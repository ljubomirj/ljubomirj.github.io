<div class="tweet">
<a href="https://x.com/osanseviero/status/1922165500538229090">https://x.com/osanseviero/status/1922165500538229090</a><br>
Omar Sanseviero <a href="https://x.com/osanseviero">@osanseviero</a><br>
Thanks for the fantastic feedback!<br>
6:42 AM Â· May 13, 2025<br>
</div>

<div class="tweet">
<a href="https://x.com/ljupc0/status/1921660533578588403">https://x.com/ljupc0/status/1921660533578588403</a><br>
Ljubomir Josifovski <a href="https://x.com/ljupc0">@ljupc0</a><br>
Mixture-of-Experts MoE in addition to dense model variants please! It's so so much faster in terms for tokens per second on localhost.<br>
The number of active parameters makes a huge difference on a laptop. M2 mbp runs Gemma-3-27b and comparable dense Qwen3-32B at ~4-6 tps. But MoE Qwen3-30B-A3B runs at ~20-40 tps (!!) (esp when 0.6B speculative decode works well). And that makes for a world of difference in the user experience.<br>
More context 256K maybe even 512K would be very useful too.<br>
Do keep the QAT training please - that was just excellent! Hope all other OS models switch to QAT too.<br>
9:15 PM Â· May 11, 2025<br>
</div>

<div class="tweet">
<a href="https://x.com/osanseviero/status/1921636582873800746">https://x.com/osanseviero/status/1921636582873800746</a><br>
Omar Sanseviero <a href="https://x.com/osanseviero">@osanseviero</a><br>
Gemma just passed 150 million downloads and over 70k variants on Hugging FaceðŸš€ðŸš€ðŸš€<br>
What would you like to see in the next Gemma versions?<br>
7:40 PM Â· May 11, 2025<br>
</div>

