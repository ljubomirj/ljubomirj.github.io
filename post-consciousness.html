<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Conciousness</title>
    <link rel="stylesheet" href="style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

    <!-- Sidebar is loaded dynamically -->
    <div id="sidebar"></div>

    <div id="content">

        <h1>Consciousness</h1>

        <h2>Q: What is your current working definition of consciousness?</h2>

        <p>Everything I've heard from <a href="http://bach.ai/">Joscha Bach</a> (JB) makes every sense to me. (and <a href="https://thoughtforms.life/">Michael Levin</a>) I have not got much to contribute on top of that. So just enumerating things heard and remembered (even if not super-faithfully) here. To have a brief in one place.</p>

        <p>The last thing that personally puzzled was: why is consciousness such a big deal? Yeah we have reasons to think "there is something there", and we can't "pin it down". But our world is filled with such wonders. Why is consciousness so special? IDK. Then recently heard JB on a podcast say "Aristotle didn't think consciousness was a big deal." - and that made me feel better. (latter I heard Pinker say that Dennet thought it "not a biggie" too) Maybe I should take consciousness more seriously. But that would be like a non-beleiver that realises he'd be better off, if he could make himself beleive in a deity, when surrounded by theists. Now knowing I'm not the only one thinking it's not that big a deal, and knowing of other - infinitelly more illustrous - names, eases the discomfort.</p>

        <p>Observables that must be true about consciousness (by JB). I agree with JB and this makes every sense to me:
        <ol start="1">
            <li>Lower level, maybe even lowest level, not "the pinacle". E.g. baby that grows into a Nobel prize winner had consciousness before they became Nobel prize winners.</li>
            <li>Necessary for learning, bootstraps knowledge acquisition. Entities lacking consciousness can't learn, don't learn. Unfortunate kids with brain damage lacking consciousness never learn much, stay almost plants. Zombies in movies lose consciousness at the point of zombification. Their knowledge is frozen to that point, they learn nothing new from that point onwards.</li>
            <li>Has element of self reflection. We know that we know.<br>
This looks less solid than 1-2 above to me. It maybe we are just telling a story, simply doing what brains ordinarily do (construct models on the fly), just on a specific questions about the brain, when turning the brain instrument unto explaining itself, it creates a model. Completely like every other model the mind constructs, nothing special. Like LLM chain of reasoning: asking for self-reflection "why did you output this" - it simply comes back with another playsible story, completely unrelated to its inner workings. That the object being modelled is the brain itself, makes zero difference. It's like chip design software, running on x86 itself, designing an x86 chip, versus designing an ARM chip. Running on x86 designing x86, does as good a job as, running on x86 designing an ARM chip. No reason to suspect the software running on x86 will do a better job designing x86 compared to designing ARM.</li>
        </ol>
        </p>

        <p>Putting 1 + 2 + 3 together, I'm quessing, consciousnees is:
        <ol start="4">
            <li>Feature of ours and animals brains architectures that implements constraints important for learning quickly in the real world. Something that constrains the weight space, so we learn before we become food for others. It must be evolutionary selected if it appears in every living thing. Must confer some advantage. Learning quicker that the competition is an advantage.</li>
            <li>Constraints act on the internal representation, on the weights of the brains networks. A small controller module, that affects the network weights of the rest of the network. It's action is to constrain the other weights in some way, reduce the space of values the rest of the network weights can take. Like enforce consistency, some kind of averaging in time.</li>
            <li>Maybe enforcing something meta- about the weights, like regularisation - "prefer smaller weights", or maybe sparsity - "prefer fewer non-zero weights". (yeah regularisation and sparseness are in tension with each other) These maybe means-to-an end: ways to end up with the trillion dimensional weight space avoid some patological cases.</li>
            <li>Or maybe it's one-off, developmental, like choosing architecture, layers (what neurons never connect). So once the brain develops almost fully past childhood + teenagehood, it becomes less relevant.</li>
            <li>The puzzle that biology HI use less data than AI, is more data efficient - maybe concioussness is a factor there? If knowledge is represented as acquired (joint) probability function, just placing a bump quickly in it at the (leaves rustling,tiger)-point rather than waiting for it to occure 10 times offers survival advantage.<br>
To my mind everything that can be known about the relationship between X and Y is expressed by their joint p.d.f. \( f_{X,Y}(x,y) \). Knowing that function <a href="post-knowing.html"><b>is knowing</b></a>. Intelligence is quering it with an \( X \) observation \( x = a \), and coming back with knowledge about the goal \( Y \) from the conditional \( f_{Y|X}(y|x=a) \) that has better error profile than the unconditional marginal \( f_Y(y) \). (both conditional and marginal functions are derivable from the joint) "Better profile" is minimizing future expected surprisal. ("expected" there implies knowledge, the joint pdf; only having knowledge gives rise to expectation.) "Surprisal" is the residual part we fail to forecast.<br>
Aside: Chain-of-Reasoning is when we come up with R, which is not observed but created as an idea in our mind. R can be a possible factual like X (possible in the world). But also R can be a counter-factual, impossible to appear as observation X. Either way, this has to be such R, that P(Y|R,X) brings us closer to a "good" Y than P(Y|X) does. (NB from the joint we also have P(R|X) at our disposal too.) These CoR discrete jumps via R, (possibly accross local saddle points?) aiding searches via R, can't be steps too far. They have to be close (~20%?) to the current state, for us not to get lost. (if too far the chance of being of use drops a lot; like stepping over stones crossing a river)</li>
        </ol></p>

        <h2>Q: How would you test for machine consciousness?</h2>

        <p>General: puruse it assuming very low SNR to any evidence brought in. Via commonality - wherever I observe some consciousness effect 1-3 above, I then seek evidence to support finding 4-8 above. Via contrasting - wherever I observe consciousness effect 1-3 above, where I found 4-8 above, now I look to remove 4-8, and see if the effect 1-3 disappears. So compare and contrast both. Need to gather both positive pro- and negative anti- evidence, as in the case of consciousness, both the Q-uestion, and the A-nswer, are unknoown. There are too many moving parts for much comfort. Maybe that's the "hard problem" referenced??</p>

        <p>Concrete steps:
        <ol start="9">
            <li>Given it's shared by all living brains, assume it's shared by all AI brains. Take a bunch of LLM-s. Then look - along the lines of the work of <a href="https://www.evlab.mit.edu/">Evelina Fedorenko</a> seeking commonnality in common language space - look find common things between them, that can be fit in the 1-3 observations above.</li>
            <li>Look at work of the likes like <a href="https://colah.github.io">Chris Olah</a>, try divine fatures that are structural. And than look again for that commonality in all of them, with an eye to 1-3.</li>
            <li>Assuming all current LLM-s are conscious to non-zero degree, would think of ways how to zombify them. Whatever I think I found in {9,10} above, try to destory undo that. Do the networks become less conscious now? In terms of 4-8?</li>
        </ol></p>

        <h2>Addendum on the consciousness experiments designs.</h2> <p>(even if I think it a nothing burger as learned from your Bach Re: Aristotle, Dennet)</p>

        <p>Look at known but still unresolved why/how-s differences between analogue HI (all life really, and at all levels of org hierarchy) and digital AI-s:
        <ol start="12">
            <li>Perpetual learning without end. Digital/AI systems collapse. (so we stop training before) Whereas analogue/HI systems (all live systems in general) don't crash and burn, learning without end doesn't undermine them. (except in psychiatric/mind ilnesses maybe?)</li>
            <li>Trully online learning. Analogue/HI has no batches, no memory except for Now, no buffers no replays, (except maybe memory consolidation while at sleep states?) is at permanent epoch 1. Whereas digial/HI have mini-batches so we need buffers, and epochs &gt;1 so we need "true" (computer) memory. That learning regime is not even that well justified by the maths of it. (more like "motivated" by somewhat hand-weavy control theories)</li>
            <li>Counter args to 12 and or 13 above, why not do them. Maybe the above 12/13 are not connected strictly to consciousness, but connected to: HI is good with many more parameters than observations, while (current) AI is good with many more observations than parameters. (but this can be folded in the experiments to test too?)</li>
        </ol></p>

        <p></p>

        <p></p>

        <p></p>

        <p><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>-- <br>LJ HPD Sun 27 Jul 2025 15:12:26 BST</p>

    </div>

    <!-- Link to the external script -->
    <script src="scripts.js"></script>

    <!--Load the sidebar html that is table of contents -->
    <script>loadSidebar();</script>

</body>
</html>
