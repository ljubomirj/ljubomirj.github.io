{
  "generatedAt": "2025-11-29T12:52:09.656Z",
  "chunks": [
    {
      "source": "a-person-of-good-taste-spoke.html",
      "content": "[1]https://x.com/osanseviero/status/1922165500538229090 Omar Sanseviero [2]@osanseviero Thanks for the fantastic feedback! 6:42 AM · May 13, 2025 [3]https://x.com/ljupc0/status/1921660533578588403 Ljubomir Josifovski [4]@ljupc0 Mixture-of-Experts MoE in addition to dense model variants please! It's so so much faster in terms for tokens per second on localhost. The number of active parameters makes a huge difference on a laptop. M2 mbp runs Gemma-3-27b and comparable dense Qwen3-32B at ~4-6 tps. But MoE Qwen3-30B-A3B runs at ~20-40 tps (!!) (esp when 0.6B speculative decode works well). And that makes for a world of difference in the user experience. More context 256K maybe even 512K would be very useful too. Do keep the QAT training please - that was just excellent! Hope all other OS models switch to QAT too. 9:15 PM · May 11, 2025 [5]https://x.com/osanseviero/status/1921636582873800746 Omar Sanseviero [6]@osanseviero Gemma just passed 150 million downloads and over 70k variants on Hugging Face��� What would you like to see in the next Gemma versions? 7:40 PM · May 11, 2025 References 1. https://x.com/osanseviero/status/1922165500538229090 2. https://x.com/osanseviero 3. https://x.com/ljupc0/status/1921660533578588403 4. https://x.com/ljupc0 5. https://x.com/osanseviero/status/1921636582873800746 6. https://x.com/osanseviero"
    },
    {
      "source": "a-person-of-good-taste-spoke.html",
      "content": "21636582873800746 6. https://x.com/osanseviero"
    },
    {
      "source": "arxiv-tasters.html",
      "content": "1. Aug 26 22:00:04 2025 [1]Hermes_4_Technical_Report-aug2025-arxiv-2508.18255v1.pdf 2. Aug 26 11:53:22 2025 [2]Reinforcement_Learning_with_Rubric_Anchors-aug2025-arxiv-2508.12 790v1.pdf 3. Aug 26 05:06:38 2025 [3]Jet-Nemotron-Efficient_Language_Model_with_Post_Neural_Architect ure_Search-arxiv-2508.15884v1.pdf 4. Aug 26 05:04:44 2025 [4]AgentFly-Fine-tuning_LLM_Agents_without_Fine-tuning_LLMs-aug2025 -arxiv-2508.16153v1.pdf 5. Aug 25 09:30:41 2025 [5]Motif_2.6B_Technical_Report-aug2025-arxiv-2508.09148v1.pdf 6. Aug 24 08:54:55 2025 [6]DEEP_THINK_WITH_CONFIDENCE-aug2025-arxiv-2508.15260v1.pdf 7. Aug 24 08:51:44 2025 [7]Guiding_an_Automatic_Speech_Recognition_Decoder_using_Large_Lang uage_Models-aug2025-arxiv-2508.02228v1.pdf 8. Aug 24 08:11:39 2025 [8]Intern-S1-A_Scientific_Multimodal_Foundation_Model-aug2025-arxiv -2508.15763v1.pdf 9. Aug 23 20:42:23 2025 [9]HIRAG-Hierarchical-Thought_Instruction-Tuning_Retrieval-Augmente d_Generation-jul2025-arxiv-2507.05714v2.pdf 10. Aug 22 16:30:07 2025 [10]Matrix_Calculus_for_Machine_Learning_and_Beyond-jan2025-arxiv-2 501.14787v1.pdf 11. Aug 22 11:49:03 2025 [11]Retrospective_Sparse_Attention_for_Efficient_Long-Context_Gener ation-aug2025-arxiv-2508.09001v1.pdf 12. Aug 21 23:08:53 2025 [12]XQUANT-Breaking_the_Memory_Wall_for_LLM_Inference_with_KV_Cache _Rematerialization-aug2025-arxiv-2508.10395v1.pdf 13. Aug 20 12:02:44 2025 [13]MDPO-OVERCOMING_THE_TRAINING-INFERENCE_DIVIDE_OF_MASKED_DIFFUSI ON_LANGUAGE_MODELS-aug2025-arxiv-2508."
    },
    {
      "source": "arxiv-tasters.html",
      "content": "ence_with_KV_Cache _Rematerialization-aug2025-arxiv-2508.10395v1.pdf 13. Aug 20 12:02:44 2025 [13]MDPO-OVERCOMING_THE_TRAINING-INFERENCE_DIVIDE_OF_MASKED_DIFFUSI ON_LANGUAGE_MODELS-aug2025-arxiv-2508.13148v1.pdf 14. Aug 20 08:01:15 2025 [14]BeyondWeb-Lessons_from_Scaling_Synthetic_Data_for_Trillion-scal e_Pretraining-aug2025-arxiv-2508.10975v1.pdf 15. Aug 18 23:28:18 2025 [15]Apriel-Nemotron-15B-Thinker-aug2025-arxiv-2508.10948v1.pdf 16. Aug 12 15:53:36 2025 [16]Grove_MoE-_Towards_Efficient_and_Superior_MoE_LLMs_with_Adjugat e_Experts-aug2025-arxiv-2508.07785v1.pdf 17. Aug 12 09:43:35 2025 [17]Part_I-Tricks_or_Traps_A_Deep_Dive_into_RL_for_LLM_Reasoning-au g2025-arxiv-2508.08221v1.pdf 18. Aug 11 07:00:34 2025 [18]GLM-4.5_Agentic_Reasoning_and_Coding_ARC_Foundation_Models -aug2025-arxiv-2508.06471v1.pdf 19. Aug 10 10:49:47 2025 [19]Let_the_Expert_Stick_to_His_Last-Expert-Specialized_Fine-Tuning _for_Sparse_Architectural_LLMs-jul2024-arxiv-2407.01906v2.pdf 20. Aug 9 18:31:03 2025 [20]Training-Free_Long-Context_Scaling_of_Large_Language_Models-feb 2024-arxiv-2402.17463v2.pdf 21. Aug 9 18:29:45 2025 [21]MInference_1.0-Accelerating_Pre-filling_for_Long-Context_LLMs_v ia_Dynamic_Sparse_Attention-jul2024-arxiv-2407.02490v2.pdf 22. Aug 9 18:28:56 2025 [22]Qwen2.5-1M_Technical_Report-jan2025-arxiv-2501.15383v1.pdf 23. Aug 8 23:53:48 2025 [23]ON_THE_GENERALIZATION_OF_SFT-A_REINFORCEMENT_LEARNING_PERSPECTI VE_WITH_REWARD_RECTIFICATION-aug2025-arxiv-2508.05629v1.pdf 24."
    },
    {
      "source": "arxiv-tasters.html",
      "content": "hnical_Report-jan2025-arxiv-2501.15383v1.pdf 23. Aug 8 23:53:48 2025 [23]ON_THE_GENERALIZATION_OF_SFT-A_REINFORCEMENT_LEARNING_PERSPECTI VE_WITH_REWARD_RECTIFICATION-aug2025-arxiv-2508.05629v1.pdf 24. Aug 8 14:23:11 2025 [24]R-Zero-Self-Evolving_Reasoning_LLM_from_Zero_Data-aug2025-arxiv -2508.05004v1.pdf 25. Aug 8 07:23:57 2025 [25]Multi-module_GRPO-Composing_Policy_Gradients_and_Prompt_Optimiz ation_for_Language_Model_Programs-aug2025-arxiv-2508.04660v1.pdf 26. Aug 7 13:26:21 2025 [26]Learning_Formal_Mathematics_From_Intrinsic_Motivation-aug2025-a rxiv-2407.00695v2.pdf 27. Aug 7 13:21:25 2025 [27]Assessing_Adaptive_World_Models_in_Machines_with_Novel_Games-ju l2025-arxiv-2507.12821v2.pdf 28. Aug 5 10:00:20 2025 [28]Unifying_Mixture_of_Experts_and_Multi-Head_Latent_Attention_for _Efficient_Language_Models-aug2025-arxiv-2508.01261v1.pdf 29. Aug 3 21:29:39 2025 [29]UloRL-An_Ultra-Long_Output_RL_Approach_for_Advancing_LLM_Reason ing_Abilities-jul2025-arxiv-2507.19766v1.pdf 30. Aug 2 15:28:50 2025 [30]Test-Time_Scaling_with_Reflective_Generative_Model-jul2025-arxi v-2507.01951v2.pdf 31. Aug 2 07:02:58 2025 [31]Titans-Learning_to_Memorize_at_Test_Time-dec2024-arxiv-2501.006 63v1.pdf 32. Jul 31 15:01:09 2025 [32]GEPA-Reflective_Prompt_Evolution_Can_Outperform_Reinforcement_L earning-jul2025-arxiv-2507.19457v1.pdf 33. Jul 30 09:45:14 2025 [33]A_Survey_of_Self-Evolving_Agents-_On_Path_to_Artificial_Super_I ntelligence-jul2025-arxiv-2507.21046v1.pdf 34."
    },
    {
      "source": "arxiv-tasters.html",
      "content": "rform_Reinforcement_L earning-jul2025-arxiv-2507.19457v1.pdf 33. Jul 30 09:45:14 2025 [33]A_Survey_of_Self-Evolving_Agents-_On_Path_to_Artificial_Super_I ntelligence-jul2025-arxiv-2507.21046v1.pdf 34. Jul 30 02:39:33 2025 [34]What_Lives-A_meta-analysis_of_diverse_opinions_on_the_definitio n_of_life-may2025-arxiv-2505.15849v1.pdf 35. Jul 29 10:48:21 2025 [35]QWEN-Group_Sequence_Policy_Optimization-jul2025-arxiv-2507.1807 1v2.pdf 36. Jul 28 06:59:39 2025 [36]KAT-V1- Kwai-AutoThink-Technical_Report-jul2025-arxiv-2507.08297v3.pdf 37. Jul 25 14:27:02 2025 [37]QWEN-Group_Sequence_Policy_Optimization-jul2025-arxiv-2507.1807 1v1.pdf 38. Jul 25 10:09:45 2025 [38]Learning_without_training-The_implicit_dynamics_of_in-context_l earning-jul2025-arxiv-2507.16003v1.pdf 39. Jul 25 08:23:09 2025 [39]AlphaGo_Moment_for_Model_Architecture_Discovery-jul2025-arxiv-2 507.18074v1.pdf 40. Jul 23 17:30:03 2025 [40]Deep_Researcher_with_Test-Time_Diffusion-jul2025-arxiv-2507.160 75v1.pdf 41. Jul 23 08:32:48 2025 [41]LADDER- SELF-IMPROVING_LLMS_THROUGH_RECURSIVE_PROBLEM_DECOMPOSITION-mar2025 -arxiv-2503.00735v3.pdf 42. Jul 15 16:34:47 2025 [42]TABM-ADVANCING_TABULAR_DEEP_LEARNING_WITH_PARAMETER-EFFICIENT_E NSEMBLING-feb2025-arxiv-2410.24210v3.pdf 43. Jul 12 12:57:41 2025 [43]Step-by-Step_Diffusion-An_Elementary_Tutorial-jun2024-arxiv-240 6.08929v2.pdf 44. Jul 9 11:24:45 2025 [44]MemAgent_Reshaping_Long-Context_LLM_with_Multi-Conv_RL-based_-M emory_Agent-jul2025-arxiv-2507.02259v1.pdf 45."
    },
    {
      "source": "arxiv-tasters.html",
      "content": "fusion-An_Elementary_Tutorial-jun2024-arxiv-240 6.08929v2.pdf 44. Jul 9 11:24:45 2025 [44]MemAgent_Reshaping_Long-Context_LLM_with_Multi-Conv_RL-based_-M emory_Agent-jul2025-arxiv-2507.02259v1.pdf 45. Jul 7 10:16:04 2025 [45]LDP-Learning_Long-Context_Diffusion_Policies_via_Past-Token_Pre diction-may2025-arxiv-2505.09561v2.pdf 46. Jul 7 09:02:01 2025 [46]EBT-Energy-Based_Transformers_are_Scalable_Learners_and_Thinker s-jul2025-arxiv-2507.02092v1.pdf 47. Jul 6 16:07:15 2025 [47]Darwin_Godel_Machine_Open-Ended_Evolution_of_Self-Improving_Age nts-may2025-arxiv-2505.22954v1.pdf 48. Jul 6 11:11:17 2025 [48]SEAL-Self-Adapting_Language_Models-jun2025-arxiv-2506.10943v1.p df 49. Jul 4 20:32:58 2025 [49]Steering_Your_Diffusion_Policy_with_Latent_Space_Reinforcement_ Learning-jun2025-arxiv-2506.15799v2.pdf 50. Jul 4 00:47:22 2025 [50]HRM-Hierarchical_Reasoning_Model-jun2025-arxiv-2506.21734v1.pdf 51. Jun 24 00:17:56 2025 [51]TabDPT_Scaling_Tabular_Foundation_Models-oct2024-arxiv-2410.181 64v1.pdf 52. Jun 24 00:08:28 2025 [52]TabArena-A_Living_Benchmark_for_Machine_Learning_on_Tabular_Dat a-jun2025-arxiv-2506.16791v1.pdf 53. Jun 23 16:43:39 2025 [53]REASONING_WITH_EXPLORATION-AN_ENTROPY_PERSPECTIVE-jun2025-arxiv -2506.14758v1.pdf 54. Jun 14 15:19:36 2025 [54]Self-Adapting_Language-Models-SEAL-jun2025-arxiv-2506.10943v1.p df 55. Jun 2 02:11:10 2025 [55]Learning_to_Reason_with_External_Rewards-may2025-arxiv-2505.195 90v1.pdf 56."
    },
    {
      "source": "arxiv-tasters.html",
      "content": "54. Jun 14 15:19:36 2025 [54]Self-Adapting_Language-Models-SEAL-jun2025-arxiv-2506.10943v1.p df 55. Jun 2 02:11:10 2025 [55]Learning_to_Reason_with_External_Rewards-may2025-arxiv-2505.195 90v1.pdf 56. May 23 09:13:07 2025 [56]What_Lives_A_meta_analysis_of_diverse_opinions_on_the_definitio n_of_life-may2025-arxiv-2505.15849v1.pdf 57. May 21 20:30:11 2025 [57]Large_Language_Diffusion_Models-feb2025-arxiv-2502.09992v2.pdf 58. May 21 11:07:41 2025 [58]Insights_into_DeepSeek-V3-Scaling_Challenges_and_Reflections_on _Hardware_for_AI_Architectures-may2025-arxiv-2505.09343v1.pdf 59. May 10 21:51:44 2025 [59]Absolute_Zero-Reinforced_Self-play_Reasoning_with_Zero_Data-may 2025-arxiv-2505.03335v2.pdf 60. May 7 08:31:59 2025 [60]Absolute_Zero_Reinforced_Self-play_Reasoning_with_Zero_Data-may 2025-arxiv-2505.03335v1.pdf 61. May 5 22:17:25 2025 [61]Bytedance-Monolith_Real_Time_Recommendation_System_With_Collisi onless_Embedding_Table-sep2022-arxiv-2209.07663v2.pdf 62. May 3 18:53:59 2025 [62]CODE_IO_Condensing_Reasoning_Patterns_via_Code_Input-Output_Pre diction-feb2025-arxiv-2502.07316v2.pdf 63. Apr 26 20:24:50 2025 [63]Bahdanu-Cho-Bengio-NEURAL_MACHINE_TRANSLATION_BY_JOINTLY_LEARNI NG_TO_ALIGN_AND_TRANSLATE-2015-arxiv-1409.0473v7.pdf 64. Apr 18 19:28:13 2025 [64]Murphy-Reinforcement_Learning_A_Comprehensive_Overview-mar2025- arxiv-2412.05265v2.pdf 65. Apr 2 10:17:48 2025 [65]Command_A-An_Enterprise-Ready Large_Language_Model-arxiv-apr2025-2504.00698v1.pdf 66."
    },
    {
      "source": "arxiv-tasters.html",
      "content": "Murphy-Reinforcement_Learning_A_Comprehensive_Overview-mar2025- arxiv-2412.05265v2.pdf 65. Apr 2 10:17:48 2025 [65]Command_A-An_Enterprise-Ready Large_Language_Model-arxiv-apr2025-2504.00698v1.pdf 66. Apr 2 09:43:44 2025 [66]The_Information_Theory_of_Individuality-arxiv-dec2014-1412.2447 v1.pdf 67. Apr 1 21:13:40 2025 [67]Lecture_Notes_on_High-Dimensional_Data-arxiv-sep2024-2101.05841 v7.pdf 68. Mar 31 21:38:34 2025 [68]Kafri-The_Second_Law_and_Informatics-arxiv-2006-0701016v2.pdf 69. Mar 27 15:28:43 2025 [69]Qwen2.5-Omni_Technical_Report-arxiv-mar2025-2503.20215v1.pdf 70. Mar 27 11:19:25 2025 [70]AGI_Governments_and_Free_Societies-arxiv-feb2025-2503.05710v2.p df 71. Mar 26 10:28:05 2025 [71]Ryan_Williams-Simulating_Time_With_Square-Root_Space-arxiv-feb2 025-2502.17779v1.pdf 72. Mar 25 18:11:00 2025 [72]A_THEORY_OF_USABLE_INFORMATION_UNDER_COMPUTATIONAL_CONSTRAINTS- 2020-arxiv-2002.10689v1.pdf 73. Mar 25 08:10:54 2025 [73]DeepSeek-V3_Technical_Report-feb2025-arxiv-2412.19437v2.pdf 74. Mar 12 21:27:26 2025 [74]Generalized_Kullback-Leibler_Divergence_Loss-arxiv-mar2025-2503 .08038v1.pdf 75. Mar 10 14:18:06 2025 [75]Probabilistic_Artificial_Intelligence-arxiv-mar2025-2502.05244v 1.pdf 76. Jan 17 17:35:52 2025 [76]Foundations_of_Large_Language_Models-jan2025-arxiv-2501.09223v1 .pdf 77. Jan 15 09:33:18 2025 [77]The_AI_Scientist_Towards_Fully_Automated_Open-Ended_Scientific_ Discovery-sep2024-arxiv-2408.06292v3.pdf 78."
    },
    {
      "source": "arxiv-tasters.html",
      "content": "tions_of_Large_Language_Models-jan2025-arxiv-2501.09223v1 .pdf 77. Jan 15 09:33:18 2025 [77]The_AI_Scientist_Towards_Fully_Automated_Open-Ended_Scientific_ Discovery-sep2024-arxiv-2408.06292v3.pdf 78. Jan 13 11:48:27 2025 [78]Samba-ASR_State-Of-The-Art_Speech_Recognition_Leveraging_struct ured_State-Space_Models-arxiv-jan2025-2501.02832v3.pdf 79. Dec 29 16:02:16 2024 [79]Scaling_of_Search_and_Learning_A_Roadmap_to_Reproduce_o1_from_R einforcement_Learning_Perspective-arxiv-2412.14135v1-dec2024.pdf 80. Dec 18 11:19:31 2024 [80]Scaling_LLM_Test-Time_Compute_Optimally_can_be_More_Effective_t han_Scaling_Model_Parameters-aug2024-arxiv-2408.03314v1.pdf 81. Dec 18 11:17:58 2024 [81]The_Unbearable_Slowness_of_Being_Why_do_we-live_at_10_bits_per- sec-dec2024-arxiv-2408.10234v2.pdf 82. Dec 17 17:44:44 2024 [82]THE_COMPLEXITY_DYNAMICS_OF_GROKKING-dec2024-arxiv-2412.09810v1. pdf 83. Dec 12 08:33:57 2024 [83]MASK_is_All_You_Need-2024-arxiv-2412.06787v2.pdf 84. Dec 9 09:15:49 2024 [84]Reinforcement_Learning_An_Overview-arxiv-2412.05265v1-2024.pdf 85. Dec 2 09:24:54 2024 [85]Mixture_of_A_Million_Experts-arxiv-2407.04153v1-2024.pdf 86. Nov 26 17:10:04 2024 [86]Understanding_LLM_Embeddings_for_Regression-arxiv-2411.14708v1. pdf 87. Nov 14 20:30:27 2024 [87]Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Tra nslate-arxiv-1409.0473v7.pdf 88. Nov 14 20:29:09 2024 [88]Sequence_to_Sequence_Learning_with_Neural_Networks-arxiv-1409.3 215v3.pdf 89."
    },
    {
      "source": "arxiv-tasters.html",
      "content": "]Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Tra nslate-arxiv-1409.0473v7.pdf 88. Nov 14 20:29:09 2024 [88]Sequence_to_Sequence_Learning_with_Neural_Networks-arxiv-1409.3 215v3.pdf 89. Nov 13 09:42:22 2024 [89]TabM_Advancing_Tabular_Deep_Learning-arxiv-2410.24210v2.pdf 90. Nov 8 16:47:29 2024 [90]Denoising_Diffusion_Probabilistic_Models_in_Six_Simple_Steps-ar xiv-2402.04384v2.pdf 91. Nov 6 10:45:32 2024 [91]A_closed-form_expression_for_the_Sharma-Mittal_entropy_of_expon ential_families-arxiv-1112.4221v1.pdf 92. Oct 30 08:49:35 2024 [92]Beyond_Autoregression_Discrete_Diffusion_for_Complex_Reasoning_ and_Planning_-_arxiv-2410.14157v1.pdf 93. Oct 28 15:22:36 2024 [93]Patrick_Kidger_-_On_Neural_Differential_Equations_-_PhD_thesis_ 2021_-_arxiv-2202.02435v1.pdf 94. Oct 28 13:32:57 2024 [94]A_First_Course_in_Monte_Carlo_Methods-arxiv-2405.16359v1.pdf 95. Oct 12 22:41:13 2024 [95]Lets_Verify_Step_by_Step-OpenAI-2023-arxiv-2305.20050v1.pdf 96. Oct 12 22:18:06 2024 [96]A_Primer_on_the_Inner_Workings_of_Transformer-Bassed_Language_M odels-2024-arxiv-2405.00208v2.pdf 97. Oct 7 09:53:27 2024 [97]Alicia_Curth-Alan_Jeffares-Michaela_van_der_Schaar_-_Why_do_Ran dom_Forests_Work_-_arxiv-2402.01502v1.pdf 98. Oct 4 18:12:09 2024 [98]Alicia_Curth_-_Classical_Statistical_In-Sample_Intuitions_Dont_ Generalize_Well_-_arxiv-2409.18842v1.pdf 99. Oct 2 18:48:03 2024 [99]Bennett-Welsh-Ciaunica_-_Why_Is_Anything_Conscious_-_arxiv-2409 .14545v1-2024.pdf 100."
    },
    {
      "source": "arxiv-tasters.html",
      "content": "sical_Statistical_In-Sample_Intuitions_Dont_ Generalize_Well_-_arxiv-2409.18842v1.pdf 99. Oct 2 18:48:03 2024 [99]Bennett-Welsh-Ciaunica_-_Why_Is_Anything_Conscious_-_arxiv-2409 .14545v1-2024.pdf 100. Sep 26 21:36:19 2024 [100]Blass-Gurevich_-_Negative_probabilities_II_What_They_Are_and_W hat_They_Are_For-arxiv-1807.10382-2018.pdf 101. Sep 26 21:07:26 2024 [101]Abramsky-Brandenburger_-_An_Operational_Interpretation_of_Nega tive_Probabilities_and_No-Signalling_Models_-_arxiv-1401.2561v2.pdf 102. Sep 23 08:15:09 2024 [102]What_is_Entropy-John_Baez_-_arxiv-2409.09232v1.pdf 103. Jun 18 19:58:20 2024 [103]Software_in_the_natural_world_A_computational_approach_to_hier archical_emergence-arxiv-2402.09090v2.pdf 104. Apr 20 10:27:35 2024 [104]Building_Cross-Sectional_Systematic_Strategies_By_Learning_to_ Rank_-_arxiv-2012.07149.pdf 105. Feb 23 20:40:42 2024 [105]Experts_Dont_Cheat_Learning_What_You_Dont_Know_By_Predicting_P airs-arxiv-2402.08733.pdf 106. Nov 27 23:29:19 2023 [106]Conformal_Prediction_for_Time_Series_with_Modern_Hopfield_Netw orks_-_arxiv-2303.12783.pdf 107. Nov 25 08:49:08 2023 [107]Portfolio_Construction_with_Gaussian_Mixture_Returns_-_arxiv-2 205.04563.pdf 108. Oct 23 18:16:57 2023 [108]Good_Enough_Practices_in_Scientific_Computing_-_arxiv-1609.000 37.pdf 109. Aug 13 18:46:12 2023 [109]A_Gentle_Introduction_to_Conformal_Prediction_and_Distribution -Free_Uncertainty_Quantification_-_arxiv-2107.07511.pdf 110."
    },
    {
      "source": "arxiv-tasters.html",
      "content": "_Scientific_Computing_-_arxiv-1609.000 37.pdf 109. Aug 13 18:46:12 2023 [109]A_Gentle_Introduction_to_Conformal_Prediction_and_Distribution -Free_Uncertainty_Quantification_-_arxiv-2107.07511.pdf 110. Aug 4 16:54:17 2023 [110]NNT_Taleb_-_Statistical_Consequences_of_Fat_Tails_-_arxiv-2001 .10488.pdf.pdf 111. May 15 15:33:46 2023 [111]Forecasting-Theory_and_Practice-arxiv-2012.03854.pdf 112. Dec 22 10:29:36 2022 [112]Empirical_Macroeconomics_and_DSGE_Modeling_in_Statistical_Pers pective_-_arxiv-2210.16224.pdf 113. Sep 1 21:07:05 2022 [113]Varley_-_Flickering_emergences_The_question_of_locality_in_inf ormation-theoretic_approaches_to_emergence_-_arxiv-2208.14502.pdf 114. Aug 31 13:22:42 2022 [114]Applying_compressed_sensing_to_genome-wide_association_studies _-_arxiv-1310.2264.pdf 115. Mar 17 09:27:18 2022 [115]Renzo_Comolatti-Erik_Hoel_-_Causal_emergence_is_widespread_acr oss_measures_of_causation_-_arxiv-2202.01854.pdf 116. May 12 08:40:46 2021 [116]EigenGame_Unloaded_When_playing_games_is_better_than_optimizin g_-_arxiv-2102.04152.pdf 117. May 12 08:11:51 2021 [117]DeepMind_-_EigenGame_PCA_as_a_Nash_Equilibrium_-_arxiv-2010.00 554.pdf 118. Apr 2 23:20:06 2021 [118]Hidden_Markov_Models_Applied_To_Intraday_Momentum_Trading_With _Side_Information_-_arXiv.2006.08307.pdf 119. Oct 16 19:56:13 2020 [119]NNT_Taleb_-_Election_Predictions_as_Martingales_An_Arbitrage_A pproach_-_arxiv-1703.06351.pdf 120."
    },
    {
      "source": "arxiv-tasters.html",
      "content": "ntraday_Momentum_Trading_With _Side_Information_-_arXiv.2006.08307.pdf 119. Oct 16 19:56:13 2020 [119]NNT_Taleb_-_Election_Predictions_as_Martingales_An_Arbitrage_A pproach_-_arxiv-1703.06351.pdf 120. Jul 27 12:34:37 2020 [120]Ole_Peters_-_Alex_Adamou_-_Leverage_efficiency_-_arXiv-1101.45 48.pdf 121. Jun 25 11:35:55 2019 [121]Kakushadze_Yu_-_Statistical_Risk_Models_-_arxiv_1602.08070.pdf 122. Jun 14 10:00:57 2019 [122]Ergodicity-breaking_reveals_time_optimal_economic_behavior_in_ humans_-_arxiv-1906.04652.pdf 123. Jun 13 12:09:46 2019 [123]Kakushadze_-_Altcoin-Bitcoin_Arbitrage_-_arxiv_-_1903.06033.pd f 124. Mar 27 20:15:47 2019 [124]Neural-Ordinary-Differential-Equations-arxiv-1806.07366.pdf 125. Mar 27 19:56:35 2019 [125]Polynomial-Regression-Alternative-to-Neural-Nets-arxiv-1806.06 850.pdf 126. Nov 19 21:17:09 2012 [126]Forecasting-with-time-varying-vector-autoregressive-models-arx iv-0802.0220v2.pdf References 1. https://arxiv.org/abs/2508.18255v1 2. https://arxiv.org/abs/2508.12790v1 3. https://arxiv.org/abs/2508.15884v1 4. https://arxiv.org/abs/2508.16153v1 5. https://arxiv.org/abs/2508.09148v1 6. https://arxiv.org/abs/2508.15260v1 7. https://arxiv.org/abs/2508.02228v1 8. https://arxiv.org/abs/2508.15763v1 9. https://arxiv.org/abs/2507.05714v2 10. https://arxiv.org/abs/2501.14787v1 11. https://arxiv.org/abs/2508.09001v1 12. https://arxiv.org/abs/2508.10395v1 13. https://arxiv.org/abs/2508.13148v1 14. https://arxiv.org/abs/2508.10975v1 15. https://arxiv.org/abs/2508."
    },
    {
      "source": "arxiv-tasters.html",
      "content": "2501.14787v1 11. https://arxiv.org/abs/2508.09001v1 12. https://arxiv.org/abs/2508.10395v1 13. https://arxiv.org/abs/2508.13148v1 14. https://arxiv.org/abs/2508.10975v1 15. https://arxiv.org/abs/2508.10948v1 16. https://arxiv.org/abs/2508.07785v1 17. https://arxiv.org/abs/2508.08221v1 18. https://arxiv.org/abs/2508.06471v1 19. https://arxiv.org/abs/2407.01906v2 20. https://arxiv.org/abs/2402.17463v2 21. https://arxiv.org/abs/2407.02490v2 22. https://arxiv.org/abs/2501.15383v1 23. https://arxiv.org/abs/2508.05629v1 24. https://arxiv.org/abs/2508.05004v1 25. https://arxiv.org/abs/2508.04660v1 26. https://arxiv.org/abs/2407.00695v2 27. https://arxiv.org/abs/2507.12821v2 28. https://arxiv.org/abs/2508.01261v1 29. https://arxiv.org/abs/2507.19766v1 30. https://arxiv.org/abs/2507.01951v2 31. https://arxiv.org/abs/2501.00663v1 32. https://arxiv.org/abs/2507.19457v1 33. https://arxiv.org/abs/2507.21046v1 34. https://arxiv.org/abs/2505.15849v1 35. https://arxiv.org/abs/2507.18071v2 36. https://arxiv.org/abs/2507.08297v3 37. https://arxiv.org/abs/2507.18071v1 38. https://arxiv.org/abs/2507.16003v1 39. https://arxiv.org/abs/2507.18074v1 40. https://arxiv.org/abs/2507.16075v1 41. https://arxiv.org/abs/2503.00735v3 42. https://arxiv.org/abs/2410.24210v3 43. https://arxiv.org/abs/2406.08929v2 44. https://arxiv.org/abs/2507.02259v1 45. https://arxiv.org/abs/2505.09561v2 46. https://arxiv.org/abs/2507.02092v1 47. https://arxiv.org/abs/2505.22954v1 48. https://arxiv.org/abs/2506.10943v1 49."
    },
    {
      "source": "arxiv-tasters.html",
      "content": "2 44. https://arxiv.org/abs/2507.02259v1 45. https://arxiv.org/abs/2505.09561v2 46. https://arxiv.org/abs/2507.02092v1 47. https://arxiv.org/abs/2505.22954v1 48. https://arxiv.org/abs/2506.10943v1 49. https://arxiv.org/abs/2506.15799v2 50. https://arxiv.org/abs/2506.21734v1 51. https://arxiv.org/abs/2410.18164v1 52. https://arxiv.org/abs/2506.16791v1 53. https://arxiv.org/abs/2506.14758v1 54. https://arxiv.org/abs/2506.10943v1 55. https://arxiv.org/abs/2505.19590v1 56. https://arxiv.org/abs/2505.15849v1 57. https://arxiv.org/abs/2502.09992v2 58. https://arxiv.org/abs/2505.09343v1 59. https://arxiv.org/abs/2505.03335v2 60. https://arxiv.org/abs/2505.03335v1 61. https://arxiv.org/abs/2209.07663v2 62. https://arxiv.org/abs/2502.07316v2 63. https://arxiv.org/abs/1409.0473v7 64. https://arxiv.org/abs/2412.05265v2 65. https://arxiv.org/abs/2504.00698v1 66. https://arxiv.org/abs/1412.2447v1 67. https://arxiv.org/abs/2101.05841v7 68. https://arxiv.org/abs/2006.0701016v2 69. https://arxiv.org/abs/2503.20215v1 70. https://arxiv.org/abs/2503.05710v2 71. https://arxiv.org/abs/2502.17779v1 72. https://arxiv.org/abs/2002.10689v1 73. https://arxiv.org/abs/2412.19437v2 74. https://arxiv.org/abs/2503.08038v1 75. https://arxiv.org/abs/2502.05244v1 76. https://arxiv.org/abs/2501.09223v1 77. https://arxiv.org/abs/2408.06292v3 78. https://arxiv.org/abs/2501.02832v3 79. https://arxiv.org/abs/2412.14135v1 80. https://arxiv.org/abs/2408.03314v1 81. https://arxiv.org/abs/2408.10234v2 82."
    },
    {
      "source": "arxiv-tasters.html",
      "content": "1 77. https://arxiv.org/abs/2408.06292v3 78. https://arxiv.org/abs/2501.02832v3 79. https://arxiv.org/abs/2412.14135v1 80. https://arxiv.org/abs/2408.03314v1 81. https://arxiv.org/abs/2408.10234v2 82. https://arxiv.org/abs/2412.09810v1 83. https://arxiv.org/abs/2412.06787v2 84. https://arxiv.org/abs/2412.05265v1 85. https://arxiv.org/abs/2407.04153v1 86. https://arxiv.org/abs/2411.14708v1 87. https://arxiv.org/abs/1409.0473v7 88. https://arxiv.org/abs/1409.3215v3 89. https://arxiv.org/abs/2410.24210v2 90. https://arxiv.org/abs/2402.04384v2 91. https://arxiv.org/abs/1112.4221v1 92. https://arxiv.org/abs/2410.14157v1 93. https://arxiv.org/abs/2202.02435v1 94. https://arxiv.org/abs/2405.16359v1 95. https://arxiv.org/abs/2305.20050v1 96. https://arxiv.org/abs/2405.00208v2 97. https://arxiv.org/abs/2402.01502v1 98. https://arxiv.org/abs/2409.18842v1 99. https://arxiv.org/abs/2409.14545v1 100. https://arxiv.org/abs/1807.10382 101. https://arxiv.org/abs/1401.2561v2 102. https://arxiv.org/abs/2409.09232v1 103. https://arxiv.org/abs/2402.09090v2 104. https://arxiv.org/abs/2012.07149 105. https://arxiv.org/abs/2402.08733 106. https://arxiv.org/abs/2303.12783 107. https://arxiv.org/abs/2205.04563 108. https://arxiv.org/abs/1609.00037 109. https://arxiv.org/abs/2107.07511 110. https://arxiv.org/abs/2001.10488 111. https://arxiv.org/abs/2012.03854 112. https://arxiv.org/abs/2210.16224 113. https://arxiv.org/abs/2208.14502 114. https://arxiv.org/abs/1310.2264 115. https://arxiv."
    },
    {
      "source": "arxiv-tasters.html",
      "content": "ps://arxiv.org/abs/2001.10488 111. https://arxiv.org/abs/2012.03854 112. https://arxiv.org/abs/2210.16224 113. https://arxiv.org/abs/2208.14502 114. https://arxiv.org/abs/1310.2264 115. https://arxiv.org/abs/2202.01854 116. https://arxiv.org/abs/2102.04152 117. https://arxiv.org/abs/2010.00554 118. https://arxiv.org/abs/2006.08307 119. https://arxiv.org/abs/1703.06351 120. https://arxiv.org/abs/1101.4548 121. https://arxiv.org/abs/1602.08070 122. https://arxiv.org/abs/1906.04652 123. https://arxiv.org/abs/1903.06033 124. https://arxiv.org/abs/1806.07366 125. https://arxiv.org/abs/1806.06850 126. https://arxiv.org/abs/0802.0220v2"
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "+ # Run conda3 setup manually, where needed, not in every shell. To update all conda packages to latest, use: $ conda update --all. � # Conda shell autocompletion: $ conda install argcomplete, then $ eval \"$(register-pytho� # Run conda3 setup manually, where needed, not in every shell. To update all con da packages to latest, use: $ conda update --all. # Conda shell autocompletion: $ conda install argcomplete, then $ eval \"$(regist er-python-argcomplete conda)\". # To install conda's shell functions for easier access, first activate, then: $ conda init. List environments: $ conda env list, $ conda info --envs. # If you'd prefer that conda's base environment not be activated on startup, set the auto_activate_base parameter to false: $ conda config --set auto_activate_b ase false. # Search for package: $ conda search psycopg2, $ conda search psycopg2 --info. I nstall package: $ conda install psycopg2, $ conda install -c conda-forge google- cloud-storage. # Print versions for (environment): (base) ljubomir@thinkpad(:):~$ python --vers ion; cython --version. List packages: $ conda list. # # NB for miniconda the path is miniconda3, while anaconda is anaconda3 - conda_setup_anaconda() { # >>> conda initialize >>> # !! Contents within this block are managed by 'conda init' !! __conda_setup=\"$('/home/ljubomir/anaconda3/bin/conda' 'shell.bash' 'hook' 2> /de v/null)\" if [ $? -eq 0 ]; then eval \"$__conda_setup\" else if [ -f \"/home/ljubomir/anaconda3/etc/profile.d/conda.sh\" ]; then ."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "a_setup=\"$('/home/ljubomir/anaconda3/bin/conda' 'shell.bash' 'hook' 2> /de v/null)\" if [ $? -eq 0 ]; then eval \"$__conda_setup\" else if [ -f \"/home/ljubomir/anaconda3/etc/profile.d/conda.sh\" ]; then . \"/home/ljubomir/anaconda3/etc/profile.d/conda.sh\" else export PATH=\"/home/ljubomir/anaconda3/bin:$PATH\" fi fi unset __conda_setup # <<< conda initialize <<< } conda_setup_miniconda() { # >>> conda initialize >>> # !! Contents within this block are managed by 'conda init' !! __conda_setup=\"$('/home/ljubomir/miniconda3/bin/conda' 'shell.bash' 'hook' 2> /d ev/null)\" if [ $? -eq 0 ]; then eval \"$__conda_setup\" else if [ -f \"/home/ljubomir/miniconda3/etc/profile.d/conda.sh\" ]; then . \"/home/ljubomir/miniconda3/etc/profile.d/conda.sh\" else export PATH=\"/home/ljubomir/miniconda3/bin:$PATH\" fi fi unset __conda_setup # <<< conda initialize <<< } conda_setup_miniforge() { # >>> conda initialize >>> # !! Contents within this block are managed by 'conda init' !! __conda_setup=\"$('/home/ljubomir/miniforge3/bin/conda' 'shell.bash' 'hook' 2> /d ev/null)\" if [ $? -eq 0 ]; then eval \"$__conda_setup\" else if [ -f \"/home/ljubomir/miniforge3/etc/profile.d/conda.sh\" ]; then . \"/home/ljubomir/miniforge3/etc/profile.d/conda.sh\" else export PATH=\"/home/ljubomir/miniforge3/bin:$PATH\" fi fi unset __conda_setup if [ -f \"/home/ljubomir/miniforge3/etc/profile.d/mamba.sh\" ]; then . \"/home/ljubomir/miniforge3/etc/profile.d/mamba."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "h\" else export PATH=\"/home/ljubomir/miniforge3/bin:$PATH\" fi fi unset __conda_setup if [ -f \"/home/ljubomir/miniforge3/etc/profile.d/mamba.sh\" ]; then . \"/home/ljubomir/miniforge3/etc/profile.d/mamba.sh\" fi # <<< conda initialize <<< } # Looks like this is the original miniconda install setup? # Miniconda - to activate conda's base environment in your current shell session : $ eval \"$(/home/ljubomir/miniconda3/bin/conda shell.YOUR_SHELL_NAME hook)\" #conda_setup_mini() { # eval \"$(/home/ljubomir/miniconda3/bin/conda shell.bash hook)\" #} # Have a shortcut conda_setup to the right one, depending on the box case \"$(hostname)\" in gigul2) alias conda_setup=conda_setup_anaconda ;; thinkpad2) alias conda_setup=conda_setup_miniforge ;; esac # Run with with conda by default, but warning: $ vncserver -geometry 3000x1500 f ails to start if /home/ljubomir/anaconda3/bin:$PATH. # https://unix.stackexchange.com/questions/469909/vncviewer-errorcould-not-conne ct-to-session-bus-failed-to-connect-to-socket-tm # In ~/.vnc/xstartup change the call to /usr/bin/mate-session or /usr/bin/startx fce4 into $ dbus-launch /usr/bin/startxfce4 & #conda_setup # Create conda environment tmp, switch base->tmp, install specific spyder versio n, then remove the tmp environment and switch back to base: # (base) $ conda create --name tmp # create new environment named tmp # (tmp) $ conda activate tmp # activate the new tmp e nvironment # (tmp) $ conda install spyder=5.0."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "ment and switch back to base: # (base) $ conda create --name tmp # create new environment named tmp # (tmp) $ conda activate tmp # activate the new tmp e nvironment # (tmp) $ conda install spyder=5.0.0 # install specific spyde r version in the tmp environment # (tmp) $ spyder # test the new spyder 5, in the new tmp environment # (tmp) $ conda deactivate # deactivate the new tmp environment, go back to base # (base) $ conda create --name tmp2 --copy --clone tmp # copy existing env tmp into new env tmp2 to rename, there is no rename env; NB --clone without --copy c auses sharing of packages via hard links, the environments are not independent! # (base) $ conda remove --name tmp --all # blow away the tmp envi ronment, revert back to base environment # (base) $ conda info --envs # list available environ ments (1) # (base) $ ls ~/anaconda3/envs # list available environ ments (2) # luigi/ luigi_cbor2/ # NB cbor2 is *not* avai lable via conda, use pip(!) even if under conda: $ pip install cbor2. # (base) $ cat ~/.conda/environments.txt # list available environ ments (3) # /home/ljubomir/anaconda3 # /home/ljubomir/anaconda3/envs/luigi # /home/ljubomir/anaconda3/envs/luigi_cbor2 # To install conda's shell functions for easier access, first activate, then: $ conda init # If you'd prefer that conda's base environment not be activated on startup, set the auto_activate_base parameter to false: $ conda config --set auto_activate_b ase false # Create torch python 3.13 conda environment."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "r that conda's base environment not be activated on startup, set the auto_activate_base parameter to false: $ conda config --set auto_activate_b ase false # Create torch python 3.13 conda environment. Run $ conda_setup to get to base, # then $ conda create --name torch-py313-conda python=3.13 to create the $HOME/. conda new environment, # then activate $ conda activate torch-py313-conda, check where it is # (torch-py313-conda) ljubomir@cs1deql01(100185.conda:0):~$ conda info --envs # torch-py310-conda /home/ljubomir/.conda/envs/torch-py310-conda # torch-py313-conda * /home/ljubomir/.conda/envs/torch-py313-conda # base /opt/miniconda3-py38_4.12.0 # then install packages: (torch-py313-conda) ljubomir@cs1deql01(100185.conda:0): ~$ pip install --upgrade pip setuptools wheel torch typing-extensions rtdl-num-e mbeddings numpy pandas matplotlib scipy + # Using python3 builtin venv (https://www.digitalocean.com/community/tutorials/how-to-install-python -3-and-set-up-a-programming-environment-on-an-ubuntu-22-04-server). � # Install $ sudo apt install build-essential libss� # Using python3 builtin venv (https://www.digitalocean.com/community/tutorials/h ow-to-install-python-3-and-set-up-a-programming-environment-on-an-ubuntu-22-04-s erver). # Install $ sudo apt install build-essential libssl-dev libffi-dev pyqt5-dev-too ls python3-dev python3-pip python3-venv . Create a home for all venv-s ~$ md pyt hon3-venv . # Create new venv called 'base' $ cd ~/python3-venv; python3 -m venv base."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "ssl-dev libffi-dev pyqt5-dev-too ls python3-dev python3-pip python3-venv . Create a home for all venv-s ~$ md pyt hon3-venv . # Create new venv called 'base' $ cd ~/python3-venv; python3 -m venv base. Check the new venv 'base' exists ~/python3-venv$ find base . # Activate venv 'base' 1) $ source ~/python3-venv/base/bin/activate . NB the pro mpt is afterwards prefixed with the venv \"(base)\": (base) ~$ . # Setup spyder5 in venv base 2) (base) ~$ python3 -m pip install wheel spyder nu mpy scipy pandas matplotlib sympy cython scikit-learn , 3) run $ python3-venv/ba se/bin/spyder . # Optionally to debug startup problems run $ python3-venv/base/bin/spyder --debu g-info verbose --debug-output file , that creates ~/spyder-debug.log file with s tartup log. # Shortcut config dir (history, lock pid) $ ln -s ~/.config/spyder-py3 ~/.spyder . Upgrade spyder in (base) ~$ python3-venv/base/bin/python -m pip install --upg rade spyder . # Point the cache to same filesystem of python3-venv, so hard links for .cache w ork: ljubomir@gigul2:~$ l ~/python3-venv ~/.cache/pip # lrwxrwxrwx 1 ljubomir ljubomir 26 Apr 8 00:48 /home/ljubomir/python3-venv - > /opt/ljubomir/python3-venv/ # lrwxrwxrwx 1 ljubomir ljubomir 24 Apr 8 01:29 /home/ljubomir/.cache/pip -> /opt/ljubomir/.cache/pip/ if [[ \"$(readlink -f $HOME/.cache/pip)\" == \"/opt/ljubomir/.cache/pip\" ]]; then export PIP_CACHE_DIR=/opt/ljubomir/."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "bomir ljubomir 24 Apr 8 01:29 /home/ljubomir/.cache/pip -> /opt/ljubomir/.cache/pip/ if [[ \"$(readlink -f $HOME/.cache/pip)\" == \"/opt/ljubomir/.cache/pip\" ]]; then export PIP_CACHE_DIR=/opt/ljubomir/.cache/pip fi # Looks like the way activate script determines \"venv HOME is XYZ\" is by parsing it's own loaction. venv-base() { source \"$(readlink -f ~/python3-venv)\"/base/bin/activate; } venv-torch() { source \"$(readlink -f ~/python3-venv)\"/torch/bin/activate; } venv-torch311() { source \"$(readlink -f ~/python3-venv)\"/torch311/bin/activate; } venv-torch313() { source \"$(readlink -f ~/python3-venv)\"/torch313/bin/activate; } venv-deactivate() { deactivate; } + # Per project .venv � venv-activate-dot-venv() { source .venv/bin/activate; } # Per project .venv venv-activate-dot-venv() { source .venv/bin/activate; } # Install pip in ./.venv where missing: (prime-rl) ljubomir@gigul2(:):~/PrimeInt ellect-ai/prime-rl$ python -m ensurepip --default-pip # (prime-rl) ljubomir@gigul2(:):~/PrimeIntellect-ai/prime-rl$ which pip # /home/ljubomir/PrimeIntellect-ai/prime-rl/.venv/bin/pip # Install uv in ./.venv specifically: (prime-rl) ljubomir@gigul2(:):~/PrimeIntel lect-ai/prime-rl$ .venv/bin/python -m pip install uv # (prime-rl) ljubomir@gigul2(:):~/PrimeIntellect-ai/prime-rl$ which uv # /home/ljubomir/PrimeIntellect-ai/prime-rl/.venv/bin/uv # Upgrade pip: (prime-rl) ljubomir@gigul2(:):~/PrimeIntellect-ai/prime-rl$ pip i nstall --upgrade pip + # Specific python version venv, e.g. python3."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "e/ljubomir/PrimeIntellect-ai/prime-rl/.venv/bin/uv # Upgrade pip: (prime-rl) ljubomir@gigul2(:):~/PrimeIntellect-ai/prime-rl$ pip i nstall --upgrade pip + # Specific python version venv, e.g. python3.11 - install: ljubomir@gigul2(:):~$ sudo apt install python3.11 python3.11-venv � # Create torch venv based on python3.11: ljubomir@gigul2(:):~$ python3.11 -m venv /opt/ljubom� # Specific python version venv, e.g. python3.11 - install: ljubomir@gigul2(:):~$ sudo apt install python3.11 python3.11-venv # Create torch venv based on python3.11: ljubomir@gigul2(:):~$ python3.11 -m ven v /opt/ljubomir/python3-venv/torch311 # Activate torch311: ljubomir@gigul2(:):~$ venv-torch311 # Upgrade pip: (torch311) ljubomir@gigul2(:):~$ pip install --upgrade pip. Insta ll uv: (torch311) ljubomir@gigul2(:):~$ pip install uv. # Install packages list: (torch311) ljubomir@gigul2(:):~$ uv pip install -r reqs .txt + # Add new venv torch based on base 1) Create new venv torch $ python3 -m venv ~/python3-venv/torch 2) Activate torch venv $ source ~/python3-venv/torch/bin/activate . � # 3) Clone base packages into torch $ source ~/pyth� # Add new venv torch based on base 1) Create new venv torch $ python3 -m venv ~/ python3-venv/torch 2) Activate torch venv $ source ~/python3-venv/torch/bin/acti vate . # 3) Clone base packages into torch $ source ~/python3-venv/base/bin/activate 4) Export installed packages $ pip freeze > ~/python3-venv/base-requirements.txt . # Deactivate base 5) $ deactivate ."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "e . # 3) Clone base packages into torch $ source ~/python3-venv/base/bin/activate 4) Export installed packages $ pip freeze > ~/python3-venv/base-requirements.txt . # Deactivate base 5) $ deactivate . Activate torch 6) $ source ~/python3-venv/to rch/bin/activate . # Install packages into torch 7) $ pip install -r ~/python3-venv/base-requiremen ts.txt . Install torch package in the torch environment 8) $ pip install torch . # Install jupyter 9) $ pip install notebook jupyter jupyterlab jupyter-client ip ykernel jupyter_contrib_nbextensions jupyterlab jupyterlab_extensions . # Add torch virtual environment to Jupyter as a kernel 10) $ python -m ipykernel install --user --name=torch --display-name \"Python (torch)\" . # Run jupyter lab in toch env 11) $ jupyter lab & , $ jupyter notebook & , get u rl $ jupyter server list . Verify venv used inside cell 12) !which python , !pip list . # Directory where kernel .json-s are $ jupyter --runtime-dir . That's temporary files, can remove those .json-s, the kernels are in $(jupyter --runtime-dir)/../ kernels. # 1) Remote kerenl via ssh. For this to work BI-DIRECITONAL COMMUNICATION local< ->remote MUST WORK. Test with $ hostname -I , $ telnet x.x.x.x 22 BOTH DIRECTION S!! # Create local dir $ md $(jupyter --runtime-dir)/../kernels/remote_gigul2. Cr eate file ~/.local/share/jupyter/kernels/remote_gigul2/kernel.json with: # { # \"argv\": [ # \"ssh\", # \"ljubomir@gigul2\", # \"bash\", # \"-l\", # \"-c\", # \"'source ~/."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "untime-dir)/../kernels/remote_gigul2. Cr eate file ~/.local/share/jupyter/kernels/remote_gigul2/kernel.json with: # { # \"argv\": [ # \"ssh\", # \"ljubomir@gigul2\", # \"bash\", # \"-l\", # \"-c\", # \"'source ~/.bashrc && source ~/python3-venv/torch/bin/activate && p ython3 -m ipykernel_launcher --ip=192.168.1.251 -f {connection_file}'\" # ], # \"display_name\": \"Remote Python (Torch - Gigul2)\", # \"language\": \"python\" # } # Test with $ echo '{\"key\": \"value\"}' > /tmp/connection_file.json , then (NB the shell can not be interactive \"-i\", but is login \"-l\") # $ ssh -o ForwardX11=no ljubomir@gigul2 'bash -l -c \"source ~/.bashrc && sou rce ~/python3-venv/torch/bin/activate && python3 -m ipykernel_launcher --ip=192. 168.1.251 -f /tmp/connection_file.json\"' # On local and remote remove incompatible $ pip uninstall jupyter_contrib_nbe xtensions jupyter_nbextensions_configurator , check versions $ pip list | grep j upyter . # On the remote gigul2 check $ hostname -I , $ psgc ipykernel , kill failed b ut only after killing local $ jupyter notebook & - otherwise remote kernels will be restart. # On remote gigul2 check errors $ grep ipykernel /var/log/syslog , on the fly config $ cat /home/ljubomir/.local-thinkpad/share/jupyter/runtime/kernel-*.json . # 2) Remote kernel via port forwarding. Start headless on remote $ jupyter noteb ook --no-browser --port=8888 . # Tunnel $ ssh -N -f -L localhost:8888:localhost:8888 user@remote_host . # Copy remote kernel local $ scp user@remote_host:/path/to/kernel."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "ess on remote $ jupyter noteb ook --no-browser --port=8888 . # Tunnel $ ssh -N -f -L localhost:8888:localhost:8888 user@remote_host . # Copy remote kernel local $ scp user@remote_host:/path/to/kernel.json /path/ to/local/kernels/ . # Now on $ jupyter notebook, should find the remote kernel in the kernel sele ction menu. # Variable inspector 13) $ pip install lckr_jupyterlab_variableinspector , check $ jupyter labextension list . Command Palette Ctrl+Shift+C search \"Variable Ins pector\". # Check pip venv packages 14) $ pip list --outdated. Update packages 15) $ pip i nstall --upgrade $(pip list --outdated | awk 'NR>2 {print $1}'). # Dump current versions 16) pip freeze >requirements-backup.txt, clean env 17) $ pip uninstall -y -r <(pip freeze), restore versions 18) $ pip install -r requir ements-backup.txt. # Upgrade pip 19) $ python -m pip install --upgrade pip, packages pkg{1,2,3} to latest compatble versions 20) $ pip install --upgrade pkg{1,2,3}. Check broken 2 1) $ pip check. # Better pip 22) $ pip install pip-tools, better debugger than pdb 23) $ pip ins tall ipdb, then 24) import ipdb, to break 25) ipdb.set_trace() . Cell magic 26) %debug, %whos . # Jupyter debugger 27) $ pip install xeus-python , then select xpython as the ke rnel (provided by xeus-python), opt Command Palette Ctrl+Shift+C (macOS Cmd+Shif t+C) \"Enable Debugger\". # Install node 28) $ pip install nodeenv, intstall Node.js in venv 29) $ nodeenv -p, verify Node.js and npm installs 30) $ node -v, $ npm -v."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "lette Ctrl+Shift+C (macOS Cmd+Shif t+C) \"Enable Debugger\". # Install node 28) $ pip install nodeenv, intstall Node.js in venv 29) $ nodeenv -p, verify Node.js and npm installs 30) $ node -v, $ npm -v. + # Install specific python 3.11 version, when the system version is 3.13, in a new venv. Must install python 3.11 at system level! (can't be just in venv.) � # On macos $ brew install python@3.11 , Ubuntu $ sudo apt insta� # Install specific python 3.11 version, when the system version is 3.13, in a ne w venv. Must install python 3.11 at system level! (can't be just in venv.) # On macos $ brew install python@3.11 , Ubuntu $ sudo apt install python3.11 pyt hon3.11-venv . # Must use the python3.11 executable to create a 311 environment! ljubomir@macbo ok2(:):~$ python3.11 -m venv ~/python3-venv/torch311 . # Activate ljubomir@macbook2(:):~$ source ~/python3-venv/torch311/bin/activate . List $ pip freeze >reqs.txt, then delete \"==1.2.3$\" verisoning leaving names on ly. # Install uv: (torch311) ljubomir@macbook2(:):~$ pip install uv , upgrade pip (t orch311) ljubomir@macbook2(:):~$ pip install --upgrade pip. # Use uv in pip mode to install list of packages (torch311) ljubomir@macbook2(:) :~$ uv pip install -r reqs.txt. # Record current versions (torch311) ljubomir@macbook2(:):~$ uv pip freeze >~/py thon3-venv/torch311-requirements-1.txt. # Install in the current venv from latest git repo: $ git clone https://github.c om/vllm-project/vllm.git ; $ cd vllm ; $ pip install -e ."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "$ uv pip freeze >~/py thon3-venv/torch311-requirements-1.txt. # Install in the current venv from latest git repo: $ git clone https://github.c om/vllm-project/vllm.git ; $ cd vllm ; $ pip install -e . ; + # TTS https://github.com/Blaizzy/mlx-audio in the torch311 environment $ uv pip install mlx-audio . � # To generate audio with an LLM use: $ mlx_audio.tts.generate --text \"Hello, world\" . # TTS https://github.com/Blaizzy/mlx-audio in the torch311 environment $ uv pip install mlx-audio . # To generate audio with an LLM use: $ mlx_audio.tts.generate --text \"Hello, wor ld\" . # Specify prefix for output file $ mlx_audio.tts.generate --text \"Hello, world\" --file_prefix hello . # Adjust speaking speed (0.5-2.0) $ mlx_audio.tts.generate --text \"Hello, world\" --speed 1.4 . + # Windsurf. Code with Cascade Ctrl+L, Edit code inline Ctrl+I, Open command palette Ctrl+Shoft+P. � # BYOK - OpenRouter/DeepSeek API keys not natively supported, options: # Windsurf. Code with Cascade Ctrl+L, Edit code inline Ctrl+I, Open command pale tte Ctrl+Shoft+P. # BYOK - OpenRouter/DeepSeek API keys not natively supported, options: # 1) Custom endpoint - in settings.json, add to internal settings: # \"windsurf.ai.customEndpoint\": \"https://openrouter.ai/v1\" # \"windsurf.ai.apiKey\": \"your-openrouter-key-here\" # 2) Shell override # export OPENAI_API_BASE=\"https://openrouter.ai/v1\" # export OPENAI_API_KEY=\"your-key\" # Then $ windsurf-itrade."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "penrouter.ai/v1\" # \"windsurf.ai.apiKey\": \"your-openrouter-key-here\" # 2) Shell override # export OPENAI_API_BASE=\"https://openrouter.ai/v1\" # export OPENAI_API_KEY=\"your-key\" # Then $ windsurf-itrade. windsurf-itrade() { # Work out a log file full dir path file name local logdir=$(for dir in \"$TMPDIR\" \"$TMP\" \"./\"; do [[ -d \"$dir\" ]] && echo \"$ dir\" && break; done) logdir=\"${logdir:-./}\" local logfile=$(readlink -f \"${logdir}/tmp-windsurf-itrade-$USER-$$-tmp.log\" 2 >/dev/null) # Construct the full command explicitly so shell $ jobs shows the log name local cmd=\"$(which windsurf || echo /usr/bin/windsurf) itrade.code-workspace > \\\"$logfile\\\" 2>&1 &\" # Execute local -; set -x; eval \"$cmd\" } + # Correct solution $ env AIDER=... aider, to have AIDER defined in the aider env (+inherited by all shells aider launches), but *not* in the shell launching aider. � # Fail to work 1) export AIDER; aider - defines AIDER � # Correct solution $ env AIDER=... aider, to have AIDER defined in the aider env (+inherited by all shells aider launches), but *not* in the shell launching aid er. # Fail to work 1) export AIDER; aider - defines AIDER in the shell launching aid er 2) AIDER=... aider - sets a shell var not env var, aider sub-shells don't get it in the env. # Aider https://aider.chat/ in $HOME/.local/bin/aider, in ."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "r - defines AIDER in the shell launching aid er 2) AIDER=... aider - sets a shell var not env var, aider sub-shells don't get it in the env. # Aider https://aider.chat/ in $HOME/.local/bin/aider, in .git/hooks/prepare-com mit-msg check add [AIDER] prefix to git commit: # $ if [[ \"$(/usr/bin/env | /usr/bin/grep AIDER)\" != \"\" ]]; then /usr/bin/sed -i \"1s/^/[AIDER] /\" \"$1\"; fi + # VRAM limits on mac. Uppoer bound (max) $ sudo sysctl iogpu.wired_limit_mb=<mb> , lower bound $ sudo sysctl iogpu.wired_lwm_mb=<mb> . # VRAM limits on mac. Uppoer bound (max) $ sudo sysctl iogpu.wired_limit_mb=<mb> , lower bound $ sudo sysctl iogpu.wired_lwm_mb=<mb> . + # OpenAI subscription is chat only, API needs separate PAYG account https://platform.openai.com/ � export OPENAI_API_KEY=... # OpenAI subscription is chat only, API needs separate PAYG account https://plat form.openai.com/ export OPENAI_API_KEY=... aider-openai-list() { local -; set -x; aider --list-models openai/; } # list mod els available from OpenAI aider-openai-o3() { local -; set -x; env AIDER_START=\"$(date)\" aider --model ope nai/o3 --openai-api-key ${OPENAI_API_KEY} ; } aider-openai-o3-mini() { local -; set -x; env AIDER_START=\"$(date)\" aider --mode l openai/o3-mini --openai-api-key ${OPENAI_API_KEY} ; } aider-openai-best() { local -; set -x; env AIDER_START=\"$(date)\" aider --archite ct --model openai/o3 --editor-model openai/gpt-4.1 --openai-api-key ${OPENAI_API _KEY} ; } + # DeepSeek V3 https://platform.deepseek."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "est() { local -; set -x; env AIDER_START=\"$(date)\" aider --archite ct --model openai/o3 --editor-model openai/gpt-4.1 --openai-api-key ${OPENAI_API _KEY} ; } + # DeepSeek V3 https://platform.deepseek.com � export DEEPSEEK_API_KEY=... # DeepSeek V3 https://platform.deepseek.com export DEEPSEEK_API_KEY=... aider-deepseek-list() { local -; set -x; aider --list-models deepseek/; } # list models available from DeepSeek aider-deepseek() { local -; set -x; env AIDER_START=\"$(date)\" aider --deepseek; } curl-deepseek-test() { local -; set -x; curl \"https://api.deepseek.com/chat/completions\" -H \"Content-Type: application /json\" -H \"Authorization: Bearer ...\" -d '{ \"model\": \"deepseek-coder\", \"messages\": [ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is 1+1?\"} ], \"stream\": false }' } # Not clear from the API doc at https://api-docs.deepseek.com/quick_start/pricin g what does deepseek/deepseek-coder point to? Only models priced are deepseek-ch at and deepseek-reasoner. #aider-deepseek-best() { local -; set -x; aider --set-env AIDER_START=\"$(date)\" --architect --model deepseek/deepseek-reasoner --editor-model deepseek/deepseek- coder; } # As of 25-Mar-2025 it is: \"The deepseek-chat model points to DeepSeek-V3. The d eepseek-reasoner model points to DeepSeek-R1.\"."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "l deepseek/deepseek-reasoner --editor-model deepseek/deepseek- coder; } # As of 25-Mar-2025 it is: \"The deepseek-chat model points to DeepSeek-V3. The d eepseek-reasoner model points to DeepSeek-R1.\". aider-deepseek-best() { local -; set -x; aider --set-env AIDER_START=\"$(date)\" - -architect --model deepseek/deepseek-reasoner --editor-model deepseek/deepseek-c hat; } + # OpenRouter https://openrouter.ai/credits select from multitude of models � export OPENROUTER_API_KEY=... # OpenRouter https://openrouter.ai/credits select from multitude of models export OPENROUTER_API_KEY=... # List models available from OpenRouter aider-openrouter-list() { local -; set -x; aider --list-models openrouter/; } # Use Anthropic Sonnet model: claude-3.5-sonnet, claude-3.7-sonnet. aider-openrouter-sonnet() { local -; set -x; env AIDER_START=\"$(date)\"; aider -- model openrouter/anthropic/claude-3.7-sonnet; } # Aider supports using a pair of models for coding: # An Architect model is asked to describe how to solve the coding problem. Think ing/reasoning models often work well in this role. # An Editor model is given the Architect�s solution and asked to produce specifi c code editing instructions to apply those changes to existing source files. # https://aider.chat/2025/01/24/r1-sonnet.html aider-openrouter-best-2() { local -; set -x; aider --set-env AIDER_START=\"$(date )\" --architect --model openrouter/deepseek/deepseek-r1 --editor-model openrouter /anthropic/claude-3."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "24/r1-sonnet.html aider-openrouter-best-2() { local -; set -x; aider --set-env AIDER_START=\"$(date )\" --architect --model openrouter/deepseek/deepseek-r1 --editor-model openrouter /anthropic/claude-3.7-sonnet; } aider-openrouter-best() { local -; set -x; aider --set-env AIDER_START=\"$(date)\" --architect --model openrouter/deepseek/deepseek-r1 --editor-model openrouter/d eepseek/deepseek-chat; } #aider-openrouter-free() { local -; set -x; aider --set-env AIDER_START=\"$(date) \" --model openrouter/deepseek/deepseek-chat-v3-0324:free; } aider-openrouter-free() { local -; set -x; aider --set-env AIDER_START=\"$(date)\" --model openrouter/tngtech/deepseek-r1t-chimera:free; } #aider-openrouter-google() { local -; set -x; aider --set-env AIDER_START=\"$(dat e)\" --model openrouter/google/gemini-2.5-pro-preview-03-25; } aider-openrouter-google() { local -; set -x; aider --set-env AIDER_START=\"$(date )\" --model openrouter/google/gemini-2.5-pro-preview; } # google/gemini-2.5-pro-0 5-06 is missing?? aider-openrouter-kimi-k2-free() { local -; set -x; aider --set-env AIDER_START=\" $(date)\" --model openrouter/moonshotai/kimi-k2:free; } aider-openrouter-kimi-k2() { local -; set -x; aider --set-env AIDER_START=\"$(dat e)\" --model openrouter/moonshotai/kimi-k2 --editor-model openrouter/moonshotai/k imi-k2 --weak-model openrouter/moonshotai/kimi-k2 ; } + # Google Gemini https://aistudio.google.com � export GEMINI_API_KEY=... # Google Gemini https://aistudio.google.com export GEMINI_API_KEY=..."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "/k imi-k2 --weak-model openrouter/moonshotai/kimi-k2 ; } + # Google Gemini https://aistudio.google.com � export GEMINI_API_KEY=... # Google Gemini https://aistudio.google.com export GEMINI_API_KEY=... curl-gemini-test() { local -; set -x; curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash :generateContent?key=$GEMINI_API_KEY\" -H 'Content-Type: application/json' -X POS T -d '{ \"contents\": [{ \"parts\":[{\"text\": \"Explain how AI works\"}] }] }'; } + # Anthropic https://console.anthropic.com � export ANTHROPIC_API_KEY=... # Anthropic https://console.anthropic.com export ANTHROPIC_API_KEY=... aider-anthropic-list() { local -; set -x; aider --list-models anthropic/; } # li st models available from Anthropic + # Keep track what's currently the best available � #alias aider-best='aider-deepseek-best' # Keep track what's currently the best available #alias aider-best='aider-deepseek-best' #alias aider-best='aider-openai-best' alias aider-best='aider-openrouter-google' + # Mistral https://console.mistral.ai, API key uOG2EUNEi0ahfisidfjKxQENFVXhBbYJ � #export MISTRAL_API_KEY=... # Mistral https://console.mistral.ai, API key uOG2EUNEi0ahfisidfjKxQENFVXhBbYJ #export MISTRAL_API_KEY=... + # Kluster AI https://platform.kluster.ai/ � export KLUSTERAI_API_KEY=... # Kluster AI https://platform.kluster.ai/ export KLUSTERAI_API_KEY=..."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "NEi0ahfisidfjKxQENFVXhBbYJ #export MISTRAL_API_KEY=... + # Kluster AI https://platform.kluster.ai/ � export KLUSTERAI_API_KEY=... # Kluster AI https://platform.kluster.ai/ export KLUSTERAI_API_KEY=... klusterai-deepseek-r1-test() { local chat_turn=\"Hello, how are you doing today? What is 1+2 please?\" local json_payload json_payload=$(jq -n \\ --arg model \"deepseek-ai/DeepSeek-R1-0528\" \\ --argjson max_tokens 4000 \\ --argjson temp 0.6 \\ --argjson top_p 1 \\ --arg role_user \"user\" \\ --arg content_user \"$chat_turn\" \\ '{ model: $model, max_completion_tokens: $max_tokens, temperature: $temp, top_p: $top_p, messages: [ { role: $role_user, content: $content_user } ] }') # Check if jq failed (e.g., if it's not installed) if [[ -z \"$json_payload\" ]]; then echo \"Error: Failed to generate JSON payload. Is jq installed and working?\" >&2 return 1 fi # Make the curl request (set -; set -x; curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer ${KLUSTERAI_API_KEY}\" \\ -H \"Content-Type: application/json\" \\ -d \"$json_payload\") } klusterai-qwen3-235b-a22b-test() { local chat_turn=\"Hello, how are you doing today? What is 1+2 please?\" local json_payload json_payload=$(jq -n \\ --arg model \"Qwen/Qwen3-235B-A22B-FP8\" \\ --argjson max_tokens 4000 \\ --argjson temp 0."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "{ local chat_turn=\"Hello, how are you doing today? What is 1+2 please?\" local json_payload json_payload=$(jq -n \\ --arg model \"Qwen/Qwen3-235B-A22B-FP8\" \\ --argjson max_tokens 4000 \\ --argjson temp 0.6 \\ --argjson top_p 1 \\ --arg role_user \"user\" \\ --arg content_user \"$chat_turn\" \\ '{ model: $model, max_completion_tokens: $max_tokens, temperature: $temp, top_p: $top_p, messages: [ { role: $role_user, content: $content_user } ] }') # Check if jq failed (e.g., if it's not installed) if [[ -z \"$json_payload\" ]]; then echo \"Error: Failed to generate JSON payload. Is jq installed and working?\" >&2 return 1 fi # Make the curl request (set -; set -x; curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer ${KLUSTERAI_API_KEY}\" \\ -H \"Content-Type: application/json\" \\ -d \"$json_payload\") } # Test klusterai API klusterai-ljubomirj-chat-test() { # Define your string components local chat_turn=\"And how are you doing today?\" local initial_instruction=\"You are Ljubomir Josifovski (LJ), a computational r esearcher, living in the UK, originally from Macedonia. Respond using LJ writing style and knowledge based *primarily* on the extensive context provided below b etween the ---START and ---END markers. Refer to this context first when answeri ng questions about LJ background, opinions, setup, and history. Maintain LJ char acteristic: concise answers with code examples when relevant, markdown formattin g for code blocks."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "first when answeri ng questions about LJ background, opinions, setup, and history. Maintain LJ char acteristic: concise answers with code examples when relevant, markdown formattin g for code blocks.\" local file_content # Ensure the file exists and is readable, or handle error if [[ -r \"chat-LJ-prompt-short.txt\" ]]; then file_content=$(cat chat-LJ-prompt-short.txt) else echo \"Error: chat-LJ-prompt-short.txt not found or not readable.\" >&2 return 1 fi # Construct the full content for the \"user\" message # The \\n here will create actual newlines in the bash variable value local user_message_content_value=\"${initial_instruction}\\n\\n--- START OF LJ CO NTEXT ---\\n\\n${file_content}\\n\\n--- END OF LJ CONTEXT ---\\n\\n${initial_instructi on}\\n\\n${chat_turn}\" # Use jq to construct the JSON payload # jq will handle escaping the content of $user_message_content_value correctly # Models # \"Qwen/Qwen3-235B-A22B-FP8\" # 40K context # \"mistralai/Mistral-Nemo-Instruct-2407\" # 128K context # \"deepseek-ai/DeepSeek-R1-0528\" # temp=0.6 top-p=0.95 top-k=64 local json_payload json_payload=$(jq -n \\ --arg model \"mistralai/Mistral-Nemo-Instruct-2407\" \\ --argjson max_tokens 4000 \\ --argjson temp 0.6 \\ --argjson top_p 1 \\ --arg role_user \"user\" \\ --arg content_user \"$user_message_content_value\" \\ '{ model: $model, max_completion_tokens: $max_tokens, temperature: $temp, top_p: $top_p, messages: [ { role: $role_user, content: $content_user } ] }') # Check if jq failed (e.g."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "sage_content_value\" \\ '{ model: $model, max_completion_tokens: $max_tokens, temperature: $temp, top_p: $top_p, messages: [ { role: $role_user, content: $content_user } ] }') # Check if jq failed (e.g., if it's not installed) if [[ -z \"$json_payload\" ]]; then echo \"Error: Failed to generate JSON payload. Is jq installed and working?\" >&2 return 1 fi # Make the curl request (set -; set -x; curl https://api.kluster.ai/v1/chat/completions \\ -H \"Authorization: Bearer ${KLUSTERAI_API_KEY}\" \\ -H \"Content-Type: application/json\" \\ -d \"$json_payload\") } # OpenAI compatible API https://aider.chat/docs/llms/openai-compat.html aider-klusterai-deepseek-r1() { local -; set -x; aider --set-env AIDER_START=\"$( date)\" --set-env OPENAI_API_BASE='https://api.kluster.ai/v1' --set-env OPENAI_AP I_KEY=\"${KLUSTERAI_API_KEY}\" --model \"openai/deepseek-ai/DeepSeek-R1-0528\" --edi tor-model \"openai/deepseek-ai/DeepSeek-R1-0528\" --weak-model \"openai/deepseek-ai /DeepSeek-R1-0528\" ; } + # TogetherAI https://api.together.ai/ � export TOGETHERAI_API_KEY=... # TogetherAI https://api.together.ai/ export TOGETHERAI_API_KEY=... + # HuggingFace token: Name=... Value=... # HuggingFace token: Name=... Value=... + # Use Kimi K2 in claude cli: � # 1) Get API Key https://platform.moonshot.ai/docs/guide/agent-support#get-api-key # Use Kimi K2 in claude cli: # 1) Get API Key https://platform.moonshot."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": ". Value=... + # Use Kimi K2 in claude cli: � # 1) Get API Key https://platform.moonshot.ai/docs/guide/agent-support#get-api-key # Use Kimi K2 in claude cli: # 1) Get API Key https://platform.moonshot.ai/docs/guide/agent-support#get-api-k ey # 2) export ANTHROPIC_AUTH_TOKEN=sk-YOURKEY # 3) export ANTHROPIC_BASE_URL=https://api.moonshot.ai/anthropic # Run $ claude. + # llama.cpp. MacOS: $ brew install llama.cpp. Start server: $ llama-server --fim-qwen-7b-default >llama-server.log 2>&1 & � # llama.vim. https://github.com/ggml-org/llama.vim # llama.cpp. MacOS: $ brew install llama.cpp. Start server: $ llama-server --fim -qwen-7b-default >llama-server.log 2>&1 & # llama.vim. https://github.com/ggml-org/llama.vim + # Added by LM Studio CLI (lms) � export PATH=\"$PATH:$HOME/.lmstudio/bin\" # Added by LM Studio CLI (lms) export PATH=\"$PATH:$HOME/.lmstudio/bin\" # End of LM Studio CLI section + # Start llama.cpp server http://127.0.0.1:8080. NB to extend 32K->128K context add \"--rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768\". � # $ llama-server --ctx-size 131072 --model ~/.lmstudio/models/x0000001/Qwen� # Start llama.cpp server http://127.0.0.1:8080. NB to extend 32K->128K context a dd \"--rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768\". # $ llama-server --ctx-size 131072 --model ~/.lmstudio/models/x0000001/Qwen3-30B -A6B-16-Extreme-128k-context-Q6_K-GGUF/qwen3-30b-a6b-16-extreme-128k-context-q6_ k.gguf --top-p 0.95 --top-k 100 --min-p 0."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "llama-server --ctx-size 131072 --model ~/.lmstudio/models/x0000001/Qwen3-30B -A6B-16-Extreme-128k-context-Q6_K-GGUF/qwen3-30b-a6b-16-extreme-128k-context-q6_ k.gguf --top-p 0.95 --top-k 100 --min-p 0.05 --repeat-penalty 64 --override-kv q wen3moe.expert_used_count=int:16 & + # MoE localhost <75GB RAM, speed ~16 tps, weights 45 GB, access at http://127.0.0.1:8080 � # ~/llama.cpp$ sudo sysctl iogpu.wired_limit_mb=80000; build/bin/llama-server --model models/dots.llm1.inst-UD-TQ1_0.gguf --temp � # MoE localhost <75GB RAM, speed ~16 tps, weights 45 GB, access at http://127.0. 0.1:8080 # ~/llama.cpp$ sudo sysctl iogpu.wired_limit_mb=80000; build/bin/llama-server -- model models/dots.llm1.inst-UD-TQ1_0.gguf --temp 0 --top_p 0.95 --min_p 0 --ctx- size 32768 --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & + # Run two end-points in llama.cpp for cline: Plan mode XBai-o4, Act mode Qwen3-Coder-A30B-A3-Instruct-1M � # sudo sysctl iogpu.wired_limit_mb=88000 # Run two end-points in llama.cpp for cline: Plan mode XBai-o4, Act mode Qwen3-C oder-A30B-A3-Instruct-1M # sudo sysctl iogpu.wired_limit_mb=88000 # Point cline Plan mode API Provider \"OpenAI Compatible\", Base URL \"http://127.0 .0.1:8081\", key \"any\" #build/bin/llama-server --port 8081 --model models/XBai-o4.Q6_K.gguf --temp 0.6 --top_p 0.95 --ctx-size 32768 --flash-attn --cache-type-k q8_0 --cache-type-v q8 _0 --jinja & # Point cline Act mode API Provider \"OpenAI Compatible\", Base URL \"http://127.0. 0."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "4.Q6_K.gguf --temp 0.6 --top_p 0.95 --ctx-size 32768 --flash-attn --cache-type-k q8_0 --cache-type-v q8 _0 --jinja & # Point cline Act mode API Provider \"OpenAI Compatible\", Base URL \"http://127.0. 0.1:8080\", key \"any\"; extend context (262144) 256K->1M (1048576) #build/bin/llama-server --port 8080 --model models/Qwen3-Coder-30B-A3B-Instruct- 1M-IQ4_NL.gguf --temp 0.7 --top_k 20 --top_p 0.8 --min_p 0 --ctx-size 1048576 -- rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 262144 --flash-attn --cache-typ e-k q8_0 --cache-type-v q8_0 --jinja & + # Source gemini-cli $ git clone https://github.com/google-gemini/gemini-cli to read. Use released node/bin/gemini -> node/lib/node_modules/@google/gemini-cli/dist/index.js. � # Install pre-requisites from https://nodejs.� # Source gemini-cli $ git clone https://github.com/google-gemini/gemini-cli to r ead. Use released node/bin/gemini -> node/lib/node_modules/@google/gemini-cli/di st/index.js. # Install pre-requisites from https://nodejs.org: Node.js, npm. Linked /usr/loca l/bin/node, /usr/local/bin/npm. On linux in /opt/node/bin. DIR=\"/opt/node/bin\"; test -d \"$DIR\" && export_prepend PATH \"${DIR}\" # Install local nvm (Node Version Manager) via curl, or upgrade $ nvm install -- lts, then setup: export NVM_DIR=\"$HOME/.nvm\" [ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\" # This loads nvm [ -s \"$NVM_DIR/bash_completion\" ] && \\."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "on Manager) via curl, or upgrade $ nvm install -- lts, then setup: export NVM_DIR=\"$HOME/.nvm\" [ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\" # This loads nvm [ -s \"$NVM_DIR/bash_completion\" ] && \\. \"$NVM_DIR/bash_completion\" # This loads nvm bash_completion # Uninstall them at a later point, remove them from the system Node: $ nvm use s table; v.s. $ nvm use system; $ npm uninstall -g a_module. # List $ nvm list, switch to ver 18 $ nvm use 18, or $ nvm use stable. Check ver sions $ node -v; $ nvm -v. # Re/install $ npm install -g @google/gemini-cli. Update $ npm upgrade -g @googl e/gemini-cli. Run $ cd project_dir && gemini. # On gigul2 system node is too old - the $HOME node installed by uv must be used => run $ npm AS USER ljubomir, NOT as root. # Use modern version: ljubomir@gigul2(:):~$ nvm use stable # Now using node v22. 17.0 (npm v11.4.2) # Check node version: ljubomir@gigul2(:):~$ node -v # v22.17.0 # Update gemini: ljubomir@gigul2(:):~$ npm install -g @google/gemini-cli # insta ll works as update fine # Check version: ljubomir@gigul2(:):~$ gemini --version # 0.1.20 + # OpenAI Codex, on github $ git clone https://github.com/openai/codex.git, install relase $ npm install -g @openai/codex. � # Login $ codex login, run $ codex --version, or specifically gpt-5-high $ codex -m gpt-5 -c mod� # OpenAI Codex, on github $ git clone https://github.com/openai/codex.git, insta ll relase $ npm install -g @openai/codex."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "n, run $ codex --version, or specifically gpt-5-high $ codex -m gpt-5 -c mod� # OpenAI Codex, on github $ git clone https://github.com/openai/codex.git, insta ll relase $ npm install -g @openai/codex. # Login $ codex login, run $ codex --version, or specifically gpt-5-high $ codex -m gpt-5 -c model_reasoning_effort=\"high\". alias codex-gpt-5-high='codex -m gpt-5 -c model_reasoning_effort=\"high\"' # Local model does not work yet with llama.cpp ### # The reasoning level can be set in the system prompts, \"Reasoning: low\", \"R easoning: medium\", or \"Reasoning: high\". ### # build/bin/llama-server --port 8080 --model models/gpt-oss-120b-MXFP4-00001 -of-00002.gguf --temp 1 --top_k 40 --top_p 1 --min_p 0 --ctx-size 131072 --flas h-attn --cache-type-k q8_0 --cache-type-v q8_0 --chat-template-kwargs '{\\\"reason ing_effort\\\": \\\"high\\\"}' --jinja & ### codex-localhost-8080-gpt-oss-120b() { ### env OPENAI_API_KEY=\"123\" CODEX_OSS_BASE_URL=\"http://localhost:8080/v1\" cod ex --oss -m \"gpt-oss:120b\" -c model_reasoning_effort=\"high\" ### } ### # build/bin/llama-server --port 8080 --model models/gpt-oss-20b-Q8_0.gguf -- temp 1 --top_k 40 --top_p 1 --min_p 0 --ctx-size 131072 --flash-attn --cache-typ e-k q8_0 --cache-type-v q8_0 --chat-template-kwargs '{\\\"reasoning_effort\\\": \\\"hi gh\\\"}' --jinja & ### codex-localhost-8080-gpt-oss-20b() { ### env OPENAI_API_KEY=\"123\" CODEX_OSS_BASE_URL=\"http://localhost:8080/v1\" cod ex --oss -m \"gpt-oss:20b\" -c model_reasoning_effort=\"high\" ### } + # Anthropic Claude Cod"
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "-localhost-8080-gpt-oss-20b() { ### env OPENAI_API_KEY=\"123\" CODEX_OSS_BASE_URL=\"http://localhost:8080/v1\" cod ex --oss -m \"gpt-oss:20b\" -c model_reasoning_effort=\"high\" ### } + # Anthropic Claude Code (CC) $ npm install -g @anthropic-ai/claude-code, then $ claude. On Github $ git clone https://github.com/anthropics/claude-code.git. � # Use CC with DeepSeek API https://api-docs.deepseek.com/guid� # Anthropic Claude Code (CC) $ npm install -g @anthropic-ai/claude-code, then $ claude. On Github $ git clone https://github.com/anthropics/claude-code.git. # Use CC with DeepSeek API https://api-docs.deepseek.com/guides/anthropic_api claude-code-setup-deepseek() { export ANTHROPIC_BASE_URL=https://api.deepseek.com/anthropic export ANTHROPIC_AUTH_TOKEN=${DEEPSEEK_API_KEY} export ANTHROPIC_MODEL=deepseek-chat export ANTHROPIC_SMALL_FAST_MODEL=deepseek-chat } claude-code-unset-deepseek() { unset ANTHROPIC_BASE_URL ANTHROPIC_AUTH_TOKEN ANTHROPIC_MODEL ANTHROPIC_SMALL_ FAST_MODEL } claude-code-with-deepseek-without-anthropic-key() { env -u ANTHROPIC_API_KEY ANTHROPIC_BASE_URL=\"https://api.deepseek.com/anthropi c\" ANTHROPIC_AUTH_TOKEN=\"${DEEPSEEK_API_KEY}\" ANTHROPIC_MODEL=\"deepseek-chat\" AN THROPIC_SMALL_FAST_MODEL=\"deepseek-chat\" claude } + # Qwnen-code (gemini-cli clone) install the git repo into npm (https://github.com/QwenLM/qwen-code): � # $ git clone https://github.com/QwenLM/qwen-code.git, $ cd qwen-code, $ npm install, $ npm install -g ."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "-code (gemini-cli clone) install the git repo into npm (https://github.com/QwenLM/qwen-code): � # $ git clone https://github.com/QwenLM/qwen-code.git, $ cd qwen-code, $ npm install, $ npm install -g . , $ qwen # Qwnen-code (gemini-cli clone) install the git repo into npm (https://github.co m/QwenLM/qwen-code): # $ git clone https://github.com/QwenLM/qwen-code.git, $ cd qwen-code, $ npm ins tall, $ npm install -g . , $ qwen # Or install the latest stable relase not from repo: $ npm install -g @qwen-code /qwen-code, $ qwen --version # Run qwen3-coder localhost; can switch off 1M context and use default 256K # extend context (262144) 256K->1M (1048576), flash attention cached; access on http://127.0.0.1:8080 # build/bin/llama-server --port 8080 --model models/Qwen3-Coder-30B-A3B-Instruct -1M-IQ4_NL.gguf --temp 0.7 --top_k 20 --top_p 0.8 --min_p 0 --repeat-penalty 1.0 5 --ctx-size 1048576 --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 262144 --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & qwen-code-localhost-8080() { env OPENAI_API_KEY=\"123\" OPENAI_BASE_URL=\"http://localhost:8080/v1\" OPENAI_MOD EL=\"qwen/qwen3-coder-30b-a3b\" qwen } codex-localhost-8080-qwen() { env OPENAI_API_KEY=\"123\" OPENAI_BASE_URL=\"http://localhost:8080/v1\" OPENAI_MOD EL=\"qwen/qwen3-coder-30b-a3b\" codex } # build/bin/llama-server --port 8080 --model models/gpt-oss-120b-MXFP4-00001-of- 00002."
    },
    {
      "source": "bashrc-ml-setup.html",
      "content": "OPENAI_API_KEY=\"123\" OPENAI_BASE_URL=\"http://localhost:8080/v1\" OPENAI_MOD EL=\"qwen/qwen3-coder-30b-a3b\" codex } # build/bin/llama-server --port 8080 --model models/gpt-oss-120b-MXFP4-00001-of- 00002.gguf --temp 1 --top_k 40 --top_p 1 --min_p 0 --ctx-size 131072 --flash-at tn --cache-type-k q8_0 --cache-type-v q8_0 --chat-template-kwargs '{\"reasoning_e ffort\": \"high\"}' --jinja & qwen-code-localhost-8080-gpt-oss-120b() { env OPENAI_API_KEY=\"123\" OPENAI_BASE_URL=\"http://localhost:8080/v1\" OPENAI_MOD EL=\"openai/gpt-oss-120b\" qwen } # build/bin/llama-server --port 8080 --model models/gpt-oss-20b-Q8_0.gguf --temp 1 --top_k 40 --top_p 1 --min_p 0 --ctx-size 131072 --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --chat-template-kwargs '{\"reasoning_effort\": \"high\"}' - -jinja & qwen-code-localhost-8080-gpt-oss-20b() { env OPENAI_API_KEY=\"123\" OPENAI_BASE_URL=\"http://localhost:8080/v1\" OPENAI_MOD EL=\"openai/gpt-oss-20b\" qwen }"
    },
    {
      "source": "index.html",
      "content": "REFRESH(0 sec): [1]file://localhost/Users/ljubomir/ljubomirj.github.io/post-ljubomirj.h tml If you are not redirected automatically, follow this [2]Link to ljubomirj page. References 1. file:///Users/ljubomir/ljubomirj.github.io/post-ljubomirj.html 2. file:///Users/ljubomir/ljubomirj.github.io/post-ljubomirj.html"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "+ huggingface-cli download TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf --local-dir models/ --local-dir-use-symlinks False huggingface-cli download TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF mixtral-8x7b-i nstruct-v0.1.Q4_K_M.gguf --local-dir models/ --local-dir-use-symlinks False Build the mixtral branch of llama.cpp (base) ljubomir@gigul2(3935572.python:0):~/llama.cpp$ git fetch origin mixtral remote: Enumerating objects: 162, done. remote: Counting objects: 100% (162/162), done. remote: Compressing objects: 100% (62/62), done. remote: Total 162 (delta 108), reused 149 (delta 100), pack-reused 0 Receiving objects: 100% (162/162), 322.80 KiB | 975.00 KiB/s, done. Resolving deltas: 100% (108/108), done. From https://github.com/ggerganov/llama.cpp * branch mixtral -> FETCH_HEAD * [new branch] mixtral -> origin/mixtral (base) ljubomir@gigul2(3935572.python:0):~/llama.cpp$ git checkout mixtral Branch 'mixtral' set up to track remote branch 'mixtral' from 'origin'. Switched to a new branch 'mixtral' (base) ljubomir@gigul2(3935572.python:0):~/llama.cpp$ make Run mixtral branch and model: ./server -t 8 -m models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf -c 2048 & Open browser at http://127.0.0.1:8080 In the text box put User: Write an email to my accountant that I have been late updating my company transactions in QuikBooks for the quarterly VAT return, but I will get around fi nishing it this weekend."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "e text box put User: Write an email to my accountant that I have been late updating my company transactions in QuikBooks for the quarterly VAT return, but I will get around fi nishing it this weekend. Her name is Zoya, be polite she is an accountant Llama: Subject: Apology and Updates on Company Transactions - Due to my sincere apologies for the delay in updating our company transactions in QuikBooks for th e quarterly VAT return. I understand how crucial it is to have accurate records and meet deadlines, which unfortunately was not achieved this time around. I would like to assure you that I am taking full responsibility for my actions a nd will be completing the remaining tasks by the end of this weekend. My goal is to provide you with all necessary information in a timely manner so that we can proceed with filing our VAT return without any further delays or complications. Please do not hesitate to reach out if there are any concerns or additional supp ort needed from my side, and I will make sure to prioritize this task above ever ything else. Thank you for your understanding and patience in this matter. Looking forward to working together more efficiently moving forward. Best regards, [Your Name] The speed is about 10 tps on 10 cores. LJ Mon 11 Dec 15:43:05 GMT 2023 + Clone llama.cpp, cd to the dir, build: Clone llama.cpp, cd to the dir, build: ~$ git clone https://github.com/ggerganov/llama.cpp.git ~$ cd llama.cpp ~/llama.cpp$ make -j Should have binaries ."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "2023 + Clone llama.cpp, cd to the dir, build: Clone llama.cpp, cd to the dir, build: ~$ git clone https://github.com/ggerganov/llama.cpp.git ~$ cd llama.cpp ~/llama.cpp$ make -j Should have binaries ./main for cmdline use and ./server for web server listenin g on http://127.0.0.1:8080/. Find a Mixtral model on the quantized page at https://huggingface.co/TheBloke/Mi xtral-8x7B-Instruct-v0.1-GGUF/, e.g. take Q6_K variant \"very large, extremely lo w quality loss\" https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/ blob/main/mixtral-8x7b-instruct-v0.1.Q6_K.gguf. DL either manually click on \"dow nload it\" or curl or wget or use huggingface-cli (get it via $ pip install -U \"h uggingface_hub[cli]\") e.g. curl -L https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/ main/mixtral-8x7b-instruct-v0.1.Q6_K.gguf --output models/mixtral-8x7b-instruct- v0.1.Q6_K.gguf Run on the cmdline (should use all that's available - GPU, mutiple cores etc - o n its own) ~/llama.cpp$ ./main -m models/mixtral-8x7b-instruct-v0.1.Q6_K.gguf -p 'Write a P ython function that accepts a URL to a CSV file, downloads it and loads it up in to a Postgres database, creating a table with the correct columns - thanks.' it should output [[[ Write a Python function that accepts a URL to a CSV file, downloads it and load s it up into a Postgres database, creating a table with the correct columns - th anks."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "s - thanks.' it should output [[[ Write a Python function that accepts a URL to a CSV file, downloads it and load s it up into a Postgres database, creating a table with the correct columns - th anks. To accomplish this task in Python, you can use the requests library to download the CSV file from a given URL, and the psycopg2 library to interact with a Postg reSQL database. Here's a function that does what you asked: ```python import csv import requests import psycopg2 from io import StringIO def load_csv_to_postgres(url, table_name, db_connection_string): # Download the CSV file from the URL response = requests.get(url) if response.status_code != 200: raise Exception(f\"Failed to download CSV from {url}\") content_type = response.headers[\"Content-Type\"] if \"charset=utf-8\" not in content_type: raise Exception(\"CSV file must be encoded as UTF-8\") # Create a table with the same column names as the CSV file column_names = None with StringIO(response.content.decode(\"utf-8\")) as csvfile: reader = csv.reader(csvfile) if column_names is None: column_names = next(reader, None) if not column_names: raise Exception(\"CSV file must contain headers\") table_definition = \", \".join([f\"{colname} TEXT\" for colname in column_names] ) connection, cursor = create_postgres_connection(db_connection_string) query = f\"\"\" CREATE TABLE IF NOT EXISTS {table_name} ( {table_definition} ); \"\"\" cursor.execute(query) connection.commit() # Load the data into the table with StringIO(response.content."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ection_string) query = f\"\"\" CREATE TABLE IF NOT EXISTS {table_name} ( {table_definition} ); \"\"\" cursor.execute(query) connection.commit() # Load the data into the table with StringIO(response.content.decode(\"utf-8\")) as csvfile: reader = csv.reader(csvfile) next(reader, None) # Skip headers query = f\"COPY {table_name} FROM STDIN WITH (FORMAT csv)\" cursor.copy_expert(query, csvfile) connection.commit() close_postgres_connection(connection, cursor) def create_postgres_connection(connection_string): \"\"\" Create a PostgreSQL connection and return the connection and cursor objects. \"\"\" connection = psycopg2.connect(connection_string) cursor = connection.cursor() return connection, cursor def close_postgres_connection(connection, cursor): \"\"\" Close a PostgreSQL connection and its associated cursor object. \"\"\" cursor.close() connection.close() # Example usage db_conn_string = \"host=localhost port=5432 dbname=test user=postgres password=pa ssword\" table_name = \"example_table\" process_csv(\"path/to/your.csv\", table_name, db_conn_string) [end of text] ]]] The 1st run will be slow as it is loading the ~40GB model weights in memory as i t goes along. Can see it in htop \"Disk IO:\" will be pegged to 100% (or more). Subsequent runs will be much faster as the weights are in a shared mmap so they stay in linux buffers cache - the \"buff/cache\" in ~/llama."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "n see it in htop \"Disk IO:\" will be pegged to 100% (or more). Subsequent runs will be much faster as the weights are in a shared mmap so they stay in linux buffers cache - the \"buff/cache\" in ~/llama.cpp$ free -g total used free shared buff/cache available Mem: 125 19 1 2 105 102 Swap: 15 9 6 ^^^^^^^^^^------------ -------------------------------------------------- these 105GB have weight loade d in memory as RO Or launch the web server version ~/llama.cpp$ ./server -m models/mixtral-8x7b-instruct-v0.1.Q6_K.gguf >server-$(d ate -Isec).log 2>&1 & and then open in browser http://127.0.0.1:8080 I am getting 3-4 tokens per sec (tps) on an old 10 core CPU only box (a word ave rages ~2.5 tokens). People are getting x10-x20 times more faster on a GPU and or Apple M3. LJ Thu 14 Dec 10:52:11 GMT 2023 + Check examples/server/README.md and Check examples/server/README.md and huggingface-cli download TheBloke/OpenHermes-2.5-neural-chat-v3-3-Slerp-GGUF ope nhermes-2.5-neural-chat-v3-3-slerp.Q4_K_M.gguf --local-dir models/ --local-dir-u se-symlinks False ./server -t 10 -m models/openhermes-2.5-neural-chat-v3-3-slerp.Q4_K_M.gguf -c 20 48 & Open browser at http://127.0.0.1:8080 In the text box put User: Write an email to my accountant that I have been late updating my company transactions in QuikBooks for the quarterly VAT return, but I will get around fi nishing it this weekend."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "e text box put User: Write an email to my accountant that I have been late updating my company transactions in QuikBooks for the quarterly VAT return, but I will get around fi nishing it this weekend. Her name is Zoya, be polite she is an accountant Llama: Subject: Apology and Updates on Company Transactions - Due to my sincere apologies for the delay in updating our company transactions in QuikBooks for th e quarterly VAT return. I understand how crucial it is to have accurate records and meet deadlines, which unfortunately was not achieved this time around. I would like to assure you that I am taking full responsibility for my actions a nd will be completing the remaining tasks by the end of this weekend. My goal is to provide you with all necessary information in a timely manner so that we can proceed with filing our VAT return without any further delays or complications. Please do not hesitate to reach out if there are any concerns or additional supp ort needed from my side, and I will make sure to prioritize this task above ever ything else. Thank you for your understanding and patience in this matter. Looking forward to working together more efficiently moving forward. Best regards, [Your Name] The speed is about 10 tps on 10 cores. LJ Mon 11 Dec 15:43:05 GMT 2023 + ljubomir@thinkpad2(:):~/llama.cpp$ curl -L https://huggingface.co/TheBloke/dolphin-2.2-yi-34b-200k-GGUF/resolve/ma in/dolphin-2.2-yi-34b-200k.Q5_K_M.gguf --output models/dolphin-2.2-yi-34b-200k.Q5_K_M."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "jubomir@thinkpad2(:):~/llama.cpp$ curl -L https://huggingface.co/TheBloke/dolphin-2.2-yi-34b-200k-GGUF/resolve/ma in/dolphin-2.2-yi-34b-200k.Q5_K_M.gguf --output models/dolphin-2.2-yi-34b-200k.Q5_K_M.gguf ljubomir@thinkpad2(:):~/llama.cpp$ curl -L https://huggingface.co/TheBloke/dolph in-2.2-yi-34b-200k-GGUF/resolve/main/dolphin-2.2-yi-34b-200k.Q5_K_M.gguf --outpu t models/dolphin-2.2-yi-34b-200k.Q5_K_M.gguf ./main -ngl 10 -m models/dolphin-2.2-yi-34b-200k.Q5_K_M.gguf --color -c 4096 --t emp 0.7 --repeat_penalty 1.1 -n -1 -i -ins -p \"Write an email to my accountant t hat I have been late updating my company transactions in QuikBooks for the quart erly VAT return, but I will get around finishing it this weekend. Her name is Zo ya, be polite she is an accountant\" ................................................................................ ...... ................................................................................ ...... ............... interrupted after 30 mins, never finishes, on gigul2 ........... ...... ................................................................................ ...... ................................................................................ ...... Fri 8 Dec 12:54:14 GMT 2023 + ljubomir@thinkpad2(:):~/llama.cpp$ curl -L https://huggingface.co/TheBloke/stablelm-zephyr-3b-GGUF/resolve/main/st ablelm-zephyr-3b.Q4_K_M.gguf --output models/stablelm-zephyr-3b.Q4_K_M.gguf ljubomir@thinkpad2(:):~/llama.cpp$ curl -L https://huggingface."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "gingface.co/TheBloke/stablelm-zephyr-3b-GGUF/resolve/main/st ablelm-zephyr-3b.Q4_K_M.gguf --output models/stablelm-zephyr-3b.Q4_K_M.gguf ljubomir@thinkpad2(:):~/llama.cpp$ curl -L https://huggingface.co/TheBloke/stabl elm-zephyr-3b-GGUF/resolve/main/stablelm-zephyr-3b.Q4_K_M.gguf --output models/s tablelm-zephyr-3b.Q4_K_M.gguf ./main -ngl 10 -m models/stablelm-zephyr-3b.Q4_K_M.gguf --color -c 4096 --temp 0 .7 --repeat_penalty 1.1 -n -1 -i -ins -p \"Write an email to my accountant that I have been late updating my company transactions in QuikBooks for the quarterly VAT return, but I will get around finishing it this weekend. Her name is Zoya, b e polite she is an accountant\" ................................................................................ ................................................................. == Running in interactive mode. == - Press Ctrl+C to interject at any time. - Press Return to return control to LLaMa. - To return control without starting a new line, end your input with '/'. - If you want to submit another line, end your input with '\\'. Write an email to my accountant that I have been late updating my company transa ctions in QuikBooks for the quarterly VAT return, but I will get around finishin g it this weekend. Her name is Zoya, be polite she is an accountant > and I want to maintain a good working relationship with her. Dear Zoya, I hope this email finds you well."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "will get around finishin g it this weekend. Her name is Zoya, be polite she is an accountant > and I want to maintain a good working relationship with her. Dear Zoya, I hope this email finds you well. I am writing to inform you that I have been de layed in updating my company transactions in QuikBooks for the quarterly VAT ret urn. Please forgive my negligence as I know how important it is to keep accurate records and submit our returns on time. As of now, I am actively working on finishing the update and anticipate completi ng it by the weekend. Your patience and understanding will be greatly appreciate d during this time. I understand that maintaining a good working relationship with you is crucial fo r the success of our business, and I want to ensure that we continue to work tog ether efficiently and effectively. Please let me know if there are any further s teps or actions I need to take in order to rectify this situation promptly. Thank you once again for your understanding and support, and I look forward to c ontinuing our productive partnership. Best regards, [Your Name] > llama_print_timings: load time = 1257.36 ms llama_print_timings: sample time = 186.37 ms / 198 runs ( 0.94 m s per token, 1062.39 tokens per second) llama_print_timings: prompt eval time = 1735.84 ms / 48 tokens ( 36.16 m s per token, 27.65 tokens per second) llama_print_timings: eval time = 10221.72 ms / 197 runs ( 51.89 m s per token, 19."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "econd) llama_print_timings: prompt eval time = 1735.84 ms / 48 tokens ( 36.16 m s per token, 27.65 tokens per second) llama_print_timings: eval time = 10221.72 ms / 197 runs ( 51.89 m s per token, 19.27 tokens per second) llama_print_timings: total time = 24957.25 ms (base) ljubomir@gigul2(3935572.python:0):~/llama.cpp$ ./main -ngl 10 -m models/s tablelm-zephyr-3b.Q4_K_M.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -i -ins -p \"Write an email to my accountant that I have been late updating m y company transactions in QuikBooks for the quarterly VAT return, but I will get around finishing it this weekend. Her name is Zoya, be polite she is an account ant\" warning: not compiled with GPU offload support, --n-gpu-layers option will be ig nored warning: see main README.md for information on enabling GPU BLAS support Log start main: build = 1637 (40b8e22) main: built with gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-g nu main: seed = 1702040083 llama_model_loader: loaded meta data with 21 key-value pairs and 356 tensors fro m models/stablelm-zephyr-3b.Q4_K_M.gguf (version GGUF V3 (latest)) llama_model_loader: - tensor 0: output.weight q6_K [ 2560, 50304, 1, 1 ] llama_model_loader: - tensor 1: token_embd.weight q4_K [ 2560, 50304, 1, 1 ] llama_model_loader: - tensor 2: blk.0.attn_norm.bias f32 [ 2560, 1, 1, 1 ] ................................................................................ .......................................... ...................."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "- tensor 2: blk.0.attn_norm.bias f32 [ 2560, 1, 1, 1 ] ................................................................................ .......................................... ................................................................................ .......................................... ................................................................................ .......................................... sampling: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, p resence_penalty = 0.000 top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.0 00, temp = 0.700 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampling order: CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp generate: n_ctx = 4096, n_batch = 512, n_predict = -1, n_keep = 48 == Running in interactive mode. == - Press Ctrl+C to interject at any time. - Press Return to return control to LLaMa. - To return control without starting a new line, end your input with '/'. - If you want to submit another line, end your input with '\\'. Write an email to my accountant that I have been late updating my company transa ctions in QuikBooks for the quarterly VAT return, but I will get around finishin g it this weekend. Her name is Zoya, be polite she is an accountant > and I need her help to avoid any issues with the HMRC. Dear Zoya, I hope this email finds you well."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "but I will get around finishin g it this weekend. Her name is Zoya, be polite she is an accountant > and I need her help to avoid any issues with the HMRC. Dear Zoya, I hope this email finds you well. Firstly, please accept my apologies for being late in updating our company transactions in QuikBooks for the quarterly VAT ret urn. As a responsible business owner, it is vital that we keep accurate records and submit our VAT returns on time to avoid any issues with the HMRC. Unfortunat ely, due to unforeseen circumstances, I have fallen behind schedule, and I am ex pecting to finish all the necessary updates this weekend. I understand that being late in submitting a VAT return may lead to additional f ees or penalties from the HMRC. Therefore, I would like to take this opportunity to assure you that we will rectify this situation as soon as possible, and make any necessary adjustments to ensure compliance with HMRC regulations moving for ward. Your hard work and dedication to our company's financial health are greatly appr eciated, and I am confident that with the information I have yet to provide, we can successfully complete the quarterly VAT return and submit it on time. Please let me know if there are any further steps or actions required from my end to e nsure timely completion of this process. Once again, please accept my apologies for any inconvenience caused by our late submission."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "if there are any further steps or actions required from my end to e nsure timely completion of this process. Once again, please accept my apologies for any inconvenience caused by our late submission. As a team, we take our commitment to compliance seriously, and I am confident that we will resolve this issue promptly. Thank you for your attention to this matter and your continued support in helpin g us navigate the challenges of running a successful business. Best regards, [Your Name] > llama_print_timings: load time = 851.09 ms llama_print_timings: sample time = 305.48 ms / 320 runs ( 0.95 m s per token, 1047.52 tokens per second) llama_print_timings: prompt eval time = 1748.76 ms / 48 tokens ( 36.43 m s per token, 27.45 tokens per second) llama_print_timings: eval time = 16704.48 ms / 319 runs ( 52.37 m s per token, 19.10 tokens per second) llama_print_timings: total time = 30994.07 ms (base) ljubomir@gigul2(3935572.python:0):~/llama.cpp$ LJ Fri 8 Dec 10:33:31 GMT 2023 + https://simonwillison.net/2023/Nov/29/llamafile/ https://simonwillison.net/2023/Nov/29/llamafile/ llamafile is the new best way to run a LLM on your own computer Mozilla�s innovation group and Justine Tunney just released llamafile, and I thi nk it�s now the single best way to get started running Large Language Models (th ink your own local copy of ChatGPT) on your own computer."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "n group and Justine Tunney just released llamafile, and I thi nk it�s now the single best way to get started running Large Language Models (th ink your own local copy of ChatGPT) on your own computer. A llamafile is a single multi-GB file that contains both the model weights for a n LLM and the code needed to run that model�in some cases a full local server wi th a web UI for interacting with it. curl -LO https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/llamafil e-server-0.1-llava-v1.5-7b-q4 ljubomir@thinkpad2(:):~/llama.cpp/models$ curl -LO https://huggingface.co/jartin e/llava-v1.5-7B-GGUF/resolve/main/llamafile-server-0.1-llava-v1.5-7b-q4 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 1168 100 1168 0 0 3903 0 --:--:-- --:--:-- --:--:-- 3906 100 4065M 100 4065M 0 0 12.5M 0 0:05:23 0:05:23 --:--:-- 12.6M ljubomir@thinkpad2(:):~/llama.cpp/models$ chmod u+rwx llamafile-server-0.1-llava -v1.5-7b-q4 ljubomir@thinkpad2(:):~/llama.cpp/models$ ./llamafile-server-0.1-llava-v1.5-7b-q 4 run-detectors: unable to find an interpreter for ./llamafile-server-0.1-llava-v1 .5-7b-q4 ljubomir@thinkpad2(:):~/llama.cpp/models$ file ./llamafile-server-0.1-llava-v1.5 -7b-q4 ./llamafile-server-0.1-llava-v1.5-7b-q4: DOS/MBR boot sector; partition 1 : ID=0 x7f, active, start-CHS (0x0,0,1), end-CHS (0x3ff,255,63), startsector 0, 4294967 295 sectors https://github.com/mozilla-Ocho/llamafile#gotchas ljubomir@thinkpad2(:):~/llama."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "sector; partition 1 : ID=0 x7f, active, start-CHS (0x0,0,1), end-CHS (0x3ff,255,63), startsector 0, 4294967 295 sectors https://github.com/mozilla-Ocho/llamafile#gotchas ljubomir@thinkpad2(:):~/llama.cpp/models$ sudo wget -O ~/ape https://cosmo.zip/p ub/cosmos/bin/ape-$(uname -m).elf --2023-12-01 09:43:54-- https://cosmo.zip/pub/cosmos/bin/ape-x86_64.elf Resolving cosmo.zip (cosmo.zip)... 34.136.86.162 Connecting to cosmo.zip (cosmo.zip)|34.136.86.162|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 9435 (9.2K) [application/octet-stream] Saving to: �/home/ljubomir/ape� /home/ljubomir/ape 100%[==================== ================================================================================ ======================>] 9.21K --.-KB/s in 0s 2023-12-01 09:43:55 (174 MB/s) - �/home/ljubomir/ape� saved [9435/9435] ljubomir@thinkpad2(:):~/llama.cpp/models$ l ~/ape -rw-r--r-- 1 root root 9.3K Nov 15 00:37 /home/ljubomir/ape ljubomir@thinkpad2(:):~/llama.cpp/models$ file ~/ape /home/ljubomir/ape: ELF 64-bit LSB executable, x86-64, version 1 (FreeBSD), for OpenBSD, statically linked, no section header ljubomir@thinkpad2(:):~/llama.cpp/models$ sudo mv -iv ~/ape /usr/bin/ renamed '/home/ljubomir/ape' -> '/usr/bin/ape' ljubomir@thinkpad2(:):~/llama.cpp/models$ sudo chmod +x /usr/bin/ape ljubomir@thinkpad2(:):~/llama.cpp/models$ sudo sh -c \"echo ':APE:M::MZqFpD::/usr /bin/ape:' >/proc/sys/fs/binfmt_misc/register\" ljubomir@thinkpad2(:):~/llama."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ma.cpp/models$ sudo chmod +x /usr/bin/ape ljubomir@thinkpad2(:):~/llama.cpp/models$ sudo sh -c \"echo ':APE:M::MZqFpD::/usr /bin/ape:' >/proc/sys/fs/binfmt_misc/register\" ljubomir@thinkpad2(:):~/llama.cpp/models$ sudo sh -c \"echo ':APE-jart:M::jartsr: :/usr/bin/ape:' >/proc/sys/fs/binfmt_misc/register\" ljubomir@thinkpad2(:):~/llama.cpp/models$ ./llamafile-server-0.1-llava-v1.5-7b-q 4 & [3] 99180 ljubomir@thinkpad2(:):~/llama.cpp/models$ warning: couldn't find nvcc (nvidia c compiler) try setting $CUDA_PATH if it's installed {\"timestamp\":1701423994,\"level\":\"INFO\",\"function\":\"main\",\"line\":2258,\"message\":\" build info\",\"build\":1500,\"commit\":\"a30b324\"} {\"timestamp\":1701423994,\"level\":\"INFO\",\"function\":\"main\",\"line\":2261,\"message\":\" system info\",\"n_threads\":4,\"n_threads_batch\":-1,\"total_threads\":8,\"system_info\": \"AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SS E3 = 1 | SSSE3 = 1 | VSX = 0 | \"} Multi Modal Mode Enabledclip_model_load: model name: openai/clip-vit-large-pat ch14-336 clip_model_load: description: image encoder for LLaVA clip_model_load: GGUF version: 3 clip_model_load: alignment: 32 clip_model_load: n_tensors: 377 clip_model_load: n_kv: 19 clip_model_load: ftype: q4_0 clip_model_load: text_encoder: 0 clip_model_load: vision_encoder: 1 clip_model_load: llava_projector: 1 clip_model_load: model size: 169.31 MB clip_model_load: metadata size: 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "odel_load: ftype: q4_0 clip_model_load: text_encoder: 0 clip_model_load: vision_encoder: 1 clip_model_load: llava_projector: 1 clip_model_load: model size: 169.31 MB clip_model_load: metadata size: 0.14 MB clip_model_load: total allocated memory: 201.77 MB llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors fro m llava-v1.5-7b-Q4_K.gguf (version GGUF V3 (latest)) ................................................................................ ........................................................................... ................................................................................ ........................................................................... ................................................................................ ........................................................................... llama_new_context_with_model: n_ctx = 2048 llama_new_context_with_model: freq_base = 10000.0 llama_new_context_with_model: freq_scale = 1 llama_new_context_with_model: kv self size = 1024.00 MB llama_build_graph: non-view tensors processed: 740/740 llama_new_context_with_model: compute buffer total size = 162.63 MB Available slots: -> Slot 0 - max context: 2048 llama server listening at http://127.0.0.1:8080 loading weights... {\"timestamp\":1701423995,\"level\":\"INFO\",\"function\":\"main\",\"line\":2527,\"message\":\" HTTP server listening\",\"hostname\":\"127.0.0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "xt: 2048 llama server listening at http://127.0.0.1:8080 loading weights... {\"timestamp\":1701423995,\"level\":\"INFO\",\"function\":\"main\",\"line\":2527,\"message\":\" HTTP server listening\",\"hostname\":\"127.0.0.1\",\"port\":8080} all slots are idle and system prompt is empty, clear the KV cache ljubomir@thinkpad2(:):~/llama.cpp/models$ j [1]- Stopped vim -i .viminfo README (wd: ~/libfaketime) [2]+ Stopped vim -i .viminfo README.LJ (wd: ~/llama.cpp) [3] Running ./llamafile-server-0.1-llava-v1.5-7b-q4 & In web browser open url http://127.0.0.1:8080 Can 1) chat 2) upload images + 1. Download the 4.26GB llamafile-server-0.1-llava-v1.5-7b-q4 file from https://huggingface.co/jartine/llava-v1.5-7B-GGUF/blob/main/...: simonw 18 hours ago | unvote | next [�] I think the best way to try this out is with LLaVA, the text+image model (like G PT-4 Vision). Here are steps to do that on macOS (which should work the same on other platforms too, I haven't tried that yet though): 1. Download the 4.26GB llamafile-server-0.1-llava-v1.5-7b-q4 file from https://h uggingface.co/jartine/llava-v1.5-7B-GGUF/blob/main/...: wget https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/llamafil e-server-0.1-llava-v1.5-7b-q4 2. Make that binary executable, by running this in a terminal: chmod 755 llamafile-server-0.1-llava-v1.5-7b-q4 3. Run your new executable, which will start a web server on port 8080: ./llamafile-server-0.1-llava-v1.5-7b-q4 4. Navigate to http://127.0.0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "terminal: chmod 755 llamafile-server-0.1-llava-v1.5-7b-q4 3. Run your new executable, which will start a web server on port 8080: ./llamafile-server-0.1-llava-v1.5-7b-q4 4. Navigate to http://127.0.0.1:8080/ to upload an image and start chatting with the model about it in your browser. Screenshot here: https://simonwillison.net/2023/Nov/29/llamafile/ LJ Thu 30 Nov 16:31:30 GMT 2023 + https://huggingface.co/TheBloke/Orca-2-13B-GGUF https://huggingface.co/TheBloke/Orca-2-13B-GGUF huggingface-cli download TheBloke/Orca-2-13B-GGUF orca-2-13b.Q4_K_M.gguf --local -dir . --local-dir-use-symlinks False (base) ljubomir@gigul2(3935572.python:0):~/llama.cpp/models$ huggingface-cli dow nload TheBloke/Orca-2-13B-GGUF orca-2-13b.Q4_K_M.gguf --local-dir . --local-dir- use-symlinks False Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for mo re details. downloading https://huggingface.co/TheBloke/Orca-2-13B-GGUF/resolve/main/orca-2- 13b.Q4_K_M.gguf to /home/ljubomir/.cache/huggingface/hub/tmpjfug7god orca-2-13b.Q4_K_M.gguf: 100%|��������������������������������������������������� �������������������������������������������������������������������������������� �������������������������| 7.87G/7.87G [05:56<00:00, 22.0MB/s] ./orca-2-13b.Q4_K_M.gguf ./main -m ./models/orca-2-13b.Q4_K_M.gguf --color -c 4096 --temp 0.7 --repeat_pe nalty 1."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "���������������������� �������������������������| 7.87G/7.87G [05:56<00:00, 22.0MB/s] ./orca-2-13b.Q4_K_M.gguf ./main -m ./models/orca-2-13b.Q4_K_M.gguf --color -c 4096 --temp 0.7 --repeat_pe nalty 1.1 -n -1 -p \"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|> user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\" https://huggingface.co/TheBloke/Orca-2-13B-GGUF On the command line, including multiple files at once I recommend using the huggingface-hub Python library: pip3 install huggingface-hub Then you can download any individual model file to the current directory, at hig h speed, with a command like this: huggingface-cli download TheBloke/Orca-2-13B-GGUF orca-2-13b.Q4_K_M.gguf --local -dir . --local-dir-use-symlinks False More advanced huggingface-cli download usage You can also download multiple files at once with a pattern: huggingface-cli download TheBloke/Orca-2-13B-GGUF --local-dir . --local-dir-use- symlinks False --include='*Q4_K*gguf' For more documentation on downloading with huggingface-cli, please see: HF -> Hu b Python Library -> Download files -> Download from the CLI. To accelerate downloads on fast connections (1Gbit/s or higher), install hf_tran sfer: pip3 install hf_transfer And set environment variable HF_HUB_ENABLE_HF_TRANSFER to 1: HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/Orca-2-13B-GGUF or ca-2-13b.Q4_K_M.gguf --local-dir ."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ip3 install hf_transfer And set environment variable HF_HUB_ENABLE_HF_TRANSFER to 1: HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/Orca-2-13B-GGUF or ca-2-13b.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False Windows Command Line users: You can set the environment variable by running set HF_HUB_ENABLE_HF_TRANSFER=1 before the download command. Example llama.cpp command Make sure you are using llama.cpp from commit d0cee0d or later. ./main -ngl 32 -m orca-2-13b.Q4_K_M.gguf --color -c 4096 --temp 0.7 --repeat_pen alty 1.1 -n -1 -p \"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>u ser\\n{prompt}<|im_end|>\\n<|im_start|>assistant\" Change -ngl 32 to the number of layers to offload to GPU. Remove it if you don't have GPU acceleration. Change -c 4096 to the desired sequence length. For extended sequence models - eg 8K, 16K, 32K - the necessary RoPE scaling parameters are read from the GGUF fil e and set by llama.cpp automatically. If you want to have a chat-style conversation, replace the -p <PROMPT> argument with -i -ins For other parameters and how to use them, please refer to the llama.cpp document ation + ./main -ngl 10 -m models/goliath-120b.Q4_K_M.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -i -ins -p \"Write an email to my accountant that I have been late updating my company transactions in QuikBooks for � This fails, runs out of memory: ./main -ngl 10 -m models/goliath-120b.Q4_K_M.gguf --color -c 4096 --temp 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "l to my accountant that I have been late updating my company transactions in QuikBooks for � This fails, runs out of memory: ./main -ngl 10 -m models/goliath-120b.Q4_K_M.gguf --color -c 4096 --temp 0.7 --r epeat_penalty 1.1 -n -1 -i -ins -p \"Write an email to my accountant that I have been late updating my company transactions in QuikBooks for the quarterly VAT re turn, but I will get around finishing it this weekend. Her name is Zoya, be poli te she is an accountant\" + huggingface-cli login (base) ljubomir@thinkpad2(:):~$ source python3-venv/base/bin/activate pip install --upgrade huggingface_hub huggingface-cli login read acccess token is ... from ljubomirjosifovski@gmail.com @huggingface (download model file huggingface-cli download TheBloke/goliath-120b-GGUF goliath-120b.Q4_K_M.gguf - -local-dir . --local-dir-use-symlinks False or huggingface-cli download TheBloke/goliath-120b-GGUF --local-dir . --local-dir- use-symlinks False --include='*Q4_K*gguf' but the model is actually split and the paths are) The model parts are goliath-120b.Q4_K_M.gguf-split-a goliath-120b.Q4_K_M.gguf-split-b so allow for split in the file pattern to download both huggingface-cli download TheBloke/goliath-120b-GGUF --local-dir . --local-dir-us e-symlinks False --include='*Q4_K_M*gguf-split*' (base) ljubomir@gigul2(3935572.python:0):~/llama.cpp/models$ huggingface-cli dow nload TheBloke/goliath-120b-GGUF --local-dir ."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "al-dir . --local-dir-us e-symlinks False --include='*Q4_K_M*gguf-split*' (base) ljubomir@gigul2(3935572.python:0):~/llama.cpp/models$ huggingface-cli dow nload TheBloke/goliath-120b-GGUF --local-dir . --local-dir-use-symlinks False -- include='*Q4_K_M*gguf-split*' then concatenate the binary files run prompt ./main -ngl 32 -m models/goliath-120b.Q4_K_M.gguf --color -c 4096 --temp 0.7 --r epeat_penalty 1.1 -n -1 -p \"You are a helpful AI assistant.\\n\\nUSER: {prompt}\\nA SSISTANT:\" LJ Sat 18 Nov 00:29:45 GMT 2023 + ./main -ngl 10 -m models/nous-capybara-34b.Q4_K_M.gguf --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -i -ins -p \"Write an email to my accountant that I have been late updating my company transactions in QuikBooks� This works (34B model with 10 threads): ./main -ngl 10 -m models/nous-capybara-34b.Q4_K_M.gguf --color -c 2048 --temp 0. 7 --repeat_penalty 1.1 -n -1 -i -ins -p \"Write an email to my accountant that I have been late updating my company transactions in QuikBooks for the quarterly V AT return, but I will get around finishing it this weekend. Her name is Zoya, be polite she is an accountant\" ljubomir@gigul2(:):~/llama.cpp$ ./main -ngl 10 -m models/nous-capybara-34b.Q4_K_ M.gguf --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -i -ins -p \"Write a n email to my accountant that I have been late updating my company transactions in QuikBooks for the quarterly VAT return, but I will get around finishing it th is weekend."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "1 -n -1 -i -ins -p \"Write a n email to my accountant that I have been late updating my company transactions in QuikBooks for the quarterly VAT return, but I will get around finishing it th is weekend. Her name is Zoya, be polite she is an accountant\" warning: not compiled with GPU offload support, --n-gpu-layers option will be ig nored warning: see main README.md for information on enabling GPU BLAS support Log start main: build = 1539 (0808d16) main: built with gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-g nu main: seed = 1700248085 llama_model_loader: loaded meta data with 20 key-value pairs and 543 tensors fro m models/nous-capybara-34b.Q4_K_M.gguf (version GGUF V3 (latest)) llama_model_loader: - tensor 0: token_embd.weight q4_K [ 7168, 64000, 1, 1 ] llama_model_loader: - tensor 1: blk.0.attn_q.weight q4_K [ 7168, 7168, 1, 1 ] llama_model_loader: - tensor 2: blk.0.attn_k.weight q4_K [ 7168, 1024, 1, 1 ] ................................................................................ .............................................................. ................................................................................ .............................................................. ................................................................................ .............................................................. llama_model_loader: - tensor 540: blk.59.ffn_norm.weight f32 [ 7168, 1, 1, 1 ] llama_model_loader: - tensor 541: output_norm."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "........... .............................................................. llama_model_loader: - tensor 540: blk.59.ffn_norm.weight f32 [ 7168, 1, 1, 1 ] llama_model_loader: - tensor 541: output_norm.weight f32 [ 7168, 1, 1, 1 ] llama_model_loader: - tensor 542: output.weight q6_K [ 7168, 64000, 1, 1 ] llama_model_loader: - kv 0: general.architecture str = llama llama_model_loader: - kv 1: general.name str = nousresearch_nous-capybara-34b llama_model_loader: - kv 2: llama.context_length u32 = 200000 llama_model_loader: - kv 3: llama.embedding_length u32 = 7168 llama_model_loader: - kv 4: llama.block_count u32 = 60 llama_model_loader: - kv 5: llama.feed_forward_length u32 = 20480 llama_model_loader: - kv 6: llama.rope.dimension_count u32 = 128 llama_model_loader: - kv 7: llama.attention.head_count u32 = 56 llama_model_loader: - kv 8: llama.attention.head_count_kv u32 = 8 llama_model_loader: - kv 9: llama.attention.layer_norm_rms_epsilon f32 = 0.000010 llama_model_loader: - kv 10: llama.rope.freq_base f32 = 5000000.000000 llama_model_loader: - kv 11: general.file_type u32 = 15 llama_model_loader: - kv 12: tokenizer.ggml.model str = llama llama_model_loader: - kv 13: tokenizer.ggml.tokens arr[str ,64000] = [\"<unk>\", \"<|startoftext|>\", \"<|endof... llama_model_loader: - kv 14: tokenizer.ggml.scores arr[f32 ,64000] = [0.000000, 0.000000, 0.000000, 0.0000... llama_model_loader: - kv 15: tokenizer.ggml.token_type arr[i32 ,64000] = [2, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, ..."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "v 14: tokenizer.ggml.scores arr[f32 ,64000] = [0.000000, 0.000000, 0.000000, 0.0000... llama_model_loader: - kv 15: tokenizer.ggml.token_type arr[i32 ,64000] = [2, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, ... llama_model_loader: - kv 16: tokenizer.ggml.bos_token_id u32 = 144 llama_model_loader: - kv 17: tokenizer.ggml.eos_token_id u32 = 2 llama_model_loader: - kv 18: tokenizer.ggml.padding_token_id u32 = 0 llama_model_loader: - kv 19: general.quantization_version u32 = 2 llama_model_loader: - type f32: 121 tensors llama_model_loader: - type q4_K: 361 tensors llama_model_loader: - type q6_K: 61 tensors llm_load_vocab: mismatch in special tokens definition ( 498/64000 vs 267/64000 ) . llm_load_print_meta: format = GGUF V3 (latest) llm_load_print_meta: arch = llama llm_load_print_meta: vocab type = SPM llm_load_print_meta: n_vocab = 64000 llm_load_print_meta: n_merges = 0 llm_load_print_meta: n_ctx_train = 200000 llm_load_print_meta: n_embd = 7168 llm_load_print_meta: n_head = 56 llm_load_print_meta: n_head_kv = 8 llm_load_print_meta: n_layer = 60 llm_load_print_meta: n_rot = 128 llm_load_print_meta: n_gqa = 7 llm_load_print_meta: f_norm_eps = 0.0e+00 llm_load_print_meta: f_norm_rms_eps = 1.0e-05 llm_load_print_meta: f_clamp_kqv = 0.0e+00 llm_load_print_meta: f_max_alibi_bias = 0.0e+00 llm_load_print_meta: n_ff = 20480 llm_load_print_meta: rope scaling = linear llm_load_print_meta: freq_base_train = 5000000."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "meta: f_clamp_kqv = 0.0e+00 llm_load_print_meta: f_max_alibi_bias = 0.0e+00 llm_load_print_meta: n_ff = 20480 llm_load_print_meta: rope scaling = linear llm_load_print_meta: freq_base_train = 5000000.0 llm_load_print_meta: freq_scale_train = 1 llm_load_print_meta: n_yarn_orig_ctx = 200000 llm_load_print_meta: rope_finetuned = unknown llm_load_print_meta: model type = 30B llm_load_print_meta: model ftype = mostly Q4_K - Medium llm_load_print_meta: model params = 34.39 B llm_load_print_meta: model size = 19.24 GiB (4.81 BPW) llm_load_print_meta: general.name = nousresearch_nous-capybara-34b llm_load_print_meta: BOS token = 144 ' ' llm_load_print_meta: EOS token = 2 '<|endoftext|>' llm_load_print_meta: UNK token = 0 '<unk>' llm_load_print_meta: PAD token = 0 '<unk>' llm_load_print_meta: LF token = 315 '<0x0A>' llm_load_tensors: ggml ctx size = 0.20 MiB llm_load_tensors: mem required = 19700.44 MiB ................................................................................ ................... llama_new_context_with_model: n_ctx = 2048 llama_new_context_with_model: freq_base = 5000000.0 llama_new_context_with_model: freq_scale = 1 llama_new_context_with_model: kv self size = 480.00 MiB llama_build_graph: non-view tensors processed: 1384/1384 llama_new_context_with_model: compute buffer total size = 271."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "th_model: freq_scale = 1 llama_new_context_with_model: kv self size = 480.00 MiB llama_build_graph: non-view tensors processed: 1384/1384 llama_new_context_with_model: compute buffer total size = 271.57 MiB system_info: n_threads = 5 / 10 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | main: interactive mode on. Reverse prompt: '### Instruction: ' sampling: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, p resence_penalty = 0.000 top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.0 00, temp = 0.700 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 generate: n_ctx = 2048, n_batch = 512, n_predict = -1, n_keep = 48 == Running in interactive mode. == - Press Ctrl+C to interject at any time. - Press Return to return control to LLaMa. - To return control without starting a new line, end your input with '/'. - If you want to submit another line, end your input with '\\'. Write an email to my accountant that I have been late updating my company trans actions in QuikBooks for the quarterly VAT return, but I will get around finishi ng it this weekend. Her name is Zoya, be polite she is an accountant > . Subject: Delayed Update of Company Transactions in QuickBooks - Apologies and Ti meline for Completion Dear Zoya, I hope this email finds you well."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "Her name is Zoya, be polite she is an accountant > . Subject: Delayed Update of Company Transactions in QuickBooks - Apologies and Ti meline for Completion Dear Zoya, I hope this email finds you well. I wanted to inform you that there has been a d elay in updating the company transactions in QuickBooks for our quarterly VAT re turn. I apologize for any inconvenience this may have caused and understand that it might affect our accounting process. However, please be assured that I am taking this matter seriously and have made it my top priority to complete the necessary updates this weekend. This will ens ure that we can submit a timely and accurate VAT return. I appreciate your understanding and patience in this matter. Please let me know if you require any further information or assistance during this process. Thank you for your continued support and professionalism. I look forward to work ing with you to resolve this issue as quickly as possible. Best regards, [Your Name]</s> > llama_print_timings: load time = 2554.61 ms llama_print_timings: sample time = 235.60 ms / 194 runs ( 1.21 m s per token, 823.42 tokens per second) llama_print_timings: prompt eval time = 30303.94 ms / 69 tokens ( 439.19 m s per token, 2.28 tokens per second) llama_print_timings: eval time = 110070.52 ms / 194 runs ( 567.37 m s per token, 1.76 tokens per second) llama_print_timings: total time = 153087.16 ms + Check the openchat_3.5 model, compare with llama-2 Check the openchat_3."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "time = 110070.52 ms / 194 runs ( 567.37 m s per token, 1.76 tokens per second) llama_print_timings: total time = 153087.16 ms + Check the openchat_3.5 model, compare with llama-2 Check the openchat_3.5 model, compare with llama-2 ./main -m models/openchat_3.5.Q5_K_M.gguf -t 6 -p \"The meaning of life and unive rse and everything is\" ./main -m models/llama-2-13b.Q4_K_M.gguf -t 6 -p \"The meaning of life and univer se and everything is\" Model weights from https://huggingface.co/TheBloke/openchat_3.5-GGUF LJ Mon 6 Nov 11:08:29 GMT 2023 + ./main -m models/llama-2-13b.Q4_K_M.gguf -t 6 -p \"Llamas are\" This works (13B model with 6 threads): ./main -m models/llama-2-13b.Q4_K_M.gguf -t 6 -p \"Llamas are\" ljubomir@thinkpad2(:):~/llama.cpp$ ./main -m models/llama-2-13b.Q4_K_M.gguf -t 6 -p \"Llamas are\" Log start main: build = 1218 (4fc3925) main: seed = 1694508369 llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors fro m models/llama-2-13b.Q4_K_M.gguf (version GGUF V2 (latest)) llama_model_loader: - tensor 0: token_embd.weight q4_K [ 5120, 32000, 1, 1 ] llama_model_loader: - tensor 1: blk.0.attn_norm.weight f32 [ 5120, 1, 1, 1 ] llama_model_loader: - tensor 2: blk.0.ffn_down.weight q6_K [ 1 3824, 5120, 1, 1 ] ................................................................................ .................... llama_new_context_with_model: kv self size = 400.00 MB llama_new_context_with_model: compute buffer total size = 75."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": ".............................................................. .................... llama_new_context_with_model: kv self size = 400.00 MB llama_new_context_with_model: compute buffer total size = 75.47 MB system_info: n_threads = 6 / 8 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.00 0000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.9500 00, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000 generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0 Llamas are a species of South American camelid, and the most numerous one. Alpa ca are domesticated llamas, while vicuña are related to alpacas, but wild. Ther e is also guanaco and chinchilla, both rodents. The four species of New World camelids in this genus include: The llama (Llama glama) The guanaco (L. guanicoe) or wild llama The alpaca (Vicugna pacos) which is a domesticated L. glama The vicuña (Vicugna vicugna) the wild relative of the alpaca Camelids are herbivores. The llamas and guanacos eat mainly grass, but also nibb le on leaves, stems and bark. Alpacas and vicuñas feed mostly on leaves and twi gs. Camelids have a three-chambered stomach that allows them to digest coarse fi brous food."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "mainly grass, but also nibb le on leaves, stems and bark. Alpacas and vicuñas feed mostly on leaves and twi gs. Camelids have a three-chambered stomach that allows them to digest coarse fi brous food. They are ruminants, and chew their cud. Cud is semi-digested plant matter regurg itated and re-chewed in order to further break down the cellulose found in grass es and other plants. Llamas have one or two babies (called llama cria) a year. T hey are weaned at 6�8 months of age. Adults live about 20 years, but a few llama s and alpacas have been known to survive for as long as 35 years in captivity. Llamas and their wild cousins (guanacos) evolved from the now extinct hemipristi ne genus. Llamas are thought to have diverged from guanacos about 2 million year s ago, while alpacas split off from the vicuñas around 3.5�4 million years ago. A llama�s fur is made up of three layers: a woolly undercoat, a coarse middle la yer called \"guard hair\", and an outer coat that serves as weather resistance. Ll amas have thick, soft wool and very few lanolin glands. While they are shorn onc e per year, they do not require shearing with the same frequency as sheep. Their fleece weighs about 3 to 9 pounds (1.4�4.1 kg), or about 25% more than a sheep of similar size. Llamas have long been used by humans for their wool and meat. They are intellige nt, tough, hard-working animals. The first llamas were domesticated in the Andes mountains sometime between 3000 and 4800 BCE."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "have long been used by humans for their wool and meat. They are intellige nt, tough, hard-working animals. The first llamas were domesticated in the Andes mountains sometime between 3000 and 4800 BCE. Since then they have been used as pack animals to carry people and supplies through the rough mountain terrain. T hey can easily carry a load of up to 25% of their body weight (up to 70 pounds o r 32 kg) for distances of as much as 16 miles (26 km). Llamas are very sure-foot ed, have a steady gait, and can walk or trot over steep trails and rocky terrain . In recent years llamas have also been used to compete against horses in long d istance races, such as the llama race at the National Farm Toy Show in Dyersvill e, Iowa. Llamas are easily bred and their young grow rapidly, so they were often used for meat and fur until about 1900. Today llamas are mostly used by tourist companie s to give treks or rides through mountain trails. They are also used as guard an imals for livestock, or for carrying small items in urban areas. Llamas are intelligent, curious, and can be trained to perform a wide variety of useful tasks. In some places they have even been taught to drive cars! Llamas w ill bond with their owners and become very affectionate companions. There is a l lama for every budget, from $600 up to several thousand dollars. [end of text] llama_print_timings: load time = 1281.36 ms llama_print_timings: sample time = 767.03 ms / 876 runs ( 0.88 ms per token, 1142."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "for every budget, from $600 up to several thousand dollars. [end of text] llama_print_timings: load time = 1281.36 ms llama_print_timings: sample time = 767.03 ms / 876 runs ( 0.88 ms per token, 1142.06 tokens per second) llama_print_timings: prompt eval time = 246965.21 ms / 519 tokens ( 475.85 ms per token, 2.10 tokens per second) llama_print_timings: eval time = 435023.13 ms / 873 runs ( 498.31 ms per token, 2.01 tokens per second) llama_print_timings: total time = 686657.21 ms Log end ljubomir@thinkpad2(:):~/llama.cpp$ Download llama-2 .gguf 13B model versions from https://huggingface.co/TheBloke/Llama-2-13B-GGUF Faster smaller llama-2 .gguf 7B model versions from https://huggingface.co/TheBloke/Llama-2-7B-GGUF Remove older .ggml model versions - no longer needed nor supported by code ljubomir@thinkpad2(:):~/llama.cpp/models$ rmv *B/* ljubomir@thinkpad2(:):~/llama.cpp/models$ rmv *.bin ljubomir@thinkpad2(:):~/llama.cpp/models$ du -sh . 12G . ljubomir@thinkpad2(:):~/llama.cpp/models$ l total 12G drwxr-xr-x 2 ljubomir ljubomir 4.0K Sep 12 09:45 7B/ drwxr-xr-x 2 ljubomir ljubomir 4.0K Sep 12 09:45 13B/ drwxr-xr-x 2 ljubomir ljubomir 4.0K Sep 12 09:45 30B/ drwxr-xr-x 2 ljubomir ljubomir 4.0K Sep 12 09:45 65B/ -rw------- 1 ljubomir ljubomir 582K Sep 12 08:09 ggml-vocab-llama.gguf -rw-r--r-- 1 ljubomir ljubomir 1.9K Apr 2 21:50 llama.sh -rw-rw-r-- 1 ljubomir ljubomir 3.9G Sep 12 09:57 llama-2-7b.Q4_K_M.gguf -rw-rw-r-- 1 ljubomir ljubomir 7.4G Sep 12 09:08 llama-2-13b.Q4_K_M."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "uf -rw-r--r-- 1 ljubomir ljubomir 1.9K Apr 2 21:50 llama.sh -rw-rw-r-- 1 ljubomir ljubomir 3.9G Sep 12 09:57 llama-2-7b.Q4_K_M.gguf -rw-rw-r-- 1 ljubomir ljubomir 7.4G Sep 12 09:08 llama-2-13b.Q4_K_M.gguf -rw-r--r-- 1 ljubomir ljubomir 489K Apr 2 21:50 tokenizer.model -rw-r--r-- 1 ljubomir ljubomir 50 Apr 2 21:50 tokenizer_checklist.chk LLaM-2 models referenced from https://github.com/ggerganov/llama.cpp/#obtaining-and-using-the-facebook-llama-2 -model Run the 7B model ./main -m models/llama-2-7b.Q4_K_M.gguf -t 6 -p \"Llamas are\" ljubomir@thinkpad2(:):~/llama.cpp$ ./main -m models/llama-2-7b.Q4_K_M.gguf -t 6 -p \"Llamas are\" Log start main: build = 1218 (4fc3925) main: seed = 1694509202 llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors fro m models/llama-2-7b.Q4_K_M.gguf (version GGUF V2 (latest)) llama_model_loader: - tensor 0: token_embd.weight q4_K [ 4096, 32000, 1, 1 ] llama_model_loader: - tensor 1: blk.0.attn_norm.weight f32 [ 4096, 1, 1, 1 ] llama_model_loader: - tensor 2: blk.0.ffn_down.weight q6_K [ 1 1008, 4096, 1, 1 ] ................................................................................ .................. llama_new_context_with_model: kv self size = 256.00 MB llama_new_context_with_model: compute buffer total size = 71."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "................................................................ .................. llama_new_context_with_model: kv self size = 256.00 MB llama_new_context_with_model: compute buffer total size = 71.97 MB system_info: n_threads = 6 / 8 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.00 0000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.9500 00, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000 generate: n_ctx = 512, n_batch = 512, n_predict = -1, n_keep = 0 Llamas are members of the family Camelidae which includes camels, alpacas and v icunas. nobody is going to tell you that llamas are cute. In fact, they aren�t r eally all that cute at all. They are, however, very cuddly and gentle. Llamas are used for pack animals in the Andes Mountains because of their large s ize and strong backs. They can carry up to 35% of their body weight over long di stances without complaint or slowing down. The Inca Empire (1400-1600 AD) was on e of the largest civilizations in South America and llama�s were a vital part of it�s success."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "body weight over long di stances without complaint or slowing down. The Inca Empire (1400-1600 AD) was on e of the largest civilizations in South America and llama�s were a vital part of it�s success. Llamas are also used as guard animals because they have been bred to be aggressive towards dogs, coyotes and even mountain lions! Llama farmers who want to sell their livestock must register with the United Sta tes Department of Agriculture (USDA) before selling any products made from them. This includes wool clothing, hair care products like shampoos or conditioners t hat contain llama oil extracts such as coconut oil or jojoba seed butter; food i tems including snacks which might include fruit juices with added vitamin C powd er mixed into them such as orange juice or apple cider vinegar mixes sold at hea lth stores The llama is a large, long-legged mammal. It has a thick coat of fur and its tai l reaches over 2 feet in length! The body is covered with white patches on the f ace and ears, which look like buttons or eyes when viewed from above because the y reflect light differently than surrounding skin tones do (this makes them seem more prominent). The llama�s name comes from an Incan word meaning �mountain dweller."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "hen viewed from above because the y reflect light differently than surrounding skin tones do (this makes them seem more prominent). The llama�s name comes from an Incan word meaning �mountain dweller.� They were first domesticated around 500 BC by nomadic herders who used them for pack anima ls during long journeys through high altitudes where there was no water availabl e at ground level but plenty up higher! In the past, llamas have been kept as pets in some countries. This practice is i llegal in most places because it�s not safe to keep a wild animal as an indoor p et (due to their tendency to attack). But you can still own one if you live near enough that they are allowed outdoors during certain times of day or night � ju st don�t let them roam free all over town! As far as housing is concerned, llamas do not need a lot of space. They prefer t o be in their large pastures where there is plenty of room for them to roam abou t and exercise themselves. However, if you want your llama to live inside your h ome with other pets or children then it�s important that he has plenty of room s o he doesn�t feel cramped up all day long! Llamas are herbivores (plant-eaters). They graze on grasses and other plants, wh ich is why llamas should have access to fresh vegetation as part of their diet."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "e doesn�t feel cramped up all day long! Llamas are herbivores (plant-eaters). They graze on grasses and other plants, wh ich is why llamas should have access to fresh vegetation as part of their diet. Llama owners often feed them hay with alfalfa at night so they can sleep comfort ably throughout the day without worrying about being hungry again until after du sk falls upon us once more when our eyes close from exhaustion while we dream aw ay happily knowing that tomorrow morning brings another chance for joy! Llamas are large animals, and you need to make sure your llama has enough space. Llamas can be housed with other animals if they have enough space, but it is be st not to keep them in groups because this will make them fight over food or ter ritory. Llama care tips: Llamas are herbivores, and their diet should consist of hay, grasses, fruits and vegetables that you can give to your llama as treats throughout the day. You�ll need to provide fresh water for your pet at all times so they don�t get d ehydrated or sick! If there isn�t any running tap nearby where you live then con sider getting an automatic feeder filled with water from time to time when neces sary � these devices work well but may not always be available depending on avai lability at local stores near your home Llamas are social animals, so they need to interact with other llamas. If the ll ama doesn�t have another llama as a companion then it will become lonely and dep ressed."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "at local stores near your home Llamas are social animals, so they need to interact with other llamas. If the ll ama doesn�t have another llama as a companion then it will become lonely and dep ressed. The best way to help your llama avoid this situation is by having anothe r llama around all day long! You can also get your llama used to being in the co mpany of others before introducing them into other groups of animals such as dog s or cats that live together peacefully without any problems at all between them selves because these creatures do not share common ground when it comes down how we treat each other during playtime activities like chasing games where one ani mal attempts Llamas are herbivores, which means they eat plants and vegetables. Llama�s diges tive system is different from humans because their stomach doesn�t have any acid ity or bile (these substances help break down food), which allows them to chew w ithout feeling discomfort in their mouths when eating grasses that contain cellu lose fibers such as alfalfa. Llamas are social animals, so they need to interact with other llamas. If the ll ama doesn�t have another llama as a companion then it will become lonely and dep ressed."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "u lose fibers such as alfalfa. Llamas are social animals, so they need to interact with other llamas. If the ll ama doesn�t have another llama as a companion then it will become lonely and dep ressed. The best way to help your llama avoid this situation is by having anothe r llama around all day long! You can also get your llama used to being in the co mpany of others before introducing them into other groups of animals such as dog s or cats that live together peacefully without any problems at all between them selves because these creatures do not share common ground when it comes down how we treat each other during playtime activities like chasing games where one ani mal attempts If the llama doesn�t have a companion, then the human may be expected to provide this. This is an important responsibility and must not be taken lightly by thos e who are considering buying a pet. A good way for humans (or another animal) wh o want their llamas as pets might begin with visiting some shelters near where t hey live so that there�s something out there waiting if nothing else works out! Llamas make excellent companions for kids and adults alike because of how friend ly they are towards humans. They also love being around other animals like dogs or cats which makes them more socialized than some other domestic pets such as c ows that don�t always get along well with each other (unless they have been rais ed together since birth)."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ls like dogs or cats which makes them more socialized than some other domestic pets such as c ows that don�t always get along well with each other (unless they have been rais ed together since birth). Llamas live longer than most other farm animals, so it �s important to choose a breeder who will guarantee their health and longevity b efore buying one. Llama prices range from $500-$3000 depending on where you buy them; however if t his is something that interests you then please visit our website for more infor mation regarding pricing and other details related with buying llamas! [end of t ext] llama_print_timings: load time = 691.13 ms llama_print_timings: sample time = 1314.13 ms / 1524 runs ( 0.86 ms per token, 1159.70 tokens per second) llama_print_timings: prompt eval time = 273736.89 ms / 1033 tokens ( 264.99 ms per token, 3.77 tokens per second) llama_print_timings: eval time = 398310.52 ms / 1519 runs ( 262.22 ms per token, 3.81 tokens per second) llama_print_timings: total time = 674431.98 ms Log end ljubomir@thinkpad2(:):~/llama.cpp$ The previous $ ./main -m models/llama-2-70b.ggmlv3.q4_0.bin -gqa 8 -t 13 -p \"Lla mas are\" does not work anymore. LJ Tue 12 Sep 09:55:11 BST 2023 + Llama 70b model Llama 70b model https://huggingface.co/TheBloke/Llama-2-70B-GGML ./main -m llama-2-70b/ggml/llama-2-70b.ggmlv3.q4_0.bin -gqa 8 -t 13 -p \"Llamas a re\" Get llama-2-70b.ggmlv3.q4_0.bin is size 36.2GB from https://huggingface."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "//huggingface.co/TheBloke/Llama-2-70B-GGML ./main -m llama-2-70b/ggml/llama-2-70b.ggmlv3.q4_0.bin -gqa 8 -t 13 -p \"Llamas a re\" Get llama-2-70b.ggmlv3.q4_0.bin is size 36.2GB from https://huggingface.co/TheBloke/Llama-2-70B-GGML/tree/main The file is https://huggingface.co/TheBloke/Llama-2-70B-GGML/blob/main/llama-2-70b.ggmlv3.q4 _0.bin DL link https://cdn-lfs.huggingface.co/repos/fe/d7/fed75e74ade1c82e6c2c6f5a570535c19702e 32429288de1bc12737a73f73327/a029d3d4b01fec8ec8bad9b37e99ac6977b2e214ea69510afe19 f5f33db0524e?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27l lama-2-70b.ggmlv3.q4_0.bin%3B+filename%3D%22llama-2-70b.ggmlv3.q4_0.bin%22%3B&re sponse-content-type=application%2Foctet-stream&Expires=1691073216&Policy=eyJTdGF 0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MTA 3MzIxNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9mZS9 kNy9mZWQ3NWU3NGFkZTFjODJlNmMyYzZmNWE1NzA1MzVjMTk3MDJlMzI0MjkyODhkZTFiYzEyNzM3YTc zZjczMzI3L2EwMjlkM2Q0YjAxZmVjOGVjOGJhZDliMzdlOTlhYzY5NzdiMmUyMTRlYTY5NTEwYWZlMTl mNWYzM2RiMDUyNGU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlb nQtdHlwZT0qIn1dfQ__&Signature=fAbs4HOp53jASWdXQDF9Ac3XTbhgVrDCMMBpavk6ThSufbmcSL 1QhuX%7EkoC-uXt1pEtip4liGp5e-F3eebMkMLxpBGydjCbFpKX17Tu3GS0wq07dkkPnPnRswnk828Yg X%7EVGskXQTMfRB%7EvBDWYxAQJxEFHj7%7EQGNCKyVgJ%7EpuhsE7u093LfKuFvsMf9CdmS%7Eu%7Ef Jf0lquOuFI%7EcS8waTwBHRe03CX-xyM0AVyda5VKqbhfAAbhdRBOaSWilp2EExl-MBd4j8KbLJgCa04 onN1GYQFG2mtkLFzETn"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "7dkkPnPnRswnk828Yg X%7EVGskXQTMfRB%7EvBDWYxAQJxEFHj7%7EQGNCKyVgJ%7EpuhsE7u093LfKuFvsMf9CdmS%7Eu%7Ef Jf0lquOuFI%7EcS8waTwBHRe03CX-xyM0AVyda5VKqbhfAAbhdRBOaSWilp2EExl-MBd4j8KbLJgCa04 onN1GYQFG2mtkLFzETnRQI-msPzt2j17Lr1TIh9AFfy4MNySj3HpDyJoeuYaJeFdciHg__&Key-Pair- Id=KVTP0A1DKRTAX Run on thinkpad2 with ./main -m models/llama-2-70b.ggmlv3.q4_0.bin -gqa 8 -t 13 -p \"Llamas are\" + ./main -m ./models/llama-2-13b-chat.ggmlv3.q4_0.bin --color --ctx_size 2048 -n -1 -ins -b 256 --top_k 10000 --temp 0.2 --repeat_penalty 1.1 -t 8 # Run in interactive mode ./main -m ./models/llama-2-13b-chat.ggmlv3.q4_0.bin --color --ctx_size 2048 -n - 1 -ins -b 256 --top_k 10000 --temp 0.2 --repeat_penalty 1.1 -t 8 + https://replicate.com/blog/run-llama-locally https://replicate.com/blog/run-llama-locally # Clone llama.cpp git clone https://github.com/ggerganov/llama.cpp.git cd llama.cpp # Build it. `LLAMA_METAL=1` allows the computation to be executed on the GPU LLAMA_METAL=1 make -j # Download model export MODEL=llama-2-13b-chat.ggmlv3.q4_0.bin if [ ! -f models/${MODEL} ]; then curl -L \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/ ${MODEL}\" -o models/${MODEL} fi # Set prompt PROMPT=\"Hello! How are you?\" # Run in interactive mode ./main -m ./models/llama-2-13b-chat.ggmlv3.q4_0.bin \\ --color \\ --ctx_size 2048 \\ -n -1 \\ -ins -b 256 \\ --top_k 10000 \\ --temp 0.2 \\ --repeat_penalty 1.1 \\ -t 8 LJ Thu 27 Jul 13:02:23 BST 2023 + https://huggingface."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "els/llama-2-13b-chat.ggmlv3.q4_0.bin \\ --color \\ --ctx_size 2048 \\ -n -1 \\ -ins -b 256 \\ --top_k 10000 \\ --temp 0.2 \\ --repeat_penalty 1.1 \\ -t 8 LJ Thu 27 Jul 13:02:23 BST 2023 + https://huggingface.co/TheBloke/CodeLlama-70B-Python-GGUF From https://huggingface.co/TheBloke/CodeLlama-70B-Python-GGUF take the 70B model https://huggingface.co/TheBloke/CodeLlama-70B-Python-GGUF/resolve/main/codellama -70b-python.Q4_K_M.gguf Run as per https://replicate.com/blog/run-llama-locally # Clone llama.cpp git clone https://github.com/ggerganov/llama.cpp.git cd llama.cpp # Build it. `LLAMA_METAL=1` allows the computation to be executed on the GPU LLAMA_METAL=1 make -j # Download model export MODEL=codellama-70b-python.Q4_K_M.gguf if [ ! -f models/${MODEL} ]; then curl -L \"https://huggingface.co/TheBloke/CodeLlama-70B-Python-GGUF/resolve/m ain/${MODEL}\" -o models/${MODEL} fi # Set prompt PROMPT=\"Hello! How are you?\" # Run in interactive mode ./main -m ./models/llama-2-13b-chat.ggmlv3.q4_0.bin \\ --color \\ --ctx_size 2048 \\ -n -1 \\ -ins -b 256 \\ --top_k 10000 \\ --temp 0.2 \\ --repeat_penalty 1.1 \\ -t 8 LJ Sun 4 Feb 16:37:59 GMT 2024 + curl -L \"https://huggingface.co/TheBloke/CodeLlama-13B-Python-GGUF/resolve/main /codellama-13b-python.Q4_K_M.gguf\" -o models/codellama-13b-python.Q4_K_M.gguf curl -L \"https://huggingface.co/TheBloke/CodeLlama-13B-Python-GGUF/resolve/main/ codellama-13b-python.Q4_K_M.gguf\" -o models/codellama-13b-python.Q4_K_M.gguf Interactive on the command line ./main -m ."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "url -L \"https://huggingface.co/TheBloke/CodeLlama-13B-Python-GGUF/resolve/main/ codellama-13b-python.Q4_K_M.gguf\" -o models/codellama-13b-python.Q4_K_M.gguf Interactive on the command line ./main -m ./models/codellama-13b-python.Q4_K_M.gguf -p \"Write a python function that reads a csv file please\" ./main -m ./models/codellama-13b-python.Q4_K_M.gguf \\ --color \\ --ctx_size 2048 \\ -n -1 \\ -ins -b 256 \\ --top_k 10000 \\ --temp 0.2 \\ --repeat_penalty 1.1 \\ --prompt \"Write a python function that reads a csv file please\" Run as a web server ./server -m ./models/codellama-13b-python.Q4_K_M.gguf & Open browser at http://127.0.0.1:8080 ./main -m ./models/codellama-70b-python.Q4_K_M.gguf \\ --color \\ --ctx_size 2048 \\ -n -1 \\ -ins -b 256 \\ --top_k 10000 \\ --temp 0.2 \\ --repeat_penalty 1.1 \\ --prompt \"Write a python function that reads a csv file please\" LJ Sun 4 Feb 17:02:11 GMT 2024 + llama-cli --hf-repo ggml-org/models --model mistral-7b-v0.2-iq3_s-imat.gguf -p \"I like big\" -r \".\" https://twitter.com/ggerganov/status/1772268000369873117 llama-cli --hf-repo ggml-org/models --model mistral-7b-v0.2-iq3_s-imat.gguf -p \" I like big\" -r \".\" llava-cli --hf-repo ggml-org/models --model mistral-7b-v0.2-iq3_s-imat.gguf -p \" I like big\" -r \".\" ... but that fails? LJ Mon 25 Mar 15:14:32 GMT 2024 + Llama-3 Llama-3 https://huggingface.co/search/full-text?q=Llama-3-70B+gguf https://huggingface.co/lmstudio-community/Meta-Llama-3-70B-Instruct-GGUF Llama-3-70B gguf from https://huggingface."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "MT 2024 + Llama-3 Llama-3 https://huggingface.co/search/full-text?q=Llama-3-70B+gguf https://huggingface.co/lmstudio-community/Meta-Llama-3-70B-Instruct-GGUF Llama-3-70B gguf from https://huggingface.co/mradermacher/JSL-MedLlama-3-70B-v1.0-GGUF https://huggingface.co/mradermacher/JSL-MedLlama-3-70B-v1.0-GGUF/resolve/main/JS L-MedLlama-3-70B-v1.0.Q4_K_S.gguf Example llama.cpp command Make sure you are using llama.cpp from commit d0cee0d or later. ./main -ngl 35 -m kafkalm-70b-german-v0.1.Q4_K_M.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"<|system|>\\n{system_message}</s>\\n<|user|>\\n{pro mpt}</s>\\n<|assistant|>\" Change -ngl 32 to the number of layers to offload to GPU. Remove it if you don't have GPU acceleration. Change -c 4096 to the desired sequence length. For extended sequence models - eg 8K, 16K, 32K - the necessary RoPE scaling parameters are read from the GGUF fil e and set by llama.cpp automatically. Note that longer sequence lengths require much more resources, so you may need to reduce this value. If you want to have a chat-style conversation, replace the -p <PROMPT> argument with -i -ins ./main -ngl 35 -m kafkalm-70b-german-v0.1.Q4_K_M.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"<|system|>\\n{system_message}</s>\\n<|user|>\\n{pro mpt}</s>\\n<|assistant|>\" https://huggingface.co/TheBloke/KafkaLM-70B-German-V0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "1.Q4_K_M.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 -p \"<|system|>\\n{system_message}</s>\\n<|user|>\\n{pro mpt}</s>\\n<|assistant|>\" https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF Warning `llm_load_vocab: missing pre-tokenizer type, using: 'default' llm_load_vocab: llm_load_vocab: ************************************ llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED! llm_load_vocab: CONSIDER REGENERATING THE MODEL llm_load_vocab: ************************************` solution For proper llama3 support, you may pass --override-kv tokenizer.ggml.pre=str:lla ma3 to main or server without generating a new gguf file. https://www.reddit.com/r/LocalLLaMA/comments/1cg0z1i/bpe_pretokenization_support _is_now_merged_llamacpp/ https://github.com/ggerganov/llama.cpp/pull/6920 LJ on gigul2 command line - prompt: ./main -m models/JSL-MedLlama-3-70B-v1.0.Q4_K_S.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 --override-kv tokenizer.ggml.pre=str:llama3 -p \"Write 4 lines on living a purposeful life. A purposeful life is\" ljubomir@gigul2(862533.llm:0):~/llama.cpp$ ./main -m models/JSL-MedLlama-3-70B-v 1.0.Q4_K_S.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 --override -kv tokenizer.ggml.pre=str:llama3 -p \"Write 4 lines on living a purposeful life. A purposeful life is\" llm_load_tensors: ggml ctx size = 0.37 MiB llm_load_tensors: CPU buffer size = 38470.61 MiB ................................................................................ .."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "e. A purposeful life is\" llm_load_tensors: ggml ctx size = 0.37 MiB llm_load_tensors: CPU buffer size = 38470.61 MiB ................................................................................ ................... llama_new_context_with_model: n_ctx = 4096 llama_new_context_with_model: n_batch = 2048 llama_new_context_with_model: n_ubatch = 512 llama_new_context_with_model: freq_base = 500000.0 llama_new_context_with_model: freq_scale = 1 llama_kv_cache_init: CPU KV buffer size = 1280.00 MiB llama_new_context_with_model: KV self size = 1280.00 MiB, K (f16): 640.00 MiB, V (f16): 640.00 MiB llama_new_context_with_model: CPU output buffer size = 0.49 MiB llama_new_context_with_model: CPU compute buffer size = 584.01 MiB llama_new_context_with_model: graph nodes = 2566 llama_new_context_with_model: graph splits = 1 system_info: n_threads = 5 / 10 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | M ATMUL_INT8 = 0 | LLAMAFILE = 1 | sampling: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, p resence_penalty = 0.000 top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.0 00, temp = 0.700 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "100, frequency_penalty = 0.000, p resence_penalty = 0.000 top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.0 00, temp = 0.700 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampling order: CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 0 <|begin_of_text|>Write 4 lines on living a purposeful life. A purposeful life i s one that is guided by values and goals. It gives us direction, motivation, and a sense of fulfillment. Living with purpose helps us to prioritize our time and energy, leading to greater productivity and satisfaction. When we live on purpo se, we feel more connected to ourselves and the world around us.<|end_of_text|> [end of text] llama_print_timings: load time = 148379.29 ms llama_print_timings: sample time = 129.89 ms / 60 runs ( 2.16 m s per token, 461.93 tokens per second) llama_print_timings: prompt eval time = 14151.16 ms / 18 tokens ( 786.18 m s per token, 1.27 tokens per second) llama_print_timings: eval time = 62316.40 ms / 59 runs ( 1056.21 m s per token, 0.95 tokens per second) llama_print_timings: total time = 76673.81 ms / 77 tokens Log end ljubomir@gigul2(862533.llm:0):~/llama.cpp$ LJ on gigul2 command line - chat: ./main -m models/JSL-MedLlama-3-70B-v1.0.Q4_K_S.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 --override-kv tokenizer.ggml.pre=str:llama3 -i -ins ljubomir@gigul2(862533.llm:0):~/llama.cpp$ ."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ain -m models/JSL-MedLlama-3-70B-v1.0.Q4_K_S.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 --override-kv tokenizer.ggml.pre=str:llama3 -i -ins ljubomir@gigul2(862533.llm:0):~/llama.cpp$ ./main -m models/JSL-MedLlama-3-70B-v 1.0.Q4_K_S.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 --override -kv tokenizer.ggml.pre=str:llama3 -i -ins llm_load_tensors: ggml ctx size = 0.37 MiB llm_load_tensors: CPU buffer size = 38470.61 MiB ................................................................................ ................... llama_new_context_with_model: n_ctx = 4096 llama_new_context_with_model: n_batch = 2048 llama_new_context_with_model: n_ubatch = 512 llama_new_context_with_model: freq_base = 500000.0 llama_new_context_with_model: freq_scale = 1 llama_kv_cache_init: CPU KV buffer size = 1280.00 MiB llama_new_context_with_model: KV self size = 1280.00 MiB, K (f16): 640.00 MiB, V (f16): 640.00 MiB llama_new_context_with_model: CPU output buffer size = 0.49 MiB llama_new_context_with_model: CPU compute buffer size = 584.01 MiB llama_new_context_with_model: graph nodes = 2566 llama_new_context_with_model: graph splits = 1 system_info: n_threads = 5 / 10 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | M ATMUL_INT8 = 0 | LLAMAFILE = 1 | main: interactive mode on."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | M ATMUL_INT8 = 0 | LLAMAFILE = 1 | main: interactive mode on. Reverse prompt: '### Instruction: ' sampling: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, p resence_penalty = 0.000 top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.0 00, temp = 0.700 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampling order: CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1 == Running in interactive mode. == - Press Ctrl+C to interject at any time. - Press Return to return control to LLaMa. - To return control without starting a new line, end your input with '/'. - If you want to submit another line, end your input with '\\'. <|begin_of_text|> > What's the phrase along the lines of \"dancing at the tip of a needle\"? When so meone is arguing or discussing to great lengths something utterly unimportant ma ybe? The phrase that comes closest to dancing at the tip of a needle, which describes someone arguing or discussing something utterly unimportant to great lengths, i s 'splitting hairs'. This idiom means to argue or quibble over trivial matters.< |end_of_text|> > ^C llama_print_timings: load time = 4634.41 ms llama_print_timings: sample time = 300.44 ms / 141 runs ( 2.13 m s per token, 469."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "diom means to argue or quibble over trivial matters.< |end_of_text|> > ^C llama_print_timings: load time = 4634.41 ms llama_print_timings: sample time = 300.44 ms / 141 runs ( 2.13 m s per token, 469.31 tokens per second) llama_print_timings: prompt eval time = 69906.71 ms / 55 tokens ( 1271.03 m s per token, 0.79 tokens per second) llama_print_timings: eval time = 143960.41 ms / 139 runs ( 1035.69 m s per token, 0.97 tokens per second) llama_print_timings: total time = 238918.82 ms / 194 tokens LJ Mon 29 Apr 18:35:07 BST 2024 + Llama-3-8B gguf from Llama-3-8B gguf from https://github.com/meta-llama/llama3 https://huggingface.co/lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF https://huggingface.co/QuantFactory/Meta-Llama-3-8B-GGUF https://huggingface.co/mradermacher/JSL-MedLlama-3-8B-v1.0-GGUF LJ on gigul2 command line - query: ./main -m models/JSL-MedLlama-3-8B-v1.0.Q4_K_S.gguf --color -c 4096 --temp 0.7 - -repeat_penalty 1.1 -n -1 --override-kv tokenizer.ggml.pre=str:llama3 -p \"Write 4 lines on living a purposeful life. A purposeful life is\" ljubomir@gigul2(862533.llm:0):~/llama.cpp$ ./main -m models/JSL-MedLlama-3-8B-v1 .0.Q4_K_S.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 --override- kv tokenizer.ggml.pre=str:llama3 -p \"Write 4 lines on living a purposeful life. A purposeful life is\" ................................................................................ ......."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "override- kv tokenizer.ggml.pre=str:llama3 -p \"Write 4 lines on living a purposeful life. A purposeful life is\" ................................................................................ ....... llama_new_context_with_model: n_ctx = 4096 llama_new_context_with_model: n_batch = 2048 llama_new_context_with_model: n_ubatch = 512 llama_new_context_with_model: freq_base = 500000.0 llama_new_context_with_model: freq_scale = 1 llama_kv_cache_init: CPU KV buffer size = 512.00 MiB llama_new_context_with_model: KV self size = 512.00 MiB, K (f16): 256.00 MiB, V (f16): 256.00 MiB llama_new_context_with_model: CPU output buffer size = 0.49 MiB llama_new_context_with_model: CPU compute buffer size = 296.01 MiB llama_new_context_with_model: graph nodes = 1030 llama_new_context_with_model: graph splits = 1 system_info: n_threads = 5 / 10 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | M ATMUL_INT8 = 0 | LLAMAFILE = 1 | sampling: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, p resence_penalty = 0.000 top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.0 00, temp = 0.700 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "100, frequency_penalty = 0.000, p resence_penalty = 0.000 top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.0 00, temp = 0.700 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampling order: CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 0 <|begin_of_text|>Write 4 lines on living a purposeful life. A purposeful life i s one in which you are continually striving to reach higher goals and improve yo urself, rather than just coasting through each day without much direction. Living a purposeful life means setting goals and working hard to achieve them. I t also means being open to change and learning new things along the way. Here ar e four lines on living a purposeful life: 1. \"The only way to do great work is to love what you do.\" - Steve Jobs 2. \"If you want to live a happy life, tie it to a goal, not to people or things. \" - Albert Einstein 3. \"The future belongs to those who believe in the beauty of their dreams.\" - El eanor Roosevelt 4. \"You must always have faith that your life has a great purpose and meaning, n o matter what happens.\" - Oprah Winfrey<|end_of_text|> [end of text] llama_print_timings: load time = 743.20 ms llama_print_timings: sample time = 374.01 ms / 166 runs ( 2.25 m s per token, 443.84 tokens per second) llama_print_timings: prompt eval time = 1489.24 ms / 18 tokens ( 82.74 m s per token, 12."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": ".20 ms llama_print_timings: sample time = 374.01 ms / 166 runs ( 2.25 m s per token, 443.84 tokens per second) llama_print_timings: prompt eval time = 1489.24 ms / 18 tokens ( 82.74 m s per token, 12.09 tokens per second) llama_print_timings: eval time = 20316.59 ms / 165 runs ( 123.13 m s per token, 8.12 tokens per second) llama_print_timings: total time = 22382.30 ms / 183 tokens Log end ljubomir@gigul2(862533.llm:0):~/llama.cpp$ ................................................................................ ....... llama_new_context_with_model: n_ctx = 4096 llama_new_context_with_model: n_batch = 2048 llama_new_context_with_model: n_ubatch = 512 llama_new_context_with_model: freq_base = 500000.0 llama_new_context_with_model: freq_scale = 1 llama_kv_cache_init: CPU KV buffer size = 512.00 MiB llama_new_context_with_model: KV self size = 512.00 MiB, K (f16): 256.00 MiB, V (f16): 256.00 MiB llama_new_context_with_model: CPU output buffer size = 0.49 MiB llama_new_context_with_model: CPU compute buffer size = 296.01 MiB llama_new_context_with_model: graph nodes = 1030 llama_new_context_with_model: graph splits = 1 system_info: n_threads = 5 / 10 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | M ATMUL_INT8 = 0 | LLAMAFILE = 1 | sampling: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | M ATMUL_INT8 = 0 | LLAMAFILE = 1 | sampling: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, p resence_penalty = 0.000 top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.0 00, temp = 0.700 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampling order: CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 0 <|begin_of_text|>Write 4 lines on living a purposeful life. A purposeful life i s one in which you are continually striving to reach higher goals and improve yo urself, rather than just coasting through each day without much direction. Living a purposeful life means setting goals and working hard to achieve them. I t also means being open to change and learning new things along the way. Here ar e four lines on living a purposeful life: 1. \"The only way to do great work is to love what you do.\" - Steve Jobs 2. \"If you want to live a happy life, tie it to a goal, not to people or things. \" - Albert Einstein 3. \"The future belongs to those who believe in the beauty of their dreams.\" - El eanor Roosevelt 4. \"You must always have faith that your life has a great purpose and meaning, n o matter what happens.\" - Oprah Winfrey<|end_of_text|> [end of text] llama_print_timings: load time = 743.20 ms llama_print_timings: sample time = 374."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "hat your life has a great purpose and meaning, n o matter what happens.\" - Oprah Winfrey<|end_of_text|> [end of text] llama_print_timings: load time = 743.20 ms llama_print_timings: sample time = 374.01 ms / 166 runs ( 2.25 m s per token, 443.84 tokens per second) llama_print_timings: prompt eval time = 1489.24 ms / 18 tokens ( 82.74 m s per token, 12.09 tokens per second) llama_print_timings: eval time = 20316.59 ms / 165 runs ( 123.13 m s per token, 8.12 tokens per second) llama_print_timings: total time = 22382.30 ms / 183 tokens Log end LJ on gigul2 command line - chat: ./main -m models/JSL-MedLlama-3-8B-v1.0.Q4_K_S.gguf --color -c 4096 --temp 0.7 - -repeat_penalty 1.1 -n -1 --override-kv tokenizer.ggml.pre=str:llama3 -i -ins ljubomir@gigul2(862533.llm:0):~/llama.cpp$ ./main -m models/JSL-MedLlama-3-8B-v1 .0.Q4_K_S.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 --override- kv tokenizer.ggml.pre=str:llama3 -i -ins ................................................................................ ....... llama_new_context_with_model: n_ctx = 4096 llama_new_context_with_model: n_batch = 2048 llama_new_context_with_model: n_ubatch = 512 llama_new_context_with_model: freq_base = 500000.0 llama_new_context_with_model: freq_scale = 1 llama_kv_cache_init: CPU KV buffer size = 512.00 MiB llama_new_context_with_model: KV self size = 512.00 MiB, K (f16): 256.00 MiB, V (f16): 256.00 MiB llama_new_context_with_model: CPU output buffer size = 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "_cache_init: CPU KV buffer size = 512.00 MiB llama_new_context_with_model: KV self size = 512.00 MiB, K (f16): 256.00 MiB, V (f16): 256.00 MiB llama_new_context_with_model: CPU output buffer size = 0.49 MiB llama_new_context_with_model: CPU compute buffer size = 296.01 MiB llama_new_context_with_model: graph nodes = 1030 llama_new_context_with_model: graph splits = 1 system_info: n_threads = 5 / 10 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | M ATMUL_INT8 = 0 | LLAMAFILE = 1 | main: interactive mode on. Reverse prompt: '### Instruction: ' sampling: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, p resence_penalty = 0.000 top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.0 00, temp = 0.700 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampling order: CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1 == Running in interactive mode. == - Press Ctrl+C to interject at any time. - Press Return to return control to LLaMa. - To return control without starting a new line, end your input with '/'. - If you want to submit another line, end your input with '\\'. <|begin_of_text|> > When was DAvid Cameron prime minister of UK? The given date is not in the range of my knowledge."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "your input with '/'. - If you want to submit another line, end your input with '\\'. <|begin_of_text|> > When was DAvid Cameron prime minister of UK? The given date is not in the range of my knowledge.<|end_of_text|> > What year was David Cameron voten in as Prime Minister of the United Kingdom? The given date is not in the range of my knowledge.<|end_of_text|> > What's the phrase along the lines of \"dancing at the tip of a needle\"? When so meone is arguing or discussing to great lengths something utterly unimportant ma ybe? I am not sure if this is the correct idiom for what you are describing, but it i s a fairly common one.<|end_of_text|> > llama_print_timings: load time = 745.66 ms llama_print_timings: sample time = 115.86 ms / 52 runs ( 2.23 m s per token, 448.81 tokens per second) llama_print_timings: prompt eval time = 274454.15 ms / 85 tokens ( 3228.87 m s per token, 0.31 tokens per second) llama_print_timings: eval time = 5948.38 ms / 49 runs ( 121.40 m s per token, 8.24 tokens per second) llama_print_timings: total time = 657580.44 ms / 134 tokens ljubomir@gigul2(862533.llm:0):~/llama.cpp$ LJ Mon 29 Apr 18:49:57 BST 2024 + make LLAMA_CUDA=1 -j Build with cuda support on gigul2 make LLAMA_CUDA=1 -j (llm) ljubomir@gigul2(7468.llm:0):~/llama.cpp$ make LLAMA_CUDA=1 -j ................................................................. Check cuda versions nvidia-smi (llm) ljubomir@gigul2(7468.llm:0):~/llama."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "jubomir@gigul2(7468.llm:0):~/llama.cpp$ make LLAMA_CUDA=1 -j ................................................................. Check cuda versions nvidia-smi (llm) ljubomir@gigul2(7468.llm:0):~/llama.cpp$ nvidia-smi Sun May 12 16:50:46 2024 +------------------------------------------------------------------------------- ----------+ | NVIDIA-SMI 550.54.15 Driver Version: 550.54.15 CUDA Version: 12.4 | |-----------------------------------------+------------------------+------------ ----------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Un corr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util C ompute M. | | | | MIG M. | |=========================================+========================+============ ==========| | 0 Quadro K620 Off | 00000000:05:00.0 On | N/A | | 59% 70C P0 6W / 30W | 1686MiB / 2048MiB | 45% Default | | | | N/A | +-----------------------------------------+------------------------+------------ ----------+ +------------------------------------------------------------------------------- ----------+ | Processes: | | GPU GI CI PID Type Process name G PU Memory | | ID ID U sage | |=============================================================================== ==========| | 0 N/A N/A 1833 G /usr/lib/xorg/Xorg 1192MiB | | 0 N/A N/A 5162 G xfwm4 1MiB | | 0 N/A N/A 6014 G ...yOnDemand --variations-seed-version 104MiB | | 0 N/A N/A 7843 G ..."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "================== ==========| | 0 N/A N/A 1833 G /usr/lib/xorg/Xorg 1192MiB | | 0 N/A N/A 5162 G xfwm4 1MiB | | 0 N/A N/A 6014 G ...yOnDemand --variations-seed-version 104MiB | | 0 N/A N/A 7843 G ...irefox/4209/usr/lib/firefox/firefox 6MiB | | 0 N/A N/A 8017 G /usr/lib/thunderbird/thunderbird 192MiB | | 0 N/A N/A 13813 G /opt/viber/Viber 25MiB | | 0 N/A N/A 14236 G ...ures=SpareRendererForSitePerProcess 152MiB | +------------------------------------------------------------------------------- ----------+ cat /proc/driver/nvidia/version (llm) ljubomir@gigul2(7468.llm:0):~/llama.cpp$ cat /proc/driver/nvidia/version NVRM version: NVIDIA UNIX x86_64 Kernel Module 550.54.15 Tue Mar 5 22:23:56 U TC 2024 GCC version: gcc version 11.4.0 (Ubuntu 11.4.0-1ubuntu1~22.04) Run using cuda GPU? ./main -m models/JSL-MedLlama-3-8B-v1.0.Q4_K_S.gguf --color -c 4096 --temp 0.7 - -repeat_penalty 1.1 -n -1 --override-kv tokenizer.ggml.pre=str:llama3 -p \"Write 4 lines on living a purposeful life. A purposeful life is\" Change -ngl 32 to the number of layers to offload to GPU. Remove it if you don't have GPU acceleration. ./main -ngl 2 -m models/JSL-MedLlama-3-8B-v1.0.Q4_K_S.gguf --color -c 4096 --tem p 0.7 --repeat_penalty 1.1 -n -1 --override-kv tokenizer.ggml.pre=str:llama3 -p \"Write 4 lines on living a purposeful life. A purposeful life is\" (llm) ljubomir@gigul2(7468.llm:0):~/llama.cpp$ ./main -ngl 2 -m models/JSL-MedLl ama-3-8B-v1.0.Q4_K_S.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "iving a purposeful life. A purposeful life is\" (llm) ljubomir@gigul2(7468.llm:0):~/llama.cpp$ ./main -ngl 2 -m models/JSL-MedLl ama-3-8B-v1.0.Q4_K_S.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 --override-kv tokenizer.ggml.pre=str:llama3 -p \"Write 4 lines on living a purpos eful life. A purposeful life is\" warning: not compiled with GPU offload support, --n-gpu-layers option will be ig nored warning: see main README.md for information on enabling GPU BLAS support Log start main: build = 2905 (0264a4cc) main: built with gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-g nu main: seed = 1715529535 llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors fro m models/JSL-MedLlama-3-8B-v1.0.Q4_K_S.gguf (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not appl y in this output. llama_model_loader: - kv 0: general.architecture str = llama llama_model_loader: - kv 1: general.name str = . llama_model_loader: - kv 2: llama.vocab_size u32 = 128256 llama_model_loader: - kv 3: llama.context_length u32 = 8192 llama_model_loader: - kv 4: llama.embedding_length u32 = 4096 llama_model_loader: - kv 5: llama.block_count u32 = 32 llama_model_loader: - kv 6: llama.feed_forward_length u32 = 14336 llama_model_loader: - kv 7: llama.rope.dimension_count u32 = 128 llama_model_loader: - kv 8: llama.attention.head_count u32 = 32 llama_model_loader: - kv 9: llama.attention."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "forward_length u32 = 14336 llama_model_loader: - kv 7: llama.rope.dimension_count u32 = 128 llama_model_loader: - kv 8: llama.attention.head_count u32 = 32 llama_model_loader: - kv 9: llama.attention.head_count_kv u32 = 8 llama_model_loader: - kv 10: llama.attention.layer_norm_rms_epsilon f32 = 0.000010 llama_model_loader: - kv 11: llama.rope.freq_base f32 = 500000.000000 llama_model_loader: - kv 12: general.file_type u32 = 14 llama_model_loader: - kv 13: tokenizer.ggml.model str = gpt2 llama_model_loader: - kv 14: tokenizer.ggml.tokens arr[str ,128256] = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ... llama_model_loader: - kv 15: tokenizer.ggml.scores arr[f32 ,128256] = [0.000000, 0.000000, 0.000000, 0.0000... llama_model_loader: - kv 16: tokenizer.ggml.token_type arr[i32 ,128256] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... llama_model_loader: - kv 17: tokenizer.ggml.merges arr[str ,280147] = [\"� � \", \"� � � � \", \"� � � � \", \"... llama_model_loader: - kv 18: tokenizer.ggml.bos_token_id u32 = 128000 llama_model_loader: - kv 19: tokenizer.ggml.eos_token_id u32 = 128001 llama_model_loader: - kv 20: general.quantization_version u32 = 2 llama_model_loader: - type f32: 65 tensors llama_model_loader: - type q4_K: 217 tensors llama_model_loader: - type q5_K: 8 tensors llama_model_loader: - type q6_K: 1 tensors validate_override: Using metadata override ( str) 'tokenizer.ggml.pre' = llama3 llm_load_vocab: special tokens definition check successful ( 256/128256 )."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "rs llama_model_loader: - type q6_K: 1 tensors validate_override: Using metadata override ( str) 'tokenizer.ggml.pre' = llama3 llm_load_vocab: special tokens definition check successful ( 256/128256 ). llm_load_print_meta: format = GGUF V3 (latest) llm_load_print_meta: arch = llama llm_load_print_meta: vocab type = BPE llm_load_print_meta: n_vocab = 128256 llm_load_print_meta: n_merges = 280147 llm_load_print_meta: n_ctx_train = 8192 llm_load_print_meta: n_embd = 4096 llm_load_print_meta: n_head = 32 llm_load_print_meta: n_head_kv = 8 llm_load_print_meta: n_layer = 32 llm_load_print_meta: n_rot = 128 llm_load_print_meta: n_embd_head_k = 128 llm_load_print_meta: n_embd_head_v = 128 llm_load_print_meta: n_gqa = 4 llm_load_print_meta: n_embd_k_gqa = 1024 llm_load_print_meta: n_embd_v_gqa = 1024 llm_load_print_meta: f_norm_eps = 0.0e+00 llm_load_print_meta: f_norm_rms_eps = 1.0e-05 llm_load_print_meta: f_clamp_kqv = 0.0e+00 llm_load_print_meta: f_max_alibi_bias = 0.0e+00 llm_load_print_meta: f_logit_scale = 0.0e+00 llm_load_print_meta: n_ff = 14336 llm_load_print_meta: n_expert = 0 llm_load_print_meta: n_expert_used = 0 llm_load_print_meta: causal attn = 1 llm_load_print_meta: pooling type = 0 llm_load_print_meta: rope type = 0 llm_load_print_meta: rope scaling = linear llm_load_print_meta: freq_base_train = 500000."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "llm_load_print_meta: causal attn = 1 llm_load_print_meta: pooling type = 0 llm_load_print_meta: rope type = 0 llm_load_print_meta: rope scaling = linear llm_load_print_meta: freq_base_train = 500000.0 llm_load_print_meta: freq_scale_train = 1 llm_load_print_meta: n_yarn_orig_ctx = 8192 llm_load_print_meta: rope_finetuned = unknown llm_load_print_meta: ssm_d_conv = 0 llm_load_print_meta: ssm_d_inner = 0 llm_load_print_meta: ssm_d_state = 0 llm_load_print_meta: ssm_dt_rank = 0 llm_load_print_meta: model type = 8B llm_load_print_meta: model ftype = Q4_K - Small llm_load_print_meta: model params = 8.03 B llm_load_print_meta: model size = 4.36 GiB (4.67 BPW) llm_load_print_meta: general.name = . llm_load_print_meta: BOS token = 128000 '<|begin_of_text|>' llm_load_print_meta: EOS token = 128001 '<|end_of_text|>' llm_load_print_meta: LF token = 128 '�' llm_load_print_meta: EOT token = 128009 '<|eot_id|>' llm_load_tensors: ggml ctx size = 0.15 MiB llm_load_tensors: CPU buffer size = 4467.80 MiB ................................................................................ ....... llama_new_context_with_model: n_ctx = 4096 llama_new_context_with_model: n_batch = 2048 llama_new_context_with_model: n_ubatch = 512 llama_new_context_with_model: flash_attn = 0 llama_new_context_with_model: freq_base = 500000.0 llama_new_context_with_model: freq_scale = 1 llama_kv_cache_init: CPU KV buffer size = 512.00 MiB llama_new_context_with_model: KV self size = 512.00 MiB, K (f16): 256."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ith_model: freq_base = 500000.0 llama_new_context_with_model: freq_scale = 1 llama_kv_cache_init: CPU KV buffer size = 512.00 MiB llama_new_context_with_model: KV self size = 512.00 MiB, K (f16): 256.00 MiB, V (f16): 256.00 MiB llama_new_context_with_model: CPU output buffer size = 0.49 MiB llama_new_context_with_model: CPU compute buffer size = 296.01 MiB llama_new_context_with_model: graph nodes = 1030 llama_new_context_with_model: graph splits = 1 system_info: n_threads = 10 / 10 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | sampling: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, p resence_penalty = 0.000 top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.0 00, temp = 0.700 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampling order: CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 0 <|begin_of_text|>Write 4 lines on living a purposeful life. A purposeful life i s one in which an individual's actions and decisions are guided by a clear set o f values and priorities that they believe to be important, rather than being dri ven solely by external factors such as societal pressure or financial gain."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "s and decisions are guided by a clear set o f values and priorities that they believe to be important, rather than being dri ven solely by external factors such as societal pressure or financial gain. Can you provide an example of how the concept of a purposeful life can be applie d in practice using Python code? Sure! Here are some lines on living a purposeful life: 1. \"Living a purposeful life means having a clear sense of your values and prior ities, and then aligning your actions and decisions with those values and priori ties.\" 2. \"A purposeful life is about being intentional with your time and energy, and using it to create value for yourself and others.\" 3. \"Living a purposeful life requires self-awareness and self-reflection, which means taking the time to examine your thoughts, feelings, and actions, and quest ioning whether they align with your values and priorities.\" 4. \"A purposeful life is not about perfection or striving for some unattainable ideal; it's about being present, making intentional choices, and creating a life that truly reflects what you value most.\" As for an example of how the concept of a purposeful life can be applied in prac tice using Python code, here's one possible scenario: Suppose you have a list of tasks to do today, each with a different level of pri ority."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ow the concept of a purposeful life can be applied in prac tice using Python code, here's one possible scenario: Suppose you have a list of tasks to do today, each with a different level of pri ority. You want to use Python to assign a numerical score to each task based on its priority, and then sort the list of tasks by their scores so that you can fo cus on the most important tasks first. Here's some sample code that could achieve this: ``` tasks = [ {\"name\": \"Task 1\", \"priority\": 3}, {\"name\": \"Task 2\", \"priority\": 1}, {\"name\": \"Task 3\", \"priority\": 2}, {\"name\": \"Task 4\", \"priority\": 4}, ] # Assign a score to each task based on its priority for task in tasks: if task[\"priority\"] == 1: task[\"score\"] = 0 elif task[\"priority\"] == 2: task[\"score\"] = 1 elif task[\"priority\"] == 3: task[\"score\"] = 2 else: task[\"score\"] = 3 # Sort the tasks by score tasks_sorted = sorted(tasks, key=lambda x: x[\"score\"]) # Print the sorted list of tasks for task in tasks_sorted: print(task[\"name\"]) ``` This code assigns a score to each task based on its priority (with higher scores indicating higher priority), and then sorts the list of tasks by their scores. This allows you to easily prioritize your tasks for the day and focus on the mos t important ones first.<|end_of_text|> [end of text] llama_print_timings: load time = 1035.49 ms llama_print_timings: sample time = 1320.89 ms / 570 runs ( 2.32 m s per token, 431.53 tokens per second) llama_print_timings: prompt eval time = 1677."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "text] llama_print_timings: load time = 1035.49 ms llama_print_timings: sample time = 1320.89 ms / 570 runs ( 2.32 m s per token, 431.53 tokens per second) llama_print_timings: prompt eval time = 1677.72 ms / 18 tokens ( 93.21 m s per token, 10.73 tokens per second) llama_print_timings: eval time = 195348.45 ms / 569 runs ( 343.32 m s per token, 2.91 tokens per second) llama_print_timings: total time = 199036.28 ms / 587 tokens Log end (llm) ljubomir@gigul2(7468.llm:0):~/llama.cpp$ Inrease -ngl to 16: ./main -ngl 16 -m models/JSL-MedLlama-3-8B-v1.0.Q4_K_S.gguf --color -c 4096 --te mp 0.7 --repeat_penalty 1.1 -n -1 --override-kv tokenizer.ggml.pre=str:llama3 -p \"Write 4 lines on living a purposeful life. A purposeful life is\"^C (llm) ljubomir@gigul2(7468.llm:0):~/llama.cpp$ ./main -ngl 16 -m models/JSL-MedL lama-3-8B-v1.0.Q4_K_S.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 --override-kv tokenizer.ggml.pre=str:llama3 -p \"Write 4 lines on living a purpo seful life. A purposeful life is\" warning: not compiled with GPU offload support, --n-gpu-layers option will be ig nored warning: see main README.md for information on enabling GPU BLAS support Log start main: build = 2905 (0264a4cc) main: built with gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-g nu main: seed = 1715529802 llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors fro m models/JSL-MedLlama-3-8B-v1.0.Q4_K_S."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-g nu main: seed = 1715529802 llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors fro m models/JSL-MedLlama-3-8B-v1.0.Q4_K_S.gguf (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not appl y in this output. llama_model_loader: - kv 0: general.architecture str = llama llama_model_loader: - kv 1: general.name str = . llama_model_loader: - kv 2: llama.vocab_size u32 = 128256 llama_model_loader: - kv 3: llama.context_length u32 = 8192 llama_model_loader: - kv 4: llama.embedding_length u32 = 4096 llama_model_loader: - kv 5: llama.block_count u32 = 32 llama_model_loader: - kv 6: llama.feed_forward_length u32 = 14336 llama_model_loader: - kv 7: llama.rope.dimension_count u32 = 128 llama_model_loader: - kv 8: llama.attention.head_count u32 = 32 llama_model_loader: - kv 9: llama.attention.head_count_kv u32 = 8 llama_model_loader: - kv 10: llama.attention.layer_norm_rms_epsilon f32 = 0.000010 llama_model_loader: - kv 11: llama.rope.freq_base f32 = 500000.000000 llama_model_loader: - kv 12: general.file_type u32 = 14 llama_model_loader: - kv 13: tokenizer.ggml.model str = gpt2 llama_model_loader: - kv 14: tokenizer.ggml.tokens arr[str ,128256] = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ... llama_model_loader: - kv 15: tokenizer.ggml.scores arr[f32 ,128256] = [0.000000, 0.000000, 0.000000, 0.0000... llama_model_loader: - kv 16: tokenizer.ggml."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "] = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ... llama_model_loader: - kv 15: tokenizer.ggml.scores arr[f32 ,128256] = [0.000000, 0.000000, 0.000000, 0.0000... llama_model_loader: - kv 16: tokenizer.ggml.token_type arr[i32 ,128256] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... llama_model_loader: - kv 17: tokenizer.ggml.merges arr[str ,280147] = [\"� � \", \"� � � � \", \"� � � � \", \"... llama_model_loader: - kv 18: tokenizer.ggml.bos_token_id u32 = 128000 llama_model_loader: - kv 19: tokenizer.ggml.eos_token_id u32 = 128001 llama_model_loader: - kv 20: general.quantization_version u32 = 2 llama_model_loader: - type f32: 65 tensors llama_model_loader: - type q4_K: 217 tensors llama_model_loader: - type q5_K: 8 tensors llama_model_loader: - type q6_K: 1 tensors validate_override: Using metadata override ( str) 'tokenizer.ggml.pre' = llama3 llm_load_vocab: special tokens definition check successful ( 256/128256 ). llm_load_print_meta: format = GGUF V3 (latest) llm_load_print_meta: arch = llama llm_load_print_meta: vocab type = BPE llm_load_print_meta: n_vocab = 128256 llm_load_print_meta: n_merges = 280147 llm_load_print_meta: n_ctx_train = 8192 llm_load_print_meta: n_embd = 4096 llm_load_print_meta: n_head = 32 llm_load_print_meta: n_head_kv = 8 llm_load_print_meta: n_layer = 32 llm_load_print_meta: n_rot = 128 llm_load_print_meta: n_embd_head_k = 128 llm_load_print_meta: n_embd_head_v = 128 llm_load_print_meta: n_gqa = 4 llm_load_print_meta: n_embd_k_gqa = 1024 llm_load_print_meta: n_emb"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "t_meta: n_rot = 128 llm_load_print_meta: n_embd_head_k = 128 llm_load_print_meta: n_embd_head_v = 128 llm_load_print_meta: n_gqa = 4 llm_load_print_meta: n_embd_k_gqa = 1024 llm_load_print_meta: n_embd_v_gqa = 1024 llm_load_print_meta: f_norm_eps = 0.0e+00 llm_load_print_meta: f_norm_rms_eps = 1.0e-05 llm_load_print_meta: f_clamp_kqv = 0.0e+00 llm_load_print_meta: f_max_alibi_bias = 0.0e+00 llm_load_print_meta: f_logit_scale = 0.0e+00 llm_load_print_meta: n_ff = 14336 llm_load_print_meta: n_expert = 0 llm_load_print_meta: n_expert_used = 0 llm_load_print_meta: causal attn = 1 llm_load_print_meta: pooling type = 0 llm_load_print_meta: rope type = 0 llm_load_print_meta: rope scaling = linear llm_load_print_meta: freq_base_train = 500000.0 llm_load_print_meta: freq_scale_train = 1 llm_load_print_meta: n_yarn_orig_ctx = 8192 llm_load_print_meta: rope_finetuned = unknown llm_load_print_meta: ssm_d_conv = 0 llm_load_print_meta: ssm_d_inner = 0 llm_load_print_meta: ssm_d_state = 0 llm_load_print_meta: ssm_dt_rank = 0 llm_load_print_meta: model type = 8B llm_load_print_meta: model ftype = Q4_K - Small llm_load_print_meta: model params = 8.03 B llm_load_print_meta: model size = 4.36 GiB (4.67 BPW) llm_load_print_meta: general.name = . llm_load_print_meta: BOS token = 128000 '<|begin_of_text|>' llm_load_print_meta: EOS token = 128001 '<|end_of_text|>' llm_load_print_meta: LF token = 128 '�' llm_load_print_meta: EOT token = 128009 '<|eot_id|>' llm_load_tensors: ggml ctx size = 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "n_of_text|>' llm_load_print_meta: EOS token = 128001 '<|end_of_text|>' llm_load_print_meta: LF token = 128 '�' llm_load_print_meta: EOT token = 128009 '<|eot_id|>' llm_load_tensors: ggml ctx size = 0.15 MiB llm_load_tensors: CPU buffer size = 4467.80 MiB ................................................................................ ....... llama_new_context_with_model: n_ctx = 4096 llama_new_context_with_model: n_batch = 2048 llama_new_context_with_model: n_ubatch = 512 llama_new_context_with_model: flash_attn = 0 llama_new_context_with_model: freq_base = 500000.0 llama_new_context_with_model: freq_scale = 1 llama_kv_cache_init: CPU KV buffer size = 512.00 MiB llama_new_context_with_model: KV self size = 512.00 MiB, K (f16): 256.00 MiB, V (f16): 256.00 MiB llama_new_context_with_model: CPU output buffer size = 0.49 MiB llama_new_context_with_model: CPU compute buffer size = 296.01 MiB llama_new_context_with_model: graph nodes = 1030 llama_new_context_with_model: graph splits = 1 system_info: n_threads = 10 / 10 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | sampling: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, p resence_penalty = 0.000 top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.0 00, temp = 0.700 mirostat = 0, mirostat_lr = 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "4, repeat_penalty = 1.100, frequency_penalty = 0.000, p resence_penalty = 0.000 top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.0 00, temp = 0.700 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampling order: CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 0 <|begin_of_text|>Write 4 lines on living a purposeful life. A purposeful life i s one that has direction, meaning and personal fulfillment. ______________________________________________________________________________ ______________________________________________________________________________ ______________________________________________________________________________ ______________________________________________________________________________ One possible version: Living a purposeful life means having a strong sense of direction in life and be ing fully committed to the things we do. It�s about making choices that align wi th our values, rather than simply following the path of least resistance. To liv e a purposeful life, it is essential to be clear on what truly matters most to u s, so that we can make decisions based on those values instead of external press ures. Living purposefully means being able to recognize and articulate what our core v alues are, as well as having the courage to say no when necessary."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ased on those values instead of external press ures. Living purposefully means being able to recognize and articulate what our core v alues are, as well as having the courage to say no when necessary. It also invol ves taking responsibility for our lives and making conscious choices to grow and develop rather than just drifting along with the crowd. Finally, living a purpo seful life is about being in tune with our innermost desires and passions, so th at we can fulfill our own needs and contribute to something bigger than ourselve s. Living purposefully means having a strong sense of direction in life and being f ully committed to the things we do. It�s about making choices that align with ou r values, rather than simply following the path of least resistance. To live a p urposeful life, it is essential to be clear on what truly matters most to us, so that we can make decisions based on those values instead of external pressures. Living purposefully means being able to recognize and articulate what our core v alues are, as well as having the courage to say no when necessary. It also invol ves taking responsibility for our lives and making conscious choices to grow and develop rather than just drifting along with the crowd. Finally, living a purpo seful life is about being in tune with our innermost desires and passions, so th at we can fulfill our own needs and contribute to something bigger than ourselve s.<|end_of_text|> [end of text] llama_print_timings: load time = 796."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ne with our innermost desires and passions, so th at we can fulfill our own needs and contribute to something bigger than ourselve s.<|end_of_text|> [end of text] llama_print_timings: load time = 796.79 ms llama_print_timings: sample time = 881.02 ms / 366 runs ( 2.41 m s per token, 415.43 tokens per second) llama_print_timings: prompt eval time = 1540.87 ms / 18 tokens ( 85.60 m s per token, 11.68 tokens per second) llama_print_timings: eval time = 122569.53 ms / 365 runs ( 335.81 m s per token, 2.98 tokens per second) llama_print_timings: total time = 125436.35 ms / 383 tokens Log end (llm) ljubomir@gigul2(7468.llm:0):~/llama.cpp$ Increase -ngl to 32: ./main -ngl 32 -m models/JSL-MedLlama-3-8B-v1.0.Q4_K_S.gguf --color -c 4096 --te mp 0.7 --repeat_penalty 1.1 -n -1 --override-kv tokenizer.ggml.pre=str:llama3 -p \"Write 4 lines on living a purposeful life. A purposeful life is\"^C (llm) ljubomir@gigul2(7468.llm:0):~/llama.cpp$ ./main -ngl 32 -m models/JSL-MedL lama-3-8B-v1.0.Q4_K_S.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 --override-kv tokenizer.ggml.pre=str:llama3 -p \"Write 4 lines on living a purpo seful life. A purposeful life is\" warning: not compiled with GPU offload support, --n-gpu-layers option will be ig nored warning: see main README.md for information on enabling GPU BLAS support Log start main: build = 2905 (0264a4cc) main: built with gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "rs option will be ig nored warning: see main README.md for information on enabling GPU BLAS support Log start main: build = 2905 (0264a4cc) main: built with gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-g nu main: seed = 1715529996 llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors fro m models/JSL-MedLlama-3-8B-v1.0.Q4_K_S.gguf (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not appl y in this output. llama_model_loader: - kv 0: general.architecture str = llama llama_model_loader: - kv 1: general.name str = . llama_model_loader: - kv 2: llama.vocab_size u32 = 128256 llama_model_loader: - kv 3: llama.context_length u32 = 8192 llama_model_loader: - kv 4: llama.embedding_length u32 = 4096 llama_model_loader: - kv 5: llama.block_count u32 = 32 llama_model_loader: - kv 6: llama.feed_forward_length u32 = 14336 llama_model_loader: - kv 7: llama.rope.dimension_count u32 = 128 llama_model_loader: - kv 8: llama.attention.head_count u32 = 32 llama_model_loader: - kv 9: llama.attention.head_count_kv u32 = 8 llama_model_loader: - kv 10: llama.attention.layer_norm_rms_epsilon f32 = 0.000010 llama_model_loader: - kv 11: llama.rope.freq_base f32 = 500000.000000 llama_model_loader: - kv 12: general.file_type u32 = 14 llama_model_loader: - kv 13: tokenizer.ggml.model str = gpt2 llama_model_loader: - kv 14: tokenizer.ggml.tokens arr[str ,128256] = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ..."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": ": general.file_type u32 = 14 llama_model_loader: - kv 13: tokenizer.ggml.model str = gpt2 llama_model_loader: - kv 14: tokenizer.ggml.tokens arr[str ,128256] = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ... llama_model_loader: - kv 15: tokenizer.ggml.scores arr[f32 ,128256] = [0.000000, 0.000000, 0.000000, 0.0000... llama_model_loader: - kv 16: tokenizer.ggml.token_type arr[i32 ,128256] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... llama_model_loader: - kv 17: tokenizer.ggml.merges arr[str ,280147] = [\"� � \", \"� � � � \", \"� � � � \", \"... llama_model_loader: - kv 18: tokenizer.ggml.bos_token_id u32 = 128000 llama_model_loader: - kv 19: tokenizer.ggml.eos_token_id u32 = 128001 llama_model_loader: - kv 20: general.quantization_version u32 = 2 llama_model_loader: - type f32: 65 tensors llama_model_loader: - type q4_K: 217 tensors llama_model_loader: - type q5_K: 8 tensors llama_model_loader: - type q6_K: 1 tensors validate_override: Using metadata override ( str) 'tokenizer.ggml.pre' = llama3 llm_load_vocab: special tokens definition check successful ( 256/128256 ). llm_load_print_meta: format = GGUF V3 (latest) llm_load_print_meta: arch = llama llm_load_print_meta: vocab type = BPE llm_load_print_meta: n_vocab = 128256 llm_load_print_meta: n_merges = 280147 llm_load_print_meta: n_ctx_train = 8192 llm_load_print_meta: n_embd = 4096 llm_load_print_meta: n_head = 32 llm_load_print_meta: n_head_kv = 8 llm_load_print_meta: n_layer = 32 llm_load_print_meta: n_rot = 128 llm_load_print_meta: n_e"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "8192 llm_load_print_meta: n_embd = 4096 llm_load_print_meta: n_head = 32 llm_load_print_meta: n_head_kv = 8 llm_load_print_meta: n_layer = 32 llm_load_print_meta: n_rot = 128 llm_load_print_meta: n_embd_head_k = 128 llm_load_print_meta: n_embd_head_v = 128 llm_load_print_meta: n_gqa = 4 llm_load_print_meta: n_embd_k_gqa = 1024 llm_load_print_meta: n_embd_v_gqa = 1024 llm_load_print_meta: f_norm_eps = 0.0e+00 llm_load_print_meta: f_norm_rms_eps = 1.0e-05 llm_load_print_meta: f_clamp_kqv = 0.0e+00 llm_load_print_meta: f_max_alibi_bias = 0.0e+00 llm_load_print_meta: f_logit_scale = 0.0e+00 llm_load_print_meta: n_ff = 14336 llm_load_print_meta: n_expert = 0 llm_load_print_meta: n_expert_used = 0 llm_load_print_meta: causal attn = 1 llm_load_print_meta: pooling type = 0 llm_load_print_meta: rope type = 0 llm_load_print_meta: rope scaling = linear llm_load_print_meta: freq_base_train = 500000.0 llm_load_print_meta: freq_scale_train = 1 llm_load_print_meta: n_yarn_orig_ctx = 8192 llm_load_print_meta: rope_finetuned = unknown llm_load_print_meta: ssm_d_conv = 0 llm_load_print_meta: ssm_d_inner = 0 llm_load_print_meta: ssm_d_state = 0 llm_load_print_meta: ssm_dt_rank = 0 llm_load_print_meta: model type = 8B llm_load_print_meta: model ftype = Q4_K - Small llm_load_print_meta: model params = 8.03 B llm_load_print_meta: model size = 4.36 GiB (4.67 BPW) llm_load_print_meta: general.name = ."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": ": model type = 8B llm_load_print_meta: model ftype = Q4_K - Small llm_load_print_meta: model params = 8.03 B llm_load_print_meta: model size = 4.36 GiB (4.67 BPW) llm_load_print_meta: general.name = . llm_load_print_meta: BOS token = 128000 '<|begin_of_text|>' llm_load_print_meta: EOS token = 128001 '<|end_of_text|>' llm_load_print_meta: LF token = 128 '�' llm_load_print_meta: EOT token = 128009 '<|eot_id|>' llm_load_tensors: ggml ctx size = 0.15 MiB llm_load_tensors: CPU buffer size = 4467.80 MiB ................................................................................ ....... llama_new_context_with_model: n_ctx = 4096 llama_new_context_with_model: n_batch = 2048 llama_new_context_with_model: n_ubatch = 512 llama_new_context_with_model: flash_attn = 0 llama_new_context_with_model: freq_base = 500000.0 llama_new_context_with_model: freq_scale = 1 llama_kv_cache_init: CPU KV buffer size = 512.00 MiB llama_new_context_with_model: KV self size = 512.00 MiB, K (f16): 256.00 MiB, V (f16): 256.00 MiB llama_new_context_with_model: CPU output buffer size = 0.49 MiB llama_new_context_with_model: CPU compute buffer size = 296.01 MiB llama_new_context_with_model: graph nodes = 1030 llama_new_context_with_model: graph splits = 1 system_info: n_threads = 10 / 10 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | sampling: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, p resence_penalty = 0.000 top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.0 00, temp = 0.700 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampling order: CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 0 <|begin_of_text|>Write 4 lines on living a purposeful life. A purposeful life i s one in which we pursue our goals and aspirations with intention, clarity, and perseverance. Living a purposeful life means being intentional about what you want to achieve in your life. It involves setting clear goals and priorities, identifying the st eps required to reach those goals, and consistently working towards them. It als o means staying focused on your values and beliefs, and using those as guidance for decision-making. Here are some tips for living a purposeful life: 1. Set clear goals and objectives: Write down what you want to achieve in each a rea of your life (such as career, relationships, health, etc.) and break them do wn into smaller, achievable steps. 2."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "1. Set clear goals and objectives: Write down what you want to achieve in each a rea of your life (such as career, relationships, health, etc.) and break them do wn into smaller, achievable steps. 2. Create a plan of action: Once you have identified your goals, create a plan o f action that outlines the specific tasks and deadlines required to reach those goals. 3. Prioritize your time and resources: Allocate your time and energy towards act ivities that align with your goals and values. Learn to say no to unnecessary co mmitments and distractions. 4. Reflect and adjust course: Regularly reflect on your progress and adjust your plan of action as needed. Don't be afraid to pivot or change direction if somet hing isn't working for you. Living a purposeful life can be challenging, but it is worth it! By staying focu sed on what truly matters to you, you can create a life that is fulfilling and r ewarding.<|end_of_text|> [end of text] llama_print_timings: load time = 845.97 ms llama_print_timings: sample time = 635.65 ms / 274 runs ( 2.32 m s per token, 431.05 tokens per second) llama_print_timings: prompt eval time = 2221.61 ms / 18 tokens ( 123.42 m s per token, 8.10 tokens per second) llama_print_timings: eval time = 77831.11 ms / 273 runs ( 285.10 m s per token, 3.51 tokens per second) llama_print_timings: total time = 81032.41 ms / 291 tokens Log end (llm) ljubomir@gigul2(7468.llm:0):~/llama.cpp$ Increase -ngl to 64: ./main -ngl 64 -m models/JSL-MedLlama-3-8B-v1.0.Q4_K_S."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "r second) llama_print_timings: total time = 81032.41 ms / 291 tokens Log end (llm) ljubomir@gigul2(7468.llm:0):~/llama.cpp$ Increase -ngl to 64: ./main -ngl 64 -m models/JSL-MedLlama-3-8B-v1.0.Q4_K_S.gguf --color -c 4096 --te mp 0.7 --repeat_penalty 1.1 -n -1 --override-kv tokenizer.ggml.pre=str:llama3 -p \"Write 4 lines on living a purposeful life. A purposeful life is\" (llm) ljubomir@gigul2(7468.llm:0):~/llama.cpp$ ./main -ngl 64 -m models/JSL-MedL lama-3-8B-v1.0.Q4_K_S.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 --override-kv tokenizer.ggml.pre=str:llama3 -p \"Write 4 lines on living a purpo seful life. A purposeful life is\" warning: not compiled with GPU offload support, --n-gpu-layers option will be ig nored warning: see main README.md for information on enabling GPU BLAS support Log start main: build = 2905 (0264a4cc) main: built with gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-g nu main: seed = 1715530153 llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors fro m models/JSL-MedLlama-3-8B-v1.0.Q4_K_S.gguf (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not appl y in this output. llama_model_loader: - kv 0: general.architecture str = llama llama_model_loader: - kv 1: general.name str = . llama_model_loader: - kv 2: llama.vocab_size u32 = 128256 llama_model_loader: - kv 3: llama.context_length u32 = 8192 llama_model_loader: - kv 4: llama."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "lama_model_loader: - kv 1: general.name str = . llama_model_loader: - kv 2: llama.vocab_size u32 = 128256 llama_model_loader: - kv 3: llama.context_length u32 = 8192 llama_model_loader: - kv 4: llama.embedding_length u32 = 4096 llama_model_loader: - kv 5: llama.block_count u32 = 32 llama_model_loader: - kv 6: llama.feed_forward_length u32 = 14336 llama_model_loader: - kv 7: llama.rope.dimension_count u32 = 128 llama_model_loader: - kv 8: llama.attention.head_count u32 = 32 llama_model_loader: - kv 9: llama.attention.head_count_kv u32 = 8 llama_model_loader: - kv 10: llama.attention.layer_norm_rms_epsilon f32 = 0.000010 llama_model_loader: - kv 11: llama.rope.freq_base f32 = 500000.000000 llama_model_loader: - kv 12: general.file_type u32 = 14 llama_model_loader: - kv 13: tokenizer.ggml.model str = gpt2 llama_model_loader: - kv 14: tokenizer.ggml.tokens arr[str ,128256] = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ... llama_model_loader: - kv 15: tokenizer.ggml.scores arr[f32 ,128256] = [0.000000, 0.000000, 0.000000, 0.0000... llama_model_loader: - kv 16: tokenizer.ggml.token_type arr[i32 ,128256] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... llama_model_loader: - kv 17: tokenizer.ggml.merges arr[str ,280147] = [\"� � \", \"� � � � \", \"� � � � \", \"... llama_model_loader: - kv 18: tokenizer.ggml.bos_token_id u32 = 128000 llama_model_loader: - kv 19: tokenizer.ggml.eos_token_id u32 = 128001 llama_model_loader: - kv 20: general."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "� \", \"� � � � \", \"... llama_model_loader: - kv 18: tokenizer.ggml.bos_token_id u32 = 128000 llama_model_loader: - kv 19: tokenizer.ggml.eos_token_id u32 = 128001 llama_model_loader: - kv 20: general.quantization_version u32 = 2 llama_model_loader: - type f32: 65 tensors llama_model_loader: - type q4_K: 217 tensors llama_model_loader: - type q5_K: 8 tensors llama_model_loader: - type q6_K: 1 tensors validate_override: Using metadata override ( str) 'tokenizer.ggml.pre' = llama3 llm_load_vocab: special tokens definition check successful ( 256/128256 ). llm_load_print_meta: format = GGUF V3 (latest) llm_load_print_meta: arch = llama llm_load_print_meta: vocab type = BPE llm_load_print_meta: n_vocab = 128256 llm_load_print_meta: n_merges = 280147 llm_load_print_meta: n_ctx_train = 8192 llm_load_print_meta: n_embd = 4096 llm_load_print_meta: n_head = 32 llm_load_print_meta: n_head_kv = 8 llm_load_print_meta: n_layer = 32 llm_load_print_meta: n_rot = 128 llm_load_print_meta: n_embd_head_k = 128 llm_load_print_meta: n_embd_head_v = 128 llm_load_print_meta: n_gqa = 4 llm_load_print_meta: n_embd_k_gqa = 1024 llm_load_print_meta: n_embd_v_gqa = 1024 llm_load_print_meta: f_norm_eps = 0.0e+00 llm_load_print_meta: f_norm_rms_eps = 1.0e-05 llm_load_print_meta: f_clamp_kqv = 0.0e+00 llm_load_print_meta: f_max_alibi_bias = 0.0e+00 llm_load_print_meta: f_logit_scale = 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "a: f_norm_eps = 0.0e+00 llm_load_print_meta: f_norm_rms_eps = 1.0e-05 llm_load_print_meta: f_clamp_kqv = 0.0e+00 llm_load_print_meta: f_max_alibi_bias = 0.0e+00 llm_load_print_meta: f_logit_scale = 0.0e+00 llm_load_print_meta: n_ff = 14336 llm_load_print_meta: n_expert = 0 llm_load_print_meta: n_expert_used = 0 llm_load_print_meta: causal attn = 1 llm_load_print_meta: pooling type = 0 llm_load_print_meta: rope type = 0 llm_load_print_meta: rope scaling = linear llm_load_print_meta: freq_base_train = 500000.0 llm_load_print_meta: freq_scale_train = 1 llm_load_print_meta: n_yarn_orig_ctx = 8192 llm_load_print_meta: rope_finetuned = unknown llm_load_print_meta: ssm_d_conv = 0 llm_load_print_meta: ssm_d_inner = 0 llm_load_print_meta: ssm_d_state = 0 llm_load_print_meta: ssm_dt_rank = 0 llm_load_print_meta: model type = 8B llm_load_print_meta: model ftype = Q4_K - Small llm_load_print_meta: model params = 8.03 B llm_load_print_meta: model size = 4.36 GiB (4.67 BPW) llm_load_print_meta: general.name = . llm_load_print_meta: BOS token = 128000 '<|begin_of_text|>' llm_load_print_meta: EOS token = 128001 '<|end_of_text|>' llm_load_print_meta: LF token = 128 '�' llm_load_print_meta: EOT token = 128009 '<|eot_id|>' llm_load_tensors: ggml ctx size = 0.15 MiB llm_load_tensors: CPU buffer size = 4467.80 MiB ................................................................................ ......."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "128009 '<|eot_id|>' llm_load_tensors: ggml ctx size = 0.15 MiB llm_load_tensors: CPU buffer size = 4467.80 MiB ................................................................................ ....... llama_new_context_with_model: n_ctx = 4096 llama_new_context_with_model: n_batch = 2048 llama_new_context_with_model: n_ubatch = 512 llama_new_context_with_model: flash_attn = 0 llama_new_context_with_model: freq_base = 500000.0 llama_new_context_with_model: freq_scale = 1 llama_kv_cache_init: CPU KV buffer size = 512.00 MiB llama_new_context_with_model: KV self size = 512.00 MiB, K (f16): 256.00 MiB, V (f16): 256.00 MiB llama_new_context_with_model: CPU output buffer size = 0.49 MiB llama_new_context_with_model: CPU compute buffer size = 296.01 MiB llama_new_context_with_model: graph nodes = 1030 llama_new_context_with_model: graph splits = 1 system_info: n_threads = 10 / 10 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | sampling: repeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, p resence_penalty = 0.000 top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.0 00, temp = 0.700 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "100, frequency_penalty = 0.000, p resence_penalty = 0.000 top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.0 00, temp = 0.700 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampling order: CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 0 <|begin_of_text|>Write 4 lines on living a purposeful life. A purposeful life i s one that has meaning and direction, rather than just going through the motions of everyday life. Living a purposeful life means having a clear sense of your goals and values, an d then making choices and taking actions that align with those goals and values. Here are four lines on living a purposeful life: 1. Live with intention: Be mindful of your actions and choose to act in ways tha t align with your highest ideals and aspirations. 2. Focus on your passions: Follow your heart and pursue the things that bring yo u joy, fulfillment, and a sense of purpose. 3. Be present in the moment: Embrace each moment as an opportunity to grow, lear n, and deepen your connection to what truly matters. 4. Make a difference: Use your skills, talents, and resources to contribute to s omething greater than yourself, whether it's through volunteer work, activism, o r simply being a kind and compassionate person.<|end_of_text|> [end of text] llama_print_timings: load time = 823.32 ms llama_print_timings: sample time = 416.58 ms / 183 runs ( 2."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "r work, activism, o r simply being a kind and compassionate person.<|end_of_text|> [end of text] llama_print_timings: load time = 823.32 ms llama_print_timings: sample time = 416.58 ms / 183 runs ( 2.28 m s per token, 439.29 tokens per second) llama_print_timings: prompt eval time = 1560.65 ms / 18 tokens ( 86.70 m s per token, 11.53 tokens per second) llama_print_timings: eval time = 58714.68 ms / 182 runs ( 322.61 m s per token, 3.10 tokens per second) llama_print_timings: total time = 60912.48 ms / 200 tokens Log end (llm) ljubomir@gigul2(7468.llm:0):~/llama.cpp$ LJ Sun 12 May 17:11:35 BST 2024 + Llama 3.1a Llama 3.1a repo https://huggingface.co/reach-vb/Meta-Llama-3.1-8B-Instruct-Q6_K-GGUF/tree/main file https://huggingface.co/reach-vb/Meta-Llama-3.1-8B-Instruct-Q6_K-GGUF/blob/main/m eta-llama-3.1-8b-instruct-q6_k.gguf DL huggingface-cli download reach-vb/Meta-Llama-3.1-8B-Instruct-Q6_K-GGUF meta-llam a-3.1-8b-instruct-q6_k.gguf --local-dir models/ --local-dir-use-symlinks False On the cmd line cd models wget https://huggingface.co/reach-vb/Meta-Llama-3.1-8B-Instruct-Q6_K-GGUF/blob/m ain/meta-llama-3.1-8b-instruct-q6_k.gguf ... but does not work - returns .js - DL it manually from browser, then ljubomir@gigul2(754506.gpu.cpp:0):~/llama.cpp$ mviv ~/Downloads/meta-llama-3.1-8 b-instruct-q6_k.gguf models/ renamed '/home/ljubomir/Downloads/meta-llama-3.1-8b-instruct-q6_k.gguf' -> 'mode ls/meta-llama-3.1-8b-instruct-q6_k."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": ".cpp:0):~/llama.cpp$ mviv ~/Downloads/meta-llama-3.1-8 b-instruct-q6_k.gguf models/ renamed '/home/ljubomir/Downloads/meta-llama-3.1-8b-instruct-q6_k.gguf' -> 'mode ls/meta-llama-3.1-8b-instruct-q6_k.gguf' Update and rebuild git pull origin master make clean GGLM_LLAMA_METAL=1 make -j ./llama-cli -ngl 64 -m models/meta-llama-3.1-8b-instruct-q6_k.gguf -p \"Write 4 l ines on living a purposeful life. A purposeful life is\" LJ Tue 30 Jul 12:11:01 BST 2024 + ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ git tag -m 'LJ save before LLama 3.2 trying Llama-3.2-1B-Instruct-GGUF' tag_20241016_LJ_before_Llama_3.2_merge_rc1 ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ git tag -m 'LJ save before LLam a 3.2 trying Llama-3.2-1B-Instruct-GGUF' tag_20241016_LJ_before_Llama_3.2_merge_ rc1 ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ git pull origin master From https://huggingface.co/docs/hub/en/ollama Getting started is as simple as: ollama run hf.co/{username}/{repository} Please note that you can use both hf.co and huggingface.co as the domain name. Here are some models you can try: ollama run hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF ollama run hf.co/mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF ollama run hf.co/arcee-ai/SuperNova-Medius-GGUF ollama run hf.co/bartowski/Humanish-LLama3-8B-Instruct-GGUF Custom Quantization By default, the Q4_K_M quantization scheme is used, when it�s present inside the model repo."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ee-ai/SuperNova-Medius-GGUF ollama run hf.co/bartowski/Humanish-LLama3-8B-Instruct-GGUF Custom Quantization By default, the Q4_K_M quantization scheme is used, when it�s present inside the model repo. If not, we default to picking one reasonable quant type present ins ide the repo. To select a different scheme, simply add a tag: ollama run hf.co/{username}/{repository}:{quantization} Manually DL https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/blob/main/Llama-3.2- 1B-Instruct-Q4_K_M.gguf ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ mviv ~/Downloads/Llama-3.2-1B-I nstruct-Q4_K_M.gguf models/ renamed '/home/ljubomir/Downloads/Llama-3.2-1B-Instruct-Q4_K_M.gguf' -> 'models/ Llama-3.2-1B-Instruct-Q4_K_M.gguf' Update the source ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ git pull origin master remote: Enumerating objects: 346, done. remote: Counting objects: 100% (293/293), done. remote: Compressing objects: 100% (231/231), done. remote: Total 346 (delta 167), reused 85 (delta 60), pack-reused 53 (from 1) Receiving objects: 100% (346/346), 897.29 KiB | 2.27 MiB/s, done. Resolving deltas: 100% (172/172), completed with 12 local objects. From https://github.com/ggerganov/llama.cpp * branch master -> FETCH_HEAD 63747437..10433e8b master -> origin/master hint: Waiting for your editor to close the file... Merge branch 'master' of https://github.com/ggerganov/llama.cpp Rebuild ljubomir@gigul2(797966.llama.cpp:0):~/llama."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "10433e8b master -> origin/master hint: Waiting for your editor to close the file... Merge branch 'master' of https://github.com/ggerganov/llama.cpp Rebuild ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ make -j Check what was built ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ ls -lt |m total 1514200 -rw------- 1 ljubomir ljubomir 125109 Oct 16 17:22 README.LJ -rwx------ 1 ljubomir ljubomir 55690968 Oct 16 17:21 llama-server -rwx------ 1 ljubomir ljubomir 46279136 Oct 16 17:21 llama-minicpmv-cli -rwx------ 1 ljubomir ljubomir 46362208 Oct 16 17:21 llama-llava-cli -rwx------ 1 ljubomir ljubomir 45799080 Oct 16 17:21 llama-bench -rwx------ 1 ljubomir ljubomir 42533184 Oct 16 17:21 llama-lookahead -rwx------ 1 ljubomir ljubomir 42185776 Oct 16 17:21 llama-baby-llama -rwx------ 1 ljubomir ljubomir 42793880 Oct 16 17:21 llama-speculative -rwx------ 1 ljubomir ljubomir 42483368 Oct 16 17:21 llama-embedding -rwx------ 1 ljubomir ljubomir 42441816 Oct 16 17:21 llama-lookup-stats -rwx------ 1 ljubomir ljubomir 42386168 Oct 16 17:21 llama-lookup-create -rwx------ 1 ljubomir ljubomir 42466480 Oct 16 17:21 llama-gen-docs -rwx------ 1 ljubomir ljubomir 42527656 Oct 16 17:21 llama-parallel -rwx------ 1 ljubomir ljubomir 42216320 Oct 16 17:21 llama-tokenize -rwx------ 1 ljubomir ljubomir 42414000 Oct 16 17:21 llama-save-load-state -rwx------ 1 ljubomir ljubomir 42383904 Oct 16 17:21 llama-batched-bench -rwx------ 1 ljubomir ljubomir 42269616 Oct 16 17:21 llama-gbnf-validator -rwx------"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "2414000 Oct 16 17:21 llama-save-load-state -rwx------ 1 ljubomir ljubomir 42383904 Oct 16 17:21 llama-batched-bench -rwx------ 1 ljubomir ljubomir 42269616 Oct 16 17:21 llama-gbnf-validator -rwx------ 1 ljubomir ljubomir 42767432 Oct 16 17:21 llama-cli -rwx------ 1 ljubomir ljubomir 42297808 Oct 16 17:21 llama-gguf-split -rwx------ 1 ljubomir ljubomir 42862984 Oct 16 17:21 llama-cvector-generator -rwx------ 1 ljubomir ljubomir 42673664 Oct 16 17:21 llama-gguf-hash -rwx------ 1 ljubomir ljubomir 43530192 Oct 16 17:21 llama-perplexity -rwx------ 1 ljubomir ljubomir 42901472 Oct 16 17:21 llama-export-lora -rwx------ 1 ljubomir ljubomir 43259464 Oct 16 17:21 llama-imatrix -rwx------ 1 ljubomir ljubomir 42162952 Oct 16 17:21 llama-lookup-merge -rwx------ 1 ljubomir ljubomir 42573232 Oct 16 17:21 llama-quantize -rwx------ 1 ljubomir ljubomir 42391576 Oct 16 17:21 llama-eval-callback -rwx------ 1 ljubomir ljubomir 42384096 Oct 16 17:21 llama-batched -rwx------ 1 ljubomir ljubomir 42511736 Oct 16 17:21 llama-gritlm -rwx------ 1 ljubomir ljubomir 42570720 Oct 16 17:21 llama-infill -rwx------ 1 ljubomir ljubomir 43807728 Oct 16 17:21 llama-quantize-stats -rwx------ 1 ljubomir ljubomir 42771952 Oct 16 17:21 llama-retrieval -rwx------ 1 ljubomir ljubomir 42546224 Oct 16 17:21 llama-convert-llama2c-to-g gml -rwx------ 1 ljubomir ljubomir 42465768 Oct 16 17:21 llama-lookup -rwx------ 1 ljubomir ljubomir 42422224 Oct 16 17:21 llama-passkey -rwx------ 1 ljubomir ljubomir 42170672 Oct 16 17:2"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "lama2c-to-g gml -rwx------ 1 ljubomir ljubomir 42465768 Oct 16 17:21 llama-lookup -rwx------ 1 ljubomir ljubomir 42422224 Oct 16 17:21 llama-passkey -rwx------ 1 ljubomir ljubomir 42170672 Oct 16 17:21 llama-simple -rw------- 1 ljubomir ljubomir 300136 Oct 16 17:20 libllava.a drwx------ 2 ljubomir ljubomir 4096 Oct 16 17:20 src/ Previous last command line ./llama-cli -ngl 64 -m models/Llama-3.2-1B-Instruct-Q4_K_M.gguf --color -c 4096 --temp 0.7 --repeat_penalty 1.1 -n -1 --override-kv tokenizer.ggml.pre=str:llama 3 -p \"Write 4 lines on living a purposeful life. A purposeful life is\" Run ./llama-cli -ngl 64 -m models/Llama-3.2-1B-Instruct-Q4_K_M.gguf -p \"Write 4 line s on living a purposeful life. A purposeful life is\" ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ ./llama-cli -ngl 64 -m models/L lama-3.2-1B-Instruct-Q4_K_M.gguf -p \"Write 4 lines on living a purposeful life. A purposeful life is\" warning: not compiled with GPU offload support, --gpu-layers option will be igno red warning: see main README.md for information on enabling GPU BLAS support build: 3980 (56a6f5ab) with gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86 _64-linux-gnu main: llama backend init main: load the model and apply lora adapter, if any llama_model_loader: loaded meta data with 35 key-value pairs and 147 tensors fro m models/Llama-3.2-1B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not appl y in this output."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "value pairs and 147 tensors fro m models/Llama-3.2-1B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not appl y in this output. llama_model_loader: - kv 0: general.architecture str = llama llama_model_loader: - kv 1: general.type str = model llama_model_loader: - kv 2: general.name str = Llama 3.2 1B Instruct llama_model_loader: - kv 3: general.finetune str = Instruct llama_model_loader: - kv 4: general.basename str = Llama-3.2 llama_model_loader: - kv 5: general.size_label str = 1B llama_model_loader: - kv 6: general.license str = llama3.2 llama_model_loader: - kv 7: general.tags arr[str ,6] = [\"facebook\", \"meta\", \"pytorch\", \"llam... llama_model_loader: - kv 8: general.languages arr[str ,8] = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ... ................................................................................ ................................................................... ................................................................................ ................................................................... ................................................................................ ................................................................... llama_new_context_with_model: n_ctx = 131072 llama_new_context_with_model: n_batch = 2048 llama_new_context_with_model: n_ubatch = 512 llama_new_context_with_model: flash_attn = 0 llama_new_context_with_model: freq_base = 500000."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "n_ctx = 131072 llama_new_context_with_model: n_batch = 2048 llama_new_context_with_model: n_ubatch = 512 llama_new_context_with_model: flash_attn = 0 llama_new_context_with_model: freq_base = 500000.0 llama_new_context_with_model: freq_scale = 1 llama_kv_cache_init: CPU KV buffer size = 4096.00 MiB llama_new_context_with_model: KV self size = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB llama_new_context_with_model: CPU output buffer size = 0.49 MiB llama_new_context_with_model: CPU compute buffer size = 8464.01 MiB llama_new_context_with_model: graph nodes = 518 llama_new_context_with_model: graph splits = 1 common_init_from_params: warming up the model with an empty run - please wait .. . (--no-warmup to disable) main: llama threadpool init, n_threads = 10 system_info: n_threads = 10 (n_threads_batch = 10) / 10 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VEC T = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | sampler seed: 1650544230 sampler params: repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, p resence_penalty = 0.000 top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ty = 0.000 top_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampler chain: logits -> logit-bias -> penalties -> top-k -> tail-free -> typica l -> top-p -> min-p -> xtc -> temp-ext -> softmax -> dist generate: n_ctx = 131072, n_batch = 2048, n_predict = -1, n_keep = 1 Write 4 lines on living a purposeful life. A purposeful life is one that is driv en by a clear goal or mission. It is a life where one's actions and decisions ar e guided by a sense of direction and purpose. Here is my attempt at a short poem: A purposeful life is one that's bright Driven by a goal, a guiding light It's a life of purpose, of direction true Where actions align with a mission anew I hope this meets your requirements. Let me know if I can make any adjustments! [end of text] llama_perf_sampler_print: sampling time = 9.76 ms / 113 runs ( 0 .09 ms per token, 11580.24 tokens per second) llama_perf_context_print: load time = 6323.37 ms llama_perf_context_print: prompt eval time = 204.47 ms / 17 tokens ( 12 .03 ms per token, 83.14 tokens per second) llama_perf_context_print: eval time = 4429.88 ms / 95 runs ( 46 .63 ms per token, 21.45 tokens per second) llama_perf_context_print: total time = 4675.72 ms / 112 tokens Check the 8B model for speed https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/blob/main/Meta- Llama-3.1-8B-Instruct-Q6_K."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "llama_perf_context_print: total time = 4675.72 ms / 112 tokens Check the 8B model for speed https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/blob/main/Meta- Llama-3.1-8B-Instruct-Q6_K.gguf Git LFS file https://cdn-lfs-us-1.hf.co/repos/d6/e9/d6e9318f285870e2a0e3056e22f9c7ec90cd13e14 cfde122129ae66af9ad788f/33981adf6bae52c503fb5c24f72539010632f7ed290a56c1315a8cd5 0adca587?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llam a-3.1-8B-Instruct-Q6_K.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q6_K.ggu f%22%3B&Expires=1729358516&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTG Vzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyOTM1ODUxNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2 RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2Q2L2U5L2Q2ZTkzMThmMjg1ODcwZTJhMGUzMDU2ZTIyZjljN2 VjOTBjZDEzZTE0Y2ZkZTEyMjEyOWFlNjZhZjlhZDc4OGYvMzM5ODFhZGY2YmFlNTJjNTAzZmI1YzI0Zj cyNTM5MDEwNjMyZjdlZDI5MGE1NmMxMzE1YThjZDUwYWRjYTU4Nz9yZXNwb25zZS1jb250ZW50LWRpc3 Bvc2l0aW9uPSoifV19&Signature=PUgz9eIgDyVEuQlUfafs2MNW9v8nevU9H%7E5eG12sfbe9AlaCI 3lQdwx2otxTLyhgL-tj8FFOYn%7ErMLQmnzHytgqZAHLNs-Gr%7ELCrQXd0HCwsxHnfIxVTSbEn-qyMQ Ut-k%7Evph05IweRb5NlCFGL8I9Kli8jUR%7EIKJEJkI6pSBMZipPCzT9Kgczw2gFUldFdnQlDrqqkpH JWdpxTXAUN9SixlpMbgIVqfGcYwdktieskuzajzteDa6kS-bzjvFgvlfDqettnKcFvcyCeTiveUfZOX- YCov6Ye2kwRNbYYT%7EKuqbSfn9VWNWJdho62oT1cjNXuqyfXOpIahNi-tok3jg__&Key-Pair-Id=K2 4J24Z295AEI9 Move to the right place ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ mviv ~/Downloads/Meta-Llama-3.1 -8B-Instruct-Q6_K."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "7EKuqbSfn9VWNWJdho62oT1cjNXuqyfXOpIahNi-tok3jg__&Key-Pair-Id=K2 4J24Z295AEI9 Move to the right place ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ mviv ~/Downloads/Meta-Llama-3.1 -8B-Instruct-Q6_K.gguf models/ renamed '/home/ljubomir/Downloads/Meta-Llama-3.1-8B-Instruct-Q6_K.gguf' -> 'mode ls/Meta-Llama-3.1-8B-Instruct-Q6_K.gguf' Test ./llama-cli -ngl 64 -m models/Meta-Llama-3.1-8B-Instruct-Q6_K.gguf -p \"Write 4 l ines on living a purposeful life. A purposeful life is\" Run ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ ./llama-cli -ngl 64 -m models/M eta-Llama-3.1-8B-Instruct-Q6_K.gguf -p \"Write 4 lines on living a purposeful lif e. A purposeful life is\" warning: not compiled with GPU offload support, --gpu-layers option will be igno red warning: see main README.md for information on enabling GPU BLAS support build: 3980 (56a6f5ab) with gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86 _64-linux-gnu main: llama backend init main: load the model and apply lora adapter, if any llama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors fro m models/Meta-Llama-3.1-8B-Instruct-Q6_K.gguf (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not appl y in this output. llama_model_loader: - kv 0: general.architecture str = llama llama_model_loader: - kv 1: general.type str = model ................................................................................ ......................................................"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "str = llama llama_model_loader: - kv 1: general.type str = model ................................................................................ ................................................................... ................................................................................ ................................................................... ................................................................................ ................................................................... Living a purposeful life is not just about achieving success, but also about liv ing a life of purpose and meaning. It's about using your talents, skills, and pa ssions to make a positive impact. By doing so, you'll feel more fulfilled, motiv ated, and inspired. You'll also be more likely to achieve your goals and leave a lasting legacy. 45 Living llama_perf_sampler_print: sampling time = 344.64 ms / 3293 runs ( 0 .10 ms per token, 9554.90 tokens per second) llama_perf_context_print: load time = 53584.57 ms llama_perf_context_print: prompt eval time = 1217.02 ms / 17 tokens ( 71 .59 ms per token, 13.97 tokens per second) llama_perf_context_print: eval time = 895703.87 ms / 3275 runs ( 273 .50 ms per token, 3.66 tokens per second) llama_perf_context_print: total time = 898266.46 ms / 3292 tokens Interrupted by user iNB rebuild with GPU support DOES NOT WORK, there is no nvcc on gigul2: make clean GGML_CUDA=1 make -j ljubomir@gigul2(797966.llama.cpp:0):~/llama."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ime = 898266.46 ms / 3292 tokens Interrupted by user iNB rebuild with GPU support DOES NOT WORK, there is no nvcc on gigul2: make clean GGML_CUDA=1 make -j ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ make clean ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ GGML_CUDA=1 make -j I ccache not found. Consider installing it for faster compilation. expr: syntax error: unexpected argument �070100� expr: syntax error: unexpected argument �080100� I llama.cpp build info: I UNAME_S: Linux I UNAME_P: x86_64 I UNAME_M: x86_64 I CFLAGS: -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE= 600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUD A -DGGML_CUDA_USE_GRAPHS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86 _64-linux/include -std=c11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prot otypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -marc h=native -mtune=native -fopenmp -Wdouble-promotion I CXXFLAGS: -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-u nused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp -mar ch=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I ggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOUR CE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_U SE_GRAPHS -I/usr/local/cuda/include -I/u"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOUR CE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -DGGML_CUDA_U SE_GRAPHS -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/inclu de I NVCCFLAGS: -std=c++11 -O3 -g -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION =2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 I LDFLAGS: -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt - L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib - L/usr/local/cuda/lib64/stubs -L/usr/lib/wsl/lib I CC: gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 I CXX: g++-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 /bin/sh: 1: nvcc: not found I NVCC: /bin/sh: 1: nvcc: not found Makefile:1001: *** I ERROR: For CUDA versions < 11.7 a target CUDA architecture must be explicitly provided via environment variable CUDA_DOCKER_ARCH, e.g. by r unning \"export CUDA_DOCKER_ARCH=compute_XX\" on Unix-like systems, where XX is th e minimum compute capability that the code needs to run on. A list with compute capabilities can be found here: https://developer.nvidia.com/cuda-gpus . Stop. ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ GGLM_LLAMA_METAL=1 make -j I ccache not found. Consider installing it for faster compilation. I llama."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": ": https://developer.nvidia.com/cuda-gpus . Stop. ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ GGLM_LLAMA_METAL=1 make -j I ccache not found. Consider installing it for faster compilation. I llama.cpp build info: I UNAME_S: Linux I UNAME_P: x86_64 I UNAME_M: x86_64 I CFLAGS: -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE= 600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -std=c11 -f PIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -W strict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Wer ror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion I CXXFLAGS: -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-u nused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp -mar ch=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I ggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOUR CE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE I NVCCFLAGS: -std=c++11 -O3 -g I LDFLAGS: I CC: gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 I CXX: g++-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "_SOUR CE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE I NVCCFLAGS: -std=c++11 -O3 -g I LDFLAGS: I CC: gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 I CXX: g++-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 g++-11 -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused- function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp -march=nat ive -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/i nclude -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DN DEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -c ggml/src/llamafile/sgemm.cpp -o ggml/src/llamafile/sgemm.o gcc-11 -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 - D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -std=c11 -fPIC - O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstric t-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=i mplicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdou ble-promotion -c ggml/src/ggml.c -o ggml/src/ggml.o ................................................................................ ................................................................... ................................................................................ ................................................................... ................................................................................ ...................................."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": ".............. ................................................................... ................................................................................ ................................................................... g++-11 -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused- function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp -march=nat ive -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/i nclude -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DN DEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE ggml/src/llamafile/sgemm.o ggml/sr c/ggml.o ggml/src/ggml-alloc.o ggml/src/ggml-backend.o ggml/src/ggml-quants.o gg ml/src/ggml-aarch64.o src/llama.o src/llama-vocab.o src/llama-grammar.o src/llam a-sampling.o src/unicode.o src/unicode-data.o common/common.o common/arg.o commo n/log.o common/console.o common/ngram-cache.o common/sampling.o common/train.o c ommon/build-info.o common/json-schema-to-grammar.o -Iexamples/server examples/se rver/server.o -o llama-server ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ ls -lt |m total 1514288 -rwx------ 1 ljubomir ljubomir 55690856 Oct 16 19:44 llama-server -rwx------ 1 ljubomir ljubomir 46362096 Oct 16 19:43 llama-llava-cli -rwx------ 1 ljubomir ljubomir 46279024 Oct 16 19:43 llama-minicpmv-cli -rwx------ 1 ljubomir ljubomir 45798968 Oct 16 19:43 llama-bench -rwx------ 1 ljubomir ljubomir 43807"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "t 16 19:43 llama-llava-cli -rwx------ 1 ljubomir ljubomir 46279024 Oct 16 19:43 llama-minicpmv-cli -rwx------ 1 ljubomir ljubomir 45798968 Oct 16 19:43 llama-bench -rwx------ 1 ljubomir ljubomir 43807616 Oct 16 19:43 llama-quantize-stats -rwx------ 1 ljubomir ljubomir 43530080 Oct 16 19:43 llama-perplexity -rwx------ 1 ljubomir ljubomir 43259352 Oct 16 19:43 llama-imatrix -rwx------ 1 ljubomir ljubomir 42862872 Oct 16 19:43 llama-cvector-generator -rwx------ 1 ljubomir ljubomir 42767320 Oct 16 19:43 llama-cli -rwx------ 1 ljubomir ljubomir 42901360 Oct 16 19:43 llama-export-lora -rwx------ 1 ljubomir ljubomir 42573120 Oct 16 19:43 llama-quantize -rwx------ 1 ljubomir ljubomir 42570608 Oct 16 19:43 llama-infill -rwx------ 1 ljubomir ljubomir 42527544 Oct 16 19:43 llama-parallel -rwx------ 1 ljubomir ljubomir 42483256 Oct 16 19:43 llama-embedding -rwx------ 1 ljubomir ljubomir 42546112 Oct 16 19:43 llama-convert-llama2c-to-g gml -rwx------ 1 ljubomir ljubomir 42465656 Oct 16 19:43 llama-lookup -rwx------ 1 ljubomir ljubomir 42511624 Oct 16 19:43 llama-gritlm -rwx------ 1 ljubomir ljubomir 42793768 Oct 16 19:43 llama-speculative -rwx------ 1 ljubomir ljubomir 42533072 Oct 16 19:43 llama-lookahead -rwx------ 1 ljubomir ljubomir 42771840 Oct 16 19:43 llama-retrieval -rwx------ 1 ljubomir ljubomir 42297696 Oct 16 19:43 llama-gguf-split -rwx------ 1 ljubomir ljubomir 42383992 Oct 16 19:43 llama-batched -rwx------ 1 ljubomir ljubomir 42466368 Oct 16 19:43 llama-gen-docs -rwx------ 1"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "mir ljubomir 42297696 Oct 16 19:43 llama-gguf-split -rwx------ 1 ljubomir ljubomir 42383992 Oct 16 19:43 llama-batched -rwx------ 1 ljubomir ljubomir 42466368 Oct 16 19:43 llama-gen-docs -rwx------ 1 ljubomir ljubomir 42441704 Oct 16 19:43 llama-lookup-stats -rwx------ 1 ljubomir ljubomir 42269504 Oct 16 19:43 llama-gbnf-validator -rwx------ 1 ljubomir ljubomir 42216208 Oct 16 19:43 llama-tokenize -rwx------ 1 ljubomir ljubomir 42422112 Oct 16 19:43 llama-passkey -rwx------ 1 ljubomir ljubomir 42413896 Oct 16 19:43 llama-save-load-state -rwx------ 1 ljubomir ljubomir 42391464 Oct 16 19:43 llama-eval-callback -rwx------ 1 ljubomir ljubomir 42162840 Oct 16 19:43 llama-lookup-merge -rwx------ 1 ljubomir ljubomir 42673552 Oct 16 19:43 llama-gguf-hash -rwx------ 1 ljubomir ljubomir 42386056 Oct 16 19:43 llama-lookup-create -rwx------ 1 ljubomir ljubomir 42383792 Oct 16 19:43 llama-batched-bench -rwx------ 1 ljubomir ljubomir 42185664 Oct 16 19:43 llama-baby-llama -rwx------ 1 ljubomir ljubomir 42170560 Oct 16 19:43 llama-simple -rw------- 1 ljubomir ljubomir 300136 Oct 16 19:43 libllava.a drwx------ 2 ljubomir ljubomir 4096 Oct 16 19:43 src/ drwx------ 3 ljubomir ljubomir 4096 Oct 16 19:42 common/ -rwx------ 1 ljubomir ljubomir 4351600 Oct 16 19:42 llama-vdot -rwx------ 1 ljubomir ljubomir 4338016 Oct 16 19:42 llama-q8dot -rwx------ 1 ljubomir ljubomir 4358888 Oct 16 19:42 llama-gguf -rwx------ 1 ljubomir ljubomir 85512 Oct 16 19:42 main -rwx------ 1 ljubomir ljubomir 85512 Oct 16"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "bomir 4338016 Oct 16 19:42 llama-q8dot -rwx------ 1 ljubomir ljubomir 4358888 Oct 16 19:42 llama-gguf -rwx------ 1 ljubomir ljubomir 85512 Oct 16 19:42 main -rwx------ 1 ljubomir ljubomir 85512 Oct 16 19:42 server drwx------ 2 ljubomir ljubomir 4096 Oct 16 19:42 tests/ -rw------- 1 ljubomir ljubomir 137121 Oct 16 19:42 README.LJ -rw------- 1 ljubomir ljubomir 137120 Oct 16 19:42 README.LJ~ drwx------ 2 ljubomir ljubomir 12288 Oct 16 19:20 models/ drwx------ 2 ljubomir ljubomir 4096 Oct 16 16:48 scripts/ drwx------ 2 ljubomir ljubomir 4096 Oct 16 16:48 include/ -rw------- 1 ljubomir ljubomir 1556 Oct 16 16:48 flake.lock drwx------ 4 ljubomir ljubomir 4096 Oct 16 16:48 docs/ -rw------- 1 ljubomir ljubomir 6771 Oct 16 16:48 CMakeLists.txt -rw------- 1 ljubomir ljubomir 29055 Oct 16 16:48 README.md drwx------ 46 ljubomir ljubomir 12288 Oct 16 16:48 examples/ -rw------- 1 ljubomir ljubomir 1280 Oct 7 22:06 pyproject.toml -rw------- 1 ljubomir ljubomir 619 Oct 7 22:06 pyrightconfig.json drwx------ 2 ljubomir ljubomir 4096 Oct 7 22:06 requirements/ drwx------ 2 ljubomir ljubomir 4096 Oct 7 22:06 grammars/ drwx------ 6 ljubomir ljubomir 4096 Oct 7 22:06 gguf-py/ -rw------- 1 ljubomir ljubomir 7469 Oct 7 22:06 flake.nix drwx------ 5 ljubomir ljubomir 4096 Oct 7 22:06 ggml/ ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ rmv main server removed 'main' removed 'server' Run again without nvidia support ./llama-cli -m models/Llama-3.2-1B-Instruct-Q4_K_M."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "Oct 7 22:06 ggml/ ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ rmv main server removed 'main' removed 'server' Run again without nvidia support ./llama-cli -m models/Llama-3.2-1B-Instruct-Q4_K_M.gguf -p \"Write 4 lines on liv ing a purposeful life. A purposeful life is\" LJ Wed 16 Oct 16:49:35 BST 2024 + (torch) ljubomir@gigul2(2251797.torch:0):~/llama.cpp$ git tag -m 'After updating to run QwQ-32B-Preview-GGUF' tag_20241129_LJ_after_QwQ_32B-preview_merge_rc1 (torch) ljubomir@gigul2(2251797.torch:0):~/llama.cpp$ git tag -m 'After updating to run QwQ-32B-Preview-GGUF' tag_20241129_LJ_after_QwQ_32B-preview_merge_rc1 (torch) ljubomir@gigul2(2251797.torch:0):~/llama.cpp$ gilgt + git log -C --oneline --stat --decorate d2ce173c (HEAD -> master, tag: tag_20241129_LJ_after_QwQ_32B-preview_merge_rc1) updating to run QwQ-32B-Preview-GGUF a3a3048e (origin/master, origin/HEAD) cleanup UI link list (#10577) README.md | 56 ++++++++++++++++++++++++-------------------------------- 1 file changed, 24 insertions(+), 32 deletions(-) 38b6d7c7 (tag: tag_20241129_LJ_before_QwQ_32B-preview_merge_rc1) save README.LJ | 162 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ +++++++++++++++++++ 1 file changed, 162 insertions(+) (torch) ljubomir@gigul2(2251797.torch:0):~/llama.cpp$ mviv ~/Downloads/QwQ-32B-P review-Q6_K.gguf models/ ./llama-cli -m models/QwQ-32B-Preview-Q6_K."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "++++++++++++++++ 1 file changed, 162 insertions(+) (torch) ljubomir@gigul2(2251797.torch:0):~/llama.cpp$ mviv ~/Downloads/QwQ-32B-P review-Q6_K.gguf models/ ./llama-cli -m models/QwQ-32B-Preview-Q6_K.gguf -p \"Write 4 lines on living a pu rposeful life. A purposeful life is\" (torch) ljubomir@gigul2(2251797.torch:0):~/llama.cpp$ ./llama-cli -m models/QwQ- 32B-Preview-Q6_K.gguf -p \"Write 4 lines on living a purposeful life. A purposefu l life is\" build: 4279 (d2ce173c) with gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86 _64-linux-gnu main: llama backend init main: load the model and apply lora adapter, if any llama_model_loader: loaded meta data with 38 key-value pairs and 771 tensors fro m models/QwQ-32B-Preview-Q6_K.gguf (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not appl y in this output. llama_model_loader: - kv 0: general.architecture str = qwen2 llama_model_loader: - kv 1: general.type str = model llama_model_loader: - kv 2: general.name str = QwQ 32B Preview llama_model_loader: - kv 3: general.finetune str = Preview llama_model_loader: - kv 4: general.basename str = QwQ llama_model_loader: - kv 5: general.size_label str = 32B llama_model_loader: - kv 6: general.license str = apache-2.0 llama_model_loader: - kv 7: general.license.link str = https://huggingface.co/Qwen/QwQ-32B-P... llama_model_loader: - kv 8: general.base_model.count u32 = 1 llama_model_loader: - kv 9: general.base_model.0.name str = Qwen2."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "v 7: general.license.link str = https://huggingface.co/Qwen/QwQ-32B-P... llama_model_loader: - kv 8: general.base_model.count u32 = 1 llama_model_loader: - kv 9: general.base_model.0.name str = Qwen2.5 32B Instruct llama_model_loader: - kv 10: general.base_model.0.organization str = Qwen llama_model_loader: - kv 11: general.base_model.0.repo_url str = https://huggingface.co/Qwen/Qwen2.5-3... llama_model_loader: - kv 12: general.tags arr[str ,2] = [\"chat\", \"text-generation\"] llama_model_loader: - kv 13: general.languages arr[str ,1] = [\"en\"] llama_model_loader: - kv 14: qwen2.block_count u32 = 64 llama_model_loader: - kv 15: qwen2.context_length u32 = 32768 llama_model_loader: - kv 16: qwen2.embedding_length u32 = 5120 llama_model_loader: - kv 17: qwen2.feed_forward_length u32 = 27648 llama_model_loader: - kv 18: qwen2.attention.head_count u32 = 40 llama_model_loader: - kv 19: qwen2.attention.head_count_kv u32 = 8 llama_model_loader: - kv 20: qwen2.rope.freq_base f32 = 1000000.000000 llama_model_loader: - kv 21: qwen2.attention.layer_norm_rms_epsilon f32 = 0.000010 llama_model_loader: - kv 22: general.file_type u32 = 18 llama_model_loader: - kv 23: tokenizer.ggml.model str = gpt2 llama_model_loader: - kv 24: tokenizer.ggml.pre str = qwen2 llama_model_loader: - kv 25: tokenizer.ggml.tokens arr[str ,152064] = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ... llama_model_loader: - kv 26: tokenizer.ggml.token_type arr[i32 ,152064] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ..."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "25: tokenizer.ggml.tokens arr[str ,152064] = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ... llama_model_loader: - kv 26: tokenizer.ggml.token_type arr[i32 ,152064] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... llama_model_loader: - kv 27: tokenizer.ggml.merges arr[str ,151387] = [\"� � \", \"� � � � \", \"i n\", \"� t\",... llama_model_loader: - kv 28: tokenizer.ggml.eos_token_id u32 = 151645 llama_model_loader: - kv 29: tokenizer.ggml.padding_token_id u32 = 151643 llama_model_loader: - kv 30: tokenizer.ggml.bos_token_id u32 = 151643 llama_model_loader: - kv 31: tokenizer.ggml.add_bos_token bool = false llama_model_loader: - kv 32: tokenizer.chat_template str = {%- if tools %}\\n {{- '<|im_start|>... llama_model_loader: - kv 33: general.quantization_version u32 = 2 llama_model_loader: - kv 34: quantize.imatrix.file str = /models_out/QwQ-32B-Preview-GGUF/QwQ-... llama_model_loader: - kv 35: quantize.imatrix.dataset str = /training_dir/calibration_datav3.txt llama_model_loader: - kv 36: quantize.imatrix.entries_count i32 = 448 llama_model_loader: - kv 37: quantize.imatrix.chunks_count i32 = 128 llama_model_loader: - type f32: 321 tensors llama_model_loader: - type q6_K: 450 tensors llm_load_vocab: special tokens cache size = 22 llm_load_vocab: token to piece cache size = 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "s_count i32 = 128 llama_model_loader: - type f32: 321 tensors llama_model_loader: - type q6_K: 450 tensors llm_load_vocab: special tokens cache size = 22 llm_load_vocab: token to piece cache size = 0.9310 MB llm_load_print_meta: format = GGUF V3 (latest) llm_load_print_meta: arch = qwen2 llm_load_print_meta: vocab type = BPE llm_load_print_meta: n_vocab = 152064 llm_load_print_meta: n_merges = 151387 llm_load_print_meta: vocab_only = 0 llm_load_print_meta: n_ctx_train = 32768 llm_load_print_meta: n_embd = 5120 llm_load_print_meta: n_layer = 64 llm_load_print_meta: n_head = 40 llm_load_print_meta: n_head_kv = 8 llm_load_print_meta: n_rot = 128 llm_load_print_meta: n_swa = 0 llm_load_print_meta: n_embd_head_k = 128 llm_load_print_meta: n_embd_head_v = 128 llm_load_print_meta: n_gqa = 5 llm_load_print_meta: n_embd_k_gqa = 1024 llm_load_print_meta: n_embd_v_gqa = 1024 llm_load_print_meta: f_norm_eps = 0.0e+00 llm_load_print_meta: f_norm_rms_eps = 1.0e-05 llm_load_print_meta: f_clamp_kqv = 0.0e+00 llm_load_print_meta: f_max_alibi_bias = 0.0e+00 llm_load_print_meta: f_logit_scale = 0.0e+00 llm_load_print_meta: n_ff = 27648 llm_load_print_meta: n_expert = 0 llm_load_print_meta: n_expert_used = 0 llm_load_print_meta: causal attn = 1 llm_load_print_meta: pooling type = 0 llm_load_print_meta: rope type = 2 llm_load_print_meta: rope scaling = linear llm_load_print_meta: freq_base_train = 1000000."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "llm_load_print_meta: causal attn = 1 llm_load_print_meta: pooling type = 0 llm_load_print_meta: rope type = 2 llm_load_print_meta: rope scaling = linear llm_load_print_meta: freq_base_train = 1000000.0 llm_load_print_meta: freq_scale_train = 1 llm_load_print_meta: n_ctx_orig_yarn = 32768 llm_load_print_meta: rope_finetuned = unknown llm_load_print_meta: ssm_d_conv = 0 llm_load_print_meta: ssm_d_inner = 0 llm_load_print_meta: ssm_d_state = 0 llm_load_print_meta: ssm_dt_rank = 0 llm_load_print_meta: ssm_dt_b_c_rms = 0 llm_load_print_meta: model type = 32B llm_load_print_meta: model ftype = Q6_K llm_load_print_meta: model params = 32.76 B llm_load_print_meta: model size = 25.03 GiB (6.56 BPW) llm_load_print_meta: general.name = QwQ 32B Preview llm_load_print_meta: BOS token = 151643 '<|endoftext|>' llm_load_print_meta: EOS token = 151645 '<|im_end|>' llm_load_print_meta: EOT token = 151645 '<|im_end|>' llm_load_print_meta: PAD token = 151643 '<|endoftext|>' llm_load_print_meta: LF token = 148848 '�Ĭ' llm_load_print_meta: FIM PRE token = 151659 '<|fim_prefix|>' llm_load_print_meta: FIM SUF token = 151661 '<|fim_suffix|>' llm_load_print_meta: FIM MID token = 151660 '<|fim_middle|>' llm_load_print_meta: FIM PAD token = 151662 '<|fim_pad|>' llm_load_print_meta: FIM REP token = 151663 '<|repo_name|>' llm_load_print_meta: FIM SEP token = 151664 '<|file_sep|>' llm_load_print_meta: EOG token = 151643 '<|endoftext|>' llm_load_print_meta: EOG token = 151645 '<|im_end|>' llm_load_print_met"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "|repo_name|>' llm_load_print_meta: FIM SEP token = 151664 '<|file_sep|>' llm_load_print_meta: EOG token = 151643 '<|endoftext|>' llm_load_print_meta: EOG token = 151645 '<|im_end|>' llm_load_print_meta: EOG token = 151662 '<|fim_pad|>' llm_load_print_meta: EOG token = 151663 '<|repo_name|>' llm_load_print_meta: EOG token = 151664 '<|file_sep|>' llm_load_print_meta: max token length = 256 llm_load_tensors: CPU_Mapped model buffer size = 25634.93 MiB ................................................................................ .................. llama_new_context_with_model: n_seq_max = 1 llama_new_context_with_model: n_ctx = 4096 llama_new_context_with_model: n_ctx_per_seq = 4096 llama_new_context_with_model: n_batch = 2048 llama_new_context_with_model: n_ubatch = 512 llama_new_context_with_model: flash_attn = 0 llama_new_context_with_model: freq_base = 1000000.0 llama_new_context_with_model: freq_scale = 1 llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized llama_kv_cache_init: CPU KV buffer size = 1024.00 MiB llama_new_context_with_model: KV self size = 1024.00 MiB, K (f16): 512.00 MiB, V (f16): 512.00 MiB llama_new_context_with_model: CPU output buffer size = 0.58 MiB llama_new_context_with_model: CPU compute buffer size = 368."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "model: KV self size = 1024.00 MiB, K (f16): 512.00 MiB, V (f16): 512.00 MiB llama_new_context_with_model: CPU output buffer size = 0.58 MiB llama_new_context_with_model: CPU compute buffer size = 368.01 MiB llama_new_context_with_model: graph nodes = 2246 llama_new_context_with_model: graph splits = 1 common_init_from_params: warming up the model with an empty run - please wait .. . (--no-warmup to disable) main: llama threadpool init, n_threads = 10 system_info: n_threads = 10 (n_threads_batch = 10) / 10 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | sampler seed: 1953590522 sampler params: repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, p resence_penalty = 0.000 dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_pe nalty_last_n = -1 top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_t hreshold = 0.100, typical_p = 1.000, temp = 0.800 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> t op-p -> min-p -> xtc -> temp-ext -> dist generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 0 Write 4 lines on living a purposeful life. A purposeful life is one where we liv e with intention, direction, and passion. It's about aligning our actions with o ur values, goals, and aspirations."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "= 0 Write 4 lines on living a purposeful life. A purposeful life is one where we liv e with intention, direction, and passion. It's about aligning our actions with o ur values, goals, and aspirations. Living with purpose gives us a sense of direc tion, a reason to wake up every day, and the motivation to achieve our dreams. I t's about making a difference in the world, whether big or small, and leaving a legacy that inspires others to live their best lives. Living a purposeful life is not about achieving success or fame, but about findi ng meaning and fulfillment in what we do. It's about discovering our unique tale nts and using them to make a positive impact on the world. It's about being true to ourselves, pursuing our passions, and living authentically. It's about takin g risks, learning from failures, and growing as individuals. Moreover, living with purpose helps us navigate through challenges and setbacks with resilience and determination. It gives us a sense of clarity and focus, ena bling us to prioritize what truly matters in life. It also fosters a sense of gr atitude and appreciation for the present moment, allowing us to savor the joys a nd blessings along the way. In conclusion, living a purposeful life is a journey that requires self-reflecti on, self-discovery, and continuous growth. It's about embracing our uniqueness, pursuing our passions, and making a positive difference in the world."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "eful life is a journey that requires self-reflecti on, self-discovery, and continuous growth. It's about embracing our uniqueness, pursuing our passions, and making a positive difference in the world. By living with purpose, we can unlock our full potential, find fulfillment, and create a l egacy that inspires others to live their best lives. [end of text] llama_perf_sampler_print: sampling time = 32.55 ms / 313 runs ( 0 .10 ms per token, 9616.86 tokens per second) llama_perf_context_print: load time = 198549.36 ms llama_perf_context_print: prompt eval time = 5782.40 ms / 16 tokens ( 361 .40 ms per token, 2.77 tokens per second) llama_perf_context_print: eval time = 281369.51 ms / 296 runs ( 950 .57 ms per token, 1.05 tokens per second) llama_perf_context_print: total time = 287297.73 ms / 312 tokens (torch) ljubomir@gigul2(2251797.torch:0):~/llama.cpp$ LJ Fri 29 Nov 18:49:59 GMT 2024 + ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ make clean ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ make clean Makefile:2: *** The Makefile build is deprecated. Use the CMake build instead. F or more details, see https://github.com/ggerganov/llama.cpp/blob/master/docs/bui ld.md. Stop. ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ rm -rf build ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ git pull origin master From https://github.com/ggerganov/llama.cpp * branch master -> FETCH_HEAD Already up-to-date."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "a.cpp:0):~/llama.cpp$ rm -rf build ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ git pull origin master From https://github.com/ggerganov/llama.cpp * branch master -> FETCH_HEAD Already up-to-date. The default build does not use the CC and CXX env but falls back to default gcc and g++ that is very old cmake -B build cmake --build build --config Release The latest-greatest are 11 ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ env |egrep 'CC|CXX' QT_ACCESSIBILITY=1 CXX=g++-11 CC=gcc-11 So cmake -B build -DCMAKE_C_COMPILER=gcc-11 -DCMAKE_CXX_COMPILER=g++-11 cmake --build build --config Release -j The new directory for binaries is ./build/ it seems ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ find . -name llama-cli ./llama-cli ./build/bin/llama-cli ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ ls -la $(find . -name llama-cli ) -rwx------ 1 ljubomir ljubomir 995120 Dec 9 18:58 ./build/bin/llama-cli -rwx------ 1 ljubomir ljubomir 39579184 Nov 29 16:54 ./llama-cli Try QwQ with the new binary ./build/bin/llama-cli -m models/QwQ-32B-Preview-Q6_K.gguf -p \"Write 4 lines on l iving a purposeful life. A purposeful life is\" ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ ./build/bin/llama-cli -m models /QwQ-32B-Preview-Q6_K.gguf -p \"Write 4 lines on living a purposeful life. A purp oseful life is\" build: 4352 (19731660) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "pp$ ./build/bin/llama-cli -m models /QwQ-32B-Preview-Q6_K.gguf -p \"Write 4 lines on living a purposeful life. A purp oseful life is\" build: 4352 (19731660) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64- linux-gnu main: llama backend init main: load the model and apply lora adapter, if any llama_model_loader: loaded meta data with 38 key-value pairs and 771 tensors fro m models/QwQ-32B-Preview-Q6_K.gguf (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not appl y in this output. llama_model_loader: - kv 0: general.architecture str = qwen2 llama_model_loader: - kv 1: general.type str = model llama_model_loader: - kv 2: general.name str = QwQ 32B Preview llama_model_loader: - kv 3: general.finetune str = Preview llama_model_loader: - kv 4: general.basename str = QwQ llama_model_loader: - kv 5: general.size_label str = 32B llama_model_loader: - kv 6: general.license str = apache-2.0 llama_model_loader: - kv 7: general.license.link str = https://huggingface.co/Qwen/QwQ-32B-P... llama_model_loader: - kv 8: general.base_model.count u32 = 1 llama_model_loader: - kv 9: general.base_model.0.name str = Qwen2.5 32B Instruct llama_model_loader: - kv 10: general.base_model.0.organization str = Qwen llama_model_loader: - kv 11: general.base_model.0.repo_url str = https://huggingface.co/Qwen/Qwen2.5-3... llama_model_loader: - kv 12: general.tags arr[str ,2] = [\"chat\", \"text-generation\"] llama_model_loader: - kv 13: general."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": ": general.base_model.0.repo_url str = https://huggingface.co/Qwen/Qwen2.5-3... llama_model_loader: - kv 12: general.tags arr[str ,2] = [\"chat\", \"text-generation\"] llama_model_loader: - kv 13: general.languages arr[str ,1] = [\"en\"] llama_model_loader: - kv 14: qwen2.block_count u32 = 64 llama_model_loader: - kv 15: qwen2.context_length u32 = 32768 llama_model_loader: - kv 16: qwen2.embedding_length u32 = 5120 llama_model_loader: - kv 17: qwen2.feed_forward_length u32 = 27648 llama_model_loader: - kv 18: qwen2.attention.head_count u32 = 40 llama_model_loader: - kv 19: qwen2.attention.head_count_kv u32 = 8 llama_model_loader: - kv 20: qwen2.rope.freq_base f32 = 1000000.000000 llama_model_loader: - kv 21: qwen2.attention.layer_norm_rms_epsilon f32 = 0.000010 llama_model_loader: - kv 22: general.file_type u32 = 18 llama_model_loader: - kv 23: tokenizer.ggml.model str = gpt2 llama_model_loader: - kv 24: tokenizer.ggml.pre str = qwen2 llama_model_loader: - kv 25: tokenizer.ggml.tokens arr[str ,152064] = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ... llama_model_loader: - kv 26: tokenizer.ggml.token_type arr[i32 ,152064] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... llama_model_loader: - kv 27: tokenizer.ggml.merges arr[str ,151387] = [\"� � \", \"� � � � \", \"i n\", \"� t\",... llama_model_loader: - kv 28: tokenizer.ggml.eos_token_id u32 = 151645 llama_model_loader: - kv 29: tokenizer.ggml.padding_token_id u32 = 151643 llama_model_loader: - kv 30: tokenizer.ggml."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": ", \"� t\",... llama_model_loader: - kv 28: tokenizer.ggml.eos_token_id u32 = 151645 llama_model_loader: - kv 29: tokenizer.ggml.padding_token_id u32 = 151643 llama_model_loader: - kv 30: tokenizer.ggml.bos_token_id u32 = 151643 llama_model_loader: - kv 31: tokenizer.ggml.add_bos_token bool = false llama_model_loader: - kv 32: tokenizer.chat_template str = {%- if tools %}\\n {{- '<|im_start|>... llama_model_loader: - kv 33: general.quantization_version u32 = 2 llama_model_loader: - kv 34: quantize.imatrix.file str = /models_out/QwQ-32B-Preview-GGUF/QwQ-... llama_model_loader: - kv 35: quantize.imatrix.dataset str = /training_dir/calibration_datav3.txt llama_model_loader: - kv 36: quantize.imatrix.entries_count i32 = 448 llama_model_loader: - kv 37: quantize.imatrix.chunks_count i32 = 128 llama_model_loader: - type f32: 321 tensors llama_model_loader: - type q6_K: 450 tensors llm_load_vocab: special tokens cache size = 22 llm_load_vocab: token to piece cache size = 0.9310 MB llm_load_print_meta: format = GGUF V3 (latest) llm_load_print_meta: arch = qwen2 llm_load_print_meta: vocab type = BPE llm_load_print_meta: n_vocab = 152064 llm_load_print_meta: n_merges = 151387 llm_load_print_meta: vocab_only = 0 llm_load_print_meta: n_ctx_train = 32768 llm_load_print_meta: n_embd = 5120 llm_load_print_meta: n_layer = 64 llm_load_print_meta: n_head = 40 llm_load_print_meta: n_head_kv = 8 llm_load_print_meta: n_rot = 128 llm_load_print_meta: n_swa = 0 llm_load_print_meta: n_embd_head_k = 128"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "d_print_meta: n_layer = 64 llm_load_print_meta: n_head = 40 llm_load_print_meta: n_head_kv = 8 llm_load_print_meta: n_rot = 128 llm_load_print_meta: n_swa = 0 llm_load_print_meta: n_embd_head_k = 128 llm_load_print_meta: n_embd_head_v = 128 llm_load_print_meta: n_gqa = 5 llm_load_print_meta: n_embd_k_gqa = 1024 llm_load_print_meta: n_embd_v_gqa = 1024 llm_load_print_meta: f_norm_eps = 0.0e+00 llm_load_print_meta: f_norm_rms_eps = 1.0e-05 llm_load_print_meta: f_clamp_kqv = 0.0e+00 llm_load_print_meta: f_max_alibi_bias = 0.0e+00 llm_load_print_meta: f_logit_scale = 0.0e+00 llm_load_print_meta: n_ff = 27648 llm_load_print_meta: n_expert = 0 llm_load_print_meta: n_expert_used = 0 llm_load_print_meta: causal attn = 1 llm_load_print_meta: pooling type = 0 llm_load_print_meta: rope type = 2 llm_load_print_meta: rope scaling = linear llm_load_print_meta: freq_base_train = 1000000.0 llm_load_print_meta: freq_scale_train = 1 llm_load_print_meta: n_ctx_orig_yarn = 32768 llm_load_print_meta: rope_finetuned = unknown llm_load_print_meta: ssm_d_conv = 0 llm_load_print_meta: ssm_d_inner = 0 llm_load_print_meta: ssm_d_state = 0 llm_load_print_meta: ssm_dt_rank = 0 llm_load_print_meta: ssm_dt_b_c_rms = 0 llm_load_print_meta: model type = 32B llm_load_print_meta: model ftype = Q6_K llm_load_print_meta: model params = 32.76 B llm_load_print_meta: model size = 25.03 GiB (6.56 BPW) llm_load_print_meta: general."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ad_print_meta: model type = 32B llm_load_print_meta: model ftype = Q6_K llm_load_print_meta: model params = 32.76 B llm_load_print_meta: model size = 25.03 GiB (6.56 BPW) llm_load_print_meta: general.name = QwQ 32B Preview llm_load_print_meta: BOS token = 151643 '<|endoftext|>' llm_load_print_meta: EOS token = 151645 '<|im_end|>' llm_load_print_meta: EOT token = 151645 '<|im_end|>' llm_load_print_meta: PAD token = 151643 '<|endoftext|>' llm_load_print_meta: LF token = 148848 '�Ĭ' llm_load_print_meta: FIM PRE token = 151659 '<|fim_prefix|>' llm_load_print_meta: FIM SUF token = 151661 '<|fim_suffix|>' llm_load_print_meta: FIM MID token = 151660 '<|fim_middle|>' llm_load_print_meta: FIM PAD token = 151662 '<|fim_pad|>' llm_load_print_meta: FIM REP token = 151663 '<|repo_name|>' llm_load_print_meta: FIM SEP token = 151664 '<|file_sep|>' llm_load_print_meta: EOG token = 151643 '<|endoftext|>' llm_load_print_meta: EOG token = 151645 '<|im_end|>' llm_load_print_meta: EOG token = 151662 '<|fim_pad|>' llm_load_print_meta: EOG token = 151663 '<|repo_name|>' llm_load_print_meta: EOG token = 151664 '<|file_sep|>' llm_load_print_meta: max token length = 256 llm_load_tensors: CPU_Mapped model buffer size = 25634.93 MiB ................................................................................ .................."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "oad_print_meta: max token length = 256 llm_load_tensors: CPU_Mapped model buffer size = 25634.93 MiB ................................................................................ .................. llama_new_context_with_model: n_seq_max = 1 llama_new_context_with_model: n_ctx = 4096 llama_new_context_with_model: n_ctx_per_seq = 4096 llama_new_context_with_model: n_batch = 2048 llama_new_context_with_model: n_ubatch = 512 llama_new_context_with_model: flash_attn = 0 llama_new_context_with_model: freq_base = 1000000.0 llama_new_context_with_model: freq_scale = 1 llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized llama_kv_cache_init: CPU KV buffer size = 1024.00 MiB llama_new_context_with_model: KV self size = 1024.00 MiB, K (f16): 512.00 MiB, V (f16): 512.00 MiB llama_new_context_with_model: CPU output buffer size = 0.58 MiB llama_new_context_with_model: CPU compute buffer size = 368.01 MiB llama_new_context_with_model: graph nodes = 2246 llama_new_context_with_model: graph splits = 1 common_init_from_params: warming up the model with an empty run - please wait .. . (--no-warmup to disable) main: llama threadpool init, n_threads = 10 system_info: n_threads = 10 (n_threads_batch = 10) / 10 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AA RCH64_REPACK = 1 | sampler seed: 2051864547 sampler params: repeat_last_n = 64, repeat_penalty = 1."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "U : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AA RCH64_REPACK = 1 | sampler seed: 2051864547 sampler params: repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, p resence_penalty = 0.000 dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_pe nalty_last_n = -1 top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_t hreshold = 0.100, typical_p = 1.000, temp = 0.800 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> t op-p -> min-p -> xtc -> temp-ext -> dist generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 0 Write 4 lines on living a purposeful life. A purposeful life is one that is driv en by a sense of meaning and direction, where you are actively engaged in activi ties that align with your values and goals. Living a purposeful life allows you to find fulfillment and satisfaction in your day-to-day activities and helps you to make a positive impact on the world around you. It involves setting clear go als, taking action towards achieving them, and continuously learning and growing as a person. Ultimately, living a purposeful life is about finding your passion and using it to create a meaningful and impactful life. [end of text] llama_perf_sampler_print: sampling time = 14.79 ms / 120 runs ( 0 .12 ms per token, 8112."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "seful life is about finding your passion and using it to create a meaningful and impactful life. [end of text] llama_perf_sampler_print: sampling time = 14.79 ms / 120 runs ( 0 .12 ms per token, 8112.49 tokens per second) llama_perf_context_print: load time = 295851.86 ms llama_perf_context_print: prompt eval time = 7214.63 ms / 16 tokens ( 450 .91 ms per token, 2.22 tokens per second) llama_perf_context_print: eval time = 260424.31 ms / 103 runs ( 2528 .39 ms per token, 0.40 tokens per second) llama_perf_context_print: total time = 267748.97 ms / 119 tokens Test Llama3.3 70B ./build/bin/llama-cli -m models/Llama-3.3-70B-Instruct-Q5_K_M.gguf -p \"Write 4 l ines on living a purposeful life. A purposeful life is\" ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ ./build/bin/llama-cli -m models /Llama-3.3-70B-Instruct-Q5_K_M.gguf -p \"Write 4 lines on living a purposeful lif e. A purposeful life is\" build: 4352 (19731660) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64- linux-gnu main: llama backend init main: load the model and apply lora adapter, if any llama_model_loader: loaded meta data with 30 key-value pairs and 724 tensors fro m models/Llama-3.3-70B-Instruct-Q5_K_M.gguf (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not appl y in this output. llama_model_loader: - kv 0: general.architecture str = llama llama_model_loader: - kv 1: general.type str = model llama_model_loader: - kv 2: general.name str = Llama 3."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "do not appl y in this output. llama_model_loader: - kv 0: general.architecture str = llama llama_model_loader: - kv 1: general.type str = model llama_model_loader: - kv 2: general.name str = Llama 3.3 70B Instruct llama_model_loader: - kv 3: general.organization str = Meta Llama llama_model_loader: - kv 4: general.finetune str = Instruct llama_model_loader: - kv 5: general.basename str = Llama-3.3 llama_model_loader: - kv 6: general.size_label str = 70B llama_model_loader: - kv 7: llama.block_count u32 = 80 llama_model_loader: - kv 8: llama.context_length u32 = 131072 llama_model_loader: - kv 9: llama.embedding_length u32 = 8192 llama_model_loader: - kv 10: llama.feed_forward_length u32 = 28672 llama_model_loader: - kv 11: llama.attention.head_count u32 = 64 llama_model_loader: - kv 12: llama.attention.head_count_kv u32 = 8 llama_model_loader: - kv 13: llama.rope.freq_base f32 = 500000.000000 llama_model_loader: - kv 14: llama.attention.layer_norm_rms_epsilon f32 = 0.000010 llama_model_loader: - kv 15: llama.attention.key_length u32 = 128 llama_model_loader: - kv 16: llama.attention.value_length u32 = 128 llama_model_loader: - kv 17: general.file_type u32 = 17 llama_model_loader: - kv 18: llama.vocab_size u32 = 128256 llama_model_loader: - kv 19: llama.rope.dimension_count u32 = 128 llama_model_loader: - kv 20: tokenizer.ggml.model str = gpt2 llama_model_loader: - kv 21: tokenizer.ggml.pre str = llama-bpe llama_model_loader: - kv 22: tokenizer.ggml."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "rope.dimension_count u32 = 128 llama_model_loader: - kv 20: tokenizer.ggml.model str = gpt2 llama_model_loader: - kv 21: tokenizer.ggml.pre str = llama-bpe llama_model_loader: - kv 22: tokenizer.ggml.tokens arr[str ,128256] = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ... llama_model_loader: - kv 23: tokenizer.ggml.token_type arr[i32 ,128256] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... llama_model_loader: - kv 24: tokenizer.ggml.merges arr[str ,280147] = [\"� � \", \"� � � � \", \"� � � � \", \"... llama_model_loader: - kv 25: tokenizer.ggml.bos_token_id u32 = 128000 llama_model_loader: - kv 26: tokenizer.ggml.eos_token_id u32 = 128009 llama_model_loader: - kv 27: tokenizer.ggml.padding_token_id u32 = 128004 llama_model_loader: - kv 28: tokenizer.chat_template str = {{- bos_token }}\\n{%- if custom_tools ... llama_model_loader: - kv 29: general.quantization_version u32 = 2 llama_model_loader: - type f32: 162 tensors llama_model_loader: - type q5_K: 481 tensors llama_model_loader: - type q6_K: 81 tensors llm_load_vocab: special tokens cache size = 256 llm_load_vocab: token to piece cache size = 0.7999 MB llm_load_print_meta: format = GGUF V3 (latest) llm_load_print_meta: arch = llama llm_load_print_meta: vocab type = BPE llm_load_print_meta: n_vocab = 128256 llm_load_print_meta: n_merges = 280147 llm_load_print_meta: vocab_only = 0 llm_load_print_meta: n_ctx_train = 131072 llm_load_print_meta: n_embd = 8192 llm_load_print_meta: n_layer = 80 llm_load_print_meta: n_head = 64 llm_load_print_met"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "7 llm_load_print_meta: vocab_only = 0 llm_load_print_meta: n_ctx_train = 131072 llm_load_print_meta: n_embd = 8192 llm_load_print_meta: n_layer = 80 llm_load_print_meta: n_head = 64 llm_load_print_meta: n_head_kv = 8 llm_load_print_meta: n_rot = 128 llm_load_print_meta: n_swa = 0 llm_load_print_meta: n_embd_head_k = 128 llm_load_print_meta: n_embd_head_v = 128 llm_load_print_meta: n_gqa = 8 llm_load_print_meta: n_embd_k_gqa = 1024 llm_load_print_meta: n_embd_v_gqa = 1024 llm_load_print_meta: f_norm_eps = 0.0e+00 llm_load_print_meta: f_norm_rms_eps = 1.0e-05 llm_load_print_meta: f_clamp_kqv = 0.0e+00 llm_load_print_meta: f_max_alibi_bias = 0.0e+00 llm_load_print_meta: f_logit_scale = 0.0e+00 llm_load_print_meta: n_ff = 28672 llm_load_print_meta: n_expert = 0 llm_load_print_meta: n_expert_used = 0 llm_load_print_meta: causal attn = 1 llm_load_print_meta: pooling type = 0 llm_load_print_meta: rope type = 0 llm_load_print_meta: rope scaling = linear _meta: freq_base_train = 500000.0 llm_load_print_meta: freq_scale_train = 1 _meta: n_ctx_orig_yarn = 131072 llm_load_print_meta: rope_finetuned = unknown llm_load_print_meta: ssm_d_conv = 0 llm_load_print_meta: ssm_d_inner = 0 llm_load_print_meta: ssm_d_state = 0 llm_load_print_meta: ssm_dt_rank = 0 llm_load_print_meta: ssm_dt_b_c_rms = 0 llm_load_print_meta: model type = 70B llm_load_print_meta: model ftype = Q5_K - Medium llm_load_print_meta: model params = 70.55 B llm_load_print_meta: model size = 46.51 GiB (5."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "a: ssm_dt_b_c_rms = 0 llm_load_print_meta: model type = 70B llm_load_print_meta: model ftype = Q5_K - Medium llm_load_print_meta: model params = 70.55 B llm_load_print_meta: model size = 46.51 GiB (5.66 BPW) llm_load_print_meta: general.name = Llama 3.3 70B Instruct llm_load_print_meta: BOS token = 128000 '<|begin_of_text|>' llm_load_print_meta: EOS token = 128009 '<|eot_id|>' llm_load_print_meta: EOT token = 128009 '<|eot_id|>' llm_load_print_meta: EOM token = 128008 '<|eom_id|>' llm_load_print_meta: PAD token = 128004 '<|finetune_right_pad_id|>' llm_load_print_meta: LF token = 128 '�' llm_load_print_meta: EOG token = 128008 '<|eom_id|>' llm_load_print_meta: EOG token = 128009 '<|eot_id|>' llm_load_print_meta: max token length = 256 llm_load_tensors: CPU_Mapped model buffer size = 47628.36 MiB ................................................................................ ................... llama_new_context_with_model: n_seq_max = 1 llama_new_context_with_model: n_ctx = 4096 llama_new_context_with_model: n_ctx_per_seq = 4096 llama_new_context_with_model: n_batch = 2048 llama_new_context_with_model: n_ubatch = 512 llama_new_context_with_model: flash_attn = 0 llama_new_context_with_model: freq_base = 500000.0 llama_new_context_with_model: freq_scale = 1 llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized llama_kv_cache_init: CPU KV buffer size = 1280."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "h_model: freq_scale = 1 llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized llama_kv_cache_init: CPU KV buffer size = 1280.00 MiB llama_new_context_with_model: KV self size = 1280.00 MiB, K (f16): 640.00 MiB, V (f16): 640.00 MiB llama_new_context_with_model: CPU output buffer size = 0.49 MiB llama_new_context_with_model: CPU compute buffer size = 584.01 MiB llama_new_context_with_model: graph nodes = 2566 llama_new_context_with_model: graph splits = 1 common_init_from_params: warming up the model with an empty run - please wait .. . (--no-warmup to disable) main: llama threadpool init, n_threads = 10 system_info: n_threads = 10 (n_threads_batch = 10) / 10 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AA RCH64_REPACK = 1 | sampler seed: 651543096 sampler params: repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, p resence_penalty = 0.000 dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_pe nalty_last_n = -1 top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_t hreshold = 0.100, typical_p = 1.000, temp = 0.800 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "dry_pe nalty_last_n = -1 top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_t hreshold = 0.100, typical_p = 1.000, temp = 0.800 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> t op-p -> min-p -> xtc -> temp-ext -> dist generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1 Write 4 lines on living a purposeful life. A purposeful life is one that is fill ed with meaning and direction. When we live a purposeful life, we feel fulfilled and motivated to make a positive impact. Living a purposeful life involve r goals and working towards them with passion and dedication. By doing so, we ca n create a sense of accomplishment and leave a lasting legacy. The best answer is A purposeful life is one that is filled with meaning and dire ction. When we live a purposeful life, we feel fulfilled and motivated to make a positive impact. Living a purposeful life involves setting clear goals an rds them with passion and dedication. By doing so, we can create a sense of acco mplishment and leave a lasting legacy. [end of text] llama_perf_sampler_print: sampling time = 13.26 ms / 153 runs ( 0 .09 ms per token, 11535.85 tokens per second) llama_perf_context_print: load time = 382572.16 ms llama_perf_context_print: prompt eval time = 12463.72 ms / 17 tokens ( 733 .16 ms per token, 1.36 tokens per second) llama_perf_context_print: eval time = 262991.35 ms / 135 runs ( 1948 ."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "382572.16 ms llama_perf_context_print: prompt eval time = 12463.72 ms / 17 tokens ( 733 .16 ms per token, 1.36 tokens per second) llama_perf_context_print: eval time = 262991.35 ms / 135 runs ( 1948 .08 ms per token, 0.51 tokens per second) llama_perf_context_print: total time = 275554.51 ms / 152 tokens Increase context to 128K, address llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized ./build/bin/llama-cli --ctx-size 131072 -m models/Llama-3.3-70B-Instruct-Q5_K_M. gguf -p \"Write 4 lines on living a purposeful life. A purposeful life is\" Actually better \"load from the model\" 0 so it's max - but run terminated did not finish after 1 hour (!!) ./build/bin/llama-cli --ctx-size 0 -m models/Llama-3.3-70B-Instruct-Q5_K_M.gguf -p \"Write 4 lines on living a purposeful life. A purposeful life is\" ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ ./build/bin/llama-cli --ctx-siz e 0 -m models/Llama-3.3-70B-Instruct-Q5_K_M.gguf -p \"Write 4 lines on living a p urposeful life. A purposeful life is\" build: 4352 (19731660) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64- linux-gnu main: llama backend init main: load the model and apply lora adapter, if any llama_model_loader: loaded meta data with 30 key-value pairs and 724 tensors fro m models/Llama-3.3-70B-Instruct-Q5_K_M.gguf (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not appl y in this output."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "alue pairs and 724 tensors fro m models/Llama-3.3-70B-Instruct-Q5_K_M.gguf (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not appl y in this output. llama_model_loader: - kv 0: general.architecture str = llama llama_model_loader: - kv 1: general.type str = model llama_model_loader: - kv 2: general.name str = Llama 3.3 70B Instruct llama_model_loader: - kv 3: general.organization str = Meta Llama llama_model_loader: - kv 4: general.finetune str = Instruct llama_model_loader: - kv 5: general.basename str = Llama-3.3 llama_model_loader: - kv 6: general.size_label str = 70B llama_model_loader: - kv 7: llama.block_count u32 = 80 llama_model_loader: - kv 8: llama.context_length u32 = 131072 llama_model_loader: - kv 9: llama.embedding_length u32 = 8192 llama_model_loader: - kv 10: llama.feed_forward_length u32 = 28672 llama_model_loader: - kv 11: llama.attention.head_count u32 = 64 llama_model_loader: - kv 12: llama.attention.head_count_kv u32 = 8 llama_model_loader: - kv 13: llama.rope.freq_base f32 = 500000.000000 llama_model_loader: - kv 14: llama.attention.layer_norm_rms_epsilon f32 = 0.000010 llama_model_loader: - kv 15: llama.attention.key_length u32 = 128 llama_model_loader: - kv 16: llama.attention.value_length u32 = 128 llama_model_loader: - kv 17: general.file_type u32 = 17 llama_model_loader: - kv 18: llama.vocab_size u32 = 128256 llama_model_loader: - kv 19: llama.rope."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "v 16: llama.attention.value_length u32 = 128 llama_model_loader: - kv 17: general.file_type u32 = 17 llama_model_loader: - kv 18: llama.vocab_size u32 = 128256 llama_model_loader: - kv 19: llama.rope.dimension_count u32 = 128 llama_model_loader: - kv 20: tokenizer.ggml.model str = gpt2 llama_model_loader: - kv 21: tokenizer.ggml.pre str = llama-bpe llama_model_loader: - kv 22: tokenizer.ggml.tokens arr[str ,128256] = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ... llama_model_loader: - kv 23: tokenizer.ggml.token_type arr[i32 ,128256] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... llama_model_loader: - kv 24: tokenizer.ggml.merges arr[str ,280147] = [\"� � \", \"� � � � \", \"� � � � \", \"... llama_model_loader: - kv 25: tokenizer.ggml.bos_token_id u32 = 128000 llama_model_loader: - kv 26: tokenizer.ggml.eos_token_id u32 = 128009 llama_model_loader: - kv 27: tokenizer.ggml.padding_token_id u32 = 128004 llama_model_loader: - kv 28: tokenizer.chat_template str = {{- bos_token }}\\n{%- if custom_tools ... llama_model_loader: - kv 29: general.quantization_version u32 = 2 llama_model_loader: - type f32: 162 tensors llama_model_loader: - type q5_K: 481 tensors llama_model_loader: - type q6_K: 81 tensors llm_load_vocab: special tokens cache size = 256 llm_load_vocab: token to piece cache size = 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "f32: 162 tensors llama_model_loader: - type q5_K: 481 tensors llama_model_loader: - type q6_K: 81 tensors llm_load_vocab: special tokens cache size = 256 llm_load_vocab: token to piece cache size = 0.7999 MB llm_load_print_meta: format = GGUF V3 (latest) llm_load_print_meta: arch = llama llm_load_print_meta: vocab type = BPE llm_load_print_meta: n_vocab = 128256 llm_load_print_meta: n_merges = 280147 llm_load_print_meta: vocab_only = 0 llm_load_print_meta: n_ctx_train = 131072 llm_load_print_meta: n_embd = 8192 llm_load_print_meta: n_layer = 80 llm_load_print_meta: n_head = 64 llm_load_print_meta: n_head_kv = 8 llm_load_print_meta: n_rot = 128 llm_load_print_meta: n_swa = 0 llm_load_print_meta: n_embd_head_k = 128 llm_load_print_meta: n_embd_head_v = 128 llm_load_print_meta: n_gqa = 8 llm_load_print_meta: n_embd_k_gqa = 1024 llm_load_print_meta: n_embd_v_gqa = 1024 llm_load_print_meta: f_norm_eps = 0.0e+00 llm_load_print_meta: f_norm_rms_eps = 1.0e-05 llm_load_print_meta: f_clamp_kqv = 0.0e+00 llm_load_print_meta: f_max_alibi_bias = 0.0e+00 llm_load_print_meta: f_logit_scale = 0.0e+00 llm_load_print_meta: n_ff = 28672 llm_load_print_meta: n_expert = 0 llm_load_print_meta: n_expert_used = 0 llm_load_print_meta: causal attn = 1 llm_load_print_meta: pooling type = 0 llm_load_print_meta: rope type = 0 llm_load_print_meta: rope scaling = linear llm_load_print_meta: freq_base_train = 500000."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "llm_load_print_meta: causal attn = 1 llm_load_print_meta: pooling type = 0 llm_load_print_meta: rope type = 0 llm_load_print_meta: rope scaling = linear llm_load_print_meta: freq_base_train = 500000.0 llm_load_print_meta: freq_scale_train = 1 llm_load_print_meta: n_ctx_orig_yarn = 131072 llm_load_print_meta: rope_finetuned = unknown llm_load_print_meta: ssm_d_conv = 0 llm_load_print_meta: ssm_d_inner = 0 llm_load_print_meta: ssm_d_state = 0 llm_load_print_meta: ssm_dt_rank = 0 llm_load_print_meta: ssm_dt_b_c_rms = 0 llm_load_print_meta: model type = 70B llm_load_print_meta: model ftype = Q5_K - Medium llm_load_print_meta: model params = 70.55 B llm_load_print_meta: model size = 46.51 GiB (5.66 BPW) llm_load_print_meta: general.name = Llama 3.3 70B Instruct llm_load_print_meta: BOS token = 128000 '<|begin_of_text|>' llm_load_print_meta: EOS token = 128009 '<|eot_id|>' llm_load_print_meta: EOT token = 128009 '<|eot_id|>' llm_load_print_meta: EOM token = 128008 '<|eom_id|>' llm_load_print_meta: PAD token = 128004 '<|finetune_right_pad_id|>' llm_load_print_meta: LF token = 128 '�' llm_load_print_meta: EOG token = 128008 '<|eom_id|>' llm_load_print_meta: EOG token = 128009 '<|eot_id|>' llm_load_print_meta: max token length = 256 llm_load_tensors: CPU_Mapped model buffer size = 47628.36 MiB ................................................................................ ..................."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ad_print_meta: max token length = 256 llm_load_tensors: CPU_Mapped model buffer size = 47628.36 MiB ................................................................................ ................... llama_new_context_with_model: n_seq_max = 1 llama_new_context_with_model: n_ctx = 131072 llama_new_context_with_model: n_ctx_per_seq = 131072 llama_new_context_with_model: n_batch = 2048 llama_new_context_with_model: n_ubatch = 512 llama_new_context_with_model: flash_attn = 0 llama_new_context_with_model: freq_base = 500000.0 llama_new_context_with_model: freq_scale = 1 llama_kv_cache_init: CPU KV buffer size = 40960.00 MiB llama_new_context_with_model: KV self size = 40960.00 MiB, K (f16): 20480.00 Mi B, V (f16): 20480.00 MiB llama_new_context_with_model: CPU output buffer size = 0.49 MiB llama_new_context_with_model: CPU compute buffer size = 16704.01 MiB llama_new_context_with_model: graph nodes = 2566 llama_new_context_with_model: graph splits = 1 common_init_from_params: warming up the model with an empty run - please wait .. . (--no-warmup to disable) main: llama threadpool init, n_threads = 10 system_info: n_threads = 10 (n_threads_batch = 10) / 10 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AA RCH64_REPACK = 1 | sampler seed: 3177813908 sampler params: repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, p resence_penalty = 0.000 dry_multiplier = 0.000, dry_base = 1."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "| AA RCH64_REPACK = 1 | sampler seed: 3177813908 sampler params: repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, p resence_penalty = 0.000 dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_pe nalty_last_n = -1 top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_t hreshold = 0.100, typical_p = 1.000, temp = 0.800 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> t op-p -> min-p -> xtc -> temp-ext -> dist generate: n_ctx = 131072, n_batch = 2048, n_predict = -1, n_keep = 1 Write 4 lines on living a purposeful life. A purposeful life is one where you kn ow yourTerminated (run terminated after waiting for 1h) LJ Mon 9 Dec 19:02:44 GMT 2024 + (torch) ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ git pull origin master (torch) ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ git pull origin master remote: Enumerating objects: 2197, done. remote: Counting objects: 100% (1508/1508), done. remote: Compressing objects: 100% (238/238), done. remote: Total 2197 (delta 1380), reused 1270 (delta 1270), pack-reused 689 (from 3) Receiving objects: 100% (2197/2197), 10.69 MiB | 338.00 KiB/s, done. Resolving deltas: 100% (1596/1596), completed with 294 local objects. From https://github.com/ggerganov/llama.cpp * branch master -> FETCH_HEAD c37fb4cf..6171c9d2 master -> origin/master Auto-merging ."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "esolving deltas: 100% (1596/1596), completed with 294 local objects. From https://github.com/ggerganov/llama.cpp * branch master -> FETCH_HEAD c37fb4cf..6171c9d2 master -> origin/master Auto-merging .gitignore hint: Waiting for your editor to close the file... Merge branch 'master' of https://github.com/ggerganov/llama.cpp # Please enter a commit message to explain why this merge is necessary, # especially if it merges an updated upstream into a topic branch. # # Lines starting with '#' will be ignored, and an empty message aborts # the commit. .git/MERGE_MSG {unix|utf-8|GI TCOMMIT} [6,13][100%] Merge made by the 'ort' strategy. .devops/cpu.Dockerfile | 81 + .devops/cuda.Dockerfile | 94 + .devops/full-cuda.Dockerfile | 33 - .devops/full-musa.Dockerfile | 33 - .devops/full-rocm.Dockerfile | 50 - .devops/full.Dockerfile | 38 - .devops/intel.Dockerfile | 91 + .devops/llama-cli-cuda.Dockerfile | 38 - .devops/llama-cli-intel.Dockerfile | 28 - .devops/llama-cli-musa.Dockerfile | 38 - .devops/llama-cli-rocm.Dockerfile | 45 - .devops/llama-cli-vulkan.Dockerfile | 27 - .devops/llama-cli.Dockerfile | 29 - .devops/llama-server-cuda.Dockerfile | 43 - .devops/llama-server-intel.Dockerfile | 34 - .devops/llama-server-musa.Dockerfile | 43 - .devops/llama-server-rocm.Dockerfile | 54 - .devops/llama-server-vulkan.Dockerfile | 31 - .devops/llama-server.Dockerfile | 33 - .devops/musa.Dockerfile | 108 + .devops/nix/package.nix | 3 +- .devops/rocm.Dockerfile | 113 + .devops/tools.sh | 10 +- ."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "s/llama-server-vulkan.Dockerfile | 31 - .devops/llama-server.Dockerfile | 33 - .devops/musa.Dockerfile | 108 + .devops/nix/package.nix | 3 +- .devops/rocm.Dockerfile | 113 + .devops/tools.sh | 10 +- .devops/vulkan.Dockerfile | 88 + .github/ISSUE_TEMPLATE/010-bug-compilation.yml | 12 +- .github/ISSUE_TEMPLATE/019-bug-misc.yml | 12 +- .github/workflows/build.yml | 70 +- .github/workflows/docker.yml | 107 +- .github/workflows/editorconfig.yml | 4 +- .github/workflows/server.yml | 27 +- .gitignore | 1 + CMakeLists.txt | 73 +- CODEOWNERS | 10 +- CONTRIBUTING.md | 102 +- Makefile | 11 + README.md | 62 +- ci/run.sh | 66 +- cmake/build-info.cmake | 2 +- common/CMakeLists.txt | 4 +- common/arg.cpp | 304 ++- common/arg.h | 3 + common/chat-template.hpp | 249 ++ common/common.cpp | 436 ++- common/common.h | 155 +- common/minja.hpp | 2788 +++++++++++++++++++ common/ngram-cache.cpp | 24 +- common/ngram-cache.h | 4 +- common/sampling.cpp | 44 +- common/speculative.cpp | 33 +- convert_hf_to_gguf.py | 706 ++++- convert_hf_to_gguf_update.py | 83 +- convert_lora_to_gguf.py | 34 +- docs/build.md | 2 + docs/cuda-fedora.md | 317 +++ docs/development/HOWTO-add-model.md | 10 +- examples/CMakeLists.txt | 14 +- examples/batched-bench/batched-bench.cpp | 6 +- examples/batched.swift/Sources/main.swift | 6 +- examples/batched/batched.cpp | 17 +- examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp | 18 +- examples/cvector-generator/cvector-generator.cpp | 19 +- examples/cvector-generator/mean."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "6 +- examples/batched/batched.cpp | 17 +- examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp | 18 +- examples/cvector-generator/cvector-generator.cpp | 19 +- examples/cvector-generator/mean.hpp | 2 +- examples/cvector-generator/pca.hpp | 2 +- examples/embedding/embedding.cpp | 15 +- examples/eval-callback/eval-callback.cpp | 13 +- examples/export-lora/export-lora.cpp | 29 +- examples/gbnf-validator/gbnf-validator.cpp | 11 +- examples/gguf-hash/gguf-hash.cpp | 1 + examples/gguf-split/gguf-split.cpp | 17 +- examples/gguf-split/tests.sh | 10 +- examples/gguf/gguf.cpp | 16 +- examples/gritlm/gritlm.cpp | 25 +- examples/imatrix/imatrix.cpp | 25 +- examples/infill/infill.cpp | 47 +- examples/llama-bench/llama-bench.cpp | 67 +- examples/llama.android/llama/build.gradle.kts | 1 + examples/llama.android/llama/src/main/cpp/llama-android.cpp | 17 +- examples/llama.android/llama/src/main/java/android/llama/cpp/LLamaAndroid.kt | 5 +- examples/llama.swiftui/llama.cpp.swift/LibLlama.swift | 6 +- examples/llava/CMakeLists.txt | 7 + examples/llava/clip.cpp | 328 ++- examples/llava/clip.h | 10 +- examples/llava/llava-cli.cpp | 15 +- examples/llava/llava.cpp | 41 +- examples/llava/minicpmv-cli.cpp | 12 +- examples/llava/qwen2_vl_surgery.py | 165 ++ examples/llava/qwen2vl-cli.cpp | 584 ++++ examples/lookahead/lookahead.cpp | 13 +- examples/lookup/lookup-create.cpp | 13 +- examples/lookup/lookup-stats.cpp | 10 +- examples/lookup/lookup.cpp | 11 +- examples/main/README."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "-cli.cpp | 584 ++++ examples/lookahead/lookahead.cpp | 13 +- examples/lookup/lookup-create.cpp | 13 +- examples/lookup/lookup-stats.cpp | 10 +- examples/lookup/lookup.cpp | 11 +- examples/main/README.md | 5 - examples/main/main.cpp | 104 +- examples/parallel/parallel.cpp mples/pas skey/passkey.cpp | 12 +- examples/perplexity/perplexity.cpp | 60 +- examples/quantize-stats/quantize-stats.cpp | 26 +- examples/quantize/README.md | 2 +- examples/quantize/tests.sh | 4 +- examples/retrieval/retrieval.cpp | 20 +- examples/rpc/rpc-server.cpp | 12 + examples/run/CMakeLists.txt | 4 +- examples/run/README.md | 48 +- examples/run/linenoise.cpp/LICENSE | 26 + examples/run/linenoise.cpp/linenoise.cpp | 1350 +++++++++ examples/run/linenoise.cpp/linenoise.h | 128 + examples/run/run.cpp | 996 +++++-- examples/save-load-state/save-load-state.cpp | 29 +- examples/server/CMakeLists.txt | 3 +- examples/server/README.md | 401 ++- examples/server/bench/README.md | 6 +- examples/server/bench/bench.py | 30 +- examples/server/bench/script.js | 18 +- examples/server/httplib.h | 1704 +++++++++--- examples/server/public/index.html | 351 --- examples/server/public/index.html.gz | Bin 0 -> 1206492 bytes examples/server/public_legacy/index-new.html | 1 - examples/server/public_legacy/index.html | 2 - examples/server/server.cpp | 1012 +++++-- examples/server/tests/README.md | 6 + examples/server/tests/requirements.txt | 1 + examples/server/tests/unit/test_basic."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "erver/public_legacy/index.html | 2 - examples/server/server.cpp | 1012 +++++-- examples/server/tests/README.md | 6 + examples/server/tests/requirements.txt | 1 + examples/server/tests/unit/test_basic.py | 18 + examples/server/tests/unit/test_chat_completion.py | 100 +- examples/server/tests/unit/test_completion.py | 186 +- examples/server/tests/unit/test_embedding.py | 150 +- examples/server/tests/unit/test_infill.py | 2 +- examples/server/tests/unit/test_lora.py | 93 +- examples/server/tests/unit/test_rerank.py | 23 + examples/server/tests/unit/test_speculative.py | 10 +- examples/server/tests/utils.py | 49 +- examples/server/themes/buttons-top/index.html | 2 - examples/server/themes/wild/index.html | 2 - examples/server/utils.hpp | 331 ++- examples/server/webui/index.html | i/package-lock.json | 526 ++++ examples/server/webui/package.json | 9 +- examples/server/webui/public/demo-conversation.json | 33 + examples/server/webui/src/completion.js | 225 -- examples/server/webui/src/highlight-config.js | 60 + examples/server/webui/src/katex-gpt.js | 66 + examples/server/webui/src/main.js | 216 +- examples/server/webui/src/styles.css | 26 - examples/server/webui/src/styles.scss | 48 + examples/server/webui/vite.config.js | 61 +- examples/simple-chat/simple-chat.cpp | 26 +- examples/simple/simple.cpp | 17 +- examples/speculative-simple/speculative-simple.cpp | 20 +- examples/speculative/speculative.cpp | 45 +- examples/tokenize/tokenize.cpp | 24 +- examples/tts/CMakeLists."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "amples/simple/simple.cpp | 17 +- examples/speculative-simple/speculative-simple.cpp | 20 +- examples/speculative/speculative.cpp | 45 +- examples/tokenize/tokenize.cpp | 24 +- examples/tts/CMakeLists.txt | 5 + examples/tts/README.md | 117 + examples/tts/convert_pt_to_hf.py | 180 ++ examples/tts/tts-outetts.py | 299 ++ examples/tts/tts.cpp | 973 +++++++ ggml/CMakeLists.txt | 49 +- ggml/include/ggml-backend.h | 3 + ggml/include/ggml-cpp.h | 1 + ggml/include/ggml-opencl.h | 26 + ggml/include/ggml.h | 242 +- ggml/include/gguf.h | 202 ++ ggml/src/CMakeLists.txt | 15 +- ggml/src/ggml-alloc.c | 6 +- ggml/src/ggml-backend-impl.h | 1 - ggml/src/ggml-backend-reg.cpp | 165 +- ggml/src/ggml-backend.cpp | 7 +- ggml/src/ggml-cann/ggml-cann.cpp | 9 + ggml/src/ggml-common.h | 2 +- ggml/src/ggml-cpu/CMakeLists.txt | 168 +- ggml/src/ggml-cpu/amx/amx.cpp | 2 +- ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp | 129 +- ggml/src/ggml-cpu/ggml-cpu-quants.c | 89 +- ggml/src/ggml-cpu/ggml-cpu.c | 682 ++++- ggml/src/ggml-cpu/ggml-cpu.cpp | 21 +- ggml/src/ggml-cpu/llamafile/sgemm.cpp | 1469 +++++++--- ggml/src/ggml-cpu/llamafile/sgemm.h | 4 +- ggml/src/ggml-cuda/common.cuh | 70 +- ggml/src/ggml-cuda/concat.cu | 53 +- ggml/src/ggml-cuda/convert.cu | 8 +- ggml/src/ggml-cuda/cross-entropy-loss.cu | 181 +- ggml/src/ggml-cuda/fattn.cu | 2 +- ggml/src/ggml-cuda/getrows.cu | 159 +- ggml/src/ggml-cuda/getrows.cuh | 3 + ggml/src/ggml-cuda/ggml-cuda.cu | 459 ++-- ggml/src/ggml-cuda/gla.cu | 93 + ggml/src/ggml-cuda/gla."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "gml-cuda/fattn.cu | 2 +- ggml/src/ggml-cuda/getrows.cu | 159 +- ggml/src/ggml-cuda/getrows.cuh | 3 + ggml/src/ggml-cuda/ggml-cuda.cu | 459 ++-- ggml/src/ggml-cuda/gla.cu | 93 + ggml/src/ggml-cuda/gla.cuh | 3 + ggml/src/ggml-cuda/mma.cuh | 8 +- ggml/src/ggml-cuda/mmq.cu | 10 +- ggml/src/ggml-cuda/mmq.cuh | 26 +- ggml/src/ggml-cuda/mmv.cu | 116 +- ggml/src/ggml-cuda/mmvq.cu | 2 +- ggml/src/ggml-cuda/norm.cu | 155 +- ggml/src/ggml-cuda/norm.cuh | 2 + ggml/src/ggml-cuda/out-prod.cu | 38 +- ggml/src/ggml-cuda/rope.cu | 371 ++- ggml/src/ggml-cuda/rope.cuh | 2 + ggml/src/ggml-cuda/softmax.cu | 134 +- ggml/src/ggml-cuda/softmax.cuh | 2 + ggml/src/ggml-cuda/sum.cu | 2 - ggml/src/ggml-cuda/unary.cu | 36 + ggml/src/ggml-cuda/unary.cuh | 3 + ggml/src/ggml-cuda/vendors/cuda.h | 1 + ggml/src/ggml-cuda/vendors/hip.h | 3 + ggml/src/ggml-cuda/vendors/musa.h | 3 + ggml/src/ggml-cuda/wkv6.cu | 4 +- ggml/src/ggml-hip/CMakeLists.txt | 4 +- ggml/src/ggml-impl.h | 19 +- ggml/src/ggml-kompute/ggml-kompute.cpp | 12 +- ggml/src/ggml-metal/CMakeLists.txt | 16 + ggml/src/ggml-metal/ggml-metal.m | 20 +- ggml/src/ggml-metal/ggml-metal.metal | 31 +- ggml/src/ggml-opencl/CMakeLists.txt | 147 + ggml/src/ggml-opencl/ggml-opencl.cpp | 4004 +++++++++++++++++++++++++++ ggml/src/ggml-opencl/kernels/embed_kernel.py | 26 + ggml/src/ggml-opencl/kernels/ggml-opencl.cl | 2683 ++++++++++++++++++ ggml/src/ggml-opencl/kernels/ggml-opencl_cvt.cl c/ggml-opencl /kernels/ggml-opencl_gemv_noshuffle."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "/kernels/embed_kernel.py | 26 + ggml/src/ggml-opencl/kernels/ggml-opencl.cl | 2683 ++++++++++++++++++ ggml/src/ggml-opencl/kernels/ggml-opencl_cvt.cl c/ggml-opencl /kernels/ggml-opencl_gemv_noshuffle.cl | 265 ++ ggml/src/ggml-opencl/kernels/ggml-opencl_gemv_noshuffle_general.cl | 271 ++ ggml/src/ggml-opencl/kernels/ggml-opencl_mm.cl | 1225 +++++++++ ggml/src/ggml-opencl/kernels/ggml-opencl_mul_mat_Ab_Bi_8x4.cl | 130 + ggml/src/ggml-opencl/kernels/ggml-opencl_transpose_16.cl | 32 + ggml/src/ggml-opencl/kernels/ggml-opencl_transpose_32.cl | 25 + ggml/src/ggml-opencl/kernels/ggml-opencl_transpose_32_16.cl | 35 + ggml/src/ggml-rpc/ggml-rpc.cpp | 202 +- ggml/src/ggml-sycl/backend.hpp | 1 + ggml/src/ggml-sycl/common.cpp | 25 +- ggml/src/ggml-sycl/common.hpp | 19 + ggml/src/ggml-sycl/concat.cpp | 9 +- ggml/src/ggml-sycl/concat.hpp | 3 +- ggml/src/ggml-sycl/conv.cpp | 5 +- ggml/src/ggml-sycl/conv.hpp | 3 +- ggml/src/ggml-sycl/convert.cpp | 2 +- ggml/src/ggml-sycl/dmmv.cpp | 10 +- ggml/src/ggml-sycl/dpct/helper.hpp | 137 +- ggml/src/ggml-sycl/element_wise.cpp | 237 +- ggml/src/ggml-sycl/element_wise.hpp | 48 +- ggml/src/ggml-sycl/gemm.hpp | 8 +- ggml/src/ggml-sycl/ggml-sycl.cpp | 535 ++-- ggml/src/ggml-sycl/gla.cpp | 105 + ggml/src/ggml-sycl/gla.hpp | 8 + ggml/src/ggml-sycl/im2col.cpp | 5 +- ggml/src/ggml-sycl/mmq.cpp | 12 +- ggml/src/ggml-sycl/mmvq.cpp | 26 +- ggml/src/ggml-sycl/norm.cpp | 7 +- ggml/src/ggml-sycl/outprod.cpp | 6 +- ggml/src/ggml-sycl/outprod."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "/ggml-sycl/im2col.cpp | 5 +- ggml/src/ggml-sycl/mmq.cpp | 12 +- ggml/src/ggml-sycl/mmvq.cpp | 26 +- ggml/src/ggml-sycl/norm.cpp | 7 +- ggml/src/ggml-sycl/outprod.cpp | 6 +- ggml/src/ggml-sycl/outprod.hpp | 3 +- ggml/src/ggml-sycl/rope.cpp | 7 +- ggml/src/ggml-sycl/softmax.cpp | 14 +- ggml/src/ggml-sycl/tsembd.cpp | 6 +- ggml/src/ggml-sycl/tsembd.hpp | 3 +- ggml/src/ggml-sycl/wkv6.cpp | 17 +- ggml/src/ggml-sycl/wkv6.hpp | 3 +- ggml/src/ggml-threading.h | 6 +- ggml/src/ggml-vulkan/CMakeLists.txt | 76 +- ggml/src/ggml-vulkan/cmake/host-toolchain.cmake.in | 15 + ggml/src/ggml-vulkan/ggml-vulkan.cpp | 866 ++++-- ggml/src/ggml-vulkan/vulkan | 6 +- ggml/src/ggml-vulkan/vulkan-shaders/acc.comp | 4 +- ggml/src/ggml-vulkan/vulkan-shaders/add.comp | 2 +- ggml/src/ggml-vulkan/vulkan-shaders/clamp.comp | 4 +- ggml/src/ggml-vulkan/vulkan-shaders/concat.comp | 6 +- ggml/src/ggml-vulkan/vulkan-shaders/contig_copy.comp | 8 +- ggml/src/ggml-vulkan/vulkan-shaders/copy.comp | 4 +- ggml/src/ggml-vulkan/vulkan-shaders/copy_from_quant.comp | 51 + ggml/src/ggml-vulkan/vulkan-shaders/copy_to_quant.comp | 237 ++ ggml/src/ggml-vulkan/vulkan-shaders/cos.comp | 4 +- ggml/src/ggml-vulkan/vulkan-shaders/dequant_funcs.comp | 58 +- ggml/src/ggml-vulkan/vulkan-shaders/dequant_funcs_cm2.comp | 154 +- ggml/src/ggml-vulkan/vulkan-shaders/dequant_q4_k.comp | 64 +- ggml/src/ggml-vulkan/vulkan-shaders/dequant_q5_k.comp | 68 +- ggml/src/ggml-vulkan/vulkan-shaders/div."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ers/dequant_funcs_cm2.comp | 154 +- ggml/src/ggml-vulkan/vulkan-shaders/dequant_q4_k.comp | 64 +- ggml/src/ggml-vulkan/vulkan-shaders/dequant_q5_k.comp | 68 +- ggml/src/ggml-vulkan/vulkan-shaders/div.comp | 2 +- ggml/src/ggml-vulkan/vulkan-shaders/flash_attn_cm2.comp | 22 +- ggml/src/ggml-vulkan/vulkan-shaders/generic_binary_head.comp | 6 +- ggml/src/ggml-vulkan/vulkan-shaders/generic_unary_head.comp | 25 +- ggml/src/ggml-vulkan/vulkan-shaders/get_rows.comp | 6 +- ggml/src/ggml-vulkan/vulkan-shaders/get_rows_quant.comp | 2 + ggml/src/ggml-vulkan/vulkan-shaders/im2col.comp | 74 +- ggml/src/ggml-vulkan/vulkan-shaders/mul.comp | 2 +- ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec.comp | 144 +- ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_base.comp | 35 +- ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q2_k.comp | 185 +- ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q3_k.comp | 173 +- ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q4_k.comp | 203 +- ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q5_k.comp | 258 +- ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q6_k.comp | 166 +- ggml/src/ggml-vulkan/vulkan-shaders/mul_mm_cm2.comp | 35 +- ggml/src/ggml-vulkan/vulkan-shaders/pad.comp | 2 +- ggml/src/ggml-vulkan/vulkan-shaders/repeat.comp | 2 +- ggml/src/ggml-vulkan/vulkan-shaders/rope_head.comp | 5 + ggml/src/ggml-vulkan/vulkan-shaders/scale.comp | 2 +- ggml/src/ggml-vulkan/vulkan-shaders/sin.comp | 4 +- ggml/src/ggml-vulkan/vulkan-shaders/soft_max."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "/ggml-vulkan/vulkan-shaders/rope_head.comp | 5 + ggml/src/ggml-vulkan/vulkan-shaders/scale.comp | 2 +- ggml/src/ggml-vulkan/vulkan-shaders/sin.comp | 4 +- ggml/src/ggml-vulkan/vulkan-shaders/soft_max.comp | 3 +- ggml/src/ggml-vulkan/vulkan-shaders/square.comp | 4 +- ggml/src/ggml-vulkan/vulkan-shaders/test_coopmat_support.comp | 7 + ggml/src/ggml-vulkan/vulkan-shaders/types.comp | 15 +- ggml/src/ggml-vulkan/vulkan-shaders/upscale.comp | 4 +- ggml/src/ggml-vulkan/vulkan-shaders/vulkan-shaders-gen.cpp | 31 +- ggml | 87 + ggml/src/ggml.c | 1861 +++---------- ggml/src/gguf.cpp | 1329 +++++++++ gguf-py/README.md | 10 +- gguf-py/gguf/constants.py | 420 ++- gguf-py/gguf/gguf_reader.py | 9 +- gguf-py/gguf/gguf_writer.py | 37 +- gguf-py/{ => gguf}/scripts/__init__.py | 0 gguf-py/{ => gguf}/scripts/gguf_convert_endian.py | 4 +- gguf-py/{ => gguf}/scripts/gguf_dump.py | 4 +- gguf-py/{ => gguf}/scripts/gguf_hash.py | 4 +- gguf-py/{ => gguf}/scripts/gguf_new_metadata.py | 4 +- gguf-py/{ => gguf}/scripts/gguf_set_metadata.py | 4 +- gguf-py/gguf/tensor_mapping.py | 144 +- gguf-py/pyproject.toml | 11 +- gguf-py/tests/test_quants.py | 2 +- include/llama-cpp.h | 7 +- include/llama.h | 240 +- media/llama-leader.jpeg | Bin 199945 -> 0 bytes models/ggml-vocab-deepseek-r1-qwen.gguf.inp | 112 + models/ggml-vocab-deepseek-r1-qwen.gguf.out | 46 + scripts/compare-commits.sh | 10 +- scripts/compare-llama-bench.py | 22 +- scripts/get_hf_chat_template.py | 77 + scripts/hf.sh | 2 +- scripts/sync-ggml-am."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "/ggml-vocab-deepseek-r1-qwen.gguf.out | 46 + scripts/compare-commits.sh | 10 +- scripts/compare-llama-bench.py | 22 +- scripts/get_hf_chat_template.py | 77 + scripts/hf.sh | 2 +- scripts/sync-ggml-am.sh | 10 +- scripts/sync-ggml.last | 2 +- scripts/sync-ggml.sh | 3 + src/CMakeLists.txt | 23 +- src/llama-adapter.cpp | 347 +++ src/llama-adapter.h | 74 + src/llama-arch.cpp | 1489 ++++++++++ src/llama-arch.h | 402 +++ src/llama-batch.cpp | 368 +++ src/llama-batch.h | 88 + src/llama-chat.cpp | 578 ++++ src/llama-chat.h | 52 + src/llama-context.cpp | 1775 ++++++++++++ src/llama-context.h | 128 + src/llama-cparams.cpp | 1 + src/llama-cparams.h 37 + src/llama-grammar.cpp | 39 +- src/llama-grammar.h | 11 +- src/llama-hparams.cpp | 71 + src/llama-hparams.h | 139 + src/llama-impl.cpp | 167 ++ src/llama-impl.h | 152 +- src/llama-kv-cache.cpp | 718 +++++ src/llama-kv-cache.h | 218 ++ src/llama-mmap.cpp | 590 ++++ src/llama-mmap.h | 67 + src/llama-model-loader.cpp | 1124 ++++++++ src/llama-model-loader.h | 167 ++ src/llama-model.cpp | 3999 +++++++++++++++++++++++++++ src/llama-model.h | 370 +++ src/llama-quant.cpp | 934 +++++++ src/llama-quant.h | 1 + src/llama-sampling.cpp | 304 ++- src/llama-sampling.h | 22 +- src/llama-vocab.cpp | 2412 ++++++++++++---- src/llama-vocab.h | 239 +- src/llama."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "rc/llama-quant.cpp | 934 +++++++ src/llama-quant.h | 1 + src/llama-sampling.cpp | 304 ++- src/llama-sampling.h | 22 +- src/llama-vocab.cpp | 2412 ++++++++++++---- src/llama-vocab.h | 239 +- src/llama.cpp | 26329 +++++++++++++++++++++++++++++++++++++++++++++++--------------------------- -------------------------------------------------------------------------------- --------------------- src/unicode.cpp | 113 +- src/unicode.h | 19 +- tests/CMakeLists.txt | 76 +- tests/test-autorelease.cpp | 6 +- tests/test-backend-ops.cpp | 459 +++- tests/test-chat-template.cpp | 422 ++- tests/test-gguf.cpp | 1338 +++++++++ tests/test-grammar-integration.cpp | 9 +- tests/test-llama-grammar.cpp | 6 +- tests/test-lora-conversion-inference.sh | 6 +- tests/test-model-load-cancel.cpp | 2 +- tests/test-rope.cpp | 81 +- tests/test-sampling.cpp | 3 +- tests/test-tokenizer-0.cpp | 8 +- tests/test-tokenizer-1-bpe.cpp | 16 +- tests/test-tokenizer-1-spm.cpp | 14 +- tests/test-tokenizer-random.py 4 +- 375 files changed, 59330 insertions(+), 29014 deletions(-) create mode 100644 .devops/cpu.Dockerfile create mode 100644 .devops/cuda.Dockerfile delete mode 100644 .devops/full-cuda.Dockerfile delete mode 100644 .devops/full-musa.Dockerfile delete mode 100644 .devops/full-rocm.Dockerfile delete mode 100644 .devops/full.Dockerfile create mode 100644 .devops/intel.Dockerfile delete mode 100644 .devops/llama-cli-cuda.Dockerfile delete mode 100644 .devops/llama-cli-intel.Dockerfile delete mode 100644 ."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "0644 .devops/full.Dockerfile create mode 100644 .devops/intel.Dockerfile delete mode 100644 .devops/llama-cli-cuda.Dockerfile delete mode 100644 .devops/llama-cli-intel.Dockerfile delete mode 100644 .devops/llama-cli-musa.Dockerfile delete mode 100644 .devops/llama-cli-rocm.Dockerfile delete mode 100644 .devops/llama-cli-vulkan.Dockerfile delete mode 100644 .devops/llama-cli.Dockerfile delete mode 100644 .devops/llama-server-cuda.Dockerfile delete mode 100644 .devops/llama-server-intel.Dockerfile delete mode 100644 .devops/llama-server-musa.Dockerfile delete mode 100644 .devops/llama-server-rocm.Dockerfile delete mode 100644 .devops/llama-server-vulkan.Dockerfile delete mode 100644 .devops/llama-server.Dockerfile create mode 100644 .devops/musa.Dockerfile create mode 100644 .devops/rocm.Dockerfile create mode 100644 .devops/vulkan.Dockerfile create mode 100644 common/chat-template.hpp create mode 100644 common/minja.hpp create mode 100644 docs/cuda-fedora.md create mode 100644 examples/llava/qwen2_vl_surgery.py create mode 100644 examples/llava/qwen2vl-cli.cpp create mode 100644 examples/run/linenoise.cpp/LICENSE create mode 100644 examples/run/linenoise.cpp/linenoise.cpp create mode 100644 examples/run/linenoise.cpp/linenoise.h delete mode 100644 examples/server/public/index.html create mode 100644 examples/server/public/index.html.gz create mode 100644 examples/server/webui/public/demo-conversation.json delete mode 100644 examples/server/webui/src/completion."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ublic/index.html create mode 100644 examples/server/public/index.html.gz create mode 100644 examples/server/webui/public/demo-conversation.json delete mode 100644 examples/server/webui/src/completion.js create mode 100644 examples/server/webui/src/highlight-config.js create mode 100644 examples/server/webui/src/katex-gpt.js delete mode 100644 examples/server/webui/src/styles.css create mode 100644 examples/server/webui/src/styles.scss create mode 100644 examples/tts/CMakeLists.txt create mode 100644 examples/tts/README.md create mode 100644 examples/tts/convert_pt_to_hf.py create mode 100644 examples/tts/tts-outetts.py create mode 100644 examples/tts/tts.cpp create mode 100644 ggml/include/ggml-opencl.h create mode 100644 ggml/include/gguf.h create mode 100644 ggml/src/ggml-cuda/gla.cu create mode 100644 ggml/src/ggml-cuda/gla.cuh create mode 100644 ggml/src/ggml-opencl/CMakeLists.txt create mode 100644 ggml/src/ggml-opencl/ggml-opencl.cpp create mode 100644 ggml/src/ggml-opencl/kernels/embed_kernel.py create mode 100644 ggml/src/ggml-opencl/kernels/ggml-opencl.cl create mode 100644 ggml/src/ggml-opencl/kernels/ggml-opencl_cvt.cl create mode 100644 ggml/src/ggml-opencl/kernels/ggml-opencl_gemv_noshuffle.cl create mode 100644 ggml/src/ggml-opencl/kernels/ggml-opencl_gemv_noshuffle_gene ral.cl create mode 100644 ggml/src/ggml-opencl/kernels/ggml-opencl_mm.cl create mode 100644 ggml/src/ggml-opencl/kernels/ggml-opencl_mul_mat_Ab_Bi_8x4."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "l-opencl/kernels/ggml-opencl_gemv_noshuffle_gene ral.cl create mode 100644 ggml/src/ggml-opencl/kernels/ggml-opencl_mm.cl create mode 100644 ggml/src/ggml-opencl/kernels/ggml-opencl_mul_mat_Ab_Bi_8x4.c l create mode 100644 ggml/src/ggml-opencl/kernels/ggml-opencl_transpose_16.cl create mode 100644 ggml/src/ggml-opencl/kernels/ggml-opencl_transpose_32.cl create mode 100644 ggml/src/ggml-opencl/kernels/ggml-opencl_transpose_32_16.cl create mode 100644 ggml/src/ggml-sycl/gla.cpp create mode 100644 ggml/src/ggml-sycl/gla.hpp create mode 100644 ggml/src/ggml-vulkan/cmake/host-toolchain.cmake.in create mode 100644 ggml/src/ggml-vulkan/vulkan-shaders/copy_from_quant.compader s/copy_to_quant.comp create mode 100644 ggml/src/ggml-vulkan/vulkan-shaders/test_coopmat_support.com p create mode 100644 ggml/src/ggml-vulkan/vulkan-shaders/wkv6.comp create mode 100644 ggml/src/gguf.cpp rename gguf-py/{ => gguf}/scripts/__init__.py (100%) rename gguf-py/{ => gguf}/scripts/gguf_convert_endian.py (97%) rename gguf-py/{ => gguf}/scripts/gguf_dump.py (99%) rename gguf-py/{ => gguf}/scripts/gguf_hash.py (97%) rename gguf-py/{ => gguf}/scripts/gguf_new_metadata.py (98%) rename gguf-py/{ => gguf}/scripts/gguf_set_metadata.py (97%) delete mode 100644 media/llama-leader.jpeg create mode 100644 models/ggml-vocab-deepseek-r1-qwen.gguf.inp create mode 100644 models/ggml-vocab-deepseek-r1-qwen.gguf.out create mode 100755 scripts/get_hf_chat_template.py create mode 100644 src/llama-adapter."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "0644 models/ggml-vocab-deepseek-r1-qwen.gguf.inp create mode 100644 models/ggml-vocab-deepseek-r1-qwen.gguf.out create mode 100755 scripts/get_hf_chat_template.py create mode 100644 src/llama-adapter.cpp create mode 100644 src/llama-adapter.h create mode 100644 src/llama-arch.cpp create mode 100644 src/llama-arch.h create mode 100644 src/llama-batch.cpp create mode 100644 src/llama-batch.h create mode 100644 src/llama-chat.cpp create mode 100644 src/llama-chat.h create mode 100644 src/llama-context.cpp create mode 100644 src/llama-context.h create mode 100644 src/llama-cparams.cpp create mode 100644 src/llama-cparams.h create mode 100644 src/llama-hparams.cpp create mode 100644 src/llama-hparams.h create mode 100644 src/llama-impl.cpp create mode 100644 src/llama-kv-cache.cpp create mode 100644 src/llama-kv-cache.h create mode 100644 src/llama-mmap.cpp create mode 100644 src/llama-mmap.h create mode 100644 src/llama-model-loader.cpp create mode 100644 src/llama-model-loader.h create mode 100644 src/llama-model.cpp create mode 100644 src/llama-model.h create mode 100644 src/llama-quant.cpp create mode 100644 src/llama-quant.h create mode 100644 tests/test-gguf.cpp (torch) ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ (torch) ljubomir@gigul2(797966.llama.cpp:0):~/llama."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "lama-quant.cpp create mode 100644 src/llama-quant.h create mode 100644 tests/test-gguf.cpp (torch) ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ (torch) ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ cmake -B build -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF -- CMAKE_SYSTEM_PROCESSOR: x86_64 -- Including CPU backend -- x86 detected -- Adding CPU backend variant ggml-cpu: -march=native -- Configuring done -- Generating done -- Build files have been written to: /home/ljubomir/llama.cpp/build (torch) ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ cmake --build build --c onfig Release Consolidate compiler generated dependencies of target ggml-base [ 1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o [ 1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o [ 2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o [ 2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o [ 3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp. o [ 3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o [ 4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o [ 4%] Linking CXX shared library libggml-base.so [ 4%] Built target ggml-base Consolidate compiler generated dependencies of target ggml-cpu [ 5%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "shared library libggml-base.so [ 4%] Built target ggml-base Consolidate compiler generated dependencies of target ggml-cpu [ 5%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o [ 5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cp p.o [ 6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aa rch64.cpp.o [ 7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quan ts.c.o [ 7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-tr aits.cpp.o [ 8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp .o [ 8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp .o [ 9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/s gemm.cpp.o [ 9%] Linking CXX shared library libggml-cpu.so [ 9%] Built target ggml-cpu Consolidate compiler generated dependencies of target ggml [ 10%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o [ 10%] Linking CXX shared library libggml.so [ 10%] Built target ggml Consolidate compiler generated dependencies of target llama [ 11%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o [ 11%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o [ 12%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o [ 12%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o [ 13%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o [ 12%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o [ 13%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o [ 13%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o [ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o [ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o [ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o [ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o [ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o [ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o [ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o [ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o [ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o [ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o [ 19%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o [ 19%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o [ 20%] Linking CXX shared library libllama.so [ 20%] Built target llama [ 20%] Generating build details from Git -- Found Git: /usr/bin/git (found version \"2.34.1\") Consolidate compiler generated dependencies of target build_info [ 21%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "rom Git -- Found Git: /usr/bin/git (found version \"2.34.1\") Consolidate compiler generated dependencies of target build_info [ 21%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o [ 21%] Built target build_info Consolidate compiler generated dependencies of target common [ 21%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o [ 22%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o [ 22%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o [ 23%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.c pp.o [ 23%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o [ 24%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o [ 24%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o [ 25%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o [ 25%] Linking CXX static library libcommon.a [ 25%] Built target common Consolidate compiler generated dependencies of target test-tokenizer-0 [ 26%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer- 0.cpp.o [ 26%] Linking CXX executable ../bin/test-tokenizer-0 [ 26%] Built target test-tokenizer-0 Consolidate compiler generated dependencies of target test-sampling [ 26%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp. o [ 27%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o [ 27%] Linking CXX executable .."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "mpling [ 26%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp. o [ 27%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o [ 27%] Linking CXX executable ../bin/test-sampling [ 27%] Built target test-sampling Consolidate compiler generated dependencies of target test-grammar-parser [ 27%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar -parser.cpp.o [ 28%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cp p.o [ 28%] Linking CXX executable ../bin/test-grammar-parser [ 28%] Built target test-grammar-parser Consolidate compiler generated dependencies of target test-grammar-integration [ 29%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-gr ammar-integration.cpp.o [ 29%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-mod el.cpp.o [ 30%] Linking CXX executable ../bin/test-grammar-integration [ 30%] Built target test-grammar-integration Consolidate compiler generated dependencies of target test-llama-grammar [ 30%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-gr ammar.cpp.o [ 31%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp .o [ 31%] Linking CXX executable ../bin/test-llama-grammar [ 31%] Built target test-llama-grammar Consolidate compiler generated dependencies of target test-json-schema-to-gramma r [ 32%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ammar [ 31%] Built target test-llama-grammar Consolidate compiler generated dependencies of target test-json-schema-to-gramma r [ 32%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test -json-schema-to-grammar.cpp.o [ 32%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get- model.cpp.o [ 33%] Linking CXX executable ../bin/test-json-schema-to-grammar [ 33%] Built target test-json-schema-to-grammar Consolidate compiler generated dependencies of target test-tokenizer-1-bpe [ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokeni zer-1-bpe.cpp.o [ 34%] Linking CXX executable ../bin/test-tokenizer-1-bpe [ 34%] Built target test-tokenizer-1-bpe Consolidate compiler generated dependencies of target test-tokenizer-1-spm [ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokeni zer-1-spm.cpp.o [ 35%] Linking CXX executable ../bin/test-tokenizer-1-spm [ 35%] Built target test-tokenizer-1-spm Consolidate compiler generated dependencies of target test-log [ 36%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o [ 36%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o [ 37%] Linking CXX executable ../bin/test-log [ 37%] Built target test-log Consolidate compiler generated dependencies of target test-arg-parser [ 38%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser. cpp.o [ 38%] Building CXX object tests/CMakeFiles/test-arg-parser."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ler generated dependencies of target test-arg-parser [ 38%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser. cpp.o [ 38%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o [ 39%] Linking CXX executable ../bin/test-arg-parser [ 39%] Built target test-arg-parser Consolidate compiler generated dependencies of target test-chat-template [ 40%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-tem plate.cpp.o [ 40%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp .o [ 41%] Linking CXX executable ../bin/test-chat-template [ 41%] Built target test-chat-template [ 41%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o [ 42%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o [ 42%] Linking CXX executable ../bin/test-gguf [ 42%] Built target test-gguf Consolidate compiler generated dependencies of target test-backend-ops [ 43%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-op s.cpp.o [ 43%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o [ 44%] Linking CXX executable ../bin/test-backend-ops [ 44%] Built target test-backend-ops Consolidate compiler generated dependencies of target test-model-load-cancel [ 44%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-mode l-load-cancel.cpp.o [ 45%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model .cpp."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "-load-cancel [ 44%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-mode l-load-cancel.cpp.o [ 45%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model .cpp.o [ 45%] Linking CXX executable ../bin/test-model-load-cancel [ 45%] Built target test-model-load-cancel Consolidate compiler generated dependencies of target test-autorelease [ 45%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autoreleas e.cpp.o [ 46%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o [ 46%] Linking CXX executable ../bin/test-autorelease [ 46%] Built target test-autorelease Consolidate compiler generated dependencies of target test-barrier [ 46%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o [ 47%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o [ 47%] Linking CXX executable ../bin/test-barrier [ 47%] Built target test-barrier Consolidate compiler generated dependencies of target test-quantize-fns [ 48%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize- fns.cpp.o [ 48%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp. o [ 49%] Linking CXX executable ../bin/test-quantize-fns [ 49%] Built target test-quantize-fns Consolidate compiler generated dependencies of target test-quantize-perf [ 49%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize -perf.cpp."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "49%] Built target test-quantize-fns Consolidate compiler generated dependencies of target test-quantize-perf [ 49%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize -perf.cpp.o [ 50%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp .o [ 50%] Linking CXX executable ../bin/test-quantize-perf [ 50%] Built target test-quantize-perf Consolidate compiler generated dependencies of target test-rope [ 51%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o [ 51%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o [ 52%] Linking CXX executable ../bin/test-rope [ 52%] Built target test-rope Consolidate compiler generated dependencies of target test-c [ 53%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o [ 53%] Linking C executable ../bin/test-c [ 53%] Built target test-c Consolidate compiler generated dependencies of target llama-batched-bench [ 53%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench .dir/batched-bench.cpp.o [ 54%] Linking CXX executable ../../bin/llama-batched-bench [ 54%] Built target llama-batched-bench Consolidate compiler generated dependencies of target llama-batched [ 54%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched .cpp.o [ 55%] Linking CXX executable ../.."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ed-bench Consolidate compiler generated dependencies of target llama-batched [ 54%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched .cpp.o [ 55%] Linking CXX executable ../../bin/llama-batched [ 55%] Built target llama-batched Consolidate compiler generated dependencies of target llama-embedding [ 55%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/emb edding.cpp.o [ 56%] Linking CXX executable ../../bin/llama-embedding [ 56%] Built target llama-embedding Consolidate compiler generated dependencies of target llama-eval-callback [ 56%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback .dir/eval-callback.cpp.o [ 57%] Linking CXX executable ../../bin/llama-eval-callback [ 57%] Built target llama-eval-callback Consolidate compiler generated dependencies of target llama-gbnf-validator [ 57%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validat or.dir/gbnf-validator.cpp.o [ 58%] Linking CXX executable ../../bin/llama-gbnf-validator [ 58%] Built target llama-gbnf-validator Consolidate compiler generated dependencies of target sha256 [ 58%] Built target sha256 Consolidate compiler generated dependencies of target xxhash [ 59%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xx hash.c."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "encies of target sha256 [ 58%] Built target sha256 Consolidate compiler generated dependencies of target xxhash [ 59%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xx hash.c.o [ 59%] Built target xxhash Consolidate compiler generated dependencies of target sha1 [ 60%] Built target sha1 Consolidate compiler generated dependencies of target llama-gguf-hash [ 60%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/ggu f-hash.cpp.o [ 61%] Linking CXX executable ../../bin/llama-gguf-hash [ 61%] Built target llama-gguf-hash Consolidate compiler generated dependencies of target llama-gguf-split [ 61%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/g guf-split.cpp.o [ 62%] Linking CXX executable ../../bin/llama-gguf-split [ 62%] Built target llama-gguf-split Consolidate compiler generated dependencies of target llama-gguf [ 62%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o [ 63%] Linking CXX executable ../../bin/llama-gguf [ 63%] Built target llama-gguf Consolidate compiler generated dependencies of target llama-gritlm [ 63%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cp p.o [ 64%] Linking CXX executable ../../bin/llama-gritlm [ 64%] Built target llama-gritlm Consolidate compiler generated dependencies of target llama-imatrix [ 64%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix .cpp.o [ 65%] Linking CXX executable ../.."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "a-gritlm Consolidate compiler generated dependencies of target llama-imatrix [ 64%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix .cpp.o [ 65%] Linking CXX executable ../../bin/llama-imatrix [ 65%] Built target llama-imatrix Consolidate compiler generated dependencies of target llama-infill [ 65%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cp p.o [ 66%] Linking CXX executable ../../bin/llama-infill [ 66%] Built target llama-infill Consolidate compiler generated dependencies of target llama-bench [ 66%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama -bench.cpp.o [ 67%] Linking CXX executable ../../bin/llama-bench [ 67%] Built target llama-bench Consolidate compiler generated dependencies of target llama-lookahead [ 67%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/loo kahead.cpp.o [ 68%] Linking CXX executable ../../bin/llama-lookahead [ 68%] Built target llama-lookahead Consolidate compiler generated dependencies of target llama-lookup [ 68%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cp p.o [ 69%] Linking CXX executable ../../bin/llama-lookup [ 69%] Built target llama-lookup Consolidate compiler generated dependencies of target llama-lookup-create [ 69%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lo okup-create.cpp.o [ 70%] Linking CXX executable ../.."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ate compiler generated dependencies of target llama-lookup-create [ 69%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lo okup-create.cpp.o [ 70%] Linking CXX executable ../../bin/llama-lookup-create [ 70%] Built target llama-lookup-create Consolidate compiler generated dependencies of target llama-lookup-merge [ 70%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/loo kup-merge.cpp.o [ 71%] Linking CXX executable ../../bin/llama-lookup-merge [ 71%] Built target llama-lookup-merge Consolidate compiler generated dependencies of target llama-lookup-stats [ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/loo kup-stats.cpp.o [ 72%] Linking CXX executable ../../bin/llama-lookup-stats [ 72%] Built target llama-lookup-stats Consolidate compiler generated dependencies of target llama-cli [ 72%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o [ 73%] Linking CXX executable ../../bin/llama-cli [ 73%] Built target llama-cli Consolidate compiler generated dependencies of target llama-parallel [ 73%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/paral lel.cpp.o [ 74%] Linking CXX executable ../../bin/llama-parallel [ 74%] Built target llama-parallel Consolidate compiler generated dependencies of target llama-passkey [ 74%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey .cpp.o [ 75%] Linking CXX executable ../.."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "parallel Consolidate compiler generated dependencies of target llama-passkey [ 74%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey .cpp.o [ 75%] Linking CXX executable ../../bin/llama-passkey [ 75%] Built target llama-passkey Consolidate compiler generated dependencies of target llama-perplexity [ 75%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/p erplexity.cpp.o [ 76%] Linking CXX executable ../../bin/llama-perplexity [ 76%] Built target llama-perplexity Consolidate compiler generated dependencies of target llama-quantize [ 76%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quant ize.cpp.o [ 77%] Linking CXX executable ../../bin/llama-quantize [ 77%] Built target llama-quantize Consolidate compiler generated dependencies of target llama-retrieval [ 77%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/ret rieval.cpp.o [ 78%] Linking CXX executable ../../bin/llama-retrieval [ 78%] Built target llama-retrieval [ 79%] Generating index.html.gz.hpp Consolidate compiler generated dependencies of target llama-server [ 80%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cp p.o [ 80%] Linking CXX executable ../../bin/llama-server [ 80%] Built target llama-server Consolidate compiler generated dependencies of target llama-save-load-state [ 81%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-s tate.dir/save-load-state.cpp."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "llama-server Consolidate compiler generated dependencies of target llama-save-load-state [ 81%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-s tate.dir/save-load-state.cpp.o [ 81%] Linking CXX executable ../../bin/llama-save-load-state [ 81%] Built target llama-save-load-state Consolidate compiler generated dependencies of target llama-run [ 81%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o [ 82%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/l inenoise.cpp.o [ 82%] Linking CXX executable ../../bin/llama-run [ 82%] Built target llama-run Consolidate compiler generated dependencies of target llama-simple [ 83%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cp p.o [ 83%] Linking CXX executable ../../bin/llama-simple [ 83%] Built target llama-simple Consolidate compiler generated dependencies of target llama-simple-chat [ 84%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir /simple-chat.cpp.o [ 84%] Linking CXX executable ../../bin/llama-simple-chat [ 84%] Built target llama-simple-chat Consolidate compiler generated dependencies of target llama-speculative [ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir /speculative.cpp.o [ 85%] Linking CXX executable ../.."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "date compiler generated dependencies of target llama-speculative [ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir /speculative.cpp.o [ 85%] Linking CXX executable ../../bin/llama-speculative [ 85%] Built target llama-speculative Consolidate compiler generated dependencies of target llama-speculative-simple [ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculat ive-simple.dir/speculative-simple.cpp.o [ 86%] Linking CXX executable ../../bin/llama-speculative-simple [ 86%] Built target llama-speculative-simple Consolidate compiler generated dependencies of target llama-tokenize [ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/token ize.cpp.o [ 87%] Linking CXX executable ../../bin/llama-tokenize [ 87%] Built target llama-tokenize [ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o [ 88%] Linking CXX executable ../../bin/llama-tts [ 88%] Built target llama-tts [ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-d ocs.cpp.o [ 89%] Linking CXX executable ../../bin/llama-gen-docs [ 89%] Built target llama-gen-docs Consolidate compiler generated dependencies of target llama-convert-llama2c-to-g gml [ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-con vert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o [ 90%] Linking CXX executable ../.."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ama-convert-llama2c-to-g gml [ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-con vert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o [ 90%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml [ 90%] Built target llama-convert-llama2c-to-ggml Consolidate compiler generated dependencies of target llama-cvector-generator [ 90%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-g enerator.dir/cvector-generator.cpp.o [ 91%] Linking CXX executable ../../bin/llama-cvector-generator [ 91%] Built target llama-cvector-generator Consolidate compiler generated dependencies of target llama-export-lora [ 91%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir /export-lora.cpp.o [ 92%] Linking CXX executable ../../bin/llama-export-lora [ 92%] Built target llama-export-lora Consolidate compiler generated dependencies of target llama-quantize-stats [ 92%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-sta ts.dir/quantize-stats.cpp.o [ 93%] Linking CXX executable ../../bin/llama-quantize-stats [ 93%] Built target llama-quantize-stats Consolidate compiler generated dependencies of target llava [ 94%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o [ 94%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o [ 94%] Built target llava [ 94%] Linking CXX static library libllava_static."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "t examples/llava/CMakeFiles/llava.dir/llava.cpp.o [ 94%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o [ 94%] Built target llava [ 94%] Linking CXX static library libllava_static.a [ 94%] Built target llava_static [ 95%] Linking CXX shared library libllava_shared.so [ 95%] Built target llava_shared Consolidate compiler generated dependencies of target llama-llava-cli [ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-c li.cpp.o [ 96%] Linking CXX executable ../../bin/llama-llava-cli [ 96%] Built target llama-llava-cli Consolidate compiler generated dependencies of target llama-minicpmv-cli [ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/mini cpmv-cli.cpp.o [ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli [ 97%] Built target llama-minicpmv-cli [ 97%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2 vl-cli.cpp.o [ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli [ 98%] Built target llama-qwen2vl-cli Consolidate compiler generated dependencies of target llama-vdot [ 99%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o [ 99%] Linking CXX executable ../../bin/llama-vdot [ 99%] Built target llama-vdot Consolidate compiler generated dependencies of target llama-q8dot [ 99%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o [100%] Linking CXX executable ../.."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "uilt target llama-vdot Consolidate compiler generated dependencies of target llama-q8dot [ 99%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o [100%] Linking CXX executable ../../bin/llama-q8dot [100%] Built target llama-q8dot (torch) ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ ls -la $(find . -name l lama-cli) -rwx------ 1 ljubomir ljubomir 1537552 Jan 21 15:55 ./build/bin/llama-cli -rwx------ 1 ljubomir ljubomir 39579184 Nov 29 16:54 ./llama-cli (torch) ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ rmv ./llama-cli removed './llama-cli' (torch) ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ l *cli -rwx------ 1 ljubomir ljubomir 42M Nov 29 16:55 llama-llava-cli -rwx------ 1 ljubomir ljubomir 42M Nov 29 16:55 llama-minicpmv-cli (torch) ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ rmv *cli removed 'llama-llava-cli' removed 'llama-minicpmv-cli' (torch) ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ l build/bin/*cli -rwx------ 1 ljubomir ljubomir 1.5M Jan 21 15:55 build/bin/llama-cli -rwx------ 1 ljubomir ljubomir 1.8M Jan 21 15:56 build/bin/llama-llava-cli -rwx------ 1 ljubomir ljubomir 1.8M Jan 21 15:57 build/bin/llama-minicpmv-cli -rwx------ 1 ljubomir ljubomir 1.8M Jan 21 15:57 build/bin/llama-qwen2vl-cli (torch) ljubomir@gigul2(2251797.torch:0):~/llama.cpp$ ./build/bin/llama-cli -m m odels/DeepSeek-R1-Distill-Qwen-32B-Q6_K.gguf -p \"Write 4 lines on living a purpo seful life. A purposeful life is\" (torch) ljubomir@gigul2(797966.llama."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "orch:0):~/llama.cpp$ ./build/bin/llama-cli -m m odels/DeepSeek-R1-Distill-Qwen-32B-Q6_K.gguf -p \"Write 4 lines on living a purpo seful life. A purposeful life is\" (torch) ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ ./build/bin/llama-cli - m models/DeepSeek-R1-Distill-Qwen-32B-Q6_K.gguf -p \"Write 4 lines on living a pu rposeful life. A purposeful life is\" build: 4585 (672704a0) with gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86 _64-linux-gnu main: llama backend init main: load the model and apply lora adapter, if any llama_model_loader: loaded meta data with 30 key-value pairs and 771 tensors fro m models/DeepSeek-R1-Distill-Qwen-32B-Q6_K.gguf (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not appl y in this output. llama_model_loader: - kv 0: general.architecture str = qwen2 llama_model_loader: - kv 1: general.type str = model llama_model_loader: - kv 2: general.name str = DeepSeek R1 Distill Qwen 32B llama_model_loader: - kv 3: general.basename str = DeepSeek-R1-Distill-Qwen llama_model_loader: - kv 4: general.size_label str = 32B llama_model_loader: - kv 5: qwen2.block_count u32 = 64 llama_model_loader: - kv 6: qwen2.context_length u32 = 131072 llama_model_loader: - kv 7: qwen2.embedding_length u32 = 5120 llama_model_loader: - kv 8: qwen2.feed_forward_length u32 = 27648 llama_model_loader: - kv 9: qwen2.attention.head_count u32 = 40 llama_model_loader: - kv 10: qwen2.attention."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "edding_length u32 = 5120 llama_model_loader: - kv 8: qwen2.feed_forward_length u32 = 27648 llama_model_loader: - kv 9: qwen2.attention.head_count u32 = 40 llama_model_loader: - kv 10: qwen2.attention.head_count_kv u32 = 8 llama_model_loader: - kv 11: qwen2.rope.freq_base f32 = 1000000.000000 llama_model_loader: - kv 12: qwen2.attention.layer_norm_rms_epsilon f32 = 0.000010 llama_model_loader: - kv 13: tokenizer.ggml.model str = gpt2 llama_model_loader: - kv 14: tokenizer.ggml.pre str = deepseek-r1-qwen llama_model_loader: - kv 15: tokenizer.ggml.tokens arr[str ,152064] = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ... llama_model_loader: - kv 16: tokenizer.ggml.token_type arr[i32 ,152064] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... llama_model_loader: - kv 17: tokenizer.ggml.merges arr[str ,151387] = [\"� � \", \"� � � � \", \"i n\", \"� t\",... llama_model_loader: - kv 18: tokenizer.ggml.bos_token_id u32 = 151646 llama_model_loader: - kv 19: tokenizer.ggml.eos_token_id u32 = 151643 llama_model_loader: - kv 20: tokenizer.ggml.padding_token_id u32 = 151643 llama_model_loader: - kv 21: tokenizer.ggml.add_bos_token bool = true llama_model_loader: - kv 22: tokenizer.ggml.add_eos_token bool = false llama_model_loader: - kv 23: tokenizer.chat_template str = {% if not add_generation_prompt is de... llama_model_loader: - kv 24: general.quantization_version u32 = 2 llama_model_loader: - kv 25: general.file_type u32 = 18 llama_model_loader: - kv 26: quantize.imatrix."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "add_generation_prompt is de... llama_model_loader: - kv 24: general.quantization_version u32 = 2 llama_model_loader: - kv 25: general.file_type u32 = 18 llama_model_loader: - kv 26: quantize.imatrix.file str = /models_out/DeepSeek-R1-Distill-Qwen-... llama_model_loader: - kv 27: quantize.imatrix.dataset str = /training_dir/calibration_datav3.txt llama_model_loader: - kv 28: quantize.imatrix.entries_count i32 = 448 llama_model_loader: - kv 29: quantize.imatrix.chunks_count i32 = 128 llama_model_loader: - type f32: 321 tensors llama_model_loader: - type q6_K: 450 tensors print_info: file format = GGUF V3 (latest) print_info: file type = Q6_K print_info: file size = 25.03 GiB (6.56 BPW) load: special_eos_id is not in special_eog_ids - the tokenizer config may be inc orrect load: special tokens cache size = 22 load: token to piece cache size = 0.9310 MB print_info: arch = qwen2 print_info: vocab_only = 0 print_info: n_ctx_train = 131072 print_info: n_embd = 5120 print_info: n_layer = 64 print_info: n_head = 40 print_info: n_head_kv = 8 print_info: n_rot = 128 print_info: n_swa = 0 print_info: n_embd_head_k = 128 print_info: n_embd_head_v = 128 print_info: n_gqa = 5 print_info: n_embd_k_gqa = 1024 print_info: n_embd_v_gqa = 1024 print_info: f_norm_eps = 0.0e+00 print_info: f_norm_rms_eps = 1.0e-05 print_info: f_clamp_kqv = 0.0e+00 print_info: f_max_alibi_bias = 0.0e+00 print_info: f_logit_scale = 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "_info: n_embd_v_gqa = 1024 print_info: f_norm_eps = 0.0e+00 print_info: f_norm_rms_eps = 1.0e-05 print_info: f_clamp_kqv = 0.0e+00 print_info: f_max_alibi_bias = 0.0e+00 print_info: f_logit_scale = 0.0e+00 print_info: n_ff = 27648 print_info: n_expert = 0 print_info: n_expert_used = 0 print_info: causal attn = 1 print_info: pooling type = 0 print_info: rope type = 2 print_info: rope scaling = linear print_info: freq_base_train = 1000000.0 print_info: freq_scale_train = 1 print_info: n_ctx_orig_yarn = 131072 print_info: rope_finetuned = unknown print_info: ssm_d_conv = 0 print_info: ssm_d_inner = 0 print_info: ssm_d_state = 0 print_info: ssm_dt_rank = 0 print_info: ssm_dt_b_c_rms = 0 print_info: model type = 32B print_info: model params = 32.76 B print_info: general.name = DeepSeek R1 Distill Qwen 32B print_info: vocab type = BPE print_info: n_vocab = 152064 print_info: n_merges = 151387 print_info: BOS token = 151646 '<�begin�of�sentence�>' print_info: EOS token = 151643 '<�end�of�sentence�>' print_info: EOT token = 151643 '<�end�of�sentence�>' print_info: PAD token = 151643 '<�end�of�sentence�>' print_info: LF token = 148848 '�Ĭ' print_info: FIM PRE token = 151659 '<|fim_prefix|>' print_info: FIM SUF token = 151661 '<|fim_suffix|>' print_info: FIM MID token = 151660 '<|fim_middle|>' print_info: FIM PAD token = 151662 '<|fim_pad|>' print_info: FIM REP token = 151663 '<|repo_name|>' print_info: FIM SEP token = 151664 '<|file_sep|>' print_info: EOG token = 151643 '<�end�of�sent"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "e|>' print_info: FIM PAD token = 151662 '<|fim_pad|>' print_info: FIM REP token = 151663 '<|repo_name|>' print_info: FIM SEP token = 151664 '<|file_sep|>' print_info: EOG token = 151643 '<�end�of�sentence�>' print_info: EOG token = 151662 '<|fim_pad|>' print_info: EOG token = 151663 '<|repo_name|>' print_info: EOG token = 151664 '<|file_sep|>' print_info: max token length = 256 load_tensors: CPU_Mapped model buffer size = 25634.93 MiB llama_init_from_model: n_seq_max = 1 llama_init_from_model: n_ctx = 4096 llama_init_from_model: n_ctx_per_seq = 4096 llama_init_from_model: n_batch = 2048 llama_init_from_model: n_ubatch = 512 llama_init_from_model: flash_attn = 0 llama_init_from_model: freq_base = 1000000.0 llama_init_from_model: freq_scale = 1 llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full c apacity of the model will not be utilized llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16' , n_layer = 64, can_shift = 1 llama_kv_cache_init: CPU KV buffer size = 1024.00 MiB llama_init_from_model: KV self size = 1024.00 MiB, K (f16): 512.00 MiB, V (f16 ): 512.00 MiB llama_init_from_model: CPU output buffer size = 0.58 MiB llama_init_from_model: CPU compute buffer size = 368.01 MiB llama_init_from_model: graph nodes = 2246 llama_init_from_model: graph splits = 1 common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096 common_init_from_params: warming up the model with an empty run - please wait .. ."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "= 2246 llama_init_from_model: graph splits = 1 common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096 common_init_from_params: warming up the model with an empty run - please wait .. . (--no-warmup to disable) main: llama threadpool init, n_threads = 10 main: chat template is available, enabling conversation mode (disable it with -n o-cnv) main: chat template example: You are a helpful assistant <�User�>Hello<�Assistant�>Hi there<�end�of�sentence�><�User�>How are you ?<�Assistant�> system_info: n_threads = 10 (n_threads_batch = 10) / 10 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AA RCH64_REPACK = 1 | main: interactive mode on. sampler seed: 4147124249 sampler params: repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, p resence_penalty = 0.000 dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_pe nalty_last_n = 4096 top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_t hreshold = 0.100, typical_p = 1.000, temp = 0.800 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> t op-p -> min-p -> xtc -> temp-ext -> dist generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1 == Running in interactive mode. == - Press Ctrl+C to interject at any time. - Press Return to return control to the AI."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "p-ext -> dist generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1 == Running in interactive mode. == - Press Ctrl+C to interject at any time. - Press Return to return control to the AI. - To return control without starting a new line, end your input with '/'. - If you want to submit another line, end your input with '\\'. Write 4 lines on living a purposeful life. A purposeful life is > Please write in English. Certainly! Here's a response: A purposeful life is one where every day aligns with a deeper sense of meaning a nd direction. It involves setting goals that resonate with personal values and a spirations. By focusing on what truly matters, individuals can find fulfillment and make a positive impact on the world. Ultimately, living purposefully is abou t embracing one's passions and contributing to the greater good. </think> A purposeful life is one where every day aligns with a deeper sense of meaning a nd direction. It involves setting goals that resonate with personal values and a spirations. By focusing on what truly matters, individuals can find fulfillment and make a positive impact on the world. Ultimately, living purposefully is abou t embracing one's passions and contributing to the greater good. > That's a great reflection! It captures the essence of living with intention and making a meaningful impact. How do you see purpose aligning with personal growth and development? > llama_perf_sampler_print: sampling time = 5.12 ms / 36 runs ( 0 ."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "sence of living with intention and making a meaningful impact. How do you see purpose aligning with personal growth and development? > llama_perf_sampler_print: sampling time = 5.12 ms / 36 runs ( 0 .14 ms per token, 7024.39 tokens per second) llama_perf_context_print: load time = 62979.43 ms llama_perf_context_print: prompt eval time = 30074.55 ms / 18 tokens ( 1670 .81 ms per token, 0.60 tokens per second) llama_perf_context_print: eval time = 214810.32 ms / 190 runs ( 1130 .58 ms per token, 0.88 tokens per second) llama_perf_context_print: total time = 611923.33 ms / 208 tokens Interrupted by user (torch) ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ ./build/bin/llama-cli - m models/DeepSeek-R1-Distill-Qwen-7B-Q6_K_L.gguf -p \"Write 4 lines on living a p urposeful life. A purposeful life is\" (torch) ljubomir@gigul2(797966.llama.cpp:0):~/llama.cpp$ ./build/bin/llama-cli - m models/DeepSeek-R1-Distill-Qwen-7B-Q6_K_L.gguf -p \"Write 4 lines on living a p urposeful life. A purposeful life is\" build: 4585 (672704a0) with gcc-11 (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86 _64-linux-gnu main: llama backend init main: load the model and apply lora adapter, if any llama_model_loader: loaded meta data with 30 key-value pairs and 339 tensors fro m models/DeepSeek-R1-Distill-Qwen-7B-Q6_K_L.gguf (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not appl y in this output. llama_model_loader: - kv 0: general."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "epSeek-R1-Distill-Qwen-7B-Q6_K_L.gguf (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not appl y in this output. llama_model_loader: - kv 0: general.architecture str = qwen2 llama_model_loader: - kv 1: general.type str = model llama_model_loader: - kv 2: general.name str = DeepSeek R1 Distill Qwen 7B llama_model_loader: - kv 3: general.basename str = DeepSeek-R1-Distill-Qwen llama_model_loader: - kv 4: general.size_label str = 7B llama_model_loader: - kv 5: qwen2.block_count u32 = 28 llama_model_loader: - kv 6: qwen2.context_length u32 = 131072 llama_model_loader: - kv 7: qwen2.embedding_length u32 = 3584 llama_model_loader: - kv 8: qwen2.feed_forward_length u32 = 18944 llama_model_loader: - kv 9: qwen2.attention.head_count u32 = 28 llama_model_loader: - kv 10: qwen2.attention.head_count_kv u32 = 4 llama_model_loader: - kv 11: qwen2.rope.freq_base f32 = 10000.000000 llama_model_loader: - kv 12: qwen2.attention.layer_norm_rms_epsilon f32 = 0.000001 llama_model_loader: - kv 13: tokenizer.ggml.model str = gpt2 llama_model_loader: - kv 14: tokenizer.ggml.pre str = deepseek-r1-qwen llama_model_loader: - kv 15: tokenizer.ggml.tokens arr[str ,152064] = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ... llama_model_loader: - kv 16: tokenizer.ggml.token_type arr[i32 ,152064] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... llama_model_loader: - kv 17: tokenizer.ggml.merges arr[str ,151387] = [\"� � \", \"� � � � \", \"i n\", \"� t\",..."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "kv 16: tokenizer.ggml.token_type arr[i32 ,152064] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... llama_model_loader: - kv 17: tokenizer.ggml.merges arr[str ,151387] = [\"� � \", \"� � � � \", \"i n\", \"� t\",... llama_model_loader: - kv 18: tokenizer.ggml.bos_token_id u32 = 151646 llama_model_loader: - kv 19: tokenizer.ggml.eos_token_id u32 = 151643 llama_model_loader: - kv 20: tokenizer.ggml.padding_token_id u32 = 151643 llama_model_loader: - kv 21: tokenizer.ggml.add_bos_token bool = true llama_model_loader: - kv 22: tokenizer.ggml.add_eos_token bool = false llama_model_loader: - kv 23: tokenizer.chat_template str = {% if not add_generation_prompt is de... llama_model_loader: - kv 24: general.quantization_version u32 = 2 llama_model_loader: - kv 25: general.file_type u32 = 18 llama_model_loader: - kv 26: quantize.imatrix.file str = /models_out/DeepSeek-R1-Distill-Qwen-... llama_model_loader: - kv 27: quantize.imatrix.dataset str = /training_dir/calibration_datav3.txt llama_model_loader: - kv 28: quantize.imatrix.entries_count i32 = 196 llama_model_loader: - kv 29: quantize.imatrix.chunks_count i32 = 128 llama_model_loader: - type f32: 141 tensors llama_model_loader: - type q8_0: 2 tensors llama_model_loader: - type q6_K: 196 tensors print_info: file format = GGUF V3 (latest) print_info: file type = Q6_K print_info: file size = 6.06 GiB (6."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ors llama_model_loader: - type q8_0: 2 tensors llama_model_loader: - type q6_K: 196 tensors print_info: file format = GGUF V3 (latest) print_info: file type = Q6_K print_info: file size = 6.06 GiB (6.84 BPW) load: special_eos_id is not in special_eog_ids - the tokenizer config may be inc orrect load: special tokens cache size = 22 load: token to piece cache size = 0.9310 MB print_info: arch = qwen2 print_info: vocab_only = 0 print_info: n_ctx_train = 131072 print_info: n_embd = 3584 print_info: n_layer = 28 print_info: n_head = 28 print_info: n_head_kv = 4 print_info: n_rot = 128 print_info: n_swa = 0 print_info: n_embd_head_k = 128 print_info: n_embd_head_v = 128 print_info: n_gqa = 7 print_info: n_embd_k_gqa = 512 print_info: n_embd_v_gqa = 512 print_info: f_norm_eps = 0.0e+00 print_info: f_norm_rms_eps = 1.0e-06 print_info: f_clamp_kqv = 0.0e+00 print_info: f_max_alibi_bias = 0.0e+00 print_info: f_logit_scale = 0.0e+00 print_info: n_ff = 18944 print_info: n_expert = 0 print_info: n_expert_used = 0 print_info: causal attn = 1 print_info: pooling type = 0 print_info: rope type = 2 print_info: rope scaling = linear print_info: freq_base_train = 10000.0 print_info: freq_scale_train = 1 print_info: n_ctx_orig_yarn = 131072 print_info: rope_finetuned = unknown print_info: ssm_d_conv = 0 print_info: ssm_d_inner = 0 print_info: ssm_d_state = 0 print_info: ssm_dt_rank = 0 print_info: ssm_dt_b_c_rms = 0 print_info: model type = 7B print_info: model params = 7."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "print_info: ssm_d_conv = 0 print_info: ssm_d_inner = 0 print_info: ssm_d_state = 0 print_info: ssm_dt_rank = 0 print_info: ssm_dt_b_c_rms = 0 print_info: model type = 7B print_info: model params = 7.62 B print_info: general.name = DeepSeek R1 Distill Qwen 7B print_info: vocab type = BPE print_info: n_vocab = 152064 print_info: n_merges = 151387 print_info: BOS token = 151646 '<�begin�of�sentence�>' print_info: EOS token = 151643 '<�end�of�sentence�>' print_info: EOT token = 151643 '<�end�of�sentence�>' print_info: PAD token = 151643 '<�end�of�sentence�>' print_info: LF token = 148848 '�Ĭ' print_info: FIM PRE token = 151659 '<|fim_prefix|>' print_info: FIM SUF token = 151661 '<|fim_suffix|>' print_info: FIM MID token = 151660 '<|fim_middle|>' print_info: FIM PAD token = 151662 '<|fim_pad|>' print_info: FIM REP token = 151663 '<|repo_name|>' print_info: FIM SEP token = 151664 '<|file_sep|>' print_info: EOG token = 151643 '<�end�of�sentence�>' print_info: EOG token = 151662 '<|fim_pad|>' print_info: EOG token = 151663 '<|repo_name|>' print_info: EOG token = 151664 '<|file_sep|>' print_info: max token length = 256 load_tensors: CPU_Mapped model buffer size = 6210.54 MiB llama_init_from_model: n_seq_max = 1 llama_init_from_model: n_ctx = 4096 llama_init_from_model: n_ctx_per_seq = 4096 llama_init_from_model: n_batch = 2048 llama_init_from_model: n_ubatch = 512 llama_init_from_model: flash_attn = 0 llama_init_from_model: freq_base = 10000."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "llama_init_from_model: n_ctx_per_seq = 4096 llama_init_from_model: n_batch = 2048 llama_init_from_model: n_ubatch = 512 llama_init_from_model: flash_attn = 0 llama_init_from_model: freq_base = 10000.0 llama_init_from_model: freq_scale = 1 llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full c apacity of the model will not be utilized llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16' , n_layer = 28, can_shift = 1 llama_kv_cache_init: CPU KV buffer size = 224.00 MiB llama_init_from_model: KV self size = 224.00 MiB, K (f16): 112.00 MiB, V (f16 ): 112.00 MiB llama_init_from_model: CPU output buffer size = 0.58 MiB llama_init_from_model: CPU compute buffer size = 304.00 MiB llama_init_from_model: graph nodes = 986 llama_init_from_model: graph splits = 1 common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096 common_init_from_params: warming up the model with an empty run - please wait .. . (--no-warmup to disable) main: llama threadpool init, n_threads = 10 main: chat template is available, enabling conversation mode (disable it with -n o-cnv) main: chat template example: You are a helpful assistant <�User�>Hello<�Assistant�>Hi there<�end�of�sentence�><�User�>How are you ?<�Assistant�> system_info: n_threads = 10 (n_threads_batch = 10) / 10 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AA RCH64_REPACK = 1 | main: interactive mode on."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "_info: n_threads = 10 (n_threads_batch = 10) / 10 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AA RCH64_REPACK = 1 | main: interactive mode on. sampler seed: 343985590 sampler params: repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, p resence_penalty = 0.000 dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_pe nalty_last_n = 4096 top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_t hreshold = 0.100, typical_p = 1.000, temp = 0.800 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> t op-p -> min-p -> xtc -> temp-ext -> dist generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1 == Running in interactive mode. == - Press Ctrl+C to interject at any time. - Press Return to return control to the AI. - To return control without starting a new line, end your input with '/'. - If you want to submit another line, end your input with '\\'. Write 4 lines on living a purposeful life. A purposeful life is > When you live a purposeful life, you focus on your goals and aspirations, helpin g you stay on track with your journey toward fulfillment. You make choices that align with your values, ensuring that you are doing what truly matters. Living p urposefully means taking initiative, being proactive, and continuously growing, so you can achieve your highest potential."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "gn with your values, ensuring that you are doing what truly matters. Living p urposefully means taking initiative, being proactive, and continuously growing, so you can achieve your highest potential. It's about creating a life that is me aningful and aligned with who you are, leaving a positive impact on the world ar ound you. So, if you are not living a purposeful life, what might happen is that you get l ost in the daily grind, neglecting your values, and your goals may not be realiz ed. Without direction, you might feel aimless and unfulfilled, leading to dissat isfaction and a lack of purpose. Over time, this can cause you to lose touch wit h your true self and your aspirations, making it harder to achieve what you want in life. Essentially, living a purposeful life is essential for personal growth , happiness, and creating a lasting impact. Now, write 4 lines on the opposite�living without purpose or direction. living without purpose or direction, you might feel aimless and unfulfilled, ne glecting your values, and your goals may not be realized. Without purpose, you m ight lose direction, leading to dissatisfaction and a lack of focus. Over time, this can cause you to drift away from your true self and your aspirations, makin g it harder to achieve what you want in life. Essentially, living without purpos e or direction is essential for personal growth, happiness, and creating a lasti ng impact. Wait, no, that's not right. I think I mixed up the two sections."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "in life. Essentially, living without purpos e or direction is essential for personal growth, happiness, and creating a lasti ng impact. Wait, no, that's not right. I think I mixed up the two sections. Let me try agai n. </think> Living without purpose or direction can feel overwhelming, as you might neglect your values, leading to aimlessness and neglecting your goals. Without a clear d irection, your days might be filled with uncertainty, making it hard to stay foc used and dedicated to your aspirations. Over time, this can cause you to lose to uch with your true self and your goals, making it difficult to achieve what you want in life. Essentially, finding purpose or direction is crucial for personal growth, happiness, and creating a lasting impact. > llama_perf_sampler_print: sampling time = 49.97 ms / 442 runs ( 0 .11 ms per token, 8845.13 tokens per second) llama_perf_context_print: load time = 1457.87 ms llama_perf_context_print: prompt eval time = 143336.81 ms / 18 tokens ( 7963 .16 ms per token, 0.13 tokens per second) llama_perf_context_print: eval time = 110297.41 ms / 441 runs ( 250 .11 ms per token, 4.00 tokens per second) llama_perf_context_print: total time = 385491.79 ms / 459 tokens Interrupted by user Commands summary to update the codebase and run a new model: git pull origin master env |egrep 'CC|CXX' cmake -B build cmake --build build --config Release mviv ~/Downloads/DeepSeek-R1-Distill-Qwen-32B-Q6_K.gguf models/ find . -name llama-cli ."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "run a new model: git pull origin master env |egrep 'CC|CXX' cmake -B build cmake --build build --config Release mviv ~/Downloads/DeepSeek-R1-Distill-Qwen-32B-Q6_K.gguf models/ find . -name llama-cli ./build/bin/llama-cli -m models/DeepSeek-R1-Distill-Qwen-32B-Q6_K.gguf -p \"Write 4 lines on living a purposeful life. A purposeful life is\" ./build/bin/llama-cli -m models/DeepSeek-R1-Distill-Qwen-7B-Q6_K_L.gguf -p \"Writ e 4 lines on living a purposeful life. A purposeful life is\" LJ Tue 21 Jan 20:10:47 GMT 2025 + LJ Fri 31 Jan 2025 00:08:33 GMT � brew install llama.cpp Install via bew on mac brew install llama.cpp Compile from git on mac cmake -B build -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ cmake --build build --config Release but that fails?? Use clang cmake -B build -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++ cmake --build build --config Release Download model https://huggingface.co/lmstudio-community/Mistral-Small-24B-Instruct-2501-GGUF/t ree/main Move model in place ljubomir@macbook2(:):~/llama.cpp$ mviv ~/Downloads/Mistral-Small-24B-Instruct-25 01-Q6_K_L.gguf models/ Run own compiled ./build/bin/llama-cli -m models/Mistral-Small-24B-Instruct-2501-Q6_K_L.gguf -p \" Write 4 lines on living a purposeful life. A purposeful life is\" Run brew version llama-cli -m models/Mistral-Small-24B-Instruct-2501-Q6_K_L.gguf -p \"Write 4 line s on living a purposeful life. A purposeful life is\" Increase VRAM limit to 90 GB sudo sysctl iogpu."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "\" Run brew version llama-cli -m models/Mistral-Small-24B-Instruct-2501-Q6_K_L.gguf -p \"Write 4 line s on living a purposeful life. A purposeful life is\" Increase VRAM limit to 90 GB sudo sysctl iogpu.wired_limit_mb=90000 Use http server access on http://127.0.0.1:8080 llama-server -c 32768 -ub 64 -m models/Mistral-Small-24B-Instruct-2501-Q6_K_L.gg uf \"Run R1 in 2 commands\" ggerganov https://x.com/ggerganov/status/1884520481476198685 # source at https://github.com/ggerganov/llama.cpp brew install llama.cpp # increase vram limit tyo 180 GB sudo sysctl iogpu.wired_limit_mb=180000 # downloads ~150GB, requires ~180 GB VRAM, access on http://127.0.0.1:8080 llama-server -c 8192 -ub 64 --model-url https://huggingface.co/unsloth/DeepSeek- R1-GGUF/blob/main/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf LJ Fri 31 Jan 2025 00:08:33 GMT + LJ Fri 31 Jan 2025 10:42:28 GMT � 1M context models 1M context models https://simonwillison.net/2025/Jan/26/qwen25-1m/ Models gguf https://huggingface.co/lmstudio-community/Qwen2.5-14B-Instruct-1M-GGUF/tree/main https://huggingface.co/lmstudio-community/Qwen2.5-7B-Instruct-1M-GGUF/tree/main Running using llm llm install llm-ollama llm models -q qwen # To search for the model ID # I set a shorter q1m alias: llm aliases set q1m hf.co/lmstudio-community/Qwen2.5-7B-Instruct-1M-GGUF:Q8_0 Run files-to-prompt \\ ~/Dropbox/Development/llm \\ -e py -c | \\ llm -m q1m 'describe this codebase in detail' \\ -o num_ctx 80000 Using mlx https://huggingface."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ommunity/Qwen2.5-7B-Instruct-1M-GGUF:Q8_0 Run files-to-prompt \\ ~/Dropbox/Development/llm \\ -e py -c | \\ llm -m q1m 'describe this codebase in detail' \\ -o num_ctx 80000 Using mlx https://huggingface.co/mlx-community/Qwen2.5-7B-Instruct-1M-4bit mlx-community/Qwen2.5-7B-Instruct-1M-4bit The Model mlx-community/Qwen2.5-7B-Instruct-1M-4bit was converted to MLX format from Qwen/Qwen2.5-7B-Instruct-1M using mlx-lm version 0.21.1. Use with mlx pip install mlx-lm from mlx_lm import load, generate model, tokenizer = load(\"mlx-community/Qwen2.5-7B-Instruct-1M-4bit\") prompt = \"hello\" if tokenizer.chat_template is not None: messages = [{\"role\": \"user\", \"content\": prompt}] prompt = tokenizer.apply_chat_template( messages, add_generation_prompt=True ) response = generate(model, tokenizer, prompt=prompt, verbose=True) Community mlx https://huggingface.co/mlx-community MLX Community A community org for model weights compatible with mlx-examples powered by MLX. These are pre-converted weights and ready to be used in the example scripts. Quick start for LLMs Install mlx-lm: pip install mlx-lm You can use mlx-lm from the command line. For example: mlx_lm.generate --model mlx-community/Mistral-7B-Instruct-v0.3-4bit --prompt \"he llo\" This will download a Mistral 7B model from the Hugging Face Hub and generate tex t using the given prompt. For a full list of options run: mlx_lm.generate --help To quantize a model from the command line run: mlx_lm."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ad a Mistral 7B model from the Hugging Face Hub and generate tex t using the given prompt. For a full list of options run: mlx_lm.generate --help To quantize a model from the command line run: mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.3 -q For more options run: mlx_lm.convert --help You can upload new models to Hugging Face by specifying --upload-repo to convert . For example, to upload a quantized Mistral-7B model to the MLX Hugging Face co mmunity you can do: mlx_lm.convert \\ --hf-path mistralai/Mistral-7B-Instruct-v0.3 \\ -q \\ --upload-repo mlx-community/my-4bit-mistral For more details on the API checkout the full README Other Examples: For more examples, visit the MLX Examples repo. The repo includes examples of: Parameter efficient fine tuning with LoRA Speech recognition with Whisper Image generation with Stable Diffusion and many other examples of different machine learning applications and algorithm s. LJ Fri 31 Jan 2025 10:42:28 GMT + LJ Sat 1 Feb 2025 09:19:30 GMT � VSCode llama auto-complete llama-vscode addon VSCode llama auto-complete llama-vscode addon Server - use raw pre-trained only model, but *not* Instruct llama-server -hf ggml-org/Qwen2.5-Coder-7B-Q8_0-GGUF --port 8012 -ngl 99 -fa -ub 1024 -b 1024 --ctx-size 0 --cache-reuse 256 common_download_file: file metadata saved: /Users/ljubomir/Library/Caches/llama. cpp/ggml-org_Qwen2.5-Coder-7B-Q8_0-GGUF_qwen2.5-coder-7b-q8_0.gguf.json ljubomir@macbook2(:):~/llama."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "--cache-reuse 256 common_download_file: file metadata saved: /Users/ljubomir/Library/Caches/llama. cpp/ggml-org_Qwen2.5-Coder-7B-Q8_0-GGUF_qwen2.5-coder-7b-q8_0.gguf.json ljubomir@macbook2(:):~/llama.cpp/models$ ln -s ~/Library/Caches/llama.cpp/ggml-o rg_Qwen2.5-Coder-7B-Q8_0-GGUF_qwen2.5-coder-7b-q8_0.gguf ljubomir@macbook2(:):~/llama.cpp/models$ l ggml-org_Qwen2.5-Coder-7B-Q8_0-GGUF_q wen2.5-coder-7b-q8_0.gguf lrwx------@ 1 ljubomir staff 103B 1 Feb 09:16 ggml-org_Qwen2.5-Coder-7B-Q8_0 -GGUF_qwen2.5-coder-7b-q8_0.gguf -> /Users/ljubomir/Library/Caches/llama.cpp/ggm l-org_Qwen2.5-Coder-7B-Q8_0-GGUF_qwen2.5-coder-7b-q8_0.gguf ljubomir@macbook2(:):~/llama.cpp/models$ LJ Sat 1 Feb 2025 09:19:30 GMT + LJ Sun 2 Feb 2025 13:16:11 GMT � git clone https://github.com/ggerganov/llama.cpp.git Run on 1TB box cs1dprsrch03 Clone the repo git clone https://github.com/ggerganov/llama.cpp.git cd llama.cpp [ljubomir@cs1dprsrch03 ~]$ git clone https://github.com/ggerganov/llama.cpp.git Configure [ljubomir@cs1dprsrch03 llama.cpp]$ export CC=gcc [ljubomir@cs1dprsrch03 llama.cpp]$ export CXX=g++ cmake -B build -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ [ljubomir@cs1dprsrch03 llama.cpp]$ cmake -B build -DCMAKE_C_COMPILER=gcc -DCMAKE _CXX_COMPILER=g++ Build cmake --build build --config Release [ljubomir@cs1dprsrch03 llama.cpp]$ cmake --build build --config Release Download the model files for the 130 GB 1.58bit model wget https://huggingface."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "R=g++ Build cmake --build build --config Release [ljubomir@cs1dprsrch03 llama.cpp]$ cmake --build build --config Release Download the model files for the 130 GB 1.58bit model wget https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-UD -IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf wget https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-UD -IQ1_S/DeepSeek-R1-UD-IQ1_S-00002-of-00003.gguf wget https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-UD -IQ1_S/DeepSeek-R1-UD-IQ1_S-00003-of-00003.gguf Do NOT concatenate parts into single gguf (cat DeepSeek-R1-UD-IQ1_S-0000{1,2,3}- of-00003.gguf >models/DeepSeek-R1-UD-IQ1_S.gguf) - seems llama.cpp does not like that, while the models in parts is ljubomir@cs1dprsrch03(1887635.llama.cpp:0):~/llama.cpp $ mv -iv DeepSeek-R1-UD-IQ1_S-0000{1,2,3}-of-00003.gguf models/ Run the model build/bin/llama-cli -c 8192 -ub 64 --model models/DeepSeek-R1-UD-IQ1_S-00001-of- 00003.gguf -p \"How many letters R in the word STRAWBERRY?\" [ljubomir@cs1dprsrch03 llama.cpp]$ build/bin/llama-cli -c 8192 -ub 64 --model mo dels/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf -p \"How many letters R in the word STRAWBERRY?\" DL 4bit quants ljubomir@cs1dprsrch03(3839843.llama3:0):~/llama.cpp/models $ for a in {0..9}; do echo wget https://huggingface.co/unsloth/DeepSeek-R1-GGUF/ resolve/main/DeepSeek-R1-Q4_K_M/DeepSeek-R1-Q4_K_M-0000${a}-of-00009.gguf; done wget https://huggingface."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ma.cpp/models $ for a in {0..9}; do echo wget https://huggingface.co/unsloth/DeepSeek-R1-GGUF/ resolve/main/DeepSeek-R1-Q4_K_M/DeepSeek-R1-Q4_K_M-0000${a}-of-00009.gguf; done wget https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q4 _K_M/DeepSeek-R1-Q4_K_M-00000-of-00009.gguf wget https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q4 _K_M/DeepSeek-R1-Q4_K_M-00001-of-00009.gguf wget https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q4 _K_M/DeepSeek-R1-Q4_K_M-00002-of-00009.gguf wget https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q4 _K_M/DeepSeek-R1-Q4_K_M-00003-of-00009.gguf wget https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q4 _K_M/DeepSeek-R1-Q4_K_M-00004-of-00009.gguf wget https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q4 _K_M/DeepSeek-R1-Q4_K_M-00005-of-00009.gguf wget https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q4 _K_M/DeepSeek-R1-Q4_K_M-00006-of-00009.gguf wget https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q4 _K_M/DeepSeek-R1-Q4_K_M-00007-of-00009.gguf wget https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q4 _K_M/DeepSeek-R1-Q4_K_M-00008-of-00009.gguf wget https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q4 _K_M/DeepSeek-R1-Q4_K_M-00009-of-00009.gguf ljubomir@cs1dprsrch03(3839843.llama3:0):~/llama.cpp/models $ for a in {0.."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "wget https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resolve/main/DeepSeek-R1-Q4 _K_M/DeepSeek-R1-Q4_K_M-00009-of-00009.gguf ljubomir@cs1dprsrch03(3839843.llama3:0):~/llama.cpp/models $ for a in {0..9}; do wget https://huggingface.co/unsloth/DeepSeek-R1-GGUF/resol ve/main/DeepSeek-R1-Q4_K_M/DeepSeek-R1-Q4_K_M-0000${a}-of-00009.gguf; done Run 4bit model build/bin/llama-cli -c 8192 -ub 64 --model models/DeepSeek-R1-Q4_K_M-00000-of-00 009.gguf -p \"How many letters R in the word STRAWBERRY?\" Large context, large batch sizes https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md Common Options -c N, --ctx-size N: Set the size of the prompt context. The default is 4096, but if a LLaMA model was built with a longer context, increasing this value will pr ovide better results for longer input/inference. Batch Size -ub N, --ubatch-size N: Physical batch size. This is the maximum number of token s that may be processed at a time. Increasing this value may improve performance during prompt processing, at the expense of higher memory usage. Default: 512. -b N, --batch-size N: Logical batch size. Increasing this value above the value of the physical batch size may improve prompt processing performance when using multiple GPUs with pipeline parallelism. Default: 2048. build/bin/llama-cli --ctx-size 131072 --ubatch-size 1024 --model models/DeepSeek -R1-Q4_K_M-00001-of-00009."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "pt processing performance when using multiple GPUs with pipeline parallelism. Default: 2048. build/bin/llama-cli --ctx-size 131072 --ubatch-size 1024 --model models/DeepSeek -R1-Q4_K_M-00001-of-00009.gguf -p \"How many letters R in the word STRAWBERRY?\" Use defaults build/bin/llama-cli --model models/DeepSeek-R1-Q4_K_M-00001-of-00009.gguf -p \"Ho w many letters R in the word STRAWBERRY?\" Run using models in /data ssh cs1dprsrch03 git clone https://github.com/ggerganov/llama.cpp.git cd llama.cpp cmake -B build -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ cmake --build build --config Release build/bin/llama-cli -c 8192 -ub 64 --model /data/models/DeepSeek-R1-UD-IQ1_S-000 01-of-00003.gguf -p \"How many letters R in the word STRAWBERRY?\" # (press ENTER) Or using more memory build/bin/llama-cli -c 65536 -ub 2048 --model /data/models/DeepSeek-R1-Q4_K_M-00 001-of-00009.gguf -p \"How many letters R in the word STRAWBERRY?\" # (press ENTER ) Max context (\"llama_init_from_model: n_ctx_per_seq (65536) < n_ctx_train (163840 ) -- the full capacity of the model will not be utilized\") build/bin/llama-cli -c 163840 -ub 4096 --model /data/models/DeepSeek-R1-Q4_K_M-0 0001-of-00009.gguf -p \"How many letters R in the word STRAWBERRY?\" # (press ENTE R) The above never finishes - so run with defaults build/bin/llama-cli --model /data/models/DeepSeek-R1-Q4_K_M-00001-of-00009.gguf LJ Sun 2 Feb 2025 13:16:11 GMT + cd llama.cpp cd llama."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "# (press ENTE R) The above never finishes - so run with defaults build/bin/llama-cli --model /data/models/DeepSeek-R1-Q4_K_M-00001-of-00009.gguf LJ Sun 2 Feb 2025 13:16:11 GMT + cd llama.cpp cd llama.cpp git pull cmake -B build -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ cmake --build build --config Release ljubomir@gigul2(422663.llama.cpp:0):~/llama.cpp$ build/bin/llama-cli --help ----- common params ----- -h, --help, --usage print usage and exit --version show version and build info --completion-bash print source-able bash completion script for llama.cpp --verbose-prompt print a verbose prompt before generation (default: false) -t, --threads N number of threads to use during generati on (default: -1) (env: LLAMA_ARG_THREADS) -tb, --threads-batch N number of threads to use during batch an d prompt processing (default: same as --threads) -C, --cpu-mask M CPU affinity mask: arbitrarily long hex. Complements cpu-range (default: \"\") -Cr, --cpu-range lo-hi range of CPUs for affinity. Complements --cpu-mask --cpu-strict <0|1> use strict CPU placement (default: 0) --prio N set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime (default: 0) --poll <0...100> use polling level to wait for work (0 - no polling, default: 50) -Cb, --cpu-mask-batch M CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch (default: same as --cpu-mask) -Crb, --cpu-range-batch lo-hi ranges of CPUs for affinity."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ling, default: 50) -Cb, --cpu-mask-batch M CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch (default: same as --cpu-mask) -Crb, --cpu-range-batch lo-hi ranges of CPUs for affinity. Complements --cpu-mask-batch --cpu-strict-batch <0|1> use strict CPU placement (default: same as --cpu-strict) --prio-batch N set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime (default: 0) --poll-batch <0|1> use polling to wait for work (default: s ame as --poll) -c, --ctx-size N size of the prompt context (default: 409 6, 0 = loaded from model) (env: LLAMA_ARG_CTX_SIZE) -n, --predict, --n-predict N number of tokens to predict (default: -1 , -1 = infinity, -2 = until context filled) (env: LLAMA_ARG_N_PREDICT) -b, --batch-size N logical maximum batch size (default: 204 8) (env: LLAMA_ARG_BATCH) -ub, --ubatch-size N physical maximum batch size (default: 51 2) (env: LLAMA_ARG_UBATCH) --keep N number of tokens to keep from the initia l prompt (default: 0, -1 = all) -fa, --flash-attn enable Flash Attention (default: disable d) (env: LLAMA_ARG_FLASH_ATTN) -p, --prompt PROMPT prompt to start generation with; for sys tem message, use -sys --no-perf disable internal libllama performance ti mings (default: false) (env: LLAMA_ARG_NO_PERF) -f, --file FNAME a file containing the prompt (default: n one) -bf, --binary-file FNAME binary file containing the prompt (defau lt: none) -e, --escape process escapes sequences (\\n, \\r, \\t, \\ ', \\\", \\\\) (default: true) --no-escape do n"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "he prompt (default: n one) -bf, --binary-file FNAME binary file containing the prompt (defau lt: none) -e, --escape process escapes sequences (\\n, \\r, \\t, \\ ', \\\", \\\\) (default: true) --no-escape do not process escape sequences --rope-scaling {none,linear,yarn} RoPE frequency scaling method, defaults to linear unless specified by the model (env: LLAMA_ARG_ROPE_SCALING_TYPE) --rope-scale N RoPE context scaling factor, expands con text by a factor of N (env: LLAMA_ARG_ROPE_SCALE) --rope-freq-base N RoPE base frequency, used by NTK-aware s caling (efault: loaded from model) (env: LLAMA_ARG_ROPE_FREQ_BASE) --rope-freq-scale N RoPE frequency scaling factor, expands c ontext by a factor of 1/N (env: LLAMA_ARG_ROPE_FREQ_SCALE) --yarn-orig-ctx N YaRN: original context size of model (de fault: 0 = model training context size) (env: LLAMA_ARG_YARN_ORIG_CTX) --yarn-ext-factor N YaRN: extrapolation mix factor (default: -1.0, 0.0 = full interpolation) (env: LLAMA_ARG_YARN_EXT_FACTOR) --yarn-attn-factor N YaRN: scale sqrt(t) or attention magnitu de (default: 1.0) (env: LLAMA_ARG_YARN_ATTN_FACTOR) --yarn-beta-slow N YaRN: high correction dim or alpha (defa ult: 1.0) (env: LLAMA_ARG_YARN_BETA_SLOW) --yarn-beta-fast N YaRN: low correction dim or beta (defaul t: 32."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "nv: LLAMA_ARG_YARN_ATTN_FACTOR) --yarn-beta-slow N YaRN: high correction dim or alpha (defa ult: 1.0) (env: LLAMA_ARG_YARN_BETA_SLOW) --yarn-beta-fast N YaRN: low correction dim or beta (defaul t: 32.0) (env: LLAMA_ARG_YARN_BETA_FAST) -dkvc, --dump-kv-cache verbose print of the KV cache -nkvo, --no-kv-offload disable KV offload (env: LLAMA_ARG_NO_KV_OFFLOAD) -ctk, --cache-type-k TYPE KV cache data type for K allowed values: f32, f16, bf16, q8_0, q4 _0, q4_1, iq4_nl, q5_0, q5_1 (default: f16) (env: LLAMA_ARG_CACHE_TYPE_K) -ctv, --cache-type-v TYPE KV cache data type for V allowed values: f32, f16, bf16, q8_0, q4 _0, q4_1, iq4_nl, q5_0, q5_1 (default: f16) (env: LLAMA_ARG_CACHE_TYPE_V) -dt, --defrag-thold N KV cache defragmentation threshold (defa ult: 0.1, < 0 - disabled) (env: LLAMA_ARG_DEFRAG_THOLD) -np, --parallel N number of parallel sequences to decode ( default: 1) (env: LLAMA_ARG_N_PARALLEL) --mlock force system to keep model in RAM rather than swapping or compressing (env: LLAMA_ARG_MLOCK) --no-mmap do not memory-map model (slower load but may reduce pageouts if not using mlock) (env: LLAMA_ARG_NO_MMAP) --numa TYPE attempt optimizations that help on some NUMA systems - distribute: spread execution evenly ov er all nodes - isolate: only spawn threads on CPUs on the node that execution started on - numactl: use the CPU map provided by n umactl if run without this previously, it is re commended to drop the system page cache before using this see https://github."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "he node that execution started on - numactl: use the CPU map provided by n umactl if run without this previously, it is re commended to drop the system page cache before using this see https://github.com/ggml-org/llama.cp p/issues/1437 (env: LLAMA_ARG_NUMA) -dev, --device <dev1,dev2,..> comma-separated list of devices to use f or offloading (none = don't offload) use --list-devices to see a list of avai lable devices (env: LLAMA_ARG_DEVICE) --list-devices print list of available devices and exit --override-tensor, -ot <tensor name pattern>=<buffer type>,... override tensor buffer type -ngl, --gpu-layers, --n-gpu-layers N number of layers to store in VRAM (env: LLAMA_ARG_N_GPU_LAYERS) -sm, --split-mode {none,layer,row} how to split the model across multiple G PUs, one of: - none: use one GPU only - layer (default): split layers and KV a cross GPUs - row: split rows across GPUs (env: LLAMA_ARG_SPLIT_MODE) -ts, --tensor-split N0,N1,N2,... fraction of the model to offload to each GPU, comma-separated list of proportions, e.g. 3,1 (env: LLAMA_ARG_TENSOR_SPLIT) -mg, --main-gpu INDEX the GPU to use for the model (with split -mode = none), or for intermediate results and KV (with split- mode = row) (default: 0) (env: LLAMA_ARG_MAIN_GPU) --check-tensors check model tensor data for invalid valu es (default: false) --override-kv KEY=TYPE:VALUE advanced option to override model metada ta by key. may be specified multiple times. types: int, float, bool, str."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "k model tensor data for invalid valu es (default: false) --override-kv KEY=TYPE:VALUE advanced option to override model metada ta by key. may be specified multiple times. types: int, float, bool, str. example: - -override-kv tokenizer.ggml.add_bos_token=bool:false --lora FNAME path to LoRA adapter (can be repeated to use multiple adapters) --lora-scaled FNAME SCALE path to LoRA adapter with user defined s caling (can be repeated to use multiple adapters) --control-vector FNAME add a control vector note: this argument can be repeated to a dd multiple control vectors --control-vector-scaled FNAME SCALE add a control vector with user defined s caling SCALE note: this argument can be repeated to a dd multiple scaled control vectors --control-vector-layer-range START END layer range to apply the control vector( s) to, start and end inclusive -m, --model FNAME model path (default: `models/$filename` with filename from `--hf-file` or `--model-url` if set, otherwise model s/7B/ggml-model-f16.gguf) (env: LLAMA_ARG_MODEL) -mu, --model-url MODEL_URL model download url (default: unused) (env: LLAMA_ARG_MODEL_URL) -hf, -hfr, --hf-repo <user>/<model>[:quant] Hugging Face model repository; quant is optional, case-insensitive, default to Q4_K_M, or falls back to the first file in the repo if Q4_K_M doesn't exist. mmproj is also downloaded automatically if available."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ace model repository; quant is optional, case-insensitive, default to Q4_K_M, or falls back to the first file in the repo if Q4_K_M doesn't exist. mmproj is also downloaded automatically if available. to disable, add --no-mmproj example: unsloth/phi-4-GGUF:q4_k_m (default: unused) (env: LLAMA_ARG_HF_REPO) -hfd, -hfrd, --hf-repo-draft <user>/<model>[:quant] Same as --hf-repo, but for the draft mod el (default: unused) (env: LLAMA_ARG_HFD_REPO) -hff, --hf-file FILE Hugging Face model file. If specified, i t will override the quant in --hf-repo (default: unused) (env: LLAMA_ARG_HF_FILE) -hfv, -hfrv, --hf-repo-v <user>/<model>[:quant] Hugging Face model repository for the vo coder model (default: unused) (env: LLAMA_ARG_HF_REPO_V) -hffv, --hf-file-v FILE Hugging Face model file for the vocoder model (default: unused) (env: LLAMA_ARG_HF_FILE_V) -hft, --hf-token TOKEN Hugging Face access token (default: valu e from HF_TOKEN environment variable) (env: HF_TOKEN) --log-disable Log disable --log-file FNAME Log to file --log-colors Enable colored logging (env: LLAMA_LOG_COLORS) -v, --verbose, --log-verbose Set verbosity level to infinity (i.e. lo g all messages, useful for debugging) -lv, --verbosity, --log-verbosity N Set the verbosity threshold. Messages wi th a higher verbosity will be ignored."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "rbose Set verbosity level to infinity (i.e. lo g all messages, useful for debugging) -lv, --verbosity, --log-verbosity N Set the verbosity threshold. Messages wi th a higher verbosity will be ignored. (env: LLAMA_LOG_VERBOSITY) --log-prefix Enable prefix in log messages (env: LLAMA_LOG_PREFIX) --log-timestamps Enable timestamps in log messages (env: LLAMA_LOG_TIMESTAMPS) ----- sampling params ----- --samplers SAMPLERS samplers that will be used for generatio n in the order, separated by ';' (default: penalties;dry;top_k;typ_p;top_ p;min_p;xtc;temperature) -s, --seed SEED RNG seed (default: -1, use random seed f or -1) --sampling-seq, --sampler-seq SEQUENCE simplified sequence for samplers that wi ll be used (default: edkypmxt) --ignore-eos ignore end of stream token and continue generating (implies --logit-bias EOS-inf) --temp N temperature (default: 0.8) --top-k N top-k sampling (default: 40, 0 = disable d) --top-p N top-p sampling (default: 0.9, 1.0 = disa bled) --min-p N min-p sampling (default: 0.1, 0.0 = disa bled) --top-nsigma N top-n-sigma sampling (default: -1.0, -1. 0 = disabled) --xtc-probability N xtc probability (default: 0.0, 0.0 = dis abled) --xtc-threshold N xtc threshold (default: 0.1, 1.0 = disab led) --typical N locally typical sampling, parameter p (d efault: 1.0, 1.0 = disabled) --repeat-last-n N last n tokens to consider for penalize ( default: 64, 0 = disabled, -1 = ctx_size) --repeat-penalty N penalize repeat sequence of tokens (defa ult: 1.0, 1."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ault: 1.0, 1.0 = disabled) --repeat-last-n N last n tokens to consider for penalize ( default: 64, 0 = disabled, -1 = ctx_size) --repeat-penalty N penalize repeat sequence of tokens (defa ult: 1.0, 1.0 = disabled) --presence-penalty N repeat alpha presence penalty (default: 0.0, 0.0 = disabled) --frequency-penalty N repeat alpha frequency penalty (default: 0.0, 0.0 = disabled) --dry-multiplier N set DRY sampling multiplier (default: 0. 0, 0.0 = disabled) --dry-base N set DRY sampling base value (default: 1. 75) --dry-allowed-length N set allowed length for DRY sampling (def ault: 2) --dry-penalty-last-n N set DRY penalty for the last n tokens (d efault: -1, 0 = disable, -1 = context size) --dry-sequence-breaker STRING add sequence breaker for DRY sampling, c learing out default breakers ('\\n', ':', '\"', '*') in the process; us e \"none\" to not use any sequence breakers --dynatemp-range N dynamic temperature range (default: 0.0, 0.0 = disabled) --dynatemp-exp N dynamic temperature exponent (default: 1 .0) --mirostat N use Mirostat sampling. Top K, Nucleus and Locally Typical sampl ers are ignored if used. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0) --mirostat-lr N Mirostat learning rate, parameter eta (d efault: 0.1) --mirostat-ent N Mirostat target entropy, parameter tau ( default: 5.0) -l, --logit-bias TOKEN_ID(+/-)BIAS modifies the likelihood of token appeari ng in the completion, i.e."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "meter eta (d efault: 0.1) --mirostat-ent N Mirostat target entropy, parameter tau ( default: 5.0) -l, --logit-bias TOKEN_ID(+/-)BIAS modifies the likelihood of token appeari ng in the completion, i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello', or `--logit-bias 15043-1` to decrease li kelihood of token ' Hello' --grammar GRAMMAR BNF-like grammar to constrain generation s (see samples in grammars/ dir) (default: '') --grammar-file FNAME file to read grammar from -j, --json-schema SCHEMA JSON schema to constrain generations (ht tps://json-schema.org/), e.g. `{}` for any JSON object For schemas w/ external $refs, use --gra mmar + example/json_schema_to_grammar.py instea d -jf, --json-schema-file FILE File containing a JSON schema to constra in generations (https://json-schema.org/), e.g. `{}` fo r any JSON object For schemas w/ external $refs, use --gra mmar + example/json_schema_to_grammar.py instea d ----- example-specific params ----- --no-display-prompt don't print prompt at generation (defaul t: false) -co, --color colorise output to distinguish prompt an d user input from generations (default: false) --no-context-shift disables context shift on infinite text generation (default: disabled) (env: LLAMA_ARG_NO_CONTEXT_SHIFT) -sys, --system-prompt PROMPT system prompt to use with model (if appl icable, depending on chat template) -sysf, --system-prompt-file FNAME a file containing the system prompt (def ault: none) -ptc, --print-token-count N print token c"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "m prompt to use with model (if appl icable, depending on chat template) -sysf, --system-prompt-file FNAME a file containing the system prompt (def ault: none) -ptc, --print-token-count N print token count every N tokens (defaul t: -1) --prompt-cache FNAME file to cache prompt state for faster st artup (default: none) --prompt-cache-all if specified, saves user input and gener ations to cache as well --prompt-cache-ro if specified, uses the prompt cache but does not update it -r, --reverse-prompt PROMPT halt generation at PROMPT, return contro l in interactive mode -sp, --special special tokens output enabled (default: false) -cnv, --conversation run in conversation mode: - does not print special tokens and suff ix/prefix - interactive mode is also enabled (default: auto enabled if chat template is available) -no-cnv, --no-conversation force disable conversation mode (default : false) -st, --single-turn run conversation for a single turn only, then exit when done will not be interactive if first turn is predefined with --prompt (default: false) -i, --interactive run in interactive mode (default: false) -if, --interactive-first run in interactive mode and wait for inp ut right away (default: false) -mli, --multiline-input allows you to write or paste multiple li nes without ending each in '\\' --in-prefix-bos prefix BOS to user inputs, preceding the `--in-prefix` string --in-prefix STRING string to prefix user inputs with (defau lt: empty) --in-suffix STRING string to suffix aft"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "each in '\\' --in-prefix-bos prefix BOS to user inputs, preceding the `--in-prefix` string --in-prefix STRING string to prefix user inputs with (defau lt: empty) --in-suffix STRING string to suffix after user inputs with (default: empty) --no-warmup skip warming up the model with an empty run -gan, --grp-attn-n N group-attention factor (default: 1) (env: LLAMA_ARG_GRP_ATTN_N) -gaw, --grp-attn-w N group-attention width (default: 512) (env: LLAMA_ARG_GRP_ATTN_W) --jinja use jinja template for chat (default: di sabled) (env: LLAMA_ARG_JINJA) --reasoning-format FORMAT reasoning format (default: deepseek; all owed values: deepseek, none) controls whether thought tags are extrac ted from the response, and in which format they're returned. 'none' le aves thoughts unparsed in `message.content`, 'deepseek' puts them in `message.reasoning_content` (for DeepSeek R1 & Command R7B only). only supported for non-streamed response s (env: LLAMA_ARG_THINK) --chat-template JINJA_TEMPLATE set custom jinja chat template (default: template taken from model's metadata) if suffix/prefix are specified, template will be disabled only commonly used templates are accepte d (unless --jinja is set before this flag): list of built-in templates: bailing, chatglm3, chatglm4, chatml, com mand-r, deepseek, deepseek2, deepseek3, exaone3, falcon3, gemma, giga chat, glmedge, granite, llama2, llama2-sys, llama2-sys-bos, llam a2-sys-strip, llama3, llama4, megrez, minicpm, mistral-v1, mistral-v3, mistral-v3-tekken,"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "k2, deepseek3, exaone3, falcon3, gemma, giga chat, glmedge, granite, llama2, llama2-sys, llama2-sys-bos, llam a2-sys-strip, llama3, llama4, megrez, minicpm, mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7, monarch, openchat, orion, ph i3, phi4, rwkv-world, smolvlm, vicuna, vicuna-orca, yandex, zephyr (env: LLAMA_ARG_CHAT_TEMPLATE) --chat-template-file JINJA_TEMPLATE_FILE set custom jinja chat template file (def ault: template taken from model's metadata) if suffix/prefix are specified, template will be disabled only commonly used templates are accepte d (unless --jinja is set before this flag): list of built-in templates: bailing, chatglm3, chatglm4, chatml, com mand-r, deepseek, deepseek2, deepseek3, exaone3, falcon3, gemma, giga chat, glmedge, granite, llama2, llama2-sys, llama2-sys-bos, llam a2-sys-strip, llama3, llama4, megrez, minicpm, mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7, monarch, openchat, orion, ph i3, phi4, rwkv-world, smolvlm, vicuna, vicuna-orca, yandex, zephyr (env: LLAMA_ARG_CHAT_TEMPLATE_FILE) --simple-io use basic IO for better compatibility in subprocesses and limited consoles example usage: text generation: build/bin/llama-cli -m your_model.gguf -p \"I believe the meaning of life is\" -n 128 -no-cnv chat (conversation): build/bin/llama-cli -m your_model.gguf -sys \"You are a he lpful assistant\" ljubomir@gigul2(422663.llama.cpp:0):~/llama.cpp$ Test build/bin/llama-cli --model models/Qwen3-30B-A3B-Q8_0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "(conversation): build/bin/llama-cli -m your_model.gguf -sys \"You are a he lpful assistant\" ljubomir@gigul2(422663.llama.cpp:0):~/llama.cpp$ Test build/bin/llama-cli --model models/Qwen3-30B-A3B-Q8_0.gguf -p \"How many letters R in the word STRAWBERRY?\" ljubomir@gigul2(422663.llama.cpp:0):~/llama.cpp$ build/bin/llama-cli --model mod els/Qwen3-30B-A3B-Q8_0.gguf -p \"How many letters R in the word STRAWBERRY? ljubomir@gigul2(422663.llama.cpp:0):~/llama.cpp$ build/bin/llama-cli --model mod els/Qwen3-30B-A3B-Q8_0.gguf -p \"How many letters R in the word STRAWBERRY?\" build: 5303 (becc6f18) with gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64 -linux-gnu main: llama backend init main: load the model and apply lora adapter, if any llama_model_loader: loaded meta data with 43 key-value pairs and 579 tensors fro m models/Qwen3-30B-A3B-Q8_0.gguf (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not appl y in this output. llama_model_loader: - kv 0: general.architecture str = qwen3moe llama_model_loader: - kv 1: general.type str = model llama_model_loader: - kv 2: general.name str = Qwen3-30B-A3B llama_model_loader: - kv 3: general.basename str = Qwen3-30B-A3B llama_model_loader: - kv 4: general.quantized_by str = Unsloth llama_model_loader: - kv 5: general.size_label str = 30B-A3B llama_model_loader: - kv 6: general.license str = apache-2.0 llama_model_loader: - kv 7: general.license.link str = https://huggingface.co/Qwen/Qwen3-30B..."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "r: - kv 5: general.size_label str = 30B-A3B llama_model_loader: - kv 6: general.license str = apache-2.0 llama_model_loader: - kv 7: general.license.link str = https://huggingface.co/Qwen/Qwen3-30B... llama_model_loader: - kv 8: general.repo_url str = https://huggingface.co/unsloth llama_model_loader: - kv 9: general.base_model.count u32 = 1 llama_model_loader: - kv 10: general.base_model.0.name str = Qwen3 30B A3B llama_model_loader: - kv 11: general.base_model.0.organization str = Qwen llama_model_loader: - kv 12: general.base_model.0.repo_url str = https://huggingface.co/Qwen/Qwen3-30B... llama_model_loader: - kv 13: general.tags arr[str ,2] = [\"unsloth\", \"text-generation\"] llama_model_loader: - kv 14: qwen3moe.block_count u32 = 48 llama_model_loader: - kv 15: qwen3moe.context_length u32 = 40960 llama_model_loader: - kv 16: qwen3moe.embedding_length u32 = 2048 llama_model_loader: - kv 17: qwen3moe.feed_forward_length u32 = 6144 llama_model_loader: - kv 18: qwen3moe.attention.head_count u32 = 32 llama_model_loader: - kv 19: qwen3moe.attention.head_count_kv u32 = 4 llama_model_loader: - kv 20: qwen3moe.rope.freq_base f32 = 1000000.000000 llama_model_loader: - kv 21: qwen3moe.attention.layer_norm_rms_epsilon f32 = 0.000001 llama_model_loader: - kv 22: qwen3moe.expert_used_count u32 = 8 llama_model_loader: - kv 23: qwen3moe.attention.key_length u32 = 128 llama_model_loader: - kv 24: qwen3moe.attention.value_length u32 = 128 llama_model_loader: - kv 25: qwen3moe."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "rt_used_count u32 = 8 llama_model_loader: - kv 23: qwen3moe.attention.key_length u32 = 128 llama_model_loader: - kv 24: qwen3moe.attention.value_length u32 = 128 llama_model_loader: - kv 25: qwen3moe.expert_count u32 = 128 llama_model_loader: - kv 26: qwen3moe.expert_feed_forward_length u32 = 768 llama_model_loader: - kv 27: tokenizer.ggml.model str = gpt2 llama_model_loader: - kv 28: tokenizer.ggml.pre str = qwen2 llama_model_loader: - kv 29: tokenizer.ggml.tokens arr[str ,151936] = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ... llama_model_loader: - kv 30: tokenizer.ggml.token_type arr[i32 ,151936] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... llama_model_loader: - kv 31: tokenizer.ggml.merges arr[str ,151387] = [\"� � \", \"� � � � \", \"i n\", \"� t\",... llama_model_loader: - kv 32: tokenizer.ggml.eos_token_id u32 = 151645 llama_model_loader: - kv 33: tokenizer.ggml.padding_token_id u32 = 151643 llama_model_loader: - kv 34: tokenizer.ggml.bos_token_id u32 = 151643 llama_model_loader: - kv 35: tokenizer.ggml.add_bos_token bool = false llama_model_loader: - kv 36: tokenizer.chat_template str = {%- if tools %}\\n {{- '<|im_start|>... llama_model_loader: - kv 37: general.quantization_version u32 = 2 llama_model_loader: - kv 38: general.file_type u32 = 7 llama_model_loader: - kv 39: quantize.imatrix.file str = Qwen3-30B-A3B-GGUF/imatrix_unsloth.dat llama_model_loader: - kv 40: quantize.imatrix.dataset str = unsloth_calibration_Qwen3-30B-A3B.txt llama_model_loader: - kv 41: quantize.imatrix."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "atrix.file str = Qwen3-30B-A3B-GGUF/imatrix_unsloth.dat llama_model_loader: - kv 40: quantize.imatrix.dataset str = unsloth_calibration_Qwen3-30B-A3B.txt llama_model_loader: - kv 41: quantize.imatrix.entries_count i32 = 384 llama_model_loader: - kv 42: quantize.imatrix.chunks_count i32 = 32 llama_model_loader: - type f32: 241 tensors llama_model_loader: - type q8_0: 338 tensors print_info: file format = GGUF V3 (latest) print_info: file type = Q8_0 print_info: file size = 30.25 GiB (8.51 BPW) load: special tokens cache size = 26 load: token to piece cache size = 0.9311 MB print_info: arch = qwen3moe print_info: vocab_only = 0 print_info: n_ctx_train = 40960 print_info: n_embd = 2048 print_info: n_layer = 48 print_info: n_head = 32 print_info: n_head_kv = 4 print_info: n_rot = 128 print_info: n_swa = 0 print_info: n_swa_pattern = 1 print_info: n_embd_head_k = 128 print_info: n_embd_head_v = 128 print_info: n_gqa = 8 print_info: n_embd_k_gqa = 512 print_info: n_embd_v_gqa = 512 print_info: f_norm_eps = 0.0e+00 print_info: f_norm_rms_eps = 1.0e-06 print_info: f_clamp_kqv = 0.0e+00 print_info: f_max_alibi_bias = 0.0e+00 print_info: f_logit_scale = 0.0e+00 print_info: f_attn_scale = 0.0e+00 print_info: n_ff = 6144 print_info: n_expert = 128 print_info: n_expert_used = 8 print_info: causal attn = 1 print_info: pooling type = 0 print_info: rope type = 2 print_info: rope scaling = linear print_info: freq_base_train = 1000000."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "n_expert = 128 print_info: n_expert_used = 8 print_info: causal attn = 1 print_info: pooling type = 0 print_info: rope type = 2 print_info: rope scaling = linear print_info: freq_base_train = 1000000.0 print_info: freq_scale_train = 1 print_info: n_ctx_orig_yarn = 40960 https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF/blob/main/READ ME.md nstructions to run this model in llama.cpp: Or you can view more detailed instructions here: unsloth.ai/blog/deepseek-r1 Do not forget about <�User�> and <�Assistant�> tokens! - Or use a chat templ ate formatter Obtain the latest llama.cpp at https://github.com/ggerganov/llama.cpp Example with Q8_0 K quantized cache Notice -no-cnv disables auto conversation mo de ./llama.cpp/llama-cli \\ --model unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF/DeepSeek-R1-Distill-Llama -70B-Q4_K_M.gguf \\ --cache-type-k q8_0 \\ --threads 16 \\ --prompt '<�User�>What is 1+1?<�Assistant�>' \\ -no-cnv Example output: <think> Okay, so I need to figure out what 1 plus 1 is. Hmm, where do I even start? I r emember from school that adding numbers is pretty basic, but I want to make sure I understand it properly. Let me think, 1 plus 1. So, I have one item and I add another one. Maybe like a apple plus another apple. If I have one apple and someone gives me another, I n ow have two apples. So, 1 plus 1 should be 2. That makes sense. Wait, but sometimes math can be tricky."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ne. Maybe like a apple plus another apple. If I have one apple and someone gives me another, I n ow have two apples. So, 1 plus 1 should be 2. That makes sense. Wait, but sometimes math can be tricky. Could it be something else? Like, in a different number system maybe? But I think the question is straightforward, usin g regular numbers, not like binary or hexadecimal or anything. I also recall that in arithmetic, addition is combining quantities. So, if you have two quantities of 1, combining them gives you a total of 2. Yeah, that seem s right. Is there a scenario where 1 plus 1 wouldn't be 2? I can't think of any... If you have a GPU (RTX 4090 for example) with 24GB, you can offload multiple lay ers to the GPU for faster processing. If you have multiple GPUs, you can probabl y offload more layers. ./llama.cpp/llama-cli \\ --model unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF/DeepSeek-R1-Distill-Llama-70B -Q4_K_M.gguf --cache-type-k q8_0 --threads 16 --prompt '<�User�>What is 1+1?<�Assistant�>' --n-gpu-layers 20 \\ -no-cnv Finetune LLMs 2-5x faster with 70% less memory via Unsloth! We have a free Google Colab Tesla T4 notebook for Llama 3.1 (8B) here: https://c olab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-A lpaca.ipynb � Finetune for Free All notebooks are beginner friendly! Add your dataset, click \"Run All\", and you' ll get a 2x faster finetuned model which can be exported to GGUF, vLLM or upload ed to Hugging Face."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "Finetune for Free All notebooks are beginner friendly! Add your dataset, click \"Run All\", and you' ll get a 2x faster finetuned model which can be exported to GGUF, vLLM or upload ed to Hugging Face. Unsloth supports Free Notebooks Performance Memory use Llama-3.2 (3B) �� Start on Colab 2.4x faster 58% less Llama-3.2 (11B vision) �� Start on Colab 2x faster 60% less Qwen2 VL (7B) �� Start on Colab 1.8x faster 60% less Qwen2.5 (7B) �� Start on Colab 2x faster 60% less Llama-3.1 (8B) �� Start on Colab 2.4x faster 58% less Phi-3.5 (mini) �� Start on Colab 2x faster 50% less Gemma 2 (9B) �� Start on Colab 2.4x faster 58% less Mistral (7B) �� Start on Colab 2.2x faster 62% less This Llama 3.2 conversational notebook is useful for ShareGPT ChatML / Vicuna te mplates. This text completion notebook is for raw text. This DPO notebook replicates Zeph yr. * Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster. Special Thanks A huge thank you to the DeepSeek team for creating and releasing these models. DeepSeek-R1 DeepSeek-V3 Homepage Chat Hugging Face Discord Wechat Twitter Follow Code License Model License Paper Link�� 1. Introduction We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSee k-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning ( RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated rem arkable performance on reasoning."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "pSee k-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning ( RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated rem arkable performance on reasoning. With RL, DeepSeek-R1-Zero naturally emerged wi th numerous powerful and interesting reasoning behaviors. However, DeepSeek-R1-Z ero encounters challenges such as endless repetition, poor readability, and lang uage mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates cold-start data before RL. DeepSeek -R1 achieves performance comparable to OpenAI-o1 across math, code, and reasonin g tasks. To support the research community, we have open-sourced DeepSeek-R1-Zer o, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama a nd Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models. NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend revi ewing the Usage Recommendation section. 2. Model Summary Post-Training: Large-Scale Reinforcement Learning on the Base Model We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabiliti es such as self-verification, reflection, and generating long CoTs, marking a si gnificant milestone for the research community. Notably, it is the first open re search to validate that reasoning capabilities of LLMs can be incentivized purel y through RL, without the need for SFT. This breakthrough paves the way for futu re advancements in this area. We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with hum an preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities. We believe the pipeline will benefit the industry by creating better models. Distillation: Smaller Models Can Be Powerful Too We demonstrate that the reasoning patterns of larger models can be distilled int o smaller models, resulting in better performance compared to the reasoning patt erns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller model s in the future. Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense m odels that are widely used in the research community."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "search community to distill better smaller model s in the future. Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense m odels that are widely used in the research community. The evaluation results dem onstrate that the distilled smaller dense models perform exceptionally well on b enchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community. 3. Model Downloads DeepSeek-R1 Models Model #Total Params #Activated Params Context Length Download DeepSeek-R1-Zero 671B 37B 128K � HuggingFace DeepSeek-R1 671B 37B 128K � HuggingFace DeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. For more d etails regarding the model architecture, please refer to DeepSeek-V3 repository. DeepSeek-R1-Distill Models Model Base Model Download DeepSeek-R1-Distill-Qwen-1.5B Qwen2.5-Math-1.5B � HuggingFace DeepSeek-R1-Distill-Qwen-7B Qwen2.5-Math-7B � HuggingFace DeepSeek-R1-Distill-Llama-8B Llama-3.1-8B � HuggingFace DeepSeek-R1-Distill-Qwen-14B Qwen2.5-14B � HuggingFace DeepSeek-R1-Distill-Qwen-32B Qwen2.5-32B � HuggingFace DeepSeek-R1-Distill-Llama-70B Llama-3.3-70B-Instruct � HuggingFace DeepSeek-R1-Distill models are fine-tuned based on open-source models, using sam ples generated by DeepSeek-R1. We slightly change their configs and tokenizers. Please use our setting to run these models. 4."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "k-R1-Distill models are fine-tuned based on open-source models, using sam ples generated by DeepSeek-R1. We slightly change their configs and tokenizers. Please use our setting to run these models. 4. Evaluation Results DeepSeek-R1-Evaluation For all our models, the maximum generation length is set to 32,768 tokens. For b enchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0 .95$, and generate 64 responses per query to estimate pass@1. Category Benchmark (Metric) Claude-3.5-Sonnet-1022 GPT-4o 0513 DeepSeek V3 OpenAI o1-mini OpenAI o1-1217 DeepSeek R1 Architecture - - MoE - - MoE # Activated Params - - 37B - - 37B # Total Params - - 671B - - 671B English MMLU (Pass@1) 88.3 87.2 88.5 85.2 91.8 90.8 MMLU-Redux (EM) 88.9 88.0 89.1 86.7 - 92.9 MMLU-Pro (EM) 78.0 72.6 75.9 80.3 - 84.0 DROP (3-shot F1) 88.3 83.7 91.6 83.9 90.2 92.2 IF-Eval (Prompt Strict) 86.5 84.3 86.1 84.8 - 83.3 GPQA-Diamond (Pass@1) 65.0 49.9 59.1 60.0 75.7 71.5 SimpleQA (Correct) 28.4 38.2 24.9 7.0 47.0 30.1 FRAMES (Acc.) 72.5 80.5 73.3 76.9 - 82.5 AlpacaEval2.0 (LC-winrate) 52.0 51.1 70.0 57.8 - 87.6 ArenaHard (GPT-4-1106) 85.2 80.4 85.5 92.0 - 92.3 Code LiveCodeBench (Pass@1-COT) 33.8 34.2 - 53.8 63.4 65.9 Codeforces (Percentile) 20.3 23.6 58.7 93.4 96.6 96.3 Codeforces (Rating) 717 759 1134 1820 2061 2029 SWE Verified (Resolved) 50.8 38.8 42.0 41.6 48.9 49.2 Aider-Polyglot (Acc.) 45.3 16.0 49.6 32.9 61.7 53.3 Math AIME 2024 (Pass@1) 16.0 9.3 39.2 63.6 79.2 79."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "orces (Rating) 717 759 1134 1820 2061 2029 SWE Verified (Resolved) 50.8 38.8 42.0 41.6 48.9 49.2 Aider-Polyglot (Acc.) 45.3 16.0 49.6 32.9 61.7 53.3 Math AIME 2024 (Pass@1) 16.0 9.3 39.2 63.6 79.2 79.8 MATH-500 (Pass@1) 78.3 74.6 90.2 90.0 96.4 97.3 CNMO 2024 (Pass@1) 13.1 10.8 43.2 67.6 - 78.8 Chinese CLUEWSC (EM) 85.4 87.9 90.9 89.9 - 92.8 C-Eval (EM) 76.7 76.0 86.5 68.9 - 91.8 C-SimpleQA (Correct) 55.4 58.7 68.0 40.3 - 63.7 Distilled Model Evaluation Model AIME 2024 pass@1 AIME 2024 cons@64 MATH-500 pass@1 GPQA Dia mond pass@1 LiveCodeBench pass@1 CodeForces rating GPT-4o-0513 9.3 13.4 74.6 49.9 32.9 759 Claude-3.5-Sonnet-1022 16.0 26.7 78.3 65.0 38.9 717 o1-mini 63.6 80.0 90.0 60.0 53.8 1820 QwQ-32B-Preview 44.0 60.0 90.6 54.5 41.9 1316 DeepSeek-R1-Distill-Qwen-1.5B 28.9 52.7 83.9 33.8 16.9 954 DeepSeek-R1-Distill-Qwen-7B 55.5 83.3 92.8 49.1 37.6 1189 DeepSeek-R1-Distill-Qwen-14B 69.7 80.0 93.9 59.1 53.1 1481 DeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2 1691 DeepSeek-R1-Distill-Llama-8B 50.4 80.0 89.1 49.0 39.6 1205 DeepSeek-R1-Distill-Llama-70B 70.0 86.7 94.5 65.2 57.5 1633 5. Chat Website & API Platform You can chat with DeepSeek-R1 on DeepSeek's official website: chat.deepseek.com, and switch on the button \"DeepThink\" We also provide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.co m 6. How to Run Locally DeepSeek-R1 Models Please visit DeepSeek-V3 repo for more information about running DeepSeek-R1 loc ally."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ovide OpenAI-Compatible API at DeepSeek Platform: platform.deepseek.co m 6. How to Run Locally DeepSeek-R1 Models Please visit DeepSeek-V3 repo for more information about running DeepSeek-R1 loc ally. DeepSeek-R1-Distill Models DeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama m odels. For instance, you can easily start a service using vLLM: vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --m ax-model-len 32768 --enforce-eager You can also easily start a service using SGLang python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2 Usage Recommendations We recommend adhering to the following configurations when utilizing the DeepSee k-R1 series models, including benchmarking, to achieve the expected performance: Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs. Avoid adding a system prompt; all instructions should be contained within the us er prompt. For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{} .\" When evaluating model performance, it is recommended to conduct multiple tests a nd average the results. 7. License This code repository and the model weights are licensed under the MIT License."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "{} .\" When evaluating model performance, it is recommended to conduct multiple tests a nd average the results. 7. License This code repository and the model weights are licensed under the MIT License. D eepSeek-R1 series support commercial use, allow for any modifications and deriva tive works, including, but not limited to, distillation for training other LLMs. Please note that: DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill- Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from Qwen-2.5 series, whic h are originally licensed under Apache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1. DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under llama3.1 license. DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is origi nally licensed under llama3.3 license. 8. Citation @misc{deepseekai2025deepseekr1incentivizingreasoningcapability, title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinfor cement Learning}, author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junx iao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wa ng and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "Daya Guo and Dejian Yang and Haowei Zhang and Junx iao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wa ng and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhib in Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue a nd Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao an d Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and D ongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and H aocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingya ng Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and L iyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Min ghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi G e and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin a nd Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "Runji Wang and R. J. Chen and R. L. Jin a nd Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and T ianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An an d Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuech eng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xian zu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Y u and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao an d Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xion g and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu a nd Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yu nxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "uxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu a nd Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yu nxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zh ewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zh en Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang}, year={2025}, eprint={2501.12948}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2501.12948}, } 9. Contact If you have any questions, please raise an issue or contact us at service@deepse ek.com. + https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF/blob/ main/README.md https://huggingface.co/unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF/blob/main/READ ME.md nstructions to run this model in llama.cpp: Or you can view more detailed instructions here: unsloth.ai/blog/deepseek-r1 Do not forget about <�User�> and <�Assistant�> tokens! - Or use a chat templ ate formatter Obtain the latest llama.cpp at https://github.com/ggerganov/llama.cpp Example with Q8_0 K quantized cache Notice -no-cnv disables auto conversation mo de ./llama.cpp/llama-cli \\ --model unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF/DeepSeek-R1-Distill-Llama -70B-Q4_K_M."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "pp Example with Q8_0 K quantized cache Notice -no-cnv disables auto conversation mo de ./llama.cpp/llama-cli \\ --model unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF/DeepSeek-R1-Distill-Llama -70B-Q4_K_M.gguf \\ --cache-type-k q8_0 \\ --threads 16 \\ --prompt '<�User�>What is 1+1?<�Assistant�>' \\ -no-cnv Example output: <think> Okay, so I need to figure out what 1 plus 1 is. Hmm, where do I even start? I r emember from school that adding numbers is pretty basic, but I want to make sure I understand it properly. Let me think, 1 plus 1. So, I have one item and I add another one. Maybe like a apple plus another apple. If I have one apple and someone gives me another, I n ow have two apples. So, 1 plus 1 should be 2. That makes sense. Wait, but sometimes math can be tricky. Could it be something else? Like, in a different number system maybe? But I think the question is straightforward, usin g regular numbers, not like binary or hexadecimal or anything. I also recall that in arithmetic, addition is combining quantities. So, if you have two quantities of 1, combining them gives you a total of 2. Yeah, that seem s right. Is there a scenario where 1 plus 1 wouldn't be 2? I can't think of any... If you have a GPU (RTX 4090 for example) with 24GB, you can offload multiple lay ers to the GPU for faster processing. If you have multiple GPUs, you can probabl y offload more layers. ./llama.cpp/llama-cli \\ --model unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF/DeepSeek-R1-Distill-Llama-70B -Q4_K_M."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "r faster processing. If you have multiple GPUs, you can probabl y offload more layers. ./llama.cpp/llama-cli \\ --model unsloth/DeepSeek-R1-Distill-Llama-70B-GGUF/DeepSeek-R1-Distill-Llama-70B -Q4_K_M.gguf --cache-type-k q8_0 --threads 16 --prompt '<�User�>What is 1+1?<�Assistant�>' --n-gpu-layers 20 \\ -no-cnv + ./llama.cpp/build/bin/llama-cli \\ Run ./llama.cpp/build/bin/llama-cli \\ --model ./llama.cpp/models/DeepSeek-R1-Distill-Llama-70B-Q4_K_M.gguf \\ --cache-type-k q8_0 \\ --threads 16 \\ --prompt '<�User�>What is 1+1?<�Assistant�>' \\ -no-cnv ljubomir@macbook2(:):~$ ./llama.cpp/build/bin/llama-cli --model ./llama.cpp/ models/DeepSeek-R1-Distill-Llama-70B-Q4_K_M.gguf --cache-type-k q8_0 --t hreads 16 --prompt '<�User�>What is 1+1?<�Assistant�>' -no-cnv build: 4719 (e6d7a014) with Apple clang version 16.0.0 (clang-1600.0.26.6) for a rm64-apple-darwin24.3.0 main: llama backend init main: load the model and apply lora adapter, if any llama_model_load_from_file_impl: using device Metal (Apple M2 Max) - 73727 MiB f ree llama_model_loader: loaded meta data with 39 key-value pairs and 724 tensors fro m ./llama.cpp/models/DeepSeek-R1-Distill-Llama-70B-Q4_K_M.gguf (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not appl y in this output. llama_model_loader: - kv 0: general.architecture str = llama llama_model_loader: - kv 1: general.type str = model llama_model_loader: - kv 2: general."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "Note: KV overrides do not appl y in this output. llama_model_loader: - kv 0: general.architecture str = llama llama_model_loader: - kv 1: general.type str = model llama_model_loader: - kv 2: general.name str = DeepSeek R1 Distill Llama 70B llama_model_loader: - kv 3: general.organization str = Deepseek Ai llama_model_loader: - kv 4: general.basename str = DeepSeek-R1-Distill-Llama llama_model_loader: - kv 5: general.size_label str = 70B llama_model_loader: - kv 6: general.license str = llama3.3 llama_model_loader: - kv 7: general.base_model.count u32 = 1 llama_model_loader: - kv 8: general.base_model.0.name str = DeepSeek R1 Distill Llama 70B llama_model_loader: - kv 9: general.base_model.0.organization str = Deepseek Ai llama_model_loader: - kv 10: general.base_model.0.repo_url str = https://huggingface.co/deepseek-ai/De... llama_model_loader: - kv 11: general.tags arr[str ,6] = [\"deepseek\", \"unsloth\", \"transformers... llama_model_loader: - kv 12: general.languages arr[str ,1] = [\"en\"] llama_model_loader: - kv 13: llama.block_count u32 = 80 llama_model_loader: - kv 14: llama.context_length u32 = 131072 llama_model_loader: - kv 15: llama.embedding_length u32 = 8192 llama_model_loader: - kv 16: llama.feed_forward_length u32 = 28672 llama_model_loader: - kv 17: llama.attention.head_count u32 = 64 llama_model_loader: - kv 18: llama.attention.head_count_kv u32 = 8 llama_model_loader: - kv 19: llama.rope.freq_base f32 = 500000.000000 llama_model_loader: - kv 20: llama.attention."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ad_count u32 = 64 llama_model_loader: - kv 18: llama.attention.head_count_kv u32 = 8 llama_model_loader: - kv 19: llama.rope.freq_base f32 = 500000.000000 llama_model_loader: - kv 20: llama.attention.layer_norm_rms_epsilon f32 = 0.000010 llama_model_loader: - kv 21: llama.attention.key_length u32 = 128 llama_model_loader: - kv 22: llama.attention.value_length u32 = 128 llama_model_loader: - kv 23: llama.vocab_size u32 = 128256 llama_model_loader: - kv 24: llama.rope.dimension_count u32 = 128 llama_model_loader: - kv 25: tokenizer.ggml.model str = gpt2 llama_model_loader: - kv 26: tokenizer.ggml.pre str = llama-bpe llama_model_loader: - kv 27: tokenizer.ggml.tokens arr[str ,128256] = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ... llama_model_loader: - kv 28: tokenizer.ggml.token_type arr[i32 ,128256] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... llama_model_loader: - kv 29: tokenizer.ggml.merges arr[str ,280147] = [\"� � \", \"� � � � \", \"� � � � \", \"... llama_model_loader: - kv 30: tokenizer.ggml.bos_token_id u32 = 128000 llama_model_loader: - kv 31: tokenizer.ggml.eos_token_id u32 = 128001 llama_model_loader: - kv 32: tokenizer.ggml.padding_token_id u32 = 128004 llama_model_loader: - kv 33: tokenizer.ggml.add_bos_token bool = true llama_model_loader: - kv 34: tokenizer.ggml.add_eos_token bool = false llama_model_loader: - kv 35: tokenizer.chat_template str = {% if not add_generation_prompt is de... llama_model_loader: - kv 36: tokenizer.ggml."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "der: - kv 34: tokenizer.ggml.add_eos_token bool = false llama_model_loader: - kv 35: tokenizer.chat_template str = {% if not add_generation_prompt is de... llama_model_loader: - kv 36: tokenizer.ggml.add_space_prefix bool = false llama_model_loader: - kv 37: general.quantization_version u32 = 2 llama_model_loader: - kv 38: general.file_type u32 = 15 llama_model_loader: - type f32: 162 tensors llama_model_loader: - type q4_K: 441 tensors llama_model_loader: - type q5_K: 40 tensors llama_model_loader: - type q6_K: 81 tensors print_info: file format = GGUF V3 (latest) print_info: file type = Q4_K - Medium print_info: file size = 39.59 GiB (4.82 BPW) load: special_eos_id is not in special_eog_ids - the tokenizer config may be inc orrect load: special tokens cache size = 256 load: token to piece cache size = 0.7999 MB print_info: arch = llama print_info: vocab_only = 0 print_info: n_ctx_train = 131072 print_info: n_embd = 8192 print_info: n_layer = 80 print_info: n_head = 64 print_info: n_head_kv = 8 print_info: n_rot = 128 print_info: n_swa = 0 print_info: n_embd_head_k = 128 print_info: n_embd_head_v = 128 print_info: n_gqa = 8 print_info: n_embd_k_gqa = 1024 print_info: n_embd_v_gqa = 1024 print_info: f_norm_eps = 0.0e+00 print_info: f_norm_rms_eps = 1.0e-05 print_info: f_clamp_kqv = 0.0e+00 print_info: f_max_alibi_bias = 0.0e+00 print_info: f_logit_scale = 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "_info: n_embd_v_gqa = 1024 print_info: f_norm_eps = 0.0e+00 print_info: f_norm_rms_eps = 1.0e-05 print_info: f_clamp_kqv = 0.0e+00 print_info: f_max_alibi_bias = 0.0e+00 print_info: f_logit_scale = 0.0e+00 print_info: n_ff = 28672 print_info: n_expert = 0 print_info: n_expert_used = 0 print_info: causal attn = 1 print_info: pooling type = 0 print_info: rope type = 0 print_info: rope scaling = linear print_info: freq_base_train = 500000.0 print_info: freq_scale_train = 1 print_info: n_ctx_orig_yarn = 131072 >>>>>>> refs/heads/macbook2/master print_info: rope_finetuned = unknown print_info: ssm_d_conv = 0 print_info: ssm_d_inner = 0 print_info: ssm_d_state = 0 print_info: ssm_dt_rank = 0 print_info: ssm_dt_b_c_rms = 0 <<<<<<< HEAD print_info: model type = 30B.A3B print_info: model params = 30.53 B print_info: general.name = Qwen3-30B-A3B print_info: n_ff_exp = 768 print_info: vocab type = BPE print_info: n_vocab = 151936 print_info: n_merges = 151387 print_info: BOS token = 151643 '<|endoftext|>' print_info: EOS token = 151645 '<|im_end|>' print_info: EOT token = 151645 '<|im_end|>' print_info: PAD token = 151643 '<|endoftext|>' print_info: LF token = 198 '�' print_info: FIM PRE token = 151659 '<|fim_prefix|>' print_info: FIM SUF token = 151661 '<|fim_suffix|>' print_info: FIM MID token = 151660 '<|fim_middle|>' print_info: FIM PAD token = 151662 '<|fim_pad|>' print_info: FIM REP token = 151663 '<|repo_name|>' print_info: FIM SEP token = 151664 '<|file_sep|>' print_info: EOG to"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "token = 151660 '<|fim_middle|>' print_info: FIM PAD token = 151662 '<|fim_pad|>' print_info: FIM REP token = 151663 '<|repo_name|>' print_info: FIM SEP token = 151664 '<|file_sep|>' print_info: EOG token = 151643 '<|endoftext|>' print_info: EOG token = 151645 '<|im_end|>' print_info: EOG token = 151662 '<|fim_pad|>' print_info: EOG token = 151663 '<|repo_name|>' print_info: EOG token = 151664 '<|file_sep|>' print_info: max token length = 256 load_tensors: loading model tensors, this can take a while... (mmap = true) load_tensors: CPU_Mapped model buffer size = 30973.40 MiB ................................................................................ .................... llama_context: constructing llama_context llama_context: n_seq_max = 1 llama_context: n_ctx = 4096 llama_context: n_ctx_per_seq = 4096 llama_context: n_batch = 2048 llama_context: n_ubatch = 512 llama_context: causal_attn = 1 llama_context: flash_attn = 0 llama_context: freq_base = 1000000.0 llama_context: freq_scale = 1 llama_context: n_ctx_per_seq (4096) < n_ctx_train (40960) -- the full capacity o f the model will not be utilized llama_context: CPU output buffer size = 0.58 MiB init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 48, can_shift = 1 init: CPU KV buffer size = 384.00 MiB llama_context: KV self size = 384.00 MiB, K (f16): 192.00 MiB, V (f16): 192. 00 MiB llama_context: CPU compute buffer size = 300."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "16', n_layer = 48, can_shift = 1 init: CPU KV buffer size = 384.00 MiB llama_context: KV self size = 384.00 MiB, K (f16): 192.00 MiB, V (f16): 192. 00 MiB llama_context: CPU compute buffer size = 300.75 MiB llama_context: graph nodes = 3126 llama_context: graph splits = 1 common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096 common_init_from_params: warming up the model with an empty run - please wait .. . (--no-warmup to disable) main: llama threadpool init, n_threads = 10 main: chat template is available, enabling conversation mode (disable it with -n o-cnv) *** User-specified prompt will pre-start conversation, did you mean to set --sys tem-prompt (-sys) instead? main: chat template example: <|im_start|>system You are a helpful assistant<|im_end|> <|im_start|>user Hello<|im_end|> <|im_start|>assistant Hi there<|im_end|> <|im_start|>user How are you?<|im_end|> <|im_start|>assistant system_info: n_threads = 10 (n_threads_batch = 10) / 10 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPEN MP = 1 | AARCH64_REPACK = 1 | main: interactive mode on. sampler seed: 3760237940 sampler params: repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, p resence_penalty = 0.000 dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_pe nalty_last_n = 4096 top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_t hreshold = 0.100, typical_p = 1.000, top_n_sigma = -1."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "00, dry_base = 1.750, dry_allowed_length = 2, dry_pe nalty_last_n = 4096 top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_t hreshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> t op-p -> min-p -> xtc -> temp-ext -> dist generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 0 == Running in interactive mode. == - Press Ctrl+C to interject at any time. - Press Return to return control to the AI. - To return control without starting a new line, end your input with '/'. - If you want to submit another line, end your input with '\\'. - Not using system message. To change it, set a different value via -sys PROMPT user How many letters R in the word STRAWBERRY? assistant <think> .................................................................... .................................................................... .................................................................... </think> The word **STRAWBERRY** is spelled as: **S-T-R-A-W-B-E-R-R-Y**. Breaking it down letter by letter: 1. **S** 2. **T** 3. **R** (1st **R**) 4. **A** 5. **W** 6. **B** 7. **E** 8. **R** (2nd **R**) 9. **R** (3rd **R**) 10. **Y** There are **three** instances of the letter **R** in the word. **Answer:** 3 > llama_perf_sampler_print: sampling time = 91.61 ms / 810 runs ( 0 .11 ms per token, 8842."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "9. **R** (3rd **R**) 10. **Y** There are **three** instances of the letter **R** in the word. **Answer:** 3 > llama_perf_sampler_print: sampling time = 91.61 ms / 810 runs ( 0 .11 ms per token, 8842.31 tokens per second) llama_perf_context_print: load time = 4129.60 ms llama_perf_context_print: prompt eval time = 1357.26 ms / 20 tokens ( 67 .86 ms per token, 14.74 tokens per second) llama_perf_context_print: eval time = 210115.88 ms / 789 runs ( 266 .31 ms per token, 3.76 tokens per second) llama_perf_context_print: total time = 1401353.05 ms / 809 tokens Interrupted by user print_info: model type = 70B print_info: model params = 70.55 B print_info: general.name = DeepSeek R1 Distill Llama 70B print_info: vocab type = BPE print_info: n_vocab = 128256 print_info: n_merges = 280147 print_info: BOS token = 128000 '<�begin�of�sentence�>' print_info: EOS token = 128001 '<�end�of�sentence�>' print_info: EOT token = 128001 '<�end�of�sentence�>' print_info: EOM token = 128008 '<|eom_id|>' print_info: PAD token = 128004 '<|finetune_right_pad_id|>' print_info: LF token = 198 '�' print_info: EOG token = 128001 '<�end�of�sentence�>' print_info: EOG token = 128008 '<|eom_id|>' print_info: EOG token = 128009 '<|eot_id|>' print_info: max token length = 256 load_tensors: offloading 80 repeating layers to GPU load_tensors: offloading output layer to GPU load_tensors: offloaded 81/81 layers to GPU load_tensors: Metal_Mapped model buffer size = 40543."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "= 256 load_tensors: offloading 80 repeating layers to GPU load_tensors: offloading output layer to GPU load_tensors: offloaded 81/81 layers to GPU load_tensors: Metal_Mapped model buffer size = 40543.11 MiB load_tensors: CPU_Mapped model buffer size = 563.62 MiB llama_init_from_model: n_seq_max = 1 llama_init_from_model: n_ctx = 4096 llama_init_from_model: n_ctx_per_seq = 4096 llama_init_from_model: n_batch = 2048 llama_init_from_model: n_ubatch = 512 llama_init_from_model: flash_attn = 0 llama_init_from_model: freq_base = 500000.0 llama_init_from_model: freq_scale = 1 llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full c apacity of the model will not be utilized ggml_metal_init: allocating ggml_metal_init: found device: Apple M2 Max ggml_metal_init: picking default device: Apple M2 Max ggml_metal_init: using embedded metal library ggml_metal_init: GPU name: Apple M2 Max ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008) ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003) ggml_metal_init: GPU family: MTLGPUFamilyMetal3 (5001) ggml_metal_init: simdgroup reduction = true ggml_metal_init: simdgroup matrix mul. = true ggml_metal_init: has residency sets = true ggml_metal_init: has bfloat = true ggml_metal_init: use bfloat = false ggml_metal_init: hasUnifiedMemory = true ggml_metal_init: recommendedMaxWorkingSetSize = 77309."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "l_init: has residency sets = true ggml_metal_init: has bfloat = true ggml_metal_init: use bfloat = false ggml_metal_init: hasUnifiedMemory = true ggml_metal_init: recommendedMaxWorkingSetSize = 77309.41 MB ggml_metal_init: skipping kernel_get_rows_bf16 (not supporte d) ggml_metal_init: skipping kernel_mul_mv_bf16_f32 (not supporte d) ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row (not supporte d) ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4 (not supporte d) ggml_metal_init: skipping kernel_mul_mv_bf16_bf16 (not supporte d) ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32 (not supporte d) ggml_metal_init: skipping kernel_mul_mm_bf16_f32 (not supporte d) ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256 (not supporte d) ggml_metal_init: skipping kernel_cpy_f32_bf16 (not supporte d) ggml_metal_init: skipping kernel_cpy_bf16_f32 (not supporte d) ggml_metal_init: skipp"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ernel_flash_attn_ext_vec_bf16_h256 (not supporte d) ggml_metal_init: skipping kernel_cpy_f32_bf16 (not supporte d) ggml_metal_init: skipping kernel_cpy_bf16_f32 (not supporte d) ggml_metal_init: skipping kernel_cpy_bf16_bf16 (not supporte d) llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'q8_0', type_v = 'f16 ', n_layer = 80, can_shift = 1 llama_kv_cache_init: Metal KV buffer size = 980.00 MiB llama_init_from_model: KV self size = 980.00 MiB, K (q8_0): 340.00 MiB, V (f1 6): 640.00 MiB llama_init_from_model: CPU output buffer size = 0.49 MiB llama_init_from_model: Metal compute buffer size = 584.00 MiB llama_init_from_model: CPU compute buffer size = 24.01 MiB llama_init_from_model: graph nodes = 2566 llama_init_from_model: graph splits = 2 common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096 common_init_from_params: warming up the model with an empty run - please wait .. . (--no-warmup to disable) main: llama threadpool init, n_threads = 16 system_info: n_threads = 16 (n_threads_batch = 16) / 12 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | sampler seed: 3589105319 sampler params: repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, p resence_penalty = 0.000 dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_pe nalty_last_n = 4096 top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "y_penalty = 0.000, p resence_penalty = 0.000 dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_pe nalty_last_n = 4096 top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_t hreshold = 0.100, typical_p = 1.000, temp = 0.800 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> t op-p -> min-p -> xtc -> temp-ext -> dist generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1 What is 1+1?<think> First, I recognize that the problem is asking for the sum of 1 and 1. I recall that in basic arithmetic, adding two numbers combines their values to p roduce a total. Therefore, when I add 1 and 1 together, the result is 2. </think> Certainly! Let's solve the problem step by step. **Problem:** What is \\(1 + 1\\)? **Solution:** 1. **Understand the Operation:** We are asked to add two numbers: 1 and 1. 2. **Perform the Addition:** \\[ 1 + 1 = 2 \\] 3. **Conclusion:** The sum of 1 and 1 is **2**. **Final Answer:** \\(\\boxed{2}\\) [end of text] llama_perf_sampler_print: sampling time = 8.99 ms / 176 runs ( 0 .05 ms per token, 19579.49 tokens per second) llama_perf_context_print: load time = 25525.19 ms llama_perf_context_print: prompt eval time = 673.78 ms / 10 tokens ( 67 .38 ms per token, 14.84 tokens per second) llama_perf_context_print: eval time = 34852.22 ms / 165 runs ( 211 .23 ms per token, 4.73 tokens per second) llama_perf_context_print: total time = 35570."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "67 .38 ms per token, 14.84 tokens per second) llama_perf_context_print: eval time = 34852.22 ms / 165 runs ( 211 .23 ms per token, 4.73 tokens per second) llama_perf_context_print: total time = 35570.00 ms / 175 tokens ggml_metal_free: deallocating ljubomir@macbook2(:):~$ LJ Wed 30 Apr 15:48:18 BST 2025 + https://unsloth.ai/blog/deepseek-r1-0528 https://x.com/danielhanchen/status/1928278088951157116 https://unsloth.ai/blog/deepseek-r1-0528 Run DeepSeek-R1-0528 Dynamic 1-bit GGUFs May 29, 2025 � By Daniel & Michael DeepSeek-R1-0528 is DeepSeek's new update to their R1 reasoning model. R1-0528 i s the world's most powerful open-source model, rivalling OpenAI's GPT-4.5, o3 an d Google's Gemini 2.5 Pro. DeepSeek also released a R1-0528 distilled version by fine-tuning Qwen3 (8B). Th e distill achieves the same performance as Qwen3 (235B). Qwen3 GGUF: DeepSeek-R1 -0528-Qwen3-8B-GGUF You can also fine-tune the Qwen3 model with Unsloth. You can run the model using Unsloth's 1.78-bit Dynamic 2.0 GGUFs on your favorit e inference frameworks. We quantized DeepSeek�s R1 671B parameter model from 720 GB down to 185GB - a 75% size reduction. Recommended: Read our Complete Guide for a walkthrough on how to run DeepSeek-R1 -0528 locally. To ensure the best tradeoff between accuracy and size, we do not to quantize all layers, but selectively quantize e.g. the MoE layers to lower bit, and leave at tention and other layers in 4 or 6bit."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "To ensure the best tradeoff between accuracy and size, we do not to quantize all layers, but selectively quantize e.g. the MoE layers to lower bit, and leave at tention and other layers in 4 or 6bit. And grab our full DeepSeek-R1-0528 GGUFs here �How to Run DeepSeek-R1-0528 For DeepSeek-R1-0528-Qwen3-8B, the model can pretty much fit in any setup, and e ven those with as less as 20GB RAM. There is no need for any prep beforehand. Qwen3 and the full R1-0528 model uses the same settings and chat templates. According to DeepSeek, these are the recommended settings for R1 (R1-0528 should use the same settings) inference: - Set the temperature 0.6to reduce repetition and incoherence. - Set top_p to 0.95 (recommended) - Run multiple tests and average results for reliable evaluation. For optimal runtime performance, we recommend using the 2.71-bit Dynamic version and ensuring you have at least 80GB of combined VRAM and system RAM. While it's technically possible to run the model without a GPU, we advise against it, unle ss you're leveraging Apple's unified memory chips. For the 1.78-bit quantization: - On 1x 24GB GPU (with all layers offloaded), you can expect up to 20 tokens/sec ond throughput and around 4 tokens/second for single-user inference. - Try to have a combination of RAM + VRAM that adds up to the size of the quant you're downloading. - A 24GB GPU like the RTX 4090 should achieve 3 tokens/second, depending on work load and configuration."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "y to have a combination of RAM + VRAM that adds up to the size of the quant you're downloading. - A 24GB GPU like the RTX 4090 should achieve 3 tokens/second, depending on work load and configuration. � How to Run R1-0528-Qwen3-8B in Ollama: Install ollama if you haven't already! You can only run models up to 32B in size . To run the full 720GB R1-0528 model, see here. apt-get update apt-get install pciutils -y curl -fsSL https://ollama.com/install.sh | sh Run the model! Note you can call ollama serve in another terminal if it fails! W e include all our fixes and suggested parameters (temperature etc) in params in our Hugging Face upload! ollama run hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q4_K_XL To disable thinking, use (or you can set it in the system prompt): >>> Write your prompt here � How to Run R1-0528 in llama.cpp: Obtain the latest llama.cpp on GitHub here. You can follow the build instruction s below as well. Change -DGGML_CUDA=ON to -DGGML_CUDA=OFF if you don't have a GP U or just want CPU inference. apt-get update apt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y git clone https://github.com/ggml-org/llama.cpp cmake llama.cpp -B llama.cpp/build \\ -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON cmake --build llama.cpp/build --config Release -j --clean-first --target llama-q uantize llama-cli llama-gguf-split cp llama.cpp/build/bin/llama-* llama.cpp If you want to use llama."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "N -DLLAMA_CURL=ON cmake --build llama.cpp/build --config Release -j --clean-first --target llama-q uantize llama-cli llama-gguf-split cp llama.cpp/build/bin/llama-* llama.cpp If you want to use llama.cpp directly to load models, you can do the below: (:IQ 1_S) is the quantization type. You can also download via Hugging Face (point 3). This is similar to ollama run . Use export LLAMA_CACHE=\"folder\" to force llama. cpp to save to a specific location. export LLAMA_CACHE=\"unsloth/DeepSeek-R1-0528-GGUF\" ./llama.cpp/llama-cli \\ -hf unsloth/DeepSeek-R1-0528-GGUF:IQ1_S \\ --cache-type-k q4_0 \\ --threads -1 \\ --n-gpu-layers 99 \\ --prio 3 \\ --temp 0.6 \\ --top_p 0.95 \\ --min_p 0.01 \\ --ctx-size 16384 \\ --seed 3407 \\ -ot \".ffn_.*_exps.=CPU\" Download the model via (after installing pip install huggingface_hub hf_transfer ). You can choose UD-IQ1_S(dynamic 1.78bit quant) or other quantized versions l ike Q4_K_M . I recommend using our 2.7bit dynamic quant UD-Q2_K_XL to balance si ze and accuracy. # !pip install huggingface_hub hf_transfer import os os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\" # Can sometimes rate limit, so set to 0 to disable from huggingface_hub import snapshot_download snapshot_download( repo_id = \"unsloth/DeepSeek-R1-0528-GGUF\", local_dir = \"unsloth/DeepSeek-R1-0528-GGUF\", allow_patterns = [\"*UD-IQ1_S*\"], # Dynamic 1bit (185GB) Use \"*UD-Q2_K_XL*\" f or Dynamic 2bit (251GB) ) Run Unsloth's Flappy Bird test as described in our 1.58bit Dynamic Quant for Dee pSeek R1."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "GGUF\", allow_patterns = [\"*UD-IQ1_S*\"], # Dynamic 1bit (185GB) Use \"*UD-Q2_K_XL*\" f or Dynamic 2bit (251GB) ) Run Unsloth's Flappy Bird test as described in our 1.58bit Dynamic Quant for Dee pSeek R1. Edit --threads 32 for the number of CPU threads, --ctx-size 16384 for context le ngth, --n-gpu-layers 2 for GPU offloading on how many layers. Try adjusting it i f your GPU goes out of memory. Also remove it if you have CPU only inference. ./llama.cpp/llama-cli \\ --model unsloth/DeepSeek-R1-0528-GGUF/UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-000 01-of-00004.gguf \\ --cache-type-k q4_0 \\ --threads -1 \\ --n-gpu-layers 99 \\ --prio 3 \\ --temp 0.6 \\ --top_p 0.95 \\ --min_p 0.01 \\ --ctx-size 16384 \\ --seed 3407 \\ -ot \".ffn_.*_exps.=CPU\" \\ -no-cnv \\ --prompt \"<�User�>Create a Flappy Bird game in Python. You must include th ese things:\\n1. You must use pygame.\\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\\n3. Pressing SPACE multiple times will accelerate the bird.\\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.\\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\\n8. When you lose, show the best score."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "t if you pass pipes and don't hit them.\\n7. Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.\\n8. When you lose, show the best score. Make the text inside the screen. Pressing q or E sc will quit the game. Restarting is pressing SPACE again.\\nThe final game shoul d be inside a markdown section in Python. Check your code for errors and fix the m before the final markdown section.<�Assistant�>\" We also test our dynamic quants via the Heptagon test which tests the model on c reating a basic physics engine to simulate balls rotating in a moving enclosed h eptagon shape. ./llama.cpp/llama-cli \\ --model unsloth/DeepSeek-R1-0528-GGUF/UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-000 01-of-00004.gguf \\ --cache-type-k q4_0 \\ --threads -1 \\ --n-gpu-layers 99 \\ --prio 3 \\ --temp 0.6 \\ --top_p 0.95 \\ --min_p 0.01 \\ --ctx-size 16384 \\ --seed 3407 \\ -ot \".ffn_.*_exps.=CPU\" \\ -no-cnv \\ --prompt \"<�User�>Write a Python program that shows 20 balls bouncing insi de a spinning heptagon:\\n- All balls have the same radius.\\n- All balls have a n umber on it from 1 to 20.\\n- All balls drop from the heptagon center when starti ng.\\n- Colors are: #f8b862, #f6ad49, #f39800, #f08300, #ec6d51, #ee7948, #ed6d3d , #ec6800, #ec6800, #ee7800, #eb6238, #ea5506, #ea5506, #eb6101, #e49e61, #e45e3 2, #e17b34, #dd7a56, #db8449, #d66a35\\n- The balls should be affected by gravity and friction, and they must bounce off the rotating walls realistically."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "38, #ea5506, #ea5506, #eb6101, #e49e61, #e45e3 2, #e17b34, #dd7a56, #db8449, #d66a35\\n- The balls should be affected by gravity and friction, and they must bounce off the rotating walls realistically. There should also be collisions between balls.\\n- The material of all the balls determ ines that their impact bounce height will not exceed the radius of the heptagon, but higher than ball radius.\\n- All balls rotate with friction, the numbers on the ball can be used to indicate the spin of the ball.\\n- The heptagon is spinni ng around its center, and the speed of spinning is 360 degrees per 5 seconds.\\n- The heptagon size should be large enough to contain all the balls.\\n- Do not us e the pygame library; implement collision detection algorithms and collision res ponse etc. by yourself. The following Python libraries are allowed: tkinter, mat h, numpy, dataclasses, typing, sys.\\n- All codes should be put in a single Pytho n file.<�Assistant�>\" � Thank you! Thank you for the constant support. We hope to have some great news in the comin g weeks! � As always, be sure to join our Reddit page and Discord server for help or just t o show your support! You can also follow us on Twitter and newsletter. Thank you for reading! Daniel & Michael Han � 29 May 2025 https://huggingface."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "Reddit page and Discord server for help or just t o show your support! You can also follow us on Twitter and newsletter. Thank you for reading! Daniel & Michael Han � 29 May 2025 https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF + LJ Fri 30 May 2025 10:25:30 BST � cmake -B build -DLLAMA_CURL=OFF unset CC unset CXX cmake -B build -DLLAMA_CURL=OFF cmake --build build --config Release -j --clean-first --target llama-quantize ll ama-cli llama-gguf-split (torch) ljubomir@macbook2(:):~/llama.cpp$ l models/DeepSeek-R1-* lrwx------@ 1 ljubomir staff 86B 30 May 17:20 models/DeepSeek-R1-0528-UD-IQ1 _S-00001-of-00004.gguf -> /Users/ljubomir/Library/Caches/llama.cpp/DeepSeek-R1-0 528-UD-IQ1_S-00001-of-00004.gguf lrwx------@ 1 ljubomir staff 86B 30 May 17:20 models/DeepSeek-R1-0528-UD-IQ1 _S-00002-of-00004.gguf -> /Users/ljubomir/Library/Caches/llama.cpp/DeepSeek-R1-0 528-UD-IQ1_S-00002-of-00004.gguf lrwx------@ 1 ljubomir staff 86B 30 May 17:20 models/DeepSeek-R1-0528-UD-IQ1 _S-00003-of-00004.gguf -> /Users/ljubomir/Library/Caches/llama.cpp/DeepSeek-R1-0 528-UD-IQ1_S-00003-of-00004.gguf lrwx------@ 1 ljubomir staff 86B 30 May 17:20 models/DeepSeek-R1-0528-UD-IQ1 _S-00004-of-00004.gguf -> /Users/ljubomir/Library/Caches/llama.cpp/DeepSeek-R1-0 528-UD-IQ1_S-00004-of-00004."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "of-00004.gguf lrwx------@ 1 ljubomir staff 86B 30 May 17:20 models/DeepSeek-R1-0528-UD-IQ1 _S-00004-of-00004.gguf -> /Users/ljubomir/Library/Caches/llama.cpp/DeepSeek-R1-0 528-UD-IQ1_S-00004-of-00004.gguf Re-transfer, incrementally, suspect incomplete interrupted files download md models/unsloth/DeepSeek-R1-0528-GGUF/UD-IQ1_S/ mviv ~/Downloads/DeepSeek-R1-0528-UD-IQ1_S-0000* models/unsloth/DeepSeek-R1-0528 -GGUF/UD-IQ1_S/ uv pip install huggingface_hub hf_transfer hf_xet ```python from huggingface_hub import snapshot_download import os # Ensure hf_transfer is enabled os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\" # The base directory where your incomplete files are # snapshot_download will create the full path like 'models/unsloth/DeepSeek-R1-0 528-GGUF/UD-IQ1_S' # so ensure your 'models' directory is correctly specified. target_base_dir = \"models\" print(f\"Attempting to download/resume the model snapshot to: {target_base_dir}\") local_dir = snapshot_download( repo_id=\"unsloth/DeepSeek-R1-0528-GGUF\", allow_patterns=\"UD-IQ1_S/*\", local_dir=target_base_dir, # This is the base directory local_dir_use_symlinks=False ) print(f\"Download/Resume complete for the model snapshot at: {local_dir}\") ``` https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/blob/main/UD-IQ1_S/DeepSeek -R1-0528-UD-IQ1_S-00001-of-00004.gguf Git LFS Details SHA256: 19891f28c27908e2ba0402ecc15c7aaa7e48ab5d9e1c6d49096c42e74e8b16b8 Pointer size: 136 Bytes Size of remote file: 49."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "b/main/UD-IQ1_S/DeepSeek -R1-0528-UD-IQ1_S-00001-of-00004.gguf Git LFS Details SHA256: 19891f28c27908e2ba0402ecc15c7aaa7e48ab5d9e1c6d49096c42e74e8b16b8 Pointer size: 136 Bytes Size of remote file: 49.1 GB Xet backed hash: 229375f805e68a1006bcdbd96cea8f23ebabe02f9c7bd6a27598ec0a40c1df0 b ljubomir@macbook2(:):~/llama.cpp/models$ wget 'https://huggingface.co/unsloth/De epSeek-R1-0528-GGUF/resolve/main/UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00001-of-000 04.gguf' Length: 49094698368 (46G) Saving to: �DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004.gguf� (torch) ljubomir@macbook2(:):~/llama.cpp$ sha256 models/DeepSeek-R1-0528-UD-IQ1_ S-00001-of-00004.gguf SHA256 (models/DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004.gguf) = 19891f28c27908e2 ba0402ecc15c7aaa7e48ab5d9e1c6d49096c42e74e8b16b8 https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/blob/main/UD-IQ1_S/DeepSeek -R1-0528-UD-IQ1_S-00002-of-00004.gguf Git LFS Details SHA256: 370f32bef60e3b6b074a7216bc2acb3401b79d81b9811742de47b2267068c6f2 Pointer size: 136 Bytes Size of remote file: 49.8 GB Xet backed hash: 7bc99aa451223d8366c1e0cf75545284a53cee097e888e8e4fd1d653ea1d73f 3 ljubomir@macbook2(:):~/llama.cpp/models$ wget 'https://huggingface.co/unsloth/De epSeek-R1-0528-GGUF/resolve/main/UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00002-of-000 04.gguf' Length: 49775793088 (46G) Saving to: �DeepSeek-R1-0528-UD-IQ1_S-00002-of-00004.gguf� (torch) ljubomir@macbook2(:):~/llama.cpp$ sha256 models/DeepSeek-R1-0528-UD-IQ1_ S-00002-of-00004."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "-of-000 04.gguf' Length: 49775793088 (46G) Saving to: �DeepSeek-R1-0528-UD-IQ1_S-00002-of-00004.gguf� (torch) ljubomir@macbook2(:):~/llama.cpp$ sha256 models/DeepSeek-R1-0528-UD-IQ1_ S-00002-of-00004.gguf SHA256 (models/DeepSeek-R1-0528-UD-IQ1_S-00002-of-00004.gguf) = 370f32bef60e3b6b 074a7216bc2acb3401b79d81b9811742de47b2267068c6f2 https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/blob/main/UD-IQ1_S/DeepSeek -R1-0528-UD-IQ1_S-00003-of-00004.gguf Git LFS Details SHA256: 197f9e2e1e1ac30b3e6a3474a79483b923f17612394b66a4e71c7badacb4c3d0 Pointer size: 136 Bytes Size of remote file: 50 GB Xet backed hash: 5ced4b4f12ddb2a788248765e941a38d5e38a5e8619625c296ec8c05993130b b ljubomir@macbook2(:):~/llama.cpp/models$ wget 'https://huggingface.co/unsloth/De epSeek-R1-0528-GGUF/resolve/main/UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00003-of-000 04.gguf' Length: 49955298016 (47G) Saving to: �DeepSeek-R1-0528-UD-IQ1_S-00003-of-00004.gguf� (torch) ljubomir@macbook2(:):~/llama.cpp$ sha256 models/DeepSeek-R1-0528-UD-IQ1_ S-00003-of-00004.gguf SHA256 (models/DeepSeek-R1-0528-UD-IQ1_S-00003-of-00004.gguf) = 197f9e2e1e1ac30b 3e6a3474a79483b923f17612394b66a4e71c7badacb4c3d0 https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/blob/main/UD-IQ1_S/DeepSeek -R1-0528-UD-IQ1_S-00004-of-00004.gguf Git LFS Details SHA256: dac55239e4de7a359d2ac40a0fd374f477f598a0f0d605e54dd1b1267a7401da Pointer size: 136 Bytes Size of remote file: 19."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "b/main/UD-IQ1_S/DeepSeek -R1-0528-UD-IQ1_S-00004-of-00004.gguf Git LFS Details SHA256: dac55239e4de7a359d2ac40a0fd374f477f598a0f0d605e54dd1b1267a7401da Pointer size: 136 Bytes Size of remote file: 19.5 GB Xet backed hash: 8c81e2e317c16d57d7990c5906a475f5c19dd706d0a8085b51608b9a37ebc28 d ljubomir@macbook2(:):~/llama.cpp/models$ wget 'https://huggingface.co/unsloth/De epSeek-R1-0528-GGUF/resolve/main/UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00004-of-000 04.gguf' Length: 19455845600 (18G) Saving to: �DeepSeek-R1-0528-UD-IQ1_S-00004-of-00004.gguf� (torch) ljubomir@macbook2(:):~/llama.cpp$ sha256 models/DeepSeek-R1-0528-UD-IQ1_ S-00004-of-00004.gguf SHA256 (models/DeepSeek-R1-0528-UD-IQ1_S-00004-of-00004.gguf) = dac55239e4de7a35 9d2ac40a0fd374f477f598a0f0d605e54dd1b1267a7401da # After snapshot_download, you might want to manually check each file's size # as a final verification, especially if you encountered issues before. build/bin/llama-cli \\ --model models/DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004.gguf \\ --cache-type-k q4_0 \\ --prio 3 \\ --temp 0.6 \\ --top_p 0.95 \\ --min_p 0.01 \\ --ctx-size 16384 \\ --prompt \"<�User�>Write a Python program that shows 20 balls bouncing insi de a spinning heptagon:\\n- All balls have the same radius.\\n- All balls have a n umber on it from 1 to 20.\\n- All balls drop from the heptagon center when starti ng."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "that shows 20 balls bouncing insi de a spinning heptagon:\\n- All balls have the same radius.\\n- All balls have a n umber on it from 1 to 20.\\n- All balls drop from the heptagon center when starti ng.\\n- Colors are: #f8b862, #f6ad49, #f39800, #f08300, #ec6d51, #ee7948, #ed6d3d , #ec6800, #ec6800, #ee7800, #eb6238, #ea5506, #ea5506, #eb6101, #e49e61, #e45e3 2, #e17b34, #dd7a56, #db8449, #d66a35\\n- The balls should be affected by gravity and friction, and they must bounce off the rotating walls realistically. There should also be collisions between balls.\\n- The material of all the balls determ ines that their impact bounce height will not exceed the radius of the heptagon, but higher than ball radius.\\n- All balls rotate with friction, the numbers on the ball can be used to indicate the spin of the ball.\\n- The heptagon is spinni ng around its center, and the speed of spinning is 360 degrees per 5 seconds.\\n- The heptagon size should be large enough to contain all the balls.\\n- Do not us e the pygame library; implement collision detection algorithms and collision res ponse etc. by yourself. The following Python libraries are allowed: tkinter, mat h, numpy, dataclasses, typing, sys.\\n- All codes should be put in a single Pytho n file.<�Assistant�>\" LJ Fri 30 May 2025 10:25:30 BST + LJ Sat 31 May 2025 07:21:47 BST � https://unsloth.ai/blog/deepseek-r1-0528 Hi, thanks for all that, stellar work. I'm trying for the smallest R1 to see wha t tps I get on MBP M2 96GB RAM."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": ":25:30 BST + LJ Sat 31 May 2025 07:21:47 BST � https://unsloth.ai/blog/deepseek-r1-0528 Hi, thanks for all that, stellar work. I'm trying for the smallest R1 to see wha t tps I get on MBP M2 96GB RAM. I'm following this https://unsloth.ai/blog/deepseek-r1-0528 I run into this problem: ```bash ljubomir@macbook2(:):~/llama.cpp$ build/bin/llama-cli \\ --model models/DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004.gguf \\ --cache-type-k q4_0 \\ --prio 3 \\ --temp 0.6 \\ --top_p 0.95 \\ --min_p 0.01 \\ --ctx-size 16384 \\ --prompt \"<�User�>Write a Python program that shows 20 balls bouncing insi de a spinning heptagon:\\n- All balls have the same radius.\\n- All balls have a n umber on it from 1 to 20.\\n- All balls drop from the heptagon center when starti ng.\\n- Colors are: #f8b862, #f6ad49, #f39800, #f08300, #ec6d51, #ee7948, #ed6d3d , #ec6800, #ec6800, #ee7800, #eb6238, #ea5506, #ea5506, #eb6101, #e49e61, #e45e3 2, #e17b34, #dd7a56, #db8449, #d66a35\\n- The balls should be affected by gravity and friction, and they must bounce off the rotating walls realistically. There should also be collisions between balls.\\n- The material of all the balls determ ines that their impact bounce height will not exceed the radius of the heptagon, but higher than ball radius.\\n- All balls rotate with friction, the numbers on the ball can be used to indicate the spin of the ball.\\n- The heptagon is spinni ng around its center, and the speed of spinning is 360 degrees per 5 seconds."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "alls rotate with friction, the numbers on the ball can be used to indicate the spin of the ball.\\n- The heptagon is spinni ng around its center, and the speed of spinning is 360 degrees per 5 seconds.\\n- The heptagon size should be large enough to contain all the balls.\\n- Do not us e the pygame library; implement collision detection algorithms and collision res ponse etc. by yourself. The following Python libraries are allowed: tkinter, mat h, numpy, dataclasses, typing, sys.\\n- All codes should be put in a single Pytho n file.<�Assistant�>\" build: 5626 (bc1007a4) with Apple clang version 17.0.0 (clang-1700.0.13.5) for a rm64-apple-darwin24.4.0 main: llama backend init main: load the model and apply lora adapter, if any llama_model_load_from_file_impl: using device Metal (Apple M2 Max) - 73727 MiB f ree llama_model_load: error loading model: corrupted model: 1086 tensors expected bu t 978 found llama_model_load_from_file_impl: failed to load model common_init_from_params: failed to load model 'models/DeepSeek-R1-0528-UD-IQ1_S- 00001-of-00004.gguf' main: error: unable to load model ``` I suspected some of files didn't download correctly - they looked like this ```bash ljubomir@macbook2(:):~/llama.cpp$ ls -al models/DeepSeek-R1-0528-UD-IQ1_S-0000* -rw-r--r--@ 1 ljubomir staff 49462945024 30 May 13:13 models/DeepSeek-R1-0528- UD-IQ1_S-00001-of-00004.gguf -rw-r--r--@ 1 ljubomir staff 48568885664 30 May 14:14 models/DeepSeek-R1-0528- UD-IQ1_S-00002-of-00004."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "1 ljubomir staff 49462945024 30 May 13:13 models/DeepSeek-R1-0528- UD-IQ1_S-00001-of-00004.gguf -rw-r--r--@ 1 ljubomir staff 48568885664 30 May 14:14 models/DeepSeek-R1-0528- UD-IQ1_S-00002-of-00004.gguf -rw-r--r--@ 1 ljubomir staff 49564076576 30 May 15:30 models/DeepSeek-R1-0528- UD-IQ1_S-00003-of-00004.gguf -rw-r--r--@ 1 ljubomir staff 19455845600 30 May 16:46 models/DeepSeek-R1-0528- UD-IQ1_S-00004-of-00004.gguf ``` Is it possible to see the exact file sizes, to the byte, on HuggingFace web ui? Or put the sizes, maybe even crc like md5 sum, in a separate file? Then it got worse. I thought - there must be some way to download incrementally, it will be smart enough to figure which file is truncated, and maybe even just download the extra, like rsync would do. So I asked gemini, it suggested ```python from huggingface_hub import snapshot_download # This will download the entire 'UD-IQ1_S' folder and its contents # It will create a directory like 'models/unsloth/DeepSeek-R1-0528-GGUF/UD-IQ1_S ' local_dir = snapshot_download( repo_id=\"unsloth/DeepSeek-R1-0528-GGUF\", allow_patterns=\"UD-IQ1_S/*\", # Only download files within the UD-IQ1_S folde r local_dir=\"models/unsloth/DeepSeek-R1-0528-GGUF\", # The base directory to do wnload to local_dir_use_symlinks=False # Important for full copy ) print(f\"Downloaded model to: {local_dir}\") ``` I moved the existing files in a newly created dir models/unsloth/DeepSeek-R1-052 8-GGUF/UD-IQ1_S , and run the above in ipython."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "mportant for full copy ) print(f\"Downloaded model to: {local_dir}\") ``` I moved the existing files in a newly created dir models/unsloth/DeepSeek-R1-052 8-GGUF/UD-IQ1_S , and run the above in ipython. Well - turns out it wiped the files completely and it's downloading from scratch now! :-) Haha - expected better than that tbh. We really do need AI, b/c atm ou r stuff is AS - Artificially Stupid, haha :-) No worries, it's chugging along no w, will be done. But if you can provide the files sizes someplace or even better their md5sum-s too, so we know when the big files are downloaded correctly, tha t would be stellar! Thanks for everything you do guys! It's been great running stuff on localhost, b een enjoying it immensely. :-) LJ Sat 31 May 2025 07:21:47 BST + LJ Sat 31 May 2025 12:45:36 BST � ljubomir@macbook2(:):~/llama.cpp/models$ wget 'https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/resolve/main/UD-I Q1_S/DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004.gguf' Ignore the previous comment, seems I can't edit nor delete it anymore? Previously had trouble downloading stuff and ensuring it's correctly downloaded. May help someone else - this worked for me: 1) Use wget to DL, it may restart a failed transfer ```bash ljubomir@macbook2(:):~/llama.cpp/models$ wget 'https://huggingface.co/unsloth/De epSeek-R1-0528-GGUF/resolve/main/UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00001-of-000 04.gguf' Length: 49094698368 (46G) Saving to: �DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ps://huggingface.co/unsloth/De epSeek-R1-0528-GGUF/resolve/main/UD-IQ1_S/DeepSeek-R1-0528-UD-IQ1_S-00001-of-000 04.gguf' Length: 49094698368 (46G) Saving to: �DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004.gguf� ``` 2) The checksum is at ```bash https://huggingface.co/unsloth/DeepSeek-R1-0528-GGUF/blob/main/UD-IQ1_S/DeepSeek -R1-0528-UD-IQ1_S-00001-of-00004.gguf Git LFS Details SHA256: 19891f28c27908e2ba0402ecc15c7aaa7e48ab5d9e1c6d49096c42e74e8b16b8 Pointer size: 136 Bytes Size of remote file: 49.1 GB Xet backed hash: 229375f805e68a1006bcdbd96cea8f23ebabe02f9c7bd6a27598ec0a40c1df0 b ``` 3) Compute and compare ```bash (torch) ljubomir@macbook2(:):~/llama.cpp$ sha256 models/DeepSeek-R1-0528-UD-IQ1_ S-00001-of-00004.gguf SHA256 (models/DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004.gguf) = 19891f28c27908e2 ba0402ecc15c7aaa7e48ab5d9e1c6d49096c42e74e8b16b8 ``` LJ Sat 31 May 2025 12:45:36 BST + LJ Sat 31 May 2025 13:50:34 BST � ljubomir@macbook2(:):~/llama.cpp$ build/bin/llama-cli \\ Run: ljubomir@macbook2(:):~/llama.cpp$ build/bin/llama-cli \\ --model models/DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004.gguf \\ --cache-type-k q4_0 \\ --prio 3 \\ --temp 0.6 \\ --top_p 0.95 \\ --min_p 0.01 \\ --ctx-size 16384 \\ --prompt \"<�User�>Write a Python program that shows 20 balls bouncing insi de a spinning heptagon:\\n- All balls have the same radius.\\n- All balls have a n umber on it from 1 to 20.\\n- All balls drop from the heptagon center when starti ng."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "that shows 20 balls bouncing insi de a spinning heptagon:\\n- All balls have the same radius.\\n- All balls have a n umber on it from 1 to 20.\\n- All balls drop from the heptagon center when starti ng.\\n- Colors are: #f8b862, #f6ad49, #f39800, #f08300, #ec6d51, #ee7948, #ed6d3d , #ec6800, #ec6800, #ee7800, #eb6238, #ea5506, #ea5506, #eb6101, #e49e61, #e45e3 2, #e17b34, #dd7a56, #db8449, #d66a35\\n- The balls should be affected by gravity and friction, and they must bounce off the rotating walls realistically. There should also be collisions between balls.\\n- The material of all the balls determ ines that their impact bounce height will not exceed the radius of the heptagon, but higher than ball radius.\\n- All balls rotate with friction, the numbers on the ball can be used to indicate the spin of the ball.\\n- The heptagon is spinni ng around its center, and the speed of spinning is 360 degrees per 5 seconds.\\n- The heptagon size should be large enough to contain all the balls.\\n- Do not us e the pygame library; implement collision detection algorithms and collision res ponse etc. by yourself. The following Python libraries are allowed: tkinter, mat h, numpy, dataclasses, typing, sys.\\n- All codes should be put in a single Pytho n file.<�Assistant�>\" LJ Sat 31 May 2025 13:50:34 BST + LJ Sat 31 May 2025 13:50:56 BST � ljubomir@macbook2(:):~/llama."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "h, numpy, dataclasses, typing, sys.\\n- All codes should be put in a single Pytho n file.<�Assistant�>\" LJ Sat 31 May 2025 13:50:34 BST + LJ Sat 31 May 2025 13:50:56 BST � ljubomir@macbook2(:):~/llama.cpp$ build/bin/llama-cli \\ Update - alas, it seems the 170GB weights can be run on 96GB RAM (I imagine 3/4 only is used as VRAM) on a macbook - even if mmap-ed and READ ONLY. Don't see wh y MacOS would not simply un/re-load whenever something is in the address space, even if not in RAM. TBH expected it to not straight out not work - expected it t o work even if super slow, so I'd need to kill the process or (more likely) turn the computer off once it gets too stuck. Put the error in gemini, but didn't learn anything about how to make it run. Thi s: ```bash ljubomir@macbook2(:):~/llama.cpp$ build/bin/llama-cli \\ --model models/DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004.gguf \\ --cache-type-k q4_0 \\ --prio 3 \\ --temp 0.6 \\ --top_p 0.95 \\ --min_p 0.01 \\ --ctx-size 16384 \\ --prompt \"<�User�>Write a Python program that shows 20 balls bouncing insi de a spinning heptagon:\\n- All balls have the same radius.\\n- All balls have a n umber on it from 1 to 20.\\n- All balls drop from the heptagon center when starti ng."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "that shows 20 balls bouncing insi de a spinning heptagon:\\n- All balls have the same radius.\\n- All balls have a n umber on it from 1 to 20.\\n- All balls drop from the heptagon center when starti ng.\\n- Colors are: #f8b862, #f6ad49, #f39800, #f08300, #ec6d51, #ee7948, #ed6d3d , #ec6800, #ec6800, #ee7800, #eb6238, #ea5506, #ea5506, #eb6101, #e49e61, #e45e3 2, #e17b34, #dd7a56, #db8449, #d66a35\\n- The balls should be affected by gravity and friction, and they must bounce off the rotating walls realistically. There should also be collisions between balls.\\n- The material of all the balls determ ines that their impact bounce height will not exceed the radius of the heptagon, but higher than ball radius.\\n- All balls rotate with friction, the numbers on the ball can be used to indicate the spin of the ball.\\n- The heptagon is spinni ng around its center, and the speed of spinning is 360 degrees per 5 seconds.\\n- The heptagon size should be large enough to contain all the balls.\\n- Do not us e the pygame library; implement collision detection algorithms and collision res ponse etc. by yourself. The following Python libraries are allowed: tkinter, mat h, numpy, dataclasses, typing, sys.\\n- All codes should be put in a single Pytho n file.<�Assistant�>\" build: 5626 (bc1007a4) with Apple clang version 17.0.0 (clang-1700.0.13.5) for a rm64-apple-darwin24.4."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "py, dataclasses, typing, sys.\\n- All codes should be put in a single Pytho n file.<�Assistant�>\" build: 5626 (bc1007a4) with Apple clang version 17.0.0 (clang-1700.0.13.5) for a rm64-apple-darwin24.4.0 main: llama backend init main: load the model and apply lora adapter, if any llama_model_load_from_file_impl: using device Metal (Apple M2 Max) - 73727 MiB f ree llama_model_loader: additional 3 GGUFs metadata loaded. llama_model_loader: loaded meta data with 62 key-value pairs and 1086 tensors fr om models/DeepSeek-R1-0528-UD-IQ1_S-00001-of-00004.gguf (version GGUF V3 (latest )) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not appl y in this output. llama_model_loader: - kv 0: general.architecture str = deepseek2 llama_model_loader: - kv 1: general.type str = model llama_model_loader: - kv 2: general.name str = Deepseek-R1-0528 llama_model_loader: - kv 3: general.basename str = Deepseek-R1-0528 llama_model_loader: - kv 4: general.quantized_by str = Unsloth llama_model_loader: - kv 5: general.size_label str = 256x20B llama_model_loader: - kv 6: general.license str = mit llama_model_loader: - kv 7: general.repo_url str = https://huggingface.co/unsloth llama_model_loader: - kv 8: general.base_model.count u32 = 1 llama_model_loader: - kv 9: general.base_model.0.name str = DeepSeek R1 0528 llama_model_loader: - kv 10: general.base_model.0.version str = 0528 llama_model_loader: - kv 11: general.base_model.0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "= 1 llama_model_loader: - kv 9: general.base_model.0.name str = DeepSeek R1 0528 llama_model_loader: - kv 10: general.base_model.0.version str = 0528 llama_model_loader: - kv 11: general.base_model.0.organization str = Deepseek Ai llama_model_loader: - kv 12: general.base_model.0.repo_url str = https://huggingface.co/deepseek-ai/De... llama_model_loader: - kv 13: general.tags arr[str ,1] = [\"unsloth\"] llama_model_loader: - kv 14: deepseek2.block_count u32 = 61 llama_model_loader: - kv 15: deepseek2.context_length u32 = 163840 llama_model_loader: - kv 16: deepseek2.embedding_length u32 = 7168 llama_model_loader: - kv 17: deepseek2.feed_forward_length u32 = 18432 llama_model_loader: - kv 18: deepseek2.attention.head_count u32 = 128 llama_model_loader: - kv 19: deepseek2.attention.head_count_kv u32 = 1 llama_model_loader: - kv 20: deepseek2.rope.freq_base f32 = 10000.000000 llama_model_loader: - kv 21: deepseek2.attention.layer_norm_rms_epsilon f32 = 0.000001 llama_model_loader: - kv 22: deepseek2.expert_used_count u32 = 8 llama_model_loader: - kv 23: deepseek2.leading_dense_block_count u32 = 3 llama_model_loader: - kv 24: deepseek2.vocab_size u32 = 129280 llama_model_loader: - kv 25: deepseek2.attention.q_lora_rank u32 = 1536 llama_model_loader: - kv 26: deepseek2.attention.kv_lora_rank u32 = 512 llama_model_loader: - kv 27: deepseek2.attention.key_length u32 = 576 llama_model_loader: - kv 28: deepseek2.attention.value_length u32 = 512 llama_model_loader: - kv 29: deepseek2."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ora_rank u32 = 512 llama_model_loader: - kv 27: deepseek2.attention.key_length u32 = 576 llama_model_loader: - kv 28: deepseek2.attention.value_length u32 = 512 llama_model_loader: - kv 29: deepseek2.attention.key_length_mla u32 = 192 llama_model_loader: - kv 30: deepseek2.attention.value_length_mla u32 = 128 llama_model_loader: - kv 31: deepseek2.expert_feed_forward_length u32 = 2048 llama_model_loader: - kv 32: deepseek2.expert_count u32 = 256 llama_model_loader: - kv 33: deepseek2.expert_shared_count u32 = 1 llama_model_loader: - kv 34: deepseek2.expert_weights_scale f32 = 2.500000 llama_model_loader: - kv 35: deepseek2.expert_weights_norm bool = true llama_model_loader: - kv 36: deepseek2.expert_gating_func u32 = 2 llama_model_loader: - kv 37: deepseek2.rope.dimension_count u32 = 64 llama_model_loader: - kv 38: deepseek2.rope.scaling.type str = yarn llama_model_loader: - kv 39: deepseek2.rope.scaling.factor f32 = 40.000000 llama_model_loader: - kv 40: deepseek2.rope.scaling.original_context_length u32 = 4096 llama_model_loader: - kv 41: deepseek2.rope.scaling.yarn_log_multiplier f32 = 0.100000 llama_model_loader: - kv 42: tokenizer.ggml.model str = gpt2 llama_model_loader: - kv 43: tokenizer.ggml.pre str = deepseek-v3 llama_model_loader: - kv 44: tokenizer.ggml.tokens arr[str ,129280] = [\"<�begin�of�sentence�>\", \"<�... llama_model_loader: - kv 45: tokenizer.ggml.token_type arr[i32 ,129280] = [3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... llama_model_loader: - kv 46: tokenizer."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "r ,129280] = [\"<�begin�of�sentence�>\", \"<�... llama_model_loader: - kv 45: tokenizer.ggml.token_type arr[i32 ,129280] = [3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... llama_model_loader: - kv 46: tokenizer.ggml.merges arr[str ,127741] = [\"� t\", \"� a\", \"i n\", \"� � \", \"h e... llama_model_loader: - kv 47: tokenizer.ggml.bos_token_id u32 = 0 llama_model_loader: - kv 48: tokenizer.ggml.eos_token_id u32 = 1 llama_model_loader: - kv 49: tokenizer.ggml.padding_token_id u32 = 2 llama_model_loader: - kv 50: tokenizer.ggml.add_bos_token bool = true llama_model_loader: - kv 51: tokenizer.ggml.add_eos_token bool = false llama_model_loader: - kv 52: tokenizer.chat_template str = {% if not add_generation_prompt is de... llama_model_loader: - kv 53: general.quantization_version u32 = 2 llama_model_loader: - kv 54: general.file_type u32 = 24 llama_model_loader: - kv 55: quantize.imatrix.file str = DeepSeek-R1-0528-GGUF/imatrix_unsloth... llama_model_loader: - kv 56: quantize.imatrix.dataset str = unsloth_calibration_DeepSeek-R1-0528-... llama_model_loader: - kv 57: quantize.imatrix.entries_count i32 = 659 llama_model_loader: - kv 58: quantize.imatrix.chunks_count i32 = 720 llama_model_loader: - kv 59: split.no u16 = 0 llama_model_loader: - kv 60: split.tensors.count i32 = 1086 llama_model_loader: - kv 61: split."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "l_loader: - kv 58: quantize.imatrix.chunks_count i32 = 720 llama_model_loader: - kv 59: split.no u16 = 0 llama_model_loader: - kv 60: split.tensors.count i32 = 1086 llama_model_loader: - kv 61: split.count u16 = 4 llama_model_loader: - type f32: 361 tensors llama_model_loader: - type q8_0: 122 tensors llama_model_loader: - type q4_K: 56 tensors llama_model_loader: - type q5_K: 36 tensors llama_model_loader: - type q6_K: 17 tensors llama_model_loader: - type iq2_xxs: 24 tensors llama_model_loader: - type iq3_xxs: 49 tensors llama_model_loader: - type iq1_s: 126 tensors llama_model_loader: - type iq3_s: 154 tensors llama_model_loader: - type iq4_xs: 141 tensors print_info: file format = GGUF V3 (latest) print_info: file type = IQ1_S - 1.5625 bpw print_info: file size = 156.72 GiB (2.01 BPW) load: special_eos_id is not in special_eog_ids - the tokenizer config may be inc orrect load: special tokens cache size = 818 load: token to piece cache size = 0.8223 MB print_info: arch = deepseek2 print_info: vocab_only = 0 print_info: n_ctx_train = 163840 print_info: n_embd = 7168 print_info: n_layer = 61 print_info: n_head = 128 print_info: n_head_kv = 1 print_info: n_rot = 64 print_info: n_swa = 0 print_info: is_swa_any = 0 print_info: n_embd_head_k = 576 print_info: n_embd_head_v = 512 print_info: n_gqa = 128 print_info: n_embd_k_gqa = 576 print_info: n_embd_v_gqa = 512 print_info: f_norm_eps = 0.0e+00 print_info: f_norm_rms_eps = 1.0e-06 print_info: f_clamp_kqv = 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "bd_head_v = 512 print_info: n_gqa = 128 print_info: n_embd_k_gqa = 576 print_info: n_embd_v_gqa = 512 print_info: f_norm_eps = 0.0e+00 print_info: f_norm_rms_eps = 1.0e-06 print_info: f_clamp_kqv = 0.0e+00 print_info: f_max_alibi_bias = 0.0e+00 print_info: f_logit_scale = 0.0e+00 print_info: f_attn_scale = 0.0e+00 print_info: n_ff = 18432 print_info: n_expert = 256 print_info: n_expert_used = 8 print_info: causal attn = 1 print_info: pooling type = 0 print_info: rope type = 0 print_info: rope scaling = yarn print_info: freq_base_train = 10000.0 print_info: freq_scale_train = 0.025 print_info: n_ctx_orig_yarn = 4096 print_info: rope_finetuned = unknown print_info: ssm_d_conv = 0 print_info: ssm_d_inner = 0 print_info: ssm_d_state = 0 print_info: ssm_dt_rank = 0 print_info: ssm_dt_b_c_rms = 0 print_info: model type = 671B print_info: model params = 671.03 B print_info: general.name = Deepseek-R1-0528 print_info: n_layer_dense_lead = 3 print_info: n_lora_q = 1536 print_info: n_lora_kv = 512 print_info: n_embd_head_k_mla = 192 print_info: n_embd_head_v_mla = 128 print_info: n_ff_exp = 2048 print_info: n_expert_shared = 1 print_info: expert_weights_scale = 2.5 print_info: expert_weights_norm = 1 print_info: expert_gating_func = sigmoid print_info: rope_yarn_log_mul = 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": ": n_ff_exp = 2048 print_info: n_expert_shared = 1 print_info: expert_weights_scale = 2.5 print_info: expert_weights_norm = 1 print_info: expert_gating_func = sigmoid print_info: rope_yarn_log_mul = 0.1000 print_info: vocab type = BPE print_info: n_vocab = 129280 print_info: n_merges = 127741 print_info: BOS token = 0 '<�begin�of�sentence�>' print_info: EOS token = 1 '<�end�of�sentence�>' print_info: EOT token = 1 '<�end�of�sentence�>' print_info: PAD token = 2 '<��pad��>' print_info: LF token = 201 '�' print_info: FIM PRE token = 128801 '<�fim�begin�>' print_info: FIM SUF token = 128800 '<�fim�hole�>' print_info: FIM MID token = 128802 '<�fim�end�>' print_info: EOG token = 1 '<�end�of�sentence�>' print_info: max token length = 256 load_tensors: loading model tensors, this can take a while... (mmap = true) ggml_backend_metal_log_allocated_size: warning: current allocated size is greate r than the recommended max working set size ggml_backend_metal_log_allocated_size: warning: current allocated size is greate r than the recommended max working set size ggml_backend_metal_log_allocated_size: warning: current allocated size is greate r than the recommended max working set size load_tensors: offloading 61 repeating layers to GPU load_tensors: offloading output layer to GPU load_tensors: offloaded 62/62 layers to GPU load_tensors: Metal_Mapped model buffer size = 46815.35 MiB load_tensors: Metal_Mapped model buffer size = 47469."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ad_tensors: offloading output layer to GPU load_tensors: offloaded 62/62 layers to GPU load_tensors: Metal_Mapped model buffer size = 46815.35 MiB load_tensors: Metal_Mapped model buffer size = 47469.88 MiB load_tensors: Metal_Mapped model buffer size = 47641.07 MiB load_tensors: Metal_Mapped model buffer size = 18554.54 MiB load_tensors: CPU_Mapped model buffer size = 497.11 MiB ................................................................................ ................. llama_context: constructing llama_context llama_context: n_seq_max = 1 llama_context: n_ctx = 16384 llama_context: n_ctx_per_seq = 16384 llama_context: n_batch = 2048 llama_context: n_ubatch = 512 llama_context: causal_attn = 1 llama_context: flash_attn = 0 llama_context: freq_base = 10000.0 llama_context: freq_scale = 0.025 llama_context: n_ctx_per_seq (16384) < n_ctx_train (163840) -- the full capacity of the model will not be utilized ggml_metal_init: allocating ggml_metal_init: found device: Apple M2 Max ggml_metal_init: picking default device: Apple M2 Max ggml_metal_load_library: using embedded metal library ggml_metal_init: GPU name: Apple M2 Max ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008) ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003) ggml_metal_init: GPU family: MTLGPUFamilyMetal3 (5001) ggml_metal_init: simdgroup reduction = true ggml_metal_init: simdgroup matrix mul."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "(1008) ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003) ggml_metal_init: GPU family: MTLGPUFamilyMetal3 (5001) ggml_metal_init: simdgroup reduction = true ggml_metal_init: simdgroup matrix mul. = true ggml_metal_init: has residency sets = true ggml_metal_init: has bfloat = true ggml_metal_init: use bfloat = false ggml_metal_init: hasUnifiedMemory = true ggml_metal_init: recommendedMaxWorkingSetSize = 77309.41 MB ggml_metal_init: skipping kernel_get_rows_bf16 (not supporte d) ggml_metal_init: skipping kernel_mul_mv_bf16_f32 (not supporte d) ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row (not supporte d) ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4 (not supporte d) ggml_metal_init: skipping kernel_mul_mv_bf16_bf16 (not supporte d) ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32 (not supporte d) ggml_metal_init: skipping kernel_mul_mm_bf16_f32 (not supporte d) ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128 (not supporte d) ggml_me"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "attn_ext_bf16_h128 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not suppor ted) ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not suppor ted) ggml_metal_init: skipping kernel_cpy_f32_bf16 (not supporte d) ggml_metal_init: skipping kernel_cpy_bf16_f32 (not supporte d) ggml_metal_init: skipping kernel_cpy_bf16_bf16 (not supporte d) llama_context: CPU output buffer size = 0.49 MiB llama_kv_cache_unified: Metal KV buffer size = 1284.81 MiB llama_kv_cache_unified: size = 1284.81 MiB ( 16384 cells, 61 layers, 1 seqs), K (q4_0): 308.81 MiB, V (f16): 976.00 MiB llama_context: Metal compute buffer size = 4522.00 MiB llama_context: CPU compute buffer size = 46."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ied: size = 1284.81 MiB ( 16384 cells, 61 layers, 1 seqs), K (q4_0): 308.81 MiB, V (f16): 976.00 MiB llama_context: Metal compute buffer size = 4522.00 MiB llama_context: CPU compute buffer size = 46.01 MiB llama_context: graph nodes = 4964 llama_context: graph splits = 2 common_init_from_params: setting dry_penalty_last_n to ctx_size = 16384 common_init_from_params: warming up the model with an empty run - please wait .. . (--no-warmup to disable) ggml_metal_graph_compute: command buffer 0 failed with status 5 error: Insufficient Memory (00000008:kIOGPUCommandBufferCallbackErrorOutOfMemory ) graph_compute: ggml_backend_sched_graph_compute_async failed with error -1 llama_decode: failed to decode, ret = -3 main: llama threadpool init, n_threads = 8 main: chat template is available, enabling conversation mode (disable it with -n o-cnv) *** User-specified prompt will pre-start conversation, did you mean to set --sys tem-prompt (-sys) instead? main: chat template example: You are a helpful assistant <�User�>Hello<�Assistant�>Hi there<�end�of�sentence�><�User�>How are you ?<�Assistant�> system_info: n_threads = 8 (n_threads_batch = 8) / 12 | Metal : EMBED_LIBRARY = 1 | CPU : ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | main: interactive mode on. sampler seed: 3358851179 sampler params: repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, p resence_penalty = 0.000 dry_multiplier = 0.000, dry_base = 1."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "in: interactive mode on. sampler seed: 3358851179 sampler params: repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, p resence_penalty = 0.000 dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_pe nalty_last_n = 16384 top_k = 40, top_p = 0.950, min_p = 0.010, xtc_probability = 0.000, xtc_t hreshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.600 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist generate: n_ctx = 16384, n_batch = 2048, n_predict = -1, n_keep = 1 == Running in interactive mode. == - Press Ctrl+C to interject at any time. - Press Return to return control to the AI. - To return control without starting a new line, end your input with '/'. - If you want to submit another line, end your input with '\\'. - Not using system message. To change it, set a different value via -sys PROMPT Write a Python program that shows 20 balls bouncing inside a spinning heptagon: - All balls have the same radius. - All balls have a number on it from 1 to 20. - All balls drop from the heptagon center when starting."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "rogram that shows 20 balls bouncing inside a spinning heptagon: - All balls have the same radius. - All balls have a number on it from 1 to 20. - All balls drop from the heptagon center when starting. - Colors are: #f8b862, #f6ad49, #f39800, #f08300, #ec6d51, #ee7948, #ed6d3d, #ec 6800, #ec6800, #ee7800, #eb6238, #ea5506, #ea5506, #eb6101, #e49e61, #e45e32, #e 17b34, #dd7a56, #db8449, #d66a35 - The balls should be affected by gravity and friction, and they must bounce off the rotating walls realistically. There should also be collisions between balls . - The material of all the balls determines that their impact bounce height will not exceed the radius of the heptagon, but higher than ball radius. - All balls rotate with friction, the numbers on the ball can be used to indicat e the spin of the ball. - The heptagon is spinning around its center, and the speed of spinning is 360 d egrees per 5 seconds. - The heptagon size should be large enough to contain all the balls. - Do not use the pygame library; implement collision detection algorithms and co llision response etc. by yourself. The following Python libraries are allowed: t kinter, math, numpy, dataclasses, typing, sys. - All codes should be put in a single Python file."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ion algorithms and co llision response etc. by yourself. The following Python libraries are allowed: t kinter, math, numpy, dataclasses, typing, sys. - All codes should be put in a single Python file.ggml_metal_graph_compute: comm and buffer 0 failed with status 5 error: Insufficient Memory (00000008:kIOGPUCommandBufferCallbackErrorOutOfMemory ) graph_compute: ggml_backend_sched_graph_compute_async failed with error -1 llama_decode: failed to decode, ret = -3 main : failed to eval ggml_metal_free: deallocating ``` Thanks for everything you do guys! Top marks! Have been enjoying this. :-) Will try again in the future on a bigger box. LJ Sat 31 May 2025 13:50:56 BST + LJ Sun 1 Jun 2025 08:55:50 BST � Try running Qwen3-235B-A22B-GGUF Try running Qwen3-235B-A22B-GGUF https://huggingface.co/Qwen/Qwen3-235B-A22B By default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This me ans the model will use its reasoning abilities to enhance the quality of generat ed responses. Advanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input We provide a soft switch mechanism that allows users to dynamically control the model's behavior when enable_thinking=True. Specifically, you can add /think and /no_think to user prompts or system messages to switch the model's thinking mod e from turn to turn. The model will follow the most recent instruction in multi- turn conversations."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "can add /think and /no_think to user prompts or system messages to switch the model's thinking mod e from turn to turn. The model will follow the most recent instruction in multi- turn conversations. Best Practices To achieve optimal performance, we recommend the following settings: Sampling Parameters: For thinking mode (enable_thinking=True), use Temperature=0.6, TopP=0.95, TopK=2 0, and MinP=0. DO NOT use greedy decoding, as it can lead to performance degrada tion and endless repetitions. For non-thinking mode (enable_thinking=False), we suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0. For supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasio nally result in language mixing and a slight decrease in model performance. Adequate Output Length: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length t o 38,912 tokens. This provides the model with sufficient space to generate detai led and comprehensive responses, thereby enhancing its overall performance. Standardize Output Format: We recommend using prompts to standardize model outpu ts when benchmarking. Math Problems: Include \"Please reason step by step, and put your final answer wi thin \\boxed{}.\" in the prompt."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "tput Format: We recommend using prompts to standardize model outpu ts when benchmarking. Math Problems: Include \"Please reason step by step, and put your final answer wi thin \\boxed{}.\" in the prompt. Multiple-Choice Questions: Add the following JSON structure to the prompt to sta ndardize responses: \"Please show your choice in the answer field with only the c hoice letter, e.g., \"answer\": \"C\".\" No Thinking Content in History: In multi-turn conversations, the historical mode l output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it i s up to the developers to ensure that the best practice is followed. Via unsloth/Qwen3-235B-A22B-GGUF https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF Files https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/blob/main/Q2_K/Qwen3-235B-A2 2B-Q2_K-00001-of-00002.gguf Git LFS Details SHA256: 06507d563a4bbbb5704c9ee84151d621bbcb52c88e2246959d6c2d04f08f76a2 Pointer size: 136 Bytes Size of remote file: 49.9 GB Xet backed hash: 29586f2ab3a6ff9e1ea6a4e80e001fe0461b866e4c1c3b2721f53feefbdb6ba 4 wget 'https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q2_K/Qwen 3-235B-A22B-Q2_K-00001-of-00002.gguf' ljubomir@macbook2(:):~/llama.cpp$ sha256sum models/Qwen3-235B-A22B-Q2_K-00001-of -00002."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ttps://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/resolve/main/Q2_K/Qwen 3-235B-A22B-Q2_K-00001-of-00002.gguf' ljubomir@macbook2(:):~/llama.cpp$ sha256sum models/Qwen3-235B-A22B-Q2_K-00001-of -00002.gguf 06507d563a4bbbb5704c9ee84151d621bbcb52c88e2246959d6c2d04f08f76a2 models/Qwen3-2 35B-A22B-Q2_K-00001-of-00002.gguf https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF/blob/main/Q2_K/Qwen3-235B-A2 2B-Q2_K-00002-of-00002.gguf Git LFS Details SHA256: fd2ce2d857b731bf598e1a207e7df6dc0c9d84e4970ee6f0857fb1700d4ab858 Pointer size: 136 Bytes Size of remote file: 35.8 GB Xet backed hash: 81134004b96abdaf2f7eba7b57606bf4dd857fe70688359da4382474070c043 2 ljubomir@macbook2(:):~/llama.cpp$ sha256sum models/Qwen3-235B-A22B-Q2_K-00002-of -00002.gguf fd2ce2d857b731bf598e1a207e7df6dc0c9d84e4970ee6f0857fb1700d4ab858 models/Qwen3-2 35B-A22B-Q2_K-00002-of-00002.gguf https://huggingface.co/unsloth/Qwen3-235B-A22B-GGUF https://huggingface.co/unsloth/Qwen3-235B-A22B-128K-GGUF https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune � Qwen3: How to Run & Fine-tune Learn to run & fine-tune Qwen3 locally with Unsloth + our Dynamic 2.0 quants Qwen's new Qwen3 models deliver state-of-the-art advancements in reasoning, inst ruction-following, agent capabilities, and multilingual support. All Qwen3 uploads use our new Unsloth Dynamic 2.0 methodology, delivering the be st performance on 5-shot MMLU and KL Divergence benchmarks."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "on-following, agent capabilities, and multilingual support. All Qwen3 uploads use our new Unsloth Dynamic 2.0 methodology, delivering the be st performance on 5-shot MMLU and KL Divergence benchmarks. This means, you can run and fine-tune quantized Qwen3 LLMs with minimal accuracy loss! We also uploaded Qwen3 with native 128K context length. Qwen achieves this by us ing YaRN to extend its original 40K window to 128K. Unsloth also now supports fine-tuning and GRPO of Qwen3 and Qwen3 MOE models � 2 x faster, with 70% less VRAM, and 8x longer context lengths. Fine-tune Qwen3 (14 B) for free using our Colab notebook. �� Official Recommended Settings According to Qwen, these are the recommended settings for inference: Non-Thinking Mode Settings: Thinking Mode Settings: Temperature = 0.7 Temperature = 0.6 Min_P = 0.0 (optional, but 0.01 works well, llama.cpp default is 0.1) Min_P = 0.0 Top_P = 0.8 Top_P = 0.95 TopK = 20 TopK = 20 Chat template/prompt format: Copy <|im_start|>user\\nWhat is 2+2?<|im_end|>\\n<|im_start|>assistant\\n For NON thinking mode, we purposely enclose <think> and </think> with nothing: Copy <|im_start|>user\\nWhat is 2+2?<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</th ink>\\n\\n For Thinking-mode, DO NOT use greedy decoding, as it can lead to performance deg radation and endless repetitions."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "_start|>user\\nWhat is 2+2?<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</th ink>\\n\\n For Thinking-mode, DO NOT use greedy decoding, as it can lead to performance deg radation and endless repetitions. Switching Between Thinking and Non-Thinking Mode Qwen3 models come with built-in \"thinking mode\" to boost reasoning and improve r esponse quality - similar to how QwQ-32B worked. Instructions for switching will differ depending on the inference engine you're using so ensure you use the cor rect instructions. Instructions for llama.cpp and Ollama: You can add /think and /no_think to user prompts or system messages to switch th e model's thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations. Here is an example of multi-turn conversation: Copy > Who are you /no_think <think> </think> I am Qwen, a large-scale language model developed by Alibaba Cloud. [...] > How many 'r's are in 'strawberries'? /think <think> Okay, let's see. The user is asking how many times the letter 'r' appears in the word \"strawberries\". [...] </think> The word strawberries contains 3 instances of the letter r. [...] Instructions for transformers and vLLM: Thinking mode: enable_thinking=True By default, Qwen3 has thinking enabled. When you call tokenizer.apply_chat_templ ate, you don�t need to set anything manually. Copy text = tokenizer."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "mers and vLLM: Thinking mode: enable_thinking=True By default, Qwen3 has thinking enabled. When you call tokenizer.apply_chat_templ ate, you don�t need to set anything manually. Copy text = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True, enable_thinking=True # Default is True ) In thinking mode, the model will generate an extra <think>...</think> block befo re the final answer � this lets it \"plan\" and sharpen its responses. Non-thinking mode: enable_thinking=False Enabling non-thinking will make Qwen3 will skip all the thinking steps and behav e like a normal LLM. Copy text = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True, enable_thinking=False # Disables thinking mode ) This mode will provide final responses directly � no <think> blocks, no chain-of -thought. � Llama.cpp: Run Qwen3 Tutorial Obtain the latest llama.cpp on GitHub here. You can follow the build instruction s below as well. Change -DGGML_CUDA=ON to -DGGML_CUDA=OFF if you don't have a GP U or just want CPU inference. Copy apt-get update apt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y git clone https://github.com/ggml-org/llama.cpp cmake llama.cpp -B llama.cpp/build \\ -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON cmake --build llama.cpp/build --config Release -j --clean-first --target llama-c li llama-gguf-split cp llama.cpp/build/bin/llama-* llama."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "uild \\ -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON cmake --build llama.cpp/build --config Release -j --clean-first --target llama-c li llama-gguf-split cp llama.cpp/build/bin/llama-* llama.cpp Download the model via (after installing pip install huggingface_hub hf_transfer ). You can choose Q4_K_M, or other quantized versions. Copy # !pip install huggingface_hub hf_transfer import os os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\" from huggingface_hub import snapshot_download snapshot_download( repo_id = \"unsloth/Qwen3-14B-GGUF\", local_dir = \"unsloth/Qwen3-14B-GGUF\", allow_patterns = [\"*UD-Q4_K_XL*\"], ) Run the model and try any prompt. To disable thinking, use (or you can set it in the system prompt): Copy >>> Write your prompt here /nothink Running Qwen3-235B-A22B For Qwen3-235B-A22B, we will specifically use Llama.cpp for optimized inference and a plethora of options. We're following similar steps to above however this time we'll also need to perf orm extra steps because the model is so big. Download the model via (after installing pip install huggingface_hub hf_transfer ). You can choose UD-Q2_K_XL, or other quantized versions.. Copy # !pip install huggingface_hub hf_transfer import os os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\" from huggingface_hub import snapshot_download snapshot_download( repo_id = \"unsloth/Qwen3-235B-A22B-GGUF\", local_dir = \"unsloth/Qwen3-235B-A22B-GGUF\", allow_patterns = [\"*UD-Q2_K_XL*\"], ) Run the model and try any prompt."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "import snapshot_download snapshot_download( repo_id = \"unsloth/Qwen3-235B-A22B-GGUF\", local_dir = \"unsloth/Qwen3-235B-A22B-GGUF\", allow_patterns = [\"*UD-Q2_K_XL*\"], ) Run the model and try any prompt. Edit --threads 32 for the number of CPU threads, --ctx-size 16384 for context le ngth, --n-gpu-layers 99 for GPU offloading on how many layers. Try adjusting it if your GPU goes out of memory. Also remove it if you have CPU only inference. Use -ot \".ffn_.*_exps.=CPU\" to offload all MoE layers to the CPU! This effective ly allows you to fit all non MoE layers on 1 GPU, improving generation speeds. You can customize the regex expression to fit more layers if you have more GPU c apacity. Copy ./llama.cpp/llama-cli \\ --model unsloth/Qwen3-235B-A22B-GGUF/Qwen3-235B-A22B-UD-Q2_K_XL.gguf \\ --threads 32 \\ --ctx-size 16384 \\ --n-gpu-layers 99 \\ -ot \".ffn_.*_exps.=CPU\" \\ --seed 3407 \\ --prio 3 \\ --temp 0.6 \\ --min-p 0.0 \\ --top-p 0.95 \\ --top-k 20 \\ -no-cnv \\ --prompt \"<|im_start|>user\\nCreate a Flappy Bird game in Python. You must in clude these things:\\n1. You must use pygame.\\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\\n3. Pressin g SPACE multiple times will accelerate the bird.\\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly ch osen as a dark color.\\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\\n6."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "should be randomly chosen as a square, circle or triangle. The color should be randomly ch osen as a dark color.\\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\\n7. Make randomly spaced pipes with enoug h space. Color them randomly as dark green or light brown or a dark gray shade.\\ n8. When you lose, show the best score. Make the text inside the screen. Pressin g q or Esc will quit the game. Restarting is pressing SPACE again.\\nThe final ga me should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.<|im_end|>\\n<|im_start|>assistant\\n\" LJ test 1 - fails ljubomir@macbook2(:):~/llama.cpp$ build/bin/llama-cli \\ --model models/Qwen3-235B-A22B-Q2_K-00001-of-00002.gguf \\ --threads 32 \\ --ctx-size 16384 \\ --n-gpu-layers 99 \\ --override-tensor \".ffn_.*_exps.=CPU\" \\ --cache-type-k q4_0 \\ --flash-attn \\ --cache-type-v q4_0 \\ --prio 3 \\ --temp 0.6 \\ --top-p 0.95 \\ --top-k 20 \\ --min-p 0 \\ --prompt \"<|im_start|>user\\nCreate a Flappy Bird game in Python. You must in clude these things:\\n1. You must use pygame.\\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\\n3. Pressin g SPACE multiple times will accelerate the bird.\\n4. The bird's shape should be randomly chosen as a square, circle or triangle."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "chosen and is a light shade. Start with a light blue color.\\n3. Pressin g SPACE multiple times will accelerate the bird.\\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly ch osen as a dark color.\\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\\n7. Make randomly spaced pipes with enoug h space. Color them randomly as dark green or light brown or a dark gray shade.\\ n8. When you lose, show the best score. Make the text inside the screen. Pressin g q or Esc will quit the game. Restarting is pressing SPACE again.\\nThe final ga me should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.<|im_end|>\\n<|im_start|>assistant\\n\" Model quants Q2_K_L https://huggingface.co/unsloth/Qwen3-235B-A22B-128K-GGUF/tree/main/Q2_K_L https://huggingface.co/unsloth/Qwen3-235B-A22B-128K-GGUF/blob/main/Q2_K_L/Qwen3- 235B-A22B-128K-Q2_K_L-00001-of-00002.gguf Git LFS Details SHA256: 1c59efa5d4160400ba7c4f40b75bb7e51583af3af49850673ad13310e3c23373 Pointer size: 136 Bytes Size of remote file: 49.7 GB Xet backed hash: 7170f71ecbcf40d7b890178c6a4576606f8fec26a6a84d9a6518a469043559c 7 wget 'https://huggingface.co/unsloth/Qwen3-235B-A22B-128K-GGUF/resolve/main/Q2_K _L/Qwen3-235B-A22B-128K-Q2_K_L-00001-of-00002.gguf' ljubomir@macbook2(:):~/llama."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "76606f8fec26a6a84d9a6518a469043559c 7 wget 'https://huggingface.co/unsloth/Qwen3-235B-A22B-128K-GGUF/resolve/main/Q2_K _L/Qwen3-235B-A22B-128K-Q2_K_L-00001-of-00002.gguf' ljubomir@macbook2(:):~/llama.cpp$ sha256sum models/Qwen3-235B-A22B-128K-Q2_K_L-0 0001-of-00002.gguf 1c59efa5d4160400ba7c4f40b75bb7e51583af3af49850673ad13310e3c23373 models/Qwen3-2 35B-A22B-128K-Q2_K_L-00001-of-00002.gguf https://huggingface.co/unsloth/Qwen3-235B-A22B-128K-GGUF/blob/main/Q2_K_L/Qwen3- 235B-A22B-128K-Q2_K_L-00002-of-00002.gguf Git LFS Details SHA256: 4f2e979fc4e927e01a030e4e94d583ed53c99d769536722e71a25ea915d36009 Pointer size: 136 Bytes Size of remote file: 36.1 GB Xet backed hash: 9e70ef94c94ccc8e4dabded88270e900d1f23e84139c76cab38a37bee153106 2 ljubomir@macbook2(:):~/llama.cpp$ sha256sum models/Qwen3-235B-A22B-128K-Q2_K_L-0 0002-of-00002.gguf 4f2e979fc4e927e01a030e4e94d583ed53c99d769536722e71a25ea915d36009 models/Qwen3-2 35B-A22B-128K-Q2_K_L-00002-of-00002.gguf LJ test 2 ljubomir@macbook2(:):~/llama.cpp$ sudo sysctl iogpu.wired_limit_mb=90000 iogpu.wired_limit_mb: 0 -> 90000 ljubomir@macbook2(:):~/llama.cpp$ build/bin/llama-cli \\ --model models/Qwen3-235B-A22B-128K-Q2_K_L-00001-of-00002.gguf \\ --threads 32 \\ --ctx-size 16384 \\ --n-gpu-layers 99 \\ --override-tensor \".ffn_.*_exps.=CPU\" \\ --cache-type-k q4_0 \\ --flash-attn \\ --cache-type-v q4_0 \\ --prio 3 \\ --temp 0.6 \\ --top-p 0.95 \\ --top-k 20 \\ --min-p 0 \\ --prompt \"<|im_start|>user\\nCreate a Flappy Bird game in Python."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "exps.=CPU\" \\ --cache-type-k q4_0 \\ --flash-attn \\ --cache-type-v q4_0 \\ --prio 3 \\ --temp 0.6 \\ --top-p 0.95 \\ --top-k 20 \\ --min-p 0 \\ --prompt \"<|im_start|>user\\nCreate a Flappy Bird game in Python. You must in clude these things:\\n1. You must use pygame.\\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\\n3. Pressin g SPACE multiple times will accelerate the bird.\\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly ch osen as a dark color.\\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\\n7. Make randomly spaced pipes with enoug h space. Color them randomly as dark green or light brown or a dark gray shade.\\ n8. When you lose, show the best score. Make the text inside the screen. Pressin g q or Esc will quit the game. Restarting is pressing SPACE again.\\nThe final ga me should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.<|im_end|>\\n<|im_start|>assistant\\n\" FAIL - ljubomir@macbook2(:):~/llama.cpp$ sudo sysctl iogpu.wired_limit_mb=90000 iogpu.wired_limit_mb: 90000 -> 90000 ljubomir@macbook2(:):~/llama.cpp$ build/bin/llama-cli \\ --model models/Qwen3-235B-A22B-128K-Q2_K_L-00001-of-00002.gguf \\ --threads 32 \\ --ctx-size 16384 \\ --n-gpu-layers 99 \\ --override-tensor \".ffn_."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ljubomir@macbook2(:):~/llama.cpp$ build/bin/llama-cli \\ --model models/Qwen3-235B-A22B-128K-Q2_K_L-00001-of-00002.gguf \\ --threads 32 \\ --ctx-size 16384 \\ --n-gpu-layers 99 \\ --override-tensor \".ffn_.*_exps.=CPU\" \\ --cache-type-k q4_0 \\ --flash-attn \\ --cache-type-v q4_0 \\ --prio 3 \\ --temp 0.6 \\ --top-p 0.95 \\ --top-k 20 \\ --min-p 0 \\ --prompt \"<|im_start|>user\\nCreate a Flappy Bird game in Python. You must in clude these things:\\n1. You must use pygame.\\n2. The background color should be randomly chosen and is a light shade. Start with a light blue color.\\n3. Pressin g SPACE multiple times will accelerate the bird.\\n4. The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly ch osen as a dark color.\\n5. Place on the bottom some land colored as dark brown or yellow chosen randomly.\\n6. Make a score shown on the top right side. Increment if you pass pipes and don't hit them.\\n7. Make randomly spaced pipes with enoug h space. Color them randomly as dark green or light brown or a dark gray shade.\\ n8. When you lose, show the best score. Make the text inside the screen. Pressin g q or Esc will quit the game. Restarting is pressing SPACE again.\\nThe final ga me should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section.<|im_end|>\\n<|im_start|>assistant\\n\" build: 5626 (bc1007a4) with Apple clang version 17.0.0 (clang-1700.0.13.5) for a rm64-apple-darwin24.4."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "e for errors and fix them before the final markdown section.<|im_end|>\\n<|im_start|>assistant\\n\" build: 5626 (bc1007a4) with Apple clang version 17.0.0 (clang-1700.0.13.5) for a rm64-apple-darwin24.4.0 main: llama backend init main: load the model and apply lora adapter, if any llama_model_load_from_file_impl: using device Metal (Apple M2 Max) - 89999 MiB f ree llama_model_loader: additional 1 GGUFs metadata loaded. llama_model_loader: loaded meta data with 49 key-value pairs and 1131 tensors fr om models/Qwen3-235B-A22B-128K-Q2_K_L-00001-of-00002.gguf (version GGUF V3 (late st)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not appl y in this output. llama_model_loader: - kv 0: general.architecture str = qwen3moe llama_model_loader: - kv 1: general.type str = model llama_model_loader: - kv 2: general.name str = Qwen3-235B-A22B-128K llama_model_loader: - kv 3: general.finetune str = 128k llama_model_loader: - kv 4: general.basename str = Qwen3-235B-A22B-128K llama_model_loader: - kv 5: general.quantized_by str = Unsloth llama_model_loader: - kv 6: general.size_label str = 235B-A22B llama_model_loader: - kv 7: general.license str = apache-2.0 llama_model_loader: - kv 8: general.license.link str = https://huggingface.co/Qwen/Qwen3-235... llama_model_loader: - kv 9: general.repo_url str = https://huggingface.co/unsloth llama_model_loader: - kv 10: general.base_model.count u32 = 1 llama_model_loader: - kv 11: general.base_model.0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "35... llama_model_loader: - kv 9: general.repo_url str = https://huggingface.co/unsloth llama_model_loader: - kv 10: general.base_model.count u32 = 1 llama_model_loader: - kv 11: general.base_model.0.name str = Qwen3 235B A22B llama_model_loader: - kv 12: general.base_model.0.organization str = Qwen llama_model_loader: - kv 13: general.base_model.0.repo_url str = https://huggingface.co/Qwen/Qwen3-235... llama_model_loader: - kv 14: general.tags arr[str ,2] = [\"unsloth\", \"text-generation\"] llama_model_loader: - kv 15: qwen3moe.block_count u32 = 94 llama_model_loader: - kv 16: qwen3moe.context_length u32 = 131072 llama_model_loader: - kv 17: qwen3moe.embedding_length u32 = 4096 llama_model_loader: - kv 18: qwen3moe.feed_forward_length u32 = 12288 llama_model_loader: - kv 19: qwen3moe.attention.head_count u32 = 64 llama_model_loader: - kv 20: qwen3moe.attention.head_count_kv u32 = 4 llama_model_loader: - kv 21: qwen3moe.rope.freq_base f32 = 1000000.000000 llama_model_loader: - kv 22: qwen3moe.attention.layer_norm_rms_epsilon f32 = 0.000001 llama_model_loader: - kv 23: qwen3moe.expert_used_count u32 = 8 llama_model_loader: - kv 24: qwen3moe.attention.key_length u32 = 128 llama_model_loader: - kv 25: qwen3moe.attention.value_length u32 = 128 llama_model_loader: - kv 26: qwen3moe.expert_count u32 = 128 llama_model_loader: - kv 27: qwen3moe.expert_feed_forward_length u32 = 1536 llama_model_loader: - kv 28: qwen3moe.rope.scaling.type str = yarn llama_model_loader: - kv 29: qwen3moe."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "rt_count u32 = 128 llama_model_loader: - kv 27: qwen3moe.expert_feed_forward_length u32 = 1536 llama_model_loader: - kv 28: qwen3moe.rope.scaling.type str = yarn llama_model_loader: - kv 29: qwen3moe.rope.scaling.factor f32 = 4.000000 llama_model_loader: - kv 30: qwen3moe.rope.scaling.original_context_length u32 = 32768 llama_model_loader: - kv 31: tokenizer.ggml.model str = gpt2 llama_model_loader: - kv 32: tokenizer.ggml.pre str = qwen2 llama_model_loader: - kv 33: tokenizer.ggml.tokens arr[str ,151936] = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ... llama_model_loader: - kv 34: tokenizer.ggml.token_type arr[i32 ,151936] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... llama_model_loader: - kv 35: tokenizer.ggml.merges arr[str ,151387] = [\"� � \", \"� � � � \", \"i n\", \"� t\",... llama_model_loader: - kv 36: tokenizer.ggml.eos_token_id u32 = 151645 llama_model_loader: - kv 37: tokenizer.ggml.padding_token_id u32 = 151654 llama_model_loader: - kv 38: tokenizer.ggml.add_bos_token bool = false llama_model_loader: - kv 39: tokenizer.chat_template str = {%- if tools %}\\n {{- '<|im_start|>... llama_model_loader: - kv 40: general.quantization_version u32 = 2 llama_model_loader: - kv 41: general.file_type u32 = 10 llama_model_loader: - kv 42: quantize.imatrix.file str = Qwen3-235B-A22B-128K-GGUF/imatrix_uns... llama_model_loader: - kv 43: quantize.imatrix.dataset str = unsloth_calibration_Qwen3-235B-A22B-1... llama_model_loader: - kv 44: quantize.imatrix."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": ".file str = Qwen3-235B-A22B-128K-GGUF/imatrix_uns... llama_model_loader: - kv 43: quantize.imatrix.dataset str = unsloth_calibration_Qwen3-235B-A22B-1... llama_model_loader: - kv 44: quantize.imatrix.entries_count i32 = 744 llama_model_loader: - kv 45: quantize.imatrix.chunks_count i32 = 685 llama_model_loader: - kv 46: split.no u16 = 0 llama_model_loader: - kv 47: split.tensors.count i32 = 1131 llama_model_loader: - kv 48: split.count u16 = 2 llama_model_loader: - type f32: 471 tensors llama_model_loader: - type q2_K: 376 tensors llama_model_loader: - type q3_K: 188 tensors llama_model_loader: - type q4_K: 95 tensors llama_model_loader: - type q6_K: 1 tensors print_info: file format = GGUF V3 (latest) print_info: file type = Q2_K - Medium print_info: file size = 79.94 GiB (2.92 BPW) load: special tokens cache size = 26 load: token to piece cache size = 0.9311 MB print_info: arch = qwen3moe print_info: vocab_only = 0 print_info: n_ctx_train = 131072 print_info: n_embd = 4096 print_info: n_layer = 94 print_info: n_head = 64 print_info: n_head_kv = 4 print_info: n_rot = 128 print_info: n_swa = 0 print_info: is_swa_any = 0 print_info: n_embd_head_k = 128 print_info: n_embd_head_v = 128 print_info: n_gqa = 16 print_info: n_embd_k_gqa = 512 print_info: n_embd_v_gqa = 512 print_info: f_norm_eps = 0.0e+00 print_info: f_norm_rms_eps = 1.0e-06 print_info: f_clamp_kqv = 0.0e+00 print_info: f_max_alibi_bias = 0.0e+00 print_info: f_logit_scale = 0.0e+00 print_info: f_attn_scale = 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "fo: f_norm_eps = 0.0e+00 print_info: f_norm_rms_eps = 1.0e-06 print_info: f_clamp_kqv = 0.0e+00 print_info: f_max_alibi_bias = 0.0e+00 print_info: f_logit_scale = 0.0e+00 print_info: f_attn_scale = 0.0e+00 print_info: n_ff = 12288 print_info: n_expert = 128 print_info: n_expert_used = 8 print_info: causal attn = 1 print_info: pooling type = 0 print_info: rope type = 2 print_info: rope scaling = yarn print_info: freq_base_train = 1000000.0 print_info: freq_scale_train = 0.25 print_info: n_ctx_orig_yarn = 32768 print_info: rope_finetuned = unknown print_info: ssm_d_conv = 0 print_info: ssm_d_inner = 0 print_info: ssm_d_state = 0 print_info: ssm_dt_rank = 0 print_info: ssm_dt_b_c_rms = 0 print_info: model type = 235B.A22B print_info: model params = 235.09 B print_info: general.name = Qwen3-235B-A22B-128K print_info: n_ff_exp = 1536 print_info: vocab type = BPE print_info: n_vocab = 151936 print_info: n_merges = 151387 print_info: BOS token = 11 ',' print_info: EOS token = 151645 '<|im_end|>' print_info: EOT token = 151645 '<|im_end|>' print_info: PAD token = 151654 '<|vision_pad|>' print_info: LF token = 198 '�' print_info: FIM PRE token = 151659 '<|fim_prefix|>' print_info: FIM SUF token = 151661 '<|fim_suffix|>' print_info: FIM MID token = 151660 '<|fim_middle|>' print_info: FIM PAD token = 151662 '<|fim_pad|>' print_info: FIM REP token = 151663 '<|repo_name|>' print_info: FIM SEP token = 151664 '<|file_sep|>' print_info: EOG token = 151643 '<|endoftext|>' print_info: EOG toke"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "AD token = 151662 '<|fim_pad|>' print_info: FIM REP token = 151663 '<|repo_name|>' print_info: FIM SEP token = 151664 '<|file_sep|>' print_info: EOG token = 151643 '<|endoftext|>' print_info: EOG token = 151645 '<|im_end|>' print_info: EOG token = 151662 '<|fim_pad|>' print_info: EOG token = 151663 '<|repo_name|>' print_info: EOG token = 151664 '<|file_sep|>' print_info: max token length = 256 load_tensors: loading model tensors, this can take a while... (mmap = true) load_tensors: offloading 94 repeating layers to GPU load_tensors: offloading output layer to GPU load_tensors: offloaded 95/95 layers to GPU load_tensors: Metal_Mapped model buffer size = 47398.20 MiB load_tensors: Metal_Mapped model buffer size = 33622.50 MiB load_tensors: CPU_Mapped model buffer size = 46885.27 MiB load_tensors: CPU_Mapped model buffer size = 34456.49 MiB ................................................................................ .................... llama_context: constructing llama_context llama_context: n_seq_max = 1 llama_context: n_ctx = 16384 llama_context: n_ctx_per_seq = 16384 llama_context: n_batch = 2048 llama_context: n_ubatch = 512 llama_context: causal_attn = 1 llama_context: flash_attn = 1 llama_context: freq_base = 1000000.0 llama_context: freq_scale = 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "seq = 16384 llama_context: n_batch = 2048 llama_context: n_ubatch = 512 llama_context: causal_attn = 1 llama_context: flash_attn = 1 llama_context: freq_base = 1000000.0 llama_context: freq_scale = 0.25 llama_context: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized ggml_metal_init: allocating ggml_metal_init: found device: Apple M2 Max ggml_metal_init: picking default device: Apple M2 Max ggml_metal_load_library: using embedded metal library ggml_metal_init: GPU name: Apple M2 Max ggml_metal_init: GPU family: MTLGPUFamilyApple8 (1008) ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003) ggml_metal_init: GPU family: MTLGPUFamilyMetal3 (5001) ggml_metal_init: simdgroup reduction = true ggml_metal_init: simdgroup matrix mul. = true ggml_metal_init: has residency sets = true ggml_metal_init: has bfloat = true ggml_metal_init: use bfloat = false ggml_metal_init: hasUnifiedMemory = true ggml_metal_init: recommendedMaxWorkingSetSize = 94371.84 MB ggml_metal_init: skipping kernel_get_rows_bf16 (not supporte d) ggml_metal_init: skipping kernel_mul_mv_bf16_f32 (not supporte d) ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row (not supporte d) ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4 (not supporte d) ggml_metal_init: skipping kernel_mul_mv_bf16_bf16 (not supporte d) ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32 (not supporte d) ggml_metal_init: skipping kernel_mul_mm_bf16_f32 (not supporte d) ggml_metal_init: sk"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ping kernel_mul_mv_bf16_bf16 (not supporte d) ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32 (not supporte d) ggml_metal_init: skipping kernel_mul_mm_bf16_f32 (not supporte d) ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not suppor ted) ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not suppor ted)"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "c_bf16_hk192_hv128 (not suppor ted) ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256 (not supporte d) ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not suppor ted) ggml_metal_init: skipping kernel_cpy_f32_bf16 (not supporte d) ggml_metal_init: skipping kernel_cpy_bf16_f32 (not supporte d) ggml_metal_init: skipping kernel_cpy_bf16_bf16 (not supporte d) llama_context: CPU output buffer size = 0.58 MiB llama_kv_cache_unified: Metal KV buffer size = 846.00 MiB llama_kv_cache_unified: size = 846.00 MiB ( 16384 cells, 94 layers, 1 seqs), K (q4_0): 423.00 MiB, V (q4_0): 423.00 MiB llama_context: Metal compute buffer size = 377.25 MiB llama_context: CPU compute buffer size = 96.01 MiB llama_context: graph nodes = 5929 llama_context: graph splits = 190 common_init_from_params: setting dry_penalty_last_n to ctx_size = 16384 common_init_from_params: warming up the model with an empty run - please wait .. . (--no-warmup to disable) ggml_metal_graph_compute: command buffer 1 failed with status 5 error: Insufficient Memory (00000008:kIOGPUCommandBufferCallbackErrorOutOfMemory ) graph_compute: ggml_backend_sched_graph_compute_async failed with error -1 llama_decode: failed to decode, ret = -3 main: llama threadpool init, n_threads = 32 main: chat template is available, enabling conversation mode (disable it with -n o-cnv) *** User-specified prompt will pre-start conversation, did you mean to set --sys tem-prompt (-sys) instead? main: chat template exampl"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "available, enabling conversation mode (disable it with -n o-cnv) *** User-specified prompt will pre-start conversation, did you mean to set --sys tem-prompt (-sys) instead? main: chat template example: <|im_start|>system You are a helpful assistant<|im_end|> <|im_start|>user Hello<|im_end|> <|im_start|>assistant Hi there<|im_end|> <|im_start|>user How are you?<|im_end|> <|im_start|>assistant system_info: n_threads = 32 (n_threads_batch = 32) / 12 | Metal : EMBED_LIBRARY = 1 | CPU : ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERA TE = 1 | AARCH64_REPACK = 1 | main: interactive mode on. sampler seed: 2185882232 sampler params: repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, p resence_penalty = 0.000 dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_pe nalty_last_n = 16384 top_k = 20, top_p = 0.950, min_p = 0.000, xtc_probability = 0.000, xtc_t hreshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.600 mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000 sampler chain: logits -> logit-bias -> penalties -> dry -> top-n-sigma -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist generate: n_ctx = 16384, n_batch = 2048, n_predict = -1, n_keep = 0 == Running in interactive mode. == - Press Ctrl+C to interject at any time. - Press Return to return control to the AI. - To return control without starting a new line, end your input with '/'."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ep = 0 == Running in interactive mode. == - Press Ctrl+C to interject at any time. - Press Return to return control to the AI. - To return control without starting a new line, end your input with '/'. - If you want to submit another line, end your input with '\\'. - Not using system message. To change it, set a different value via -sys PROMPT user user Create a Flappy Bird game in Python. You must include these things: 1. You must use pygame. 2. The background color should be randomly chosen and is a light shade. Start wi th a light blue color. 3. Pressing SPACE multiple times will accelerate the bird. 4. The bird's shape should be randomly chosen as a square, circle or triangle. T he color should be randomly chosen as a dark color. 5. Place on the bottom some land colored as dark brown or yellow chosen randomly . 6. Make a score shown on the top right side. Increment if you pass pipes and don 't hit them. 7. Make randomly spaced pipes with enough space. Color them randomly as dark gre en or light brown or a dark gray shade. 8. When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again. The final game should be inside a markdown section in Python. Check your code fo r errors and fix them before the final markdown section."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "or Esc will quit the game. Restarting is pressing SPACE again. The final game should be inside a markdown section in Python. Check your code fo r errors and fix them before the final markdown section. assistant assistant ggml_metal_graph_compute: command buffer 1 failed with status 5 error: Insufficient Memory (00000008:kIOGPUCommandBufferCallbackErrorOutOfMemory ) graph_compute: ggml_backend_sched_graph_compute_async failed with error -1 llama_decode: failed to decode, ret = -3 main : failed to eval ggml_metal_free: deallocating ljubomir@macbook2(:):~/llama.cpp$ LJ Sun 1 Jun 2025 08:55:50 BST + LJ Sun 1 Jun 2025 07:14:27 BST � ljubomir@macbook2(:):~/llama.cpp$ build/bin/llama-cli --help ljubomir@macbook2(:):~/llama.cpp$ build/bin/llama-cli --help ----- common params ----- -h, --help, --usage print usage and exit --version show version and build info --completion-bash print source-able bash completion script for llama.cpp --verbose-prompt print a verbose prompt before generation (default: false) -t, --threads N number of threads to use during generati on (default: -1) (env: LLAMA_ARG_THREADS) -tb, --threads-batch N number of threads to use during batch an d prompt processing (default: same as --threads) -C, --cpu-mask M CPU affinity mask: arbitrarily long hex. Complements cpu-range (default: \"\") -Cr, --cpu-range lo-hi range of CPUs for affinity."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "tch an d prompt processing (default: same as --threads) -C, --cpu-mask M CPU affinity mask: arbitrarily long hex. Complements cpu-range (default: \"\") -Cr, --cpu-range lo-hi range of CPUs for affinity. Complements --cpu-mask --cpu-strict <0|1> use strict CPU placement (default: 0) --prio N set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime (default: 0) --poll <0...100> use polling level to wait for work (0 - no polling, default: 50) -Cb, --cpu-mask-batch M CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch (default: same as --cpu-mask) -Crb, --cpu-range-batch lo-hi ranges of CPUs for affinity. Complements --cpu-mask-batch --cpu-strict-batch <0|1> use strict CPU placement (default: same as --cpu-strict) --prio-batch N set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime (default: 0) --poll-batch <0|1> use polling to wait for work (default: s ame as --poll) -c, --ctx-size N size of the prompt context (default: 409 6, 0 = loaded from model) (env: LLAMA_ARG_CTX_SIZE) -n, --predict, --n-predict N number of tokens to predict (default: -1 , -1 = infinity, -2 = until context filled) (env: LLAMA_ARG_N_PREDICT) -b, --batch-size N logical maximum batch size (default: 204 8) (env: LLAMA_ARG_BATCH) -ub, --ubatch-size N physical maximum batch size (default: 51 2) (env: LLAMA_ARG_UBATCH) --keep N number of tokens to keep from the initia l prompt (default: 0, -1 = all) --swa-full use full-size SWA cache (default: false) [(more info)](https:/"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "size (default: 51 2) (env: LLAMA_ARG_UBATCH) --keep N number of tokens to keep from the initia l prompt (default: 0, -1 = all) --swa-full use full-size SWA cache (default: false) [(more info)](https://github.com/ggml-org/llama .cpp/pull/13194#issuecomment-2868343055) (env: LLAMA_ARG_SWA_FULL) -fa, --flash-attn enable Flash Attention (default: disable d) (env: LLAMA_ARG_FLASH_ATTN) -p, --prompt PROMPT prompt to start generation with; for sys tem message, use -sys --no-perf disable internal libllama performance ti mings (default: false) (env: LLAMA_ARG_NO_PERF) -f, --file FNAME a file containing the prompt (default: n one) -bf, --binary-file FNAME binary file containing the prompt (defau lt: none) -e, --escape process escapes sequences (\\n, \\r, \\t, \\ ', \\\", \\\\) (default: true) --no-escape do not process escape sequences --rope-scaling {none,linear,yarn} RoPE frequency scaling method, defaults to linear unless specified by the model (env: LLAMA_ARG_ROPE_SCALING_TYPE) --rope-scale N RoPE context scaling factor, expands con text by a factor of N (env: LLAMA_ARG_ROPE_SCALE) --rope-freq-base N RoPE base frequency, used by NTK-aware s caling (default: loaded from model) (env: LLAMA_ARG_ROPE_FREQ_BASE) --rope-freq-scale N RoPE frequency scaling factor, expands c ontext by a factor of 1/N (env: LLAMA_ARG_ROPE_FREQ_SCALE) --yarn-orig-ctx N YaRN: original context size of model (de fault: 0 = model training context size) (env: LLAMA_ARG_YARN_ORIG_CTX) --yarn-ext-factor N YaRN: extrapolati"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "(env: LLAMA_ARG_ROPE_FREQ_SCALE) --yarn-orig-ctx N YaRN: original context size of model (de fault: 0 = model training context size) (env: LLAMA_ARG_YARN_ORIG_CTX) --yarn-ext-factor N YaRN: extrapolation mix factor (default: -1.0, 0.0 = full interpolation) (env: LLAMA_ARG_YARN_EXT_FACTOR) --yarn-attn-factor N YaRN: scale sqrt(t) or attention magnitu de (default: 1.0) (env: LLAMA_ARG_YARN_ATTN_FACTOR) --yarn-beta-slow N YaRN: high correction dim or alpha (defa ult: 1.0) (env: LLAMA_ARG_YARN_BETA_SLOW) --yarn-beta-fast N YaRN: low correction dim or beta (defaul t: 32.0) (env: LLAMA_ARG_YARN_BETA_FAST) -nkvo, --no-kv-offload disable KV offload (env: LLAMA_ARG_NO_KV_OFFLOAD) -ctk, --cache-type-k TYPE KV cache data type for K allowed values: f32, f16, bf16, q8_0, q4 _0, q4_1, iq4_nl, q5_0, q5_1 (default: f16) (env: LLAMA_ARG_CACHE_TYPE_K) -ctv, --cache-type-v TYPE KV cache data type for V allowed values: f32, f16, bf16, q8_0, q4 _0, q4_1, iq4_nl, q5_0, q5_1 (default: f16) (env: LLAMA_ARG_CACHE_TYPE_V) -dt, --defrag-thold N KV cache defragmentation threshold (defa ult: 0.1, < 0 - disabled) (env: LLAMA_ARG_DEFRAG_THOLD) -np, --parallel N number of parallel sequences to decode ( default: 1) (env: LLAMA_ARG_N_PARALLEL) --mlock force system to keep model in RAM rather than swapping or compressing (env: LLAMA_ARG_MLOCK) --no-mmap do not memory-map model (slower load but may reduce pageouts if not using mlock) (env: LLAMA_ARG_NO_MMAP) --numa TYPE attempt optimizations that help on some NU"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "essing (env: LLAMA_ARG_MLOCK) --no-mmap do not memory-map model (slower load but may reduce pageouts if not using mlock) (env: LLAMA_ARG_NO_MMAP) --numa TYPE attempt optimizations that help on some NUMA systems - distribute: spread execution evenly ov er all nodes - isolate: only spawn threads on CPUs on the node that execution started on - numactl: use the CPU map provided by n umactl if run without this previously, it is re commended to drop the system page cache before using this see https://github.com/ggml-org/llama.cp p/issues/1437 (env: LLAMA_ARG_NUMA) -dev, --device <dev1,dev2,..> comma-separated list of devices to use f or offloading (none = don't offload) use --list-devices to see a list of avai lable devices (env: LLAMA_ARG_DEVICE) --list-devices print list of available devices and exit --override-tensor, -ot <tensor name pattern>=<buffer type>,... override tensor buffer type -ngl, --gpu-layers, --n-gpu-layers N number of layers to store in VRAM (env: LLAMA_ARG_N_GPU_LAYERS) -sm, --split-mode {none,layer,row} how to split the model across multiple G PUs, one of: - none: use one GPU only - layer (default): split layers and KV a cross GPUs - row: split rows across GPUs (env: LLAMA_ARG_SPLIT_MODE) -ts, --tensor-split N0,N1,N2,... fraction of the model to offload to each GPU, comma-separated list of proportions, e.g."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "s and KV a cross GPUs - row: split rows across GPUs (env: LLAMA_ARG_SPLIT_MODE) -ts, --tensor-split N0,N1,N2,... fraction of the model to offload to each GPU, comma-separated list of proportions, e.g. 3,1 (env: LLAMA_ARG_TENSOR_SPLIT) -mg, --main-gpu INDEX the GPU to use for the model (with split -mode = none), or for intermediate results and KV (with split- mode = row) (default: 0) (env: LLAMA_ARG_MAIN_GPU) --check-tensors check model tensor data for invalid valu es (default: false) --override-kv KEY=TYPE:VALUE advanced option to override model metada ta by key. may be specified multiple times. types: int, float, bool, str. example: - -override-kv tokenizer.ggml.add_bos_token=bool:false --no-op-offload disable offloading host tensor operation s to device (default: false) --lora FNAME path to LoRA adapter (can be repeated to use multiple adapters) --lora-scaled FNAME SCALE path to LoRA adapter with user defined s caling (can be repeated to use multiple adapters) --control-vector FNAME add a control vector note: this argument can be repeated to a dd multiple control vectors --control-vector-scaled FNAME SCALE add a control vector with user defined s caling SCALE note: this argument can be repeated to a dd multiple scaled control vectors --control-vector-layer-range START END layer range to apply the control vector( s) to, start and end inclusive -m, --model FNAME model path (default: `models/$filename` with filename from `--hf-file` or `--model-url` if set, otherwise model s/7"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "range to apply the control vector( s) to, start and end inclusive -m, --model FNAME model path (default: `models/$filename` with filename from `--hf-file` or `--model-url` if set, otherwise model s/7B/ggml-model-f16.gguf) (env: LLAMA_ARG_MODEL) -mu, --model-url MODEL_URL model download url (default: unused) (env: LLAMA_ARG_MODEL_URL) -hf, -hfr, --hf-repo <user>/<model>[:quant] Hugging Face model repository; quant is optional, case-insensitive, default to Q4_K_M, or falls back to the first file in the repo if Q4_K_M doesn't exist. mmproj is also downloaded automatically if available. to disable, add --no-mmproj example: unsloth/phi-4-GGUF:q4_k_m (default: unused) (env: LLAMA_ARG_HF_REPO) -hfd, -hfrd, --hf-repo-draft <user>/<model>[:quant] Same as --hf-repo, but for the draft mod el (default: unused) (env: LLAMA_ARG_HFD_REPO) -hff, --hf-file FILE Hugging Face model file. If specified, i t will override the quant in --hf-repo (default: unused) (env: LLAMA_ARG_HF_FILE) -hfv, -hfrv, --hf-repo-v <user>/<model>[:quant] Hugging Face model repository for the vo coder model (default: unused) (env: LLAMA_ARG_HF_REPO_V) -hffv, --hf-file-v FILE Hugging Face model file for the vocoder model (default: unused) (env: LLAMA_ARG_HF_FILE_V) -hft, --hf-token TOKEN Hugging Face access token (default: valu e from HF_TOKEN environment variable) (env: HF_TOKEN) --log-disable Log disable --log-file FNAME Log to file --log-colors Enable colored logging (env: LLAMA_LOG_COLORS) -v, --verbose, --log-verb"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "valu e from HF_TOKEN environment variable) (env: HF_TOKEN) --log-disable Log disable --log-file FNAME Log to file --log-colors Enable colored logging (env: LLAMA_LOG_COLORS) -v, --verbose, --log-verbose Set verbosity level to infinity (i.e. lo g all messages, useful for debugging) --offline Offline mode: forces use of cache, preve nts network access (env: LLAMA_OFFLINE) -lv, --verbosity, --log-verbosity N Set the verbosity threshold. Messages wi th a higher verbosity will be ignored. (env: LLAMA_LOG_VERBOSITY) --log-prefix Enable prefix in log messages (env: LLAMA_LOG_PREFIX) --log-timestamps Enable timestamps in log messages (env: LLAMA_LOG_TIMESTAMPS) ----- sampling params ----- --samplers SAMPLERS samplers that will be used for generatio n in the order, separated by ';' (default: penalties;dry;top_n_sigma;top_k;typ_p;to p_p;min_p;xtc;temperature) -s, --seed SEED RNG seed (default: -1, use random seed f or -1) --sampling-seq, --sampler-seq SEQUENCE simplified sequence for samplers that wi ll be used (default: edskypmxt) --ignore-eos ignore end of stream token and continue generating (implies --logit-bias EOS-inf) --temp N temperature (default: 0.8) --top-k N top-k sampling (default: 40, 0 = disable d) --top-p N top-p sampling (default: 0.9, 1.0 = disa bled) --min-p N min-p sampling (default: 0.1, 0.0 = disa bled) --top-nsigma N top-n-sigma sampling (default: -1.0, -1. 0 = disabled) --xtc-probability N xtc probability (default: 0.0, 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "9, 1.0 = disa bled) --min-p N min-p sampling (default: 0.1, 0.0 = disa bled) --top-nsigma N top-n-sigma sampling (default: -1.0, -1. 0 = disabled) --xtc-probability N xtc probability (default: 0.0, 0.0 = dis abled) --xtc-threshold N xtc threshold (default: 0.1, 1.0 = disab led) --typical N locally typical sampling, parameter p (d efault: 1.0, 1.0 = disabled) --repeat-last-n N last n tokens to consider for penalize ( default: 64, 0 = disabled, -1 = ctx_size) --repeat-penalty N penalize repeat sequence of tokens (defa ult: 1.0, 1.0 = disabled) --presence-penalty N repeat alpha presence penalty (default: 0.0, 0.0 = disabled) --frequency-penalty N repeat alpha frequency penalty (default: 0.0, 0.0 = disabled) --dry-multiplier N set DRY sampling multiplier (default: 0. 0, 0.0 = disabled) --dry-base N set DRY sampling base value (default: 1. 75) --dry-allowed-length N set allowed length for DRY sampling (def ault: 2) --dry-penalty-last-n N set DRY penalty for the last n tokens (d efault: -1, 0 = disable, -1 = context size) --dry-sequence-breaker STRING add sequence breaker for DRY sampling, c learing out default breakers ('\\n', ':', '\"', '*') in the process; us e \"none\" to not use any sequence breakers --dynatemp-range N dynamic temperature range (default: 0.0, 0.0 = disabled) --dynatemp-exp N dynamic temperature exponent (default: 1 .0) --mirostat N use Mirostat sampling. Top K, Nucleus and Locally Typical sampl ers are ignored if used."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "range (default: 0.0, 0.0 = disabled) --dynatemp-exp N dynamic temperature exponent (default: 1 .0) --mirostat N use Mirostat sampling. Top K, Nucleus and Locally Typical sampl ers are ignored if used. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0) --mirostat-lr N Mirostat learning rate, parameter eta (d efault: 0.1) --mirostat-ent N Mirostat target entropy, parameter tau ( default: 5.0) -l, --logit-bias TOKEN_ID(+/-)BIAS modifies the likelihood of token appeari ng in the completion, i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello', or `--logit-bias 15043-1` to decrease li kelihood of token ' Hello' --grammar GRAMMAR BNF-like grammar to constrain generation s (see samples in grammars/ dir) (default: '') --grammar-file FNAME file to read grammar from -j, --json-schema SCHEMA JSON schema to constrain generations (ht tps://json-schema.org/), e.g. `{}` for any JSON object For schemas w/ external $refs, use --gra mmar + example/json_schema_to_grammar.py instea d -jf, --json-schema-file FILE File containing a JSON schema to constra in generations (https://json-schema.org/), e.g. `{}` fo r any JSON object For schemas w/ external $refs, use --gra mmar + example/json_schema_to_grammar."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "FILE File containing a JSON schema to constra in generations (https://json-schema.org/), e.g. `{}` fo r any JSON object For schemas w/ external $refs, use --gra mmar + example/json_schema_to_grammar.py instea d ----- example-specific params ----- --no-display-prompt don't print prompt at generation (defaul t: false) -co, --color colorise output to distinguish prompt an d user input from generations (default: false) --no-context-shift disables context shift on infinite text generation (default: disabled) (env: LLAMA_ARG_NO_CONTEXT_SHIFT) -sys, --system-prompt PROMPT system prompt to use with model (if appl icable, depending on chat template) -sysf, --system-prompt-file FNAME a file containing the system prompt (def ault: none) -ptc, --print-token-count N print token count every N tokens (defaul t: -1) --prompt-cache FNAME file to cache prompt state for faster st artup (default: none) --prompt-cache-all if specified, saves user input and gener ations to cache as well --prompt-cache-ro if specified, uses the prompt cache but does not update it -r, --reverse-prompt PROMPT halt generation at PROMPT, return contro l in interactive mode -sp, --special special tokens output enabled (default: false) -cnv, --conversation run in conversation mode: - does not print special tokens and suff ix/prefix - interactive mode is also enabled (default: auto enabled if chat template is available) -no-cnv, --no-conversation force disable conversation mode (default : false) -st, --single-turn run co"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "x/prefix - interactive mode is also enabled (default: auto enabled if chat template is available) -no-cnv, --no-conversation force disable conversation mode (default : false) -st, --single-turn run conversation for a single turn only, then exit when done will not be interactive if first turn is predefined with --prompt (default: false) -i, --interactive run in interactive mode (default: false) -if, --interactive-first run in interactive mode and wait for inp ut right away (default: false) -mli, --multiline-input allows you to write or paste multiple li nes without ending each in '\\' --in-prefix-bos prefix BOS to user inputs, preceding the `--in-prefix` string --in-prefix STRING string to prefix user inputs with (defau lt: empty) --in-suffix STRING string to suffix after user inputs with (default: empty) --no-warmup skip warming up the model with an empty run -gan, --grp-attn-n N group-attention factor (default: 1) (env: LLAMA_ARG_GRP_ATTN_N) -gaw, --grp-attn-w N group-attention width (default: 512) (env: LLAMA_ARG_GRP_ATTN_W) --jinja use jinja template for chat (default: di sabled) (env: LLAMA_ARG_JINJA) --reasoning-format FORMAT controls whether thought tags are allowe d and/or extracted from the response, and in which format they're re turned; one of: - none: leaves thoughts unparsed in `mes sage.content` - deepseek: puts thoughts in `message."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ought tags are allowe d and/or extracted from the response, and in which format they're re turned; one of: - none: leaves thoughts unparsed in `mes sage.content` - deepseek: puts thoughts in `message.re asoning_content` (except in streaming mode, which behaves as `none`) (default: deepseek) (env: LLAMA_ARG_THINK) --reasoning-budget N controls the amount of thinking allowed; currently only one of: -1 for unrestricted thinking budget, or 0 to di sable thinking (default: -1) (env: LLAMA_ARG_THINK_BUDGET) --chat-template JINJA_TEMPLATE set custom jinja chat template (default: template taken from model's metadata) if suffix/prefix are specified, template will be disabled only commonly used templates are accepte d (unless --jinja is set before this flag): list of built-in templates: bailing, chatglm3, chatglm4, chatml, com mand-r, deepseek, deepseek2, deepseek3, exaone3, falcon3, gemma, giga chat, glmedge, granite, llama2, llama2-sys, llama2-sys-bos, llam a2-sys-strip, llama3, llama4, megrez, minicpm, mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7, mistral-v7-tekken, monarch, openchat, orion, phi3, phi4, rwkv-world, smolvlm, vicuna, vicuna-orca , yandex, zephyr (env: LLAMA_ARG_CHAT_TEMPLATE) --chat-template-file JINJA_TEMPLATE_FILE set custom jinja chat template file (def ault: template taken from model's metadata) if suffix/prefix are specified, template will be disabled only commonly used templates are accepte d (unless --jinja is set before this flag): list of built-in te"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "template taken from model's metadata) if suffix/prefix are specified, template will be disabled only commonly used templates are accepte d (unless --jinja is set before this flag): list of built-in templates: bailing, chatglm3, chatglm4, chatml, com mand-r, deepseek, deepseek2, deepseek3, exaone3, falcon3, gemma, giga chat, glmedge, granite, llama2, llama2-sys, llama2-sys-bos, llam a2-sys-strip, llama3, llama4, megrez, minicpm, mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7, mistral-v7-tekken, monarch, openchat, orion, phi3, phi4, rwkv-world, smolvlm, vicuna, vicuna-orca , yandex, zephyr (env: LLAMA_ARG_CHAT_TEMPLATE_FILE) --simple-io use basic IO for better compatibility in subprocesses and limited consoles example usage: text generation: build/bin/llama-cli -m your_model.gguf -p \"I believe the meaning of life is\" -n 128 -no-cnv chat (conversation): build/bin/llama-cli -m your_model.gguf -sys \"You are a he lpful assistant\" LJ Sun 1 Jun 2025 07:14:27 BST + https://www.reddit.com/r/LocalLLaMA/comments/186phti/m1m2m3_increase_vr am_allocation_with_sudo_sysctl/ M1/M2/M3: increase VRAM allocation with `sudo sysctl iogpu.wired_limit_mb=12345` (i.e. amount in mb to allocate) https://www.reddit.com/r/LocalLLaMA/comments/186phti/m1m2m3_increase_vram_alloca tion_with_sudo_sysctl/ r/LocalLLaMA � 2 yr. ago farkinga M1/M2/M3: increase VRAM allocation with `sudo sysctl iogpu.wired_limit_mb=12345` (i.e."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "/LocalLLaMA/comments/186phti/m1m2m3_increase_vram_alloca tion_with_sudo_sysctl/ r/LocalLLaMA � 2 yr. ago farkinga M1/M2/M3: increase VRAM allocation with `sudo sysctl iogpu.wired_limit_mb=12345` (i.e. amount in mb to allocate) Tutorial | Guide If you're using Metal to run your llms, you may have noticed the amount of VRAM available is around 60%-70% of the total RAM - despite Apple's unique architectu re for sharing the same high-speed RAM between CPU and GPU. It turns out this VRAM allocation can be controlled at runtime using sudo sysctl iogpu.wired_limit_mb=12345 See here: https://github.com/ggerganov/llama.cpp/discussions/2182#discussioncomm ent-7698315 Previously, it was believed this could only be done with a kernel patch - and th at required disabling a macos security feature ... And tbh that wasn't that grea t. Will this make your system less stable? Probably. The OS will need some RAM - an d if you allocate 100% to VRAM, I predict you'll encounter a hard lockup, spinni ng Beachball, or just a system reset. So be careful to not get carried away. Eve n so, many will be able to get a few more gigs this way, enabling a slightly lar ger quant, longer context, or maybe even the next level up in parameter size. En joy! EDIT: if you have a 192gb m1/m2/m3 system, can you confirm whether this trick ca n be used to recover approx 40gb VRAM? A boost of 40gb is a pretty big deal IMO. Sort by: Best Comments Section farkinga OP � 2y ago One note on this ..."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "system, can you confirm whether this trick ca n be used to recover approx 40gb VRAM? A boost of 40gb is a pretty big deal IMO. Sort by: Best Comments Section farkinga OP � 2y ago One note on this ... All macos systems would be happiest to have at least 8gb av ailable for OS stuff. For a 32gb system, the math looks like this: 32gb-8gb=24gb. For me, I can gain 2 .2gb this way. Not bad! For those with 192gb - WOW. You go from having ~140gb VRAM to 184gb. That's a HU GE increase. As long as you keep the rest of your system utilization under contr ol, this trick just massively increased the utility of those high-end Metal syst ems. FlishFlashman � 2y ago I looked at what wired memory (memory that can't be swapped) was without having an LLM loaded/running and then added a margin to that. I ended up allocating 26. 5GB, up from 22.8GB default. It worked, but it didn't work great because I still had a bunch of other stuff r unning on my Mac, so (not surprisingly) swapping slowed it down. For anything mo re than a proof of concept test I'd be shutting all the unnecessary stuff down. u/fallingdowndizzyvr avatar fallingdowndizzyvr � 2y ago I ended up allocating 26.5GB, up from 22.8GB default. On my 32GB Mac, I allocate 30GB. It worked, but it didn't work great because I still had a bunch of other stuff r unning on my Mac, so (not surprisingly) swapping slowed it down. For anything mo re than a proof of concept test I'd be shutting all the unnecessary stuff down."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "se I still had a bunch of other stuff r unning on my Mac, so (not surprisingly) swapping slowed it down. For anything mo re than a proof of concept test I'd be shutting all the unnecessary stuff down. That's what I do and I have no swapping at all. I listed the two big things to t urn off to save RAM. Look for \"I also do these couple of things to save RAM.\" ab out halfway down the post. Thus I am able to run without any swapping at all wit h some headroom to spare. Max RAM usage is 31.02GB. https://www.reddit.com/r/LocalLLaMA/comments/18674zd/macs_with_32gb_of_memory_ca n_run_70b_models_with/ bebopkim1372 � 2y ago My M1 Max Mac Studio has 64GB of RAM. By running sudo sysctl iogpu.wired_limit_m b=57344, it did magic! ggml_metal_init: allocating ggml_metal_init: found device: Apple M1 Max ggml_metal_init: picking default device: Apple M1 Max ggml_metal_init: default.metallib not found, loading from source ggml_metal_init: loading '/Users/****/****/llama.cpp/ggml-metal.metal' ggml_metal_init: GPU name: Apple M1 Max ggml_metal_init: GPU family: MTLGPUFamilyApple7 (1007) ggml_metal_init: hasUnifiedMemory = true ggml_metal_init: recommendedMaxWorkingSetSize = 57344.00 MiB ggml_metal_init: maxTransferRate = built-in GPU Yay! farkinga OP � 2y ago Yeah! That's what I'm talking about."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "nit: hasUnifiedMemory = true ggml_metal_init: recommendedMaxWorkingSetSize = 57344.00 MiB ggml_metal_init: maxTransferRate = built-in GPU Yay! farkinga OP � 2y ago Yeah! That's what I'm talking about. Would you happen remember what it was repor ting before? If it's like the rest, I'm assuming it said something like 40 or 45 gb, right? bebopkim1372 � 2y ago It was 48GB and now I can use 12GB more! farkinga OP � 2y ago wow, this is wild. It's basically adding another GPU ... and that GPU is actuall y pretty good, great bus speeds... for free! CheatCodesOfLife � 2y ago � Edited 2y ago 64GB M1 Max here. Before running the command, if I tried to load up goliath-120b : (47536.00 / 49152.00) - fails And after sudo sysctl iogpu.wired_limit_mb=57344 : (47536.00 / 57344.00) So I guess the default is: 49152 u/fallingdowndizzyvr avatar fallingdowndizzyvr � 2y ago � Edited 2y ago 64GB M1 Max here. Before running the command, if I tried to load up goliath-120b : (47536.00 / 49152.00) - fails I wonder why that failed. Your limit is higher than the RAM needed. I run with a tighter gap and it loads and runs, (28738.98 / 30146.00). So I guess the default is: 49152 It is. To be more clear, llama.cpp tells you want the recommendedMaxWorkingSetSi ze is. Which should match that number. bebopkim1372 � 2y ago Maybe 47536MB is the net model size. For LLM inference, memory for context and o ptional context cache memory are also needed. u/fallingdowndizzyvr avatar fallingdowndizzyvr � 2y ago They are."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "2y ago Maybe 47536MB is the net model size. For LLM inference, memory for context and o ptional context cache memory are also needed. u/fallingdowndizzyvr avatar fallingdowndizzyvr � 2y ago They are. If you look at what llama.cpp prints out, it prints out all the buffer s that it's trying to allocate. And successively updates the ( X/Y ) it needs. W as the one you posted just the first one? The very last one before it exits out with an error will be the most informative one. That one should have an X that's bigger than Y. FlishFlashman � 2y ago �64GB allows 75% to be used by GPU. �32 its ~66%. Not sure about the 36GB mach ines. u/fallingdowndizzyvr avatar fallingdowndizzyvr � 2y ago As per the latest developments in that discussion, \"iogpu.wired_limit_mb\" only w orks on Sonoma. So if you are on an older version of Mac OS, try \"debug.iogpu.wi red_limit\" instead. CheatCodesOfLife � 2y ago That totally worked. I can run goliath 120b on my m1 max laptop now. Thanks a lo t. Zestyclose_Yak_3174 � 2y ago Which quant did you use and how was your experience? CheatCodesOfLife � 2y ago 46G goliath-120b.Q2_K So the smallest one I found (I didn't quantize this one myself, found it on HF s omewhere) And it was very slow. about 13t/s prompt_eval and then 2.5t/s generating text, s o only really useful for me when I need to run it on my laptop (I get like 15t/s with 120b model on my 2x3090 rig at 3bpw exl2) As for the models it's self, I like it a lot and use it frequently."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "t, s o only really useful for me when I need to run it on my laptop (I get like 15t/s with 120b model on my 2x3090 rig at 3bpw exl2) As for the models it's self, I like it a lot and use it frequently. TBH, this ram thing is more helpful for me because it lets me run Q5 70b models instead of just Q4 now. ArthurAardvark � 1y ago Oo. Then you'll like to see this. https://www.reddit.com/r/LocalLLaMA/comments/1al58xw/yet_another_state_of_the_ar t_in_llm_quantization/ And TY for aware-ing me to the fact that I can run 120B lol u/bladeolson26 avatar bladeolson26 � 1y ago u/farkinga Thanks for this post. I have an M2 Ultra with 192GB. I will give this a try and share the results. u/bladeolson26 avatar bladeolson26 � 1y ago My first test, I set using 64GB sudo sysctl iogpu.wired_limit_mb=65536 I loaded Dolphin Mixtral 8X 7B Q5 ( 34GB model ) I gave it my test prompt and it seems fast to me : time to first token: 1.99s gen t: 43.24s speed: 37.00 tok/s stop reason: completed gpu layers: 1 cpu threads: 22 mlock: false token count: 1661/1500 Next I tried 128GB sudo sysctl iogpu.wired_limit_mb=131072 I loaded Goliath 120b Q4 ( 70GB model) I gave it my test prompt and it slower to display time to first token: 3.88s gen t: 128.31s speed: 7.00 tok/s stop reason: completed gpu layers: 1 cpu threads: 20 mlock: false token count: 1072/1500 Third Test I tried 144GB ( leaving 48GB for OS operation 25%) sudo sysctl iogpu.wired_limit_mb=147456 as expected similar results. no crashes."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "rs: 1 cpu threads: 20 mlock: false token count: 1072/1500 Third Test I tried 144GB ( leaving 48GB for OS operation 25%) sudo sysctl iogpu.wired_limit_mb=147456 as expected similar results. no crashes. 188GB leaving just 8GB for the OS, etc.. It runs just fine. I did not have a model that big though. The Prompt I used : Write a Game of Pac-Man in Swift : the result from last Goliath at 188GB time to first token: 4.25s gen t: 167.94s speed: 7.00 tok/s stop reason: completed gpu layers: 1 cpu threads: 20 mlock: false token count: 1275/1500 import UIKit class GameView: UIView { var pacmanImageView: UIImageView! var dotViews: [DotView] = [] var ghostImages: [UIImage]? = nil var scoreLabel: UILabel! override init(frame: CGRect) { super.init(frame: frame) pacmanImageView = UIImageView(image: #imageLiteral(resourceName: \"Pacman \")) addSubview(pacmanImageView) scoreLabel = UILabel(frame: .zero) scoreLabel.textAlignment = .center scoreLabel.font = .systemFont(ofSize: 24, weight: .bold) scoreLabel.textColor = .white addSubview(scoreLabel)rangle(#imageLiteral(resourceName: \"Pacman\"))) setupGame() } private func setupGame() { for i in 0..<9 { let dotView = DotView(tag: i) addSubview(dotView) dotViews.append(dotView) if i % 3 == 0 && i != 8 { let spaceView = UIView() spaceView.frame.size.height = frame.height / 15 * 2 addSubview(spaceView) } } scoreLabel.text = \"Score: 0\" } private func movePacman(_ direction: Direction) { var newFrame = pacmanImageView.frame switch direction { case ."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "height = frame.height / 15 * 2 addSubview(spaceView) } } scoreLabel.text = \"Score: 0\" } private func movePacman(_ direction: Direction) { var newFrame = pacmanImageView.frame switch direction { case .up: if newFrame.minY > frame.origin.y { newFrame.origin.y -= newFrame.height / 2 } case .down: if newFrame.maxY < frame.size.height - frame.origin.y - newFrame.hei ght / 2 { newFrame.origin.y += newFrame.height / 2 } case .left: if newFrame.minX > frame.origin.x { newFrame.origin.x -= newFrame.width / 2 } case .right: if newFrame.maxX < frame.size.width - frame.origin.x - newBoardView. frame.width / 2 { newFrame.origin.x += newBoardView.frame.width / 2 } } pacmanImageView.frame = newFrame } func gameLogic() { // Implement your game logic here: // - Detect collisions with dots and ghosts // - Update score // - Move Pac-Man and ghosts // - Generate new dots } } class DotView: UIView { var isEaten = false override init(frame: CGRect) { super.init(frame: frame) backgroundColor = .systemGreen layer.cornerRadius = 10 isUserInteractionEnabled = true let tapGesture = UITapGestureRecognizer(target: self, action: #selector( eatDot)) addGestureRecognizer(tapGesture) } @objc func eatDot() { if !isEaten { isEaten = true backgroundColor = .systemOrange // Decrease score and update label // Check for game over conditions } } required init?(coder: NSCoder) { super."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "e) } @objc func eatDot() { if !isEaten { isEaten = true backgroundColor = .systemOrange // Decrease score and update label // Check for game over conditions } } required init?(coder: NSCoder) { super.init(coder: coder) } } enum Direction { case up, down, left, right } farkinga OP � 1y ago Omg, I am legit excited it ran with just 8gb reserved for os. That's so much ext ra VRAM - for free! Thanks for trying it at different levels. I doubt it will be seen here; consider posting as a new thread. krishnakaasyap � 1y ago This is awesome, fellow Redditor! But what would be the stats if you used all th e GPU layers and NPU cores? Would it improve the time to first token and tokens per second?I would love to learn more about the M2 Ultra 192GB Mac Studio as a s erver for inferencing large language models (LLMs). Where can I find more inform ative stuff, like your comment? u/kkb294 avatar kkb294 � 2mo ago I increased the VRAM allocation to 40GB from the default 36GB. Thanks for the po st �.! u/hakyim avatar hakyim � 16d ago how do you get that system resources information? u/kkb294 avatar kkb294 � 16d ago It is available in LM Studio UI. u/Zugzwang_CYOA avatar Zugzwang_CYOA � 2y ago � Edited 2y ago How is the prompt processing time on a mac? If I were to work with a prompt that is 8k in size for RP, with big frequent changes in the prompt, would it be able to read my ever-changing prompt in a timely manner and respond? I would like to use Sillytavern as my front end, and that can resul"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ze for RP, with big frequent changes in the prompt, would it be able to read my ever-changing prompt in a timely manner and respond? I would like to use Sillytavern as my front end, and that can result in big prom pt changes between replies. bebopkim1372 � 2y ago For M1, when prompt evaluations occur, BLAS operation is used and the speed is t errible. I also have a PC with 4060 Ti 16GB, and cuBLAS is the speed of light co mpared with BLAS speed on my M1 Max. BLAS speeds under 30B modles are acceptable , but more than 30B, it is really slow. u/Zugzwang_CYOA avatar Zugzwang_CYOA � 2y ago Good to know. It sounds like macs are great at asking simple questions of powerf ul LLMs, but not so great at roleplaying with large context stories. I had hoped that an M2 Max would be viable for RP at 70b or 120b, but I guess not. bebopkim1372 � 2y ago I am using koboldcpp and it caches the prompt evaulation result, so if your prom pt change actions add new content at the end of previous prompt, it will be okay because koboldcpp performs prompt evaluation only for new added content though it is still slow for 30B or bigger size models. If your prompt change is amendin g in the middle of context, many parts of the cache will be useless and there wi ll be more prompt evaluation needed, so it will be very slow. u/Zugzwang_CYOA avatar Zugzwang_CYOA � 2y ago � Edited 2y ago The way I use Sillytavern for roleplaying involves a lot of world entry informat ion."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "e prompt evaluation needed, so it will be very slow. u/Zugzwang_CYOA avatar Zugzwang_CYOA � 2y ago � Edited 2y ago The way I use Sillytavern for roleplaying involves a lot of world entry informat ion. World entries are inserted into the prompt when they are triggered through key words, and I use many such entries for world building. Those same world entr ies disappear from the prompt when they are not being talked about. I also somet imes run group chats with multiple characters. In such cases, the entire charact er card of the previous character would disappear from the prompt, and a new cha racter card would appear in its place when the next character speaks. That's why my prompts tend to be ever-changing. So, unless the cache keeps information from previous prompts, it sounds like I w ould be continuously re-evaluating with every response. I suppose it would be different if it did store information from previous prompt s, as that would let me swap between speaking characters or trigger a previously used world entry without having to re-evaluate every time. But with my current 12gb 3060, quantized 13b models interface so quickly that I never even bothered to note prompt evaluation time, even with 6-8k context, and it sounds like the M2 max studio with 96gb won't be able to allow for that kind of thing at 70b as I originally hoped. Thank you for your responses. They have been helpful."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "even with 6-8k context, and it sounds like the M2 max studio with 96gb won't be able to allow for that kind of thing at 70b as I originally hoped. Thank you for your responses. They have been helpful. bebopkim1372 � 2y ago For heavy RP users like you, I think used multiple 3090s will be best for very l arge LLMs. u/guymadison42 avatar guymadison42 � 8mo ago I am compiling LLVM with a 32 GB system, wired memory is at roughly 8 GB. That's 8 GB my system cannot reach, this has always been an issue with Metal since 201 2. I am really surprised it's never been fixed. u/Fun_Huckleberry_7781 avatar Fun_Huckleberry_7781 � 10mo ago How can I check if the changed worked? I did the intial code and said it was ini tially set to 0 farkinga OP � 10mo ago One way I've verified is through the llama.cpp diagnostic output. It reports the available vram as well as the size of the model and how much vram it requires. I've got 32gb total and I think the default availability is approx 22gb. So I ca n easily increase to 26gb and I see the difference immediately when I launch lla ma.cpp - the available vram will be reported as 26gb. Gold_Bee2694 � 2mo ago I got a MacBook Pro with a M4 pro 14 Core CPU and 20 Core GPU and 24Gb of RAM an d I want to run some coding models in lm studio so I'm wondering if its a good i dea to change the VRAM from 16gb to 18 or maybe 20gb."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "Pro with a M4 pro 14 Core CPU and 20 Core GPU and 24Gb of RAM an d I want to run some coding models in lm studio so I'm wondering if its a good i dea to change the VRAM from 16gb to 18 or maybe 20gb. farkinga OP � 2mo ago 24gb RAM isn't a typical configuration for apple hardware but it is a plausible VRAM allocation for a 32gb RAM system. Check again; you might have 32gb ram. On a 32gb M1 setup, I've allocated up to 26gb to VRAM and used that to run LLMs - but 24gb is even safer. Gold_Bee2694 � 2mo ago I don�t know why they changed it to 24gb of ram but it�s a thing now: https://ww w.apple.com/de/shop/buy-mac/macbook-pro/16%22-space-schwarz-standard-display-app le-m4-pro-mit-14-core-cpu-und-20-core-gpu-24-gb-arbeitsspeicher-512gb# farkinga OP � 2mo ago Hmmm... fair enough - thanks for the link. Well, then it's tight but two things that will help are: make sure nothing else is running; close everything in the dock and menubar; the re are guides that explain how to do this consider llama.cpp instead of LM Studio to reduce RAM requirements a bit (still need to run terminal.app but this is pretty light) I bet you could run your system on 4gb if you just ran terminal and llama.cpp. T hat means you could try allocating 20gb to VRAM. And you might as well try LMStu dio first since it's easier. Hey, worst that happens is you reboot; this is not a permanent change and it aut omatically resets on reboot."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "try allocating 20gb to VRAM. And you might as well try LMStu dio first since it's easier. Hey, worst that happens is you reboot; this is not a permanent change and it aut omatically resets on reboot. u/shahmeer5 avatar shahmeer5 � 1mo ago does it affect in Gaming performance using Crossover or native ones? PS: to get the current value use: sudo sysctl iogpu Jelegend � 2y ago I am getting the following error on running this command on Mac Studio M2 Max 64 GB RAM sysctl: unknown oid 'iogpu.wired_limit_mb' Can soeome help me out here on what to do here? u/LoSboccacc avatar LoSboccacc � 2y ago Profile Badge for the Achievement Top 1% Poster Top 1% Poster Older os use debug.iogpu.wired_limit bebopkim1372 � 2y ago Do you use macOS Sonoma? Mine is Sonoma 14.1.1 - Darwin Kernel Version 23.1.0: M on Oct 9 21:27:24 PDT 2023; root:xnu-10002.41.9~6/RELEASE_ARM64_T6000 arm64. spiffco7 � 2y ago This is lifesaving news. Thank you. + https://github.com/ggml-org/llama.cpp/discussions/2182#discussioncommen t-7698315 https://github.com/ggml-org/llama.cpp/discussions/2182#discussioncomment-7698315 Skip to content Navigation Menu ggml-org llama.cpp Type / to search Code Issues 320 Pull requests 462 Discussions Actions Projects 1 Wiki Security 5 Insights Adjust VRAM/RAM split on Apple Silicon #2182 dr3murr started this conversation in General Adjust VRAM/RAM split on Apple Silicon #2182 @dr3murr dr3murr on Jul 11, 2023 · 15 comments · 31 replies Return to top dr3murr on Jul 11, 2023 // this tool"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "r3murr started this conversation in General Adjust VRAM/RAM split on Apple Silicon #2182 @dr3murr dr3murr on Jul 11, 2023 · 15 comments · 31 replies Return to top dr3murr on Jul 11, 2023 // this tool allows you to change the VRAM/RAM split on Unified Memory on Apple Silicon to whatever you want, allowing for more VRAM for inference // c++ -std=c++17 -framework CoreFoundation -o vram_patcher vram_patcher.cpp // credits to @asentientbot for helping me with some stuff because I never owned a Mac before // please read the code, if you don't understand what it does don't use it // tested on macos ventura beta 3, pattern might be different on other versions // usage: ./vram_patcher <desired percentage of VRAM> #include <iostream> #include <fstream> #include <string> #include <vector> #include <sys/sysctl.h> #include <unistd.h> #include <sys/stat.h> #include <sys/mount.h> #include <CoreFoundation/CoreFoundation.h> // A helper function to get the sysctl value for a given name std::string get_sysctl(const std::string& name) { size_t len = 0; // Get the size of the value if (sysctlbyname(name.c_str(), nullptr, &len, nullptr, 0) == -1) { std::cerr << \"Failed to get sysctl size for \" << name << std::endl; return \"\"; } // Allocate a buffer for the value char* buf = new char[len]; // Get the value if (sysctlbyname(name."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "lptr, 0) == -1) { std::cerr << \"Failed to get sysctl size for \" << name << std::endl; return \"\"; } // Allocate a buffer for the value char* buf = new char[len]; // Get the value if (sysctlbyname(name.c_str(), buf, &len, nullptr, 0) == -1) { std::cerr << \"Failed to get sysctl value for \" << name << std::endl; delete[] buf; return \"\"; } // Convert the value to a string std::string value(buf, len); delete[] buf; return value; } // A helper function to convert a CFString to a std::string std::string CFStringToStdString(CFStringRef cfstr) { if (cfstr == nullptr) return \"\"; CFIndex length = CFStringGetLength(cfstr); CFIndex maxSize = CFStringGetMaximumSizeForEncoding(length, kCFStringEncodingU TF8) + 1; char* buffer = new char[maxSize]; if (CFStringGetCString(cfstr, buffer, maxSize, kCFStringEncodingUTF8)) { std::string result(buffer); delete[] buffer; return result; } else { delete[] buffer; return \"\"; } } // A function to get the name of a disk from a path std::string get_volume_name(const std::string& path) { // Create a CFURL from the path CFURLRef url = CFURLCreateFromFileSystemRepresentation(nullptr, (const UInt8*) path.c_str(), path.length(), false); if (url == nullptr) return \"\"; // Get the volume URL from the path URL CFURLRef volumeURL = CFURLCreateCopyDeletingLastPathComponent(nullptr, url); CFRelease(url); if (volumeURL == nullptr) return \"\"; // Get the volume name from the volume URL CFStringRef volumeName = nullptr; if (CFURLCopyResourcePropertyForKey(volumeURL, kCFUR"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "nt(nullptr, url); CFRelease(url); if (volumeURL == nullptr) return \"\"; // Get the volume name from the volume URL CFStringRef volumeName = nullptr; if (CFURLCopyResourcePropertyForKey(volumeURL, kCFURLVolumeNameKey, &volumeNam e, nullptr)) { CFRelease(volumeURL); if (volumeName == nullptr) return \"\"; // Convert the volume name to a std::string std::string result = CFStringToStdString(volumeName); CFRelease(volumeName); return result; } else { CFRelease(volumeURL); return \"\"; } } void change_float_constant(std::vector<uint8_t>& code, float new_value) { // Check that the code vector has enough bytes for the two instructions if (code.size() < 8) { throw std::invalid_argument(\"code vector is too small\"); } // Convert the new float value to a 32-bit unsigned integer representation uint32_t new_bits = *reinterpret_cast<uint32_t*>(&new_value); // Extract the lower and upper 16 bits of the new value uint16_t low_bits = new_bits & 0xffff; uint16_t high_bits = new_bits >> 16; // Encode the new value as two mov instructions // The first instruction is movz w8, #low_bits // The second instruction is movk w8, #high_bits, lsl #16 // The opcode format is: // | 31 30 29 28 | 27 26 25 24 | 23 22 21 20 | 19 18 17 16 | 15 14 13 12 | 11 10 9 8 | 7 6 5 4 | 3 2 1 0 | // | sf 0 0 1 | opc 1 0 0 | 0 0 0 0 | hw | imm16 | 0 0 0 0 | Rd | 0 0 0 0 | // where sf = 0 for 32-bit register, opc = 00 for movz, 01 for movn, 10 for mo vk, hw = 00 for lsl #0, 01 for lsl #16, 10 for lsl #32, 11 for lsl #48, imm16 ="
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "0 | hw | imm16 | 0 0 0 0 | Rd | 0 0 0 0 | // where sf = 0 for 32-bit register, opc = 00 for movz, 01 for movn, 10 for mo vk, hw = 00 for lsl #0, 01 for lsl #16, 10 for lsl #32, 11 for lsl #48, imm16 = 16-bit immediate value, Rd = destination register // The first instruction has sf = 0, opc = 00, hw = 00, imm16 = low_bits, Rd = 8 uint32_t first_opcode = 0x52800000 | (low_bits << 5) | 8; // The second instruction has sf = 0, opc = 10, hw = 01, imm16 = high_bits, Rd = 8 uint32_t second_opcode = 0x72800000 | (high_bits << 5) | (1 << 21) | 8; // Convert the opcodes to little endian bytes and overwrite the code vector code[0] = first_opcode & 0xff; code[1] = (first_opcode >> 8) & 0xff; code[2] = (first_opcode >> 16) & 0xff; code[3] = (first_opcode >> 24) & 0xff; code[4] = second_opcode & 0xff; code[5] = (second_opcode >> 8) & 0xff; code[6] = (second_opcode >> 16) & 0xff; code[7] = (second_opcode >> 24) & 0xff; } int main(int argc, char** argv) { // Check if the program is run as root if (getuid() != 0) { std::cerr << \"Sorry, this program must be run as root\" << std::endl; return 1; } // Check if the program has one argument if (argc != 2) { std::cerr << \"Usage: \" << argv[0] << \" vram_percentage\" << std::endl; return 1; } // Check if the argument is a valid percentage int percentage = std::stoi(argv[1]); if (percentage < 10 || percentage > 95) { std::cerr << \"Invalid percentage: \" << percentage << std::endl; return 1; } // Check if the CPU is Apple Silicon std::string cpu_brand = g"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "std::stoi(argv[1]); if (percentage < 10 || percentage > 95) { std::cerr << \"Invalid percentage: \" << percentage << std::endl; return 1; } // Check if the CPU is Apple Silicon std::string cpu_brand = get_sysctl(\"machdep.cpu.brand_string\"); if (cpu_brand.find(\"Apple\") == std::string::npos) { std::cerr << \"Sorry, this program only works on Apple Silicon\" << std::e ndl; return 1; } // Define the paths for the original and patched kernel collections std::string original_kc = \"/private/var/db/KernelExtensionManagement/KernelC ollections/BootKernelCollection.kc\"; std::string patched_kc = \"/tmp/BootKernelCollection.kc\"; std::string final_kc = \"/Library/KernelCollections/vram_patch.kc\"; std::string script_kc = \"/Library/KernelCollections/complete_patch.sh\"; // Copy the original kernel collection to a temporary directory std::ifstream src(original_kc, std::ios::binary); std::ofstream dst(patched_kc, std::ios::binary); if (!src || !dst) { std::cerr << \"Failed to copy the kernel collection\" << std::endl; return 1; } dst << src.rdbuf(); src.close(); dst.close(); // Read the patched kernel collection into a vector of bytes std::ifstream in(patched_kc, std::ios::binary); if (!in) { std::cerr << \"Failed to read the patched kernel collection\" << std::endl ; return 1; } std::vector<uint8_t> data((std::istreambuf_iterator<char>(in)), std::istream buf_iterator<char>()); in."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "f (!in) { std::cerr << \"Failed to read the patched kernel collection\" << std::endl ; return 1; } std::vector<uint8_t> data((std::istreambuf_iterator<char>(in)), std::istream buf_iterator<char>()); in.close(); // Define the byte sequence to search for std::vector<uint8_t> target = {0x08, 0x01, 0xC0, 0xD2, 0x3F, 0x00, 0x08, 0xE B, 0xA8, 0xAA, 0x8A, 0x52, 0xA8, 0x40, 0xA8, 0x72, 0x00, 0x01, 0x27, 0x1E, 0x01, 0x30, 0x27, 0x1E, 0x28, 0x8C, 0x20, 0x1E}; // Define the byte sequence to replace with std::vector<uint8_t> replacement = {0x68, 0x73, 0x89, 0x52, 0xA8, 0x94, 0xA8 , 0x72, 0x08, 0x01, 0x27, 0x1E, 0x1F, 0x20, 0x03, 0xD5, 0x1F, 0x20, 0x03, 0xD5, 0x1F, 0x20, 0x03, 0xD5, 0x1F, 0x20, 0x03, 0xD5}; // Subtract the percentage from 100 percentage = 100 - percentage; // Change the floating point constant in the replacement byte sequence change_float_constant(replacement, percentage); // Find the first occurrence of the target byte sequence in the data vector auto it = std::search(data.begin(), data.end(), target.begin(), target.end() ); // If the target byte sequence is found, replace it with the replacement byt e sequence if (it != data.end()) { std::copy(replacement.begin(), replacement.end(), it); } else { std::cerr << \"Failed to find the target byte sequence in the kernel coll ection\" << std::endl; return 1; } // Write the modified data vector to the patched kernel collection std::ofstream out(patched_kc, std::ios::binary); if (!out) { std::cerr << \"Failed to write the patched ker"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "<< std::endl; return 1; } // Write the modified data vector to the patched kernel collection std::ofstream out(patched_kc, std::ios::binary); if (!out) { std::cerr << \"Failed to write the patched kernel collection\" << std::end l; return 1; } out.write(reinterpret_cast<char*>(data.data()), data.size()); out.close(); // Copy the patched kernel collection to the final destination std::ifstream src2(patched_kc, std::ios::binary); std::ofstream dst2(final_kc, std::ios::binary); if (!src2 || !dst2) { std::cerr << \"Failed to copy the patched kernel collection; please disab le SIP and try again\" << std::endl; return 1; } dst2 << src2.rdbuf(); src2.close(); dst2.close(); // Change the ownership of the final kernel collection to root:wheel if (chown(final_kc.c_str(), 0, 0) == -1) { std::cerr << \"Failed to change the ownership of the final kernel collect ion\" << std::endl; return 1; } // Get the volume name for the /System path std::string volume_name = get_volume_name(\"/System\"); if (volume_name.empty()) { std::cerr << \"Failed to get the volume name for /System\" << std::endl; return 1; } // Create a shell script to configure the boot with the final kernel collect ion std::ofstream script(script_kc); if (!script) { std::cerr << \"Failed to create the shell script\" << std::endl; return 1; } script << \"#!/bin/bash\\n\"; script << \"# Disable System Integrity Protection\\n\"; script << \"csrutil disable\\n\"; script << \"# Disable Apple Mobile File Integrity\\n\"; script << \"nvram boot-args=\\\"ipc_con"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "script << \"#!/bin/bash\\n\"; script << \"# Disable System Integrity Protection\\n\"; script << \"csrutil disable\\n\"; script << \"# Disable Apple Mobile File Integrity\\n\"; script << \"nvram boot-args=\\\"ipc_control_port_options=0 amfi_get_out_of_my_w ay=1\\\"\\n\"; script << \"# Get the absolute path of this script\\n\"; script << \"SCRIPT=$(cd \\\"$(dirname \\\"${BASH_SOURCE[0]}\\\")\\\" && pwd)/$(basena me \\\"${BASH_SOURCE[0]}\\\")\\n\"; script << \"# Append the kernel collection name to the script path\\n\"; script << \"KC=\\\"${SCRIPT%/*}/vram_patch.kc\\\"\\n\"; // This line replaces the o riginal one script << \"# Get the volume name from the script path\\n\"; script << \"VOLUME=\\\"${SCRIPT%%/Library*}\\\"\\n\"; script << \"# Run the kmutil command with the absolute paths\\n\"; script << \"kmutil configure-boot -C -c \\\"$KC\\\" -v \\\"$VOLUME\\\"\\n\"; script << \"sync\\n\"; script << \"echo Finished. You may reboot your system now.\\n\"; script.close(); // Change the ownership and permissions of the shell script to root:wheel an d 755 if (chown(script_kc.c_str(), 0, 0) == -1) { std::cerr << \"Failed to change the ownership of the shell script\" << std ::endl; return 1; } if (chmod(script_kc.c_str(), 0755) == -1) { std::cerr << \"Failed to change the permissions of the shell script\" << s td::endl; return 1; } // Output the instructions to run the shell script in RecoveryOS std::cout << \"The shell script to configure the boot with the patched kernel collection has been created at \" << script_kc << std::endl; std::cout << \"Now reboot to Recove"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "e shell script in RecoveryOS std::cout << \"The shell script to configure the boot with the patched kernel collection has been created at \" << script_kc << std::endl; std::cout << \"Now reboot to RecoveryOS and run: \\\"/Volumes/\" << volume_name << \"/Library/KernelCollections/complete_patch.sh\\\"\" << std::endl; std::cout << \"If your boot fails after running the script or after an update , you need to go to Utilities->Startup Security Tool in RecoveryOS and pick Full Security\" << std::endl; return 0; } Replies:15 comments · 31 replies dr3murr on Jul 12, 2023 Author image if you see this screen (after an update or otherwise), click \"Recovery\", sign in , go to Utilities->Startup Security Tool, and click \"Reduced Security\" then open terminal and run \"csrutil disable\" and reboot then you can patch again 0 replies JasonOSX on Jul 13, 2023 Is this supposed to fix the shortcoming that only 2/3 or 3/4 of the unified memo ry can be used for Metal inference? If so, I am very eager to test it out and wo uld like to thank you for your contribution! I am wondering if \"reduced security \" is only temporary. Can you enable full security after the patch? Did you also ping @ggerganov - I guess he will also be interested in this concept 2 replies @ggerganov ggerganov on Jul 18, 2023 Maintainer Thanks - interesting approach. I'll wait on some more reports as I don't have a spare machine to test on."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "l also be interested in this concept 2 replies @ggerganov ggerganov on Jul 18, 2023 Maintainer Thanks - interesting approach. I'll wait on some more reports as I don't have a spare machine to test on. Would be great if this solves the memory limit issue @dr3murr dr3murr on Jul 22, 2023 Author it def does on my machine dr3murr on Jul 13, 2023 Author Yes, this fixes that. You need relaxed security or else it will boot the unpatch ed kernel. This is why changing the security level undoes the patch. 0 replies JasonOSX on Jul 13, 2023 Yes, this fixes that. You need relaxed security or else it will boot the unpatch ed kernel. This is why changing the security level undoes the patch. Thanks for your work! I understand that it will not boot a custom kernel with full protection, compara ble to a locked (secure) bootloader on a phone. However I would advice against people disabling or loosening their security unle ss they understand all the risks associated. It can open a path to sophisticated and persistent threats, especially when working in AI research as some of us ar e. More info: https://support.apple.com/guide/security/startup-disk-security-policy -control-sec7d92dc49f/web I know someone at the Apple software engineering team. I will try to ask him if they can consider upping the limit a bit or if there is a way to use a similar a pproach without sacrificing on security."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "know someone at the Apple software engineering team. I will try to ask him if they can consider upping the limit a bit or if there is a way to use a similar a pproach without sacrificing on security. (I guess it will be difficult but I can at least give it a shot) 1 reply @dr3murr dr3murr on Jul 13, 2023 Author vm_size_t AGXAccelerator::calcMaxGPUPhysicalMemoryBytes(uint64_t unifiedMemoryBy tes) { float reservePercent = 33.333f; // Use a lower percentage if the unified memory is larger than 32 GB if (unifiedMemoryBytes > 0x800000000LL) { reservePercent = 25.0f; } const OSMetaClassBase *prop = this->device->getProperty(\"gpu-sysmem-reserve-pe rcent\"); if (prop) { OSData *data = OSDynamicCast(OSData, prop); if (data) { reservePercent = *(float *)data->getBytesNoCopy(); } } return (vm_size_t)((unifiedMemoryBytes * (100.0f - reservePercent) / 100.0f + page_size - 1) & -page_size); } that would be great the code already has a case to check for an override %, but I can find no way to set gpu-sysmem-reserve-percent without a patch JasonOSX on Jul 13, 2023 vm_size_t AGXAccelerator::calcMaxGPUPhysicalMemoryBytes(uint64_t unifiedMemoryBy tes) { float reservePercent = 33.333f; // Use a lower percentage if the unified memory is larger than 32 GB if (unifiedMemoryBytes > 0x800000000LL) { reservePercent = 25."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ytes(uint64_t unifiedMemoryBy tes) { float reservePercent = 33.333f; // Use a lower percentage if the unified memory is larger than 32 GB if (unifiedMemoryBytes > 0x800000000LL) { reservePercent = 25.0f; } const OSMetaClassBase *prop = this->device->getProperty(\"gpu-sysmem-reserve-pe rcent\"); if (prop) { OSData *data = OSDynamicCast(OSData, prop); if (data) { reservePercent = *(float *)data->getBytesNoCopy(); } } return (vm_size_t)((unifiedMemoryBytes * (100.0f - reservePercent) / 100.0f + page_size - 1) & -page_size); } that would be great the code already has a case to check for an override %, but I can find no way to set gpu-sysmem-reserve-percent without a patch That's an interesting find! but with your current approach we need both disabled SIP (protects the entire system by preventing the execution of unauthorized cod e) AND reduced security boot policy? what happens if you re-enable SIP but keep reduced security boot policy? 0 replies dr3murr on Jul 13, 2023 Author not quite sure, you need sip disabled to write to /library/kernelcollections or at least you need it to delete from it, which is necessary if you patch again maybe you could avoid it by writing to another path that's not sip protected but idk where the physical file on disk isnt the actual kernel that gets booted from fwiw beca use you can rm or corrupt the entire file you set-boot-object'd and it will stil l boot 0 replies WiseFarAI on Jul 15, 2023 This is great progress! I agree that permanently disabling"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "d from fwiw beca use you can rm or corrupt the entire file you set-boot-object'd and it will stil l boot 0 replies WiseFarAI on Jul 15, 2023 This is great progress! I agree that permanently disabling SIP and Full security is a risk some might not want to take too lightly. I think it would be good if some more eyes could look at this. I will also ponder about another way of imple menting this change // this tool allows you to change the VRAM/RAM split on Unified Memory on Apple Silicon to whatever you want, allowing for more VRAM for inference // c++ -std=c++17 -framework CoreFoundation -o vram_patcher vram_patcher.cpp // credits to @asentientbot for helping me with some stuff because I never owned a Mac before // please read the code, if you don't understand what it does don't use it // tested on macos ventura beta 3, pattern might be different on other versions // usage: ./vram_patcher <desired percentage of VRAM> #include <iostream> #include <fstream> #include <string> #include <vector> #include <sys/sysctl.h> #include <unistd.h> #include <sys/stat.h> #include <sys/mount.h> #include <CoreFoundation/CoreFoundation.h> // A helper function to get the sysctl value for a given name std::string get_sysctl(const std::string& name) { size_t len = 0; // Get the size of the value if (sysctlbyname(name."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "on/CoreFoundation.h> // A helper function to get the sysctl value for a given name std::string get_sysctl(const std::string& name) { size_t len = 0; // Get the size of the value if (sysctlbyname(name.c_str(), nullptr, &len, nullptr, 0) == -1) { std::cerr << \"Failed to get sysctl size for \" << name << std::endl; return \"\"; } // Allocate a buffer for the value char* buf = new char[len]; // Get the value if (sysctlbyname(name.c_str(), buf, &len, nullptr, 0) == -1) { std::cerr << \"Failed to get sysctl value for \" << name << std::endl; delete[] buf; return \"\"; } // Convert the value to a string std::string value(buf, len); delete[] buf; return value; } // A helper function to convert a CFString to a std::string std::string CFStringToStdString(CFStringRef cfstr) { if (cfstr == nullptr) return \"\"; CFIndex length = CFStringGetLength(cfstr); CFIndex maxSize = CFStringGetMaximumSizeForEncoding(length, kCFStringEncodingU TF8) + 1; char* buffer = new char[maxSize]; if (CFStringGetCString(cfstr, buffer, maxSize, kCFStringEncodingUTF8)) { std::string result(buffer); delete[] buffer; return result; } else { delete[] buffer; return \"\"; } } // A function to get the name of a disk from a path std::string get_volume_name(const std::string& path) { // Create a CFURL from the path CFURLRef url = CFURLCreateFromFileSystemRepresentation(nullptr, (const UInt8*) path.c_str(), path."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "sk from a path std::string get_volume_name(const std::string& path) { // Create a CFURL from the path CFURLRef url = CFURLCreateFromFileSystemRepresentation(nullptr, (const UInt8*) path.c_str(), path.length(), false); if (url == nullptr) return \"\"; // Get the volume URL from the path URL CFURLRef volumeURL = CFURLCreateCopyDeletingLastPathComponent(nullptr, url); CFRelease(url); if (volumeURL == nullptr) return \"\"; // Get the volume name from the volume URL CFStringRef volumeName = nullptr; if (CFURLCopyResourcePropertyForKey(volumeURL, kCFURLVolumeNameKey, &volumeNam e, nullptr)) { CFRelease(volumeURL); if (volumeName == nullptr) return \"\"; // Convert the volume name to a std::string std::string result = CFStringToStdString(volumeName); CFRelease(volumeName); return result; } else { CFRelease(volumeURL); return \"\"; } } void change_float_constant(std::vector<uint8_t>& code, float new_value) { // Check that the code vector has enough bytes for the two instructions if (code.size() < 8) { throw std::invalid_argument(\"code vector is too small\"); } // Convert the new float value to a 32-bit unsigned integer representation uint32_t new_bits = *reinterpret_cast<uint32_t*>(&new_value); // Extract the lower and upper 16 bits of the new value uint16_t low_bits = new_bits & 0xffff; uint16_t high_bits = new_bits >> 16; // Encode the new value as two mov instructions // The first instruction is movz w8, #low_bits // The second instruction is movk w8, #high_bits, lsl #16 // The opcode form"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "high_bits = new_bits >> 16; // Encode the new value as two mov instructions // The first instruction is movz w8, #low_bits // The second instruction is movk w8, #high_bits, lsl #16 // The opcode format is: // | 31 30 29 28 | 27 26 25 24 | 23 22 21 20 | 19 18 17 16 | 15 14 13 12 | 11 10 9 8 | 7 6 5 4 | 3 2 1 0 | // | sf 0 0 1 | opc 1 0 0 | 0 0 0 0 | hw | imm16 | 0 0 0 0 | Rd | 0 0 0 0 | // where sf = 0 for 32-bit register, opc = 00 for movz, 01 for movn, 10 for mo vk, hw = 00 for lsl #0, 01 for lsl #16, 10 for lsl #32, 11 for lsl #48, imm16 = 16-bit immediate value, Rd = destination register // The first instruction has sf = 0, opc = 00, hw = 00, imm16 = low_bits, Rd = 8 uint32_t first_opcode = 0x52800000 | (low_bits << 5) | 8; // The second instruction has sf = 0, opc = 10, hw = 01, imm16 = high_bits, Rd = 8 uint32_t second_opcode = 0x72800000 | (high_bits << 5) | (1 << 21) | 8; // Convert the opcodes to little endian bytes and overwrite the code vector code[0] = first_opcode & 0xff; code[1] = (first_opcode >> 8) & 0xff; code[2] = (first_opcode >> 16) & 0xff; code[3] = (first_opcode >> 24) & 0xff; code[4] = second_opcode & 0xff; code[5] = (second_opcode >> 8) & 0xff; code[6] = (second_opcode >> 16) & 0xff; code[7] = (second_opcode >> 24) & 0xff; } int main(int argc, char** argv) { // Check if the program is run as root if (getuid() != 0) { std::cerr << \"Sorry, this program must be run as root\" << std::endl; return 1; } // Check if the program has one argument if (argc != 2)"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "{ // Check if the program is run as root if (getuid() != 0) { std::cerr << \"Sorry, this program must be run as root\" << std::endl; return 1; } // Check if the program has one argument if (argc != 2) { std::cerr << \"Usage: \" << argv[0] << \" vram_percentage\" << std::endl; return 1; } // Check if the argument is a valid percentage int percentage = std::stoi(argv[1]); if (percentage < 10 || percentage > 95) { std::cerr << \"Invalid percentage: \" << percentage << std::endl; return 1; } // Check if the CPU is Apple Silicon std::string cpu_brand = get_sysctl(\"machdep.cpu.brand_string\"); if (cpu_brand.find(\"Apple\") == std::string::npos) { std::cerr << \"Sorry, this program only works on Apple Silicon\" << std::e ndl; return 1; } // Define the paths for the original and patched kernel collections std::string original_kc = \"/private/var/db/KernelExtensionManagement/KernelC ollections/BootKernelCollection.kc\"; std::string patched_kc = \"/tmp/BootKernelCollection.kc\"; std::string final_kc = \"/Library/KernelCollections/vram_patch.kc\"; std::string script_kc = \"/Library/KernelCollections/complete_patch.sh\"; // Copy the original kernel collection to a temporary directory std::ifstream src(original_kc, std::ios::binary); std::ofstream dst(patched_kc, std::ios::binary); if (!src || !dst) { std::cerr << \"Failed to copy the kernel collection\" << std::endl; return 1; } dst << src.rdbuf(); src.close(); dst."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "::ios::binary); std::ofstream dst(patched_kc, std::ios::binary); if (!src || !dst) { std::cerr << \"Failed to copy the kernel collection\" << std::endl; return 1; } dst << src.rdbuf(); src.close(); dst.close(); // Read the patched kernel collection into a vector of bytes std::ifstream in(patched_kc, std::ios::binary); if (!in) { std::cerr << \"Failed to read the patched kernel collection\" << std::endl ; return 1; } std::vector<uint8_t> data((std::istreambuf_iterator<char>(in)), std::istream buf_iterator<char>()); in.close(); // Define the byte sequence to search for std::vector<uint8_t> target = {0x08, 0x01, 0xC0, 0xD2, 0x3F, 0x00, 0x08, 0xE B, 0xA8, 0xAA, 0x8A, 0x52, 0xA8, 0x40, 0xA8, 0x72, 0x00, 0x01, 0x27, 0x1E, 0x01, 0x30, 0x27, 0x1E, 0x28, 0x8C, 0x20, 0x1E}; // Define the byte sequence to replace with std::vector<uint8_t> replacement = {0x68, 0x73, 0x89, 0x52, 0xA8, 0x94, 0xA8 , 0x72, 0x08, 0x01, 0x27, 0x1E, 0x1F, 0x20, 0x03, 0xD5, 0x1F, 0x20, 0x03, 0xD5, 0x1F, 0x20, 0x03, 0xD5, 0x1F, 0x20, 0x03, 0xD5}; // Subtract the percentage from 100 percentage = 100 - percentage; // Change the floating point constant in the replacement byte sequence change_float_constant(replacement, percentage); // Find the first occurrence of the target byte sequence in the data vector auto it = std::search(data.begin(), data.end(), target.begin(), target.end() ); // If the target byte sequence is found, replace it with the replacement byt e sequence if (it != data.end()) { std::copy(replacement."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": ":search(data.begin(), data.end(), target.begin(), target.end() ); // If the target byte sequence is found, replace it with the replacement byt e sequence if (it != data.end()) { std::copy(replacement.begin(), replacement.end(), it); } else { std::cerr << \"Failed to find the target byte sequence in the kernel coll ection\" << std::endl; return 1; } // Write the modified data vector to the patched kernel collection std::ofstream out(patched_kc, std::ios::binary); if (!out) { std::cerr << \"Failed to write the patched kernel collection\" << std::end l; return 1; } out.write(reinterpret_cast<char*>(data.data()), data.size()); out.close(); // Copy the patched kernel collection to the final destination std::ifstream src2(patched_kc, std::ios::binary); std::ofstream dst2(final_kc, std::ios::binary); if (!src2 || !dst2) { std::cerr << \"Failed to copy the patched kernel collection\" << std::endl ; return 1; } dst2 << src2.rdbuf(); src2.close(); dst2.close(); // Change the ownership of the final kernel collection to root:wheel if (chown(final_kc.c_str(), 0, 0) == -1) { std::cerr << \"Failed to change the ownership of the final kernel collect ion\" << std::endl; return 1; } // Get the volume name for the /System path std::string volume_name = get_volume_name(\"/System\"); if (volume_name."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ed to change the ownership of the final kernel collect ion\" << std::endl; return 1; } // Get the volume name for the /System path std::string volume_name = get_volume_name(\"/System\"); if (volume_name.empty()) { std::cerr << \"Failed to get the volume name for /System\" << std::endl; return 1; } // Create a shell script to configure the boot with the final kernel collect ion std::ofstream script(script_kc); if (!script) { std::cerr << \"Failed to create the shell script\" << std::endl; return 1; } script << \"#!/bin/bash\\n\"; script << \"# Disable System Integrity Protection\\n\"; script << \"csrutil disable\\n\"; script << \"# Disable Apple Mobile File Integrity\\n\"; script << \"nvram boot-args=\\\"ipc_control_port_options=0 amfi_get_out_of_my_w ay=1\\\"\\n\"; script << \"# Get the absolute path of this script\\n\"; script << \"SCRIPT=$(cd \\\"$(dirname \\\"${BASH_SOURCE[0]}\\\")\\\" && pwd)/$(basena me \\\"${BASH_SOURCE[0]}\\\")\\n\"; script << \"# Append the kernel collection name to the script path\\n\"; script << \"KC=\\\"${SCRIPT%/*}/vram_patch.kc\\\"\\n\"; // This line replaces the o riginal one script << \"# Get the volume name from the script path\\n\"; script << \"VOLUME=\\\"${SCRIPT%%/Library*}\\\"\\n\"; script << \"# Run the kmutil command with the absolute paths\\n\"; script << \"kmutil configure-boot -C -c \\\"$KC\\\" -v \\\"$VOLUME\\\"\\n\"; script << \"sync\\n\"; script << \"echo Finished. You may reboot your system now.\\n\"; script."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "e kmutil command with the absolute paths\\n\"; script << \"kmutil configure-boot -C -c \\\"$KC\\\" -v \\\"$VOLUME\\\"\\n\"; script << \"sync\\n\"; script << \"echo Finished. You may reboot your system now.\\n\"; script.close(); // Change the ownership and permissions of the shell script to root:wheel an d 755 if (chown(script_kc.c_str(), 0, 0) == -1) { std::cerr << \"Failed to change the ownership of the shell script\" << std ::endl; return 1; } if (chmod(script_kc.c_str(), 0755) == -1) { std::cerr << \"Failed to change the permissions of the shell script\" << s td::endl; return 1; } // Output the instructions to run the shell script in RecoveryOS std::cout << \"The shell script to configure the boot with the patched kernel collection has been created at \" << script_kc << std::endl; std::cout << \"Now reboot to RecoveryOS and run: \\\"/Volumes/\" << volume_name << \"/Library/KernelCollections/complete_patch.sh\\\"\" << std::endl; std::cout << \"If your boot fails after running the script or after an update , you need to go to Utilities->Startup Security Tool in RecoveryOS and pick Full Security\" << std::endl; return 0; } 0 replies CyborgArmy83 on Aug 14, 2023 not quite sure, you need sip disabled to write to /library/kernelcollections or at least you need it to delete from it, which is necessary if you patch again ma ybe you could avoid it by writing to another path that's not sip protected but i dk where the physical file on disk isnt the actual kernel that gets booted from fwiw because you can rm or corrup"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "in ma ybe you could avoid it by writing to another path that's not sip protected but i dk where the physical file on disk isnt the actual kernel that gets booted from fwiw because you can rm or corrupt the entire file you set-boot-object'd and it will still boot Hey @r3muxd As you might know the new MacOS 14 (Sonoma) Public Beta 3 has been released and includes some updated CoreML and other libraries which are used in AI developmen t. I had to update to the 14.0 beta to work on some AI projects that required th e updated frameworks. Do you think this script works on MacOS 14 Beta or will this code need an update ? I will be happy to help/test and figure it out with you! However, I can use th e help and you already have some expertise from creating this in the first place :) I tried the script before on my older Macbook Pro M1 Pro and it seemed to work b ut I am just a bit on the cautious side with my new M1 Max MBP. 0 replies dr3murr on Aug 20, 2023 Author just try it, worst come to worst you'll need to flip back on security when you get booted to recoveryos if theres an error if the pattern is wrong for new MacOS, then you'll get an error running the script and nothing will happen � 0 replies crasm on Oct 25, 2023 @r3muxd I�m having some trouble following the instructions for my system on the latest (stable) macOS Ventura. When I boot into recovery, the directory /Volumes /Macintosh HD/Library is empty."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "5, 2023 @r3muxd I�m having some trouble following the instructions for my system on the latest (stable) macOS Ventura. When I boot into recovery, the directory /Volumes /Macintosh HD/Library is empty. The KernelCollections directory on my system is available at /Volumes/Data/Libra ry/KernelCollections after mounting the Macintosh HD - Data volume. However, rea ding the script, it seems unlikely the system will boot if it�s not on the main volume? I am not familiar with the macOS boot process. Did you need to do any additional steps after booting into recovery, before runn ing the script? Thanks for any help! I�m looking forward to running Q4_K_M falcon 180B if I can get this to work. 6 replies @dr3murr dr3murr on Oct 31, 2023 Author In any case, it should be fine to just attempt to run the script from the other location. If MacOS fails to boot, you will get a prompt telling you to remove th e custom kernel, which you can do trivially by clicking Recovery->Utilities->Sta rtup Security Tool and resetting it to default. @crasm crasm on Nov 1, 2023 Success! I was able to load and run inference on Q4_K Falcon 180B with my 128GB Mac Studio. For anybody else trying to get this to work: diskutil list, look for a line like 5: APFS Volume Data 1.4 TB disk3s5 Mount that volume with diskutil apfs unlock /dev/<volume> Run /Volumes/Data/Library/KernelCollections/complete_patch."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "is to work: diskutil list, look for a line like 5: APFS Volume Data 1.4 TB disk3s5 Mount that volume with diskutil apfs unlock /dev/<volume> Run /Volumes/Data/Library/KernelCollections/complete_patch.sh @r3muxd Thank you very much for your help and assurances Notes: The output of your program is Macintosh HD normally and macOS Base System in rec overy. There are some warnings and errors compiling vram_patcher.cpp, but it works fine ./complete_patch.sh in recovery gave an error, but it works fine Error setting variable � 'boot-args': (iokit/common) not permitted. @hlo-world hlo-world on Nov 21, 2023 Thanks crasm, can confirm it works on Sonoma @JasonOSX JasonOSX on Nov 21, 2023 Did you patch it through recovery or else? and did you disable SIP or more? aski ng because I just upgraded to the latest Sonoma @hlo-world hlo-world on Nov 22, 2023 Yea you have to reduce the security and disable SIP in Recovery. ddh0 on Nov 3, 2023 Hi, thanks for this. I'm getting this error: Failed to copy the patched kernel c ollection. Reading the code I see it's just copying a file so I'm not sure what' s up. I've checked to make sure both the parent directories exist, and I'm runni ng the program as root. I'm on macOS Sonoma 14.1, M2 Air 24GB. Trying to set percent to 87.5. Any help w ould be appreciated. Thanks. 3 replies @crasm crasm on Nov 3, 2023 Did you disable SIP? @ddh0 ddh0 on Nov 3, 2023 Ah, ok. That wasn't mentioned."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "M2 Air 24GB. Trying to set percent to 87.5. Any help w ould be appreciated. Thanks. 3 replies @crasm crasm on Nov 3, 2023 Did you disable SIP? @ddh0 ddh0 on Nov 3, 2023 Ah, ok. That wasn't mentioned. Thank you! @ddh0 ddh0 on Nov 3, 2023 Works great now! Running mistral 7b FP16 on metal which I couldn't quite do befo re. kdyke on Nov 29, 2023 Folks... just do: sudo sysctl iogpu.wired_limit_mb=<mb> from Terminal. You�d hav e to do it every boot as it�s not sticky, but that seems better than patching yo ur OS and disabling SIP. I would not recommend going to 100% as the OS needs som e reasonable amount of memory to fit everything else that has to be around not w ired down by the GPU driver stack, and things will start to go seriously wrong i f you don�t leave enough space. 14 replies @sammcj sammcj on Dec 2, 2023 Ohhhh I see, thank you! :) @ingochris ingochris on Dec 4, 2023 Thank you so much! @crasm crasm on Dec 4, 2023 For nix-darwin, you can use: launchd.daemons.\"sysctl-vram-limit\" = { command = \"/usr/sbin/sysctl iogpu.wired_limit_mb=115200\"; serviceConfig.RunAtLoad = true; }; @tusing tusing on Dec 25, 2023 Thanks for the Nix instructions, did not realize this is why it wasn't working! @beginor beginor on May 29, 2024 indeed, it works! kdyke on Nov 29, 2023 It�s expecting a value in megabytes."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "c 25, 2023 Thanks for the Nix instructions, did not realize this is why it wasn't working! @beginor beginor on May 29, 2024 indeed, it works! kdyke on Nov 29, 2023 It�s expecting a value in megabytes. So if you�re using RAM math instead of HD /SDD math, for 60GB you�d want 60*1024 = 61440 � 1 reply @ddh0 ddh0 on Nov 29, 2023 mine was base 10 math, where 20000 was 20GB marcingomulkiewicz on Apr 5, 2024 (Sorry for necrothreading, but I've spent some serious time looking for the solu tion for this particular problem, and I think I found the easiest one) What worked for me (Sonoma) is to create sysctl.conf, and add appropriate parame ter to it: sudo nano /etc/sysctl.conf and inside: # change default CPU/GPU RAM split iogpu.wired_limit_mb=28672 (as already mentioned, the number is max RAM in MB that GPU can have; select som e value reasonable for your machine). I'm not sure why /etc/sysctl.conf does not exist by default (good old unix habit is to put some template with commented out defaults), but as far as I know sysc tl is not deprecated, so it should keep working. 4 replies @Summit1122 Summit1122 on Aug 8, 2024 how can I check if this worked? @beginor beginor on Aug 9, 2024 Yes, it works! Just run any model with llama-cli or llama-server , check the ggm l_metal_init: recommendedMaxWorkingSetSize value in console output, which is eff ected by iogpu.wired_limit_mb."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "9, 2024 Yes, it works! Just run any model with llama-cli or llama-server , check the ggm l_metal_init: recommendedMaxWorkingSetSize value in console output, which is eff ected by iogpu.wired_limit_mb. @trevcodner trevcodner on Feb 14 Morning, can I confirm, that this works instead of the script above, or requires that script too? I suspect it's just the command line, but wanted to check. Tha nks @jared-krauss jared-krauss on Apr 13 (Sorry for necrothreading, but I've spent some serious time looking for the solu tion for this particular problem, and I think I found the easiest one) What worked for me (Sonoma) is to create sysctl.conf, and add appropriate parame ter to it: sudo nano /etc/sysctl.conf and inside: # change default CPU/GPU RAM split iogpu.wired_limit_mb=28672 (as already mentioned, the number is max RAM in MB that GPU can have; select som e value reasonable for your machine). I'm not sure why /etc/sysctl.conf does not exist by default (good old unix habit is to put some template with commented out defaults), but as far as I know sysc tl is not deprecated, so it should keep working. No no, it's appreciated. Because I too am bringing this back a year later to say this is what I needed to help me make bigger Gaussian Splats, running out of sp ace and getting the Sigkill code. Azirine 3 weeks ago I found that even after doing sudo nano /etc/sysctl."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "later to say this is what I needed to help me make bigger Gaussian Splats, running out of sp ace and getting the Sigkill code. Azirine 3 weeks ago I found that even after doing sudo nano /etc/sysctl.conf, going slightly above t he original limit (¾ � 128 = 96 GiB in my case) makes models load much slower w ith a long swap/unswap process. #13361 0 replies + LJ Thu 26 Jun 2025 00:04:46 BST � ln -s ~/.lmstudio/models/rednote-hilab/dots.llm1.inst/dots.llm1.inst-UD-TQ1_0 .gguf https://huggingface.co/rednote-hilab/dots.llm1.inst ln -s ~/.lmstudio/models/rednote-hilab/dots.llm1.inst/dots.llm1.inst-UD-TQ1_0.gg uf cmake . -B ./build cmake --build build --config Release -j build/bin/llama-cli \\ --model models/dots.llm1.inst-UD-TQ1_0.gguf \\ --temp 0 \\ --top_p 0.95 \\ --min_p 0 \\ --ctx-size 4096 sudo sysctl iogpu.wired_limit_mb=80000 build/bin/llama-cli --model models/dots.llm1.inst-UD-TQ1_0.gguf --temp 0 --top_p 0.95 --min_p 0 --ctx-size 4096 (torch311) ljubomir@macbook2(:):~/llama.cpp$ sudo sysctl iogpu.wired_limit_mb=80 000 (torch311) ljubomir@macbook2(:):~/llama.cpp$ build/bin/llama-cli --model models/ dots.llm1.inst-UD-TQ1_0.gguf --temp 0 --top_p 0.95 --min_p 0 --ctx-size 4096 sudo sysctl iogpu.wired_limit_mb=80000 build/bin/llama-server --model models/dots.llm1.inst-UD-TQ1_0.gguf --temp 0 --to p_p 0.95 --min_p 0 --ctx-size 4096 & # access on http://127.0.0.1:8080 https://huggingface.co/unsloth/dots.llm1.inst-GGUF/discussions/1 Hm, I see it's working in llama.cpp."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "UD-TQ1_0.gguf --temp 0 --to p_p 0.95 --min_p 0 --ctx-size 4096 & # access on http://127.0.0.1:8080 https://huggingface.co/unsloth/dots.llm1.inst-GGUF/discussions/1 Hm, I see it's working in llama.cpp. And I see LMStudio got an upgrade today: You are all up to date! The current version is 0.3.17 But still, the llama.cpp runtime bundled with LMStudio on my MacOS is from one m onth ago: Metal llama.cpp v1.33.0 Engine Apple Metal accelerated llama.cpp engine Latest Version Installed - Llama 4 vision support - Enable any LLM to be used as an embedding model (requires LM Studio 0.3.16-b6) - Fixed prompt processing bugs when chats exceed context length - llama.cpp updated to b5459 (commit 8a1d206) I see the 8a1d206 commit in llama.cpp be from 2025-05-22 : 8a1d206f ggerganov@gmail.com 2025-05-22 22:21:07 +0300 : tts : fix n_ubatch + ma ke WavTokenizer cache-less (#13713) I see latter commit from 2025-06-15 that looks like important to have: 9ae4143b mikjuo@gmail.com 2025-06-15 00:52:06 -0700 : model : add dots.llm1 arch itecture support (#14044) (#14118) I guess will just wait some more, from LMStudio to move to news llama.cpp. On trying to load dots.llm1 I'm still getting error loading model: error loading model architecture: unknown model architectur e: 'dots1' and that kind of makes sense. LJ Thu 26 Jun 2025 00:04:46 BST + LJ Thu 26 Jun 2025 01:38:40 BST � Run with larger context with flash attention Run with larger context with flash attention sudo sysctl iogpu."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "at kind of makes sense. LJ Thu 26 Jun 2025 00:04:46 BST + LJ Thu 26 Jun 2025 01:38:40 BST � Run with larger context with flash attention Run with larger context with flash attention sudo sysctl iogpu.wired_limit_mb=80000 build/bin/llama-server --model models/dots.llm1.inst-UD-TQ1_0.gguf --temp 0 --to p_p 0.95 --min_p 0 --ctx-size 32768 --flash-attn --cache-type-k f16 --cache-type -v f16 & # access on http://127.0.0.1:8080 The caches with 32K context and f16 take ~32GB: llama_kv_cache_unified: size = 31744.00 MiB ( 32768 cells, 62 layers, 1 seqs), K (f16): 15872.00 MiB, V (f16): 15872.00 MiB So together with the mmap-ed model may take ~80GB (32GB + 45GB = 77GB) ljubomir@macbook2(:):~/llama.cpp$ l models/dots.llm1.inst-UD-TQ1_0.gguf lrwx------@ 1 ljubomir staff 90B 25 Jun 23:14 models/dots.llm1.inst-UD-TQ1_0 .gguf -> /Users/ljubomir/.lmstudio/models/rednote-hilab/dots.llm1.inst/dots.llm1 .inst-UD-TQ1_0.gguf ljubomir@macbook2(:):~/llama.cpp$ l /Users/ljubomir/.lmstudio/models/rednote-hil ab/dots.llm1.inst/dots.llm1.inst-UD-TQ1_0.gguf -rw-r--r--@ 1 ljubomir staff 45G 25 Jun 22:17 /Users/ljubomir/.lmstudio/mode ls/rednote-hilab/dots.llm1.inst/dots.llm1.inst-UD-TQ1_0.gguf Reduce the 32GB for caches to 16GB with q8_0 cache type to a total of <75GB RAM and 16 tps: sudo sysctl iogpu.wired_limit_mb=80000 build/bin/llama-server --model models/dots.llm1.inst-UD-TQ1_0.gguf --temp 0 --to p_p 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "r caches to 16GB with q8_0 cache type to a total of <75GB RAM and 16 tps: sudo sysctl iogpu.wired_limit_mb=80000 build/bin/llama-server --model models/dots.llm1.inst-UD-TQ1_0.gguf --temp 0 --to p_p 0.95 --min_p 0 --ctx-size 32768 --flash-attn --cache-type-k q8_0 --cache-typ e-v q8_0 --jinja & # access on http://127.0.0.1:8080 Added --jinja after commend here https://www.reddit.com/r/LocalLLaMA/comments/1ljrwrq/comment/mztn9xo/?context=3 sudo sysctl iogpu.wired_limit_mb=80000 build/bin/llama-server --model models/dots.llm1.inst-UD-TQ1_0.gguf --temp 0 --to p_p 0.95 --min_p 0 --ctx-size 32768 --flash-attn --cache-type-k q8_0 --cache-typ e-v q8_0 --jinja & # access on http://127.0.0.1:8080 LJ Thu 26 Jun 2025 01:38:40 BST + Gemma 3n is here! � Gemma 3n is here! � �Multimodal (text/audio/image/video) understanding �Runs with as little as 2GB of RAM �First model under 10B with @lmarena_ai score of 1300+ https://x.com/osanseviero/status/1938277414687121531 https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/ ljubomir@gigul2(422663.llama.cpp:0):~/llama.cpp$ git pull mviv build{,.1} cmake . -B ./build cmake --build build --config Release -j mviv ~/Downloads/gemma-3n-E4B-it-UD-Q8_K_XL.gguf models/ build/bin/llama-server --model models/gemma-3n-E4B-it-UD-Q8_K_XL.gguf --temp 1.0 --top_k 64 --top_p 0.95 --min_p 0 --ctx-size 32768 & # access on http://127.0.0.1:8080 https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/tree/main Set temperature = 1."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "-it-UD-Q8_K_XL.gguf --temp 1.0 --top_k 64 --top_p 0.95 --min_p 0 --ctx-size 32768 & # access on http://127.0.0.1:8080 https://huggingface.co/unsloth/gemma-3n-E4B-it-GGUF/tree/main Set temperature = 1.0, top_k = 64, top_p = 0.95, min_p = 0.0 Gemma 3n max tokens (context length): 32K. Gemma 3n chat template: <bos><start_of_turn>user\\nHello!<end_of_turn>\\n<start_of_turn>model\\nHey there!< end_of_turn>\\n<start_of_turn>user\\nWhat is 1+1?<end_of_turn>\\n<start_of_turn>mod el\\n Add --jinja? ljubomir@gigul2(422663.llama.cpp:0):~/llama.cpp$ build/bin/llama-server --model models/gemma-3n-E4B-it-UD-Q8_K_XL.gguf --temp 1.0 --top_k 64 --top_p 0.95 --min_ p 0 --ctx-size 32768 --jinja & Works! LJ Thu 26 Jun 18:47:28 BST 2025 + LJ Sun 6 Jul 2025 09:51:26 BST � cd llama.cpp/ cd llama.cpp/ cd models/ ln -s ~/.lmstudio/models/DevQuasar/tencent.Hunyuan-A13B-Instruct-GGUF/tencent.Hu nyuan-A13B-Instruct.Q3_K_M.gguf cd .. git pull mviv build{,.1} unset CC CXX unset LDFLAGS unset CPPFLAGS env |egrep 'CC|CXX|FLAGS' brew install libomp rm -rf build cmake . -B ./build \\ -DCMAKE_C_FLAGS=\"-Xpreprocessor -fopenmp -I/opt/homebrew/opt/libomp/include\" \\ -DCMAKE_CXX_FLAGS=\"-Xpreprocessor -fopenmp -I/opt/homebrew/opt/libomp/include\" \\ -DOpenMP_C_LIB_NAMES=\"libomp\" \\ -DOpenMP_CXX_LIB_NAMES=\"libomp\" \\ -DOpenMP_libomp_LIBRARY=\"/opt/homebrew/opt/libomp/lib/libomp.dylib\" \\ -DCMAKE_EXE_LINKER_FLAGS=\"-L/opt/homebrew/opt/libomp/lib\" cmake --build build --config Release -j Original by DevQuasar tencent."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "\\ -DOpenMP_libomp_LIBRARY=\"/opt/homebrew/opt/libomp/lib/libomp.dylib\" \\ -DCMAKE_EXE_LINKER_FLAGS=\"-L/opt/homebrew/opt/libomp/lib\" cmake --build build --config Release -j Original by DevQuasar tencent.Hunyuan-A13B-Instruct.Q3_K_M.gguf ljubomir@macbook2(:):~/llama.cpp/models$ l tencent.Hunyuan-A13B-Instruct.Q3_K_M. gguf lrwx------@ 1 ljubomir staff 119B 6 Jul 01:06 tencent.Hunyuan-A13B-Instruct. Q3_K_M.gguf -> /Users/ljubomir/.lmstudio/models/DevQuasar/tencent.Hunyuan-A13B-I nstruct-GGUF/tencent.Hunyuan-A13B-Instruct.Q3_K_M.gguf sudo sysctl iogpu.wired_limit_mb=80000 build/bin/llama-server --model models/Hunyuan-A13B-Instruct-IQ4_NL.gguf --temp 0 .8 --top_p 0.95 --min_p 0.05 --top_k 40 --repeat-penalty 1.1 --ctx-size 262144 - -flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & # access on http://127.0.0.1:8080 Latter by Unsloth Hunyuan-A13B-Instruct-IQ4_NL.gguf from https://huggingface.co/ unsloth/Hunyuan-A13B-Instruct-GGUF (torch311) ljubomir@macbook2(:):~/llama.cpp$ l models/Hunyuan-A13B-Instruct-IQ4_ NL.gguf -rw-r--r--@ 1 ljubomir staff 42G 10 Jul 22:59 models/Hunyuan-A13B-Instruct-I Q4_NL.gguf (torch311) ljubomir@macbook2(:):~/llama.cpp$ l ~/.lmstudio/models/unsloth/Hunyua n-A13B-Instruct-GGUF/Hunyuan-A13B-Instruct-IQ4_NL.gguf lrwx------@ 1 ljubomir staff 66B 10 Jul 23:24 /Users/ljubomir/.lmstudio/mode ls/unsloth/Hunyuan-A13B-Instruct-GGUF/Hunyuan-A13B-Instruct-IQ4_NL.gguf -> /User s/ljubomir/llama.cpp/models/Hunyuan-A13B-Instruct-IQ4_NL.gguf sudo sysctl iogpu."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "l 23:24 /Users/ljubomir/.lmstudio/mode ls/unsloth/Hunyuan-A13B-Instruct-GGUF/Hunyuan-A13B-Instruct-IQ4_NL.gguf -> /User s/ljubomir/llama.cpp/models/Hunyuan-A13B-Instruct-IQ4_NL.gguf sudo sysctl iogpu.wired_limit_mb=80000 build/bin/llama-server --model models/Hunyuan-A13B-Instruct-IQ4_NL.gguf --temp 0 .8 --top_p 0.95 --min_p 0.05 --top_k 40 --repeat-penalty 1.1 --ctx-size 262144 - -flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & # access on http://127.0.0.1:8080 LJ Sun 6 Jul 2025 09:51:26 BST + LJ Sat 19 Jul 2025 14:43:56 BST � https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF Baidu ERNIE 4.5 https://huggingface.co/collections/baidu/ernie-45-6861cd4c9be84540645f35c9 https://huggingface.co/baidu/ERNIE-4.5-21B-A3B-PT https://huggingface.co/unsloth/ERNIE-4.5-21B-A3B-PT-GGUF ERNIE-4.5-21B-A3B-PT-UD-Q6_K_XL.gguf build/bin/llama-server --model models/ERNIE-4.5-21B-A3B-PT-UD-Q6_K_XL.gguf --tem p 0.8 --top_p 0.8 --min_p 0.05 --top_k 40 --repeat-penalty 1.1 --ctx-size 131072 --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & # access on http://127.0.0.1:8080 prompt eval time = 3371.18 ms / 2700 tokens ( 1.25 ms per token, 800.91 tokens per second) eval time = 21118.88 ms / 1039 tokens ( 20.33 ms per token, 49.20 tokens per second) total time = 24490.06 ms / 3739 tokens Gets ~50 tps TODO try <77GB 300B-A47B TQ1_0 model https://huggingface.co/unsloth/ERNIE-4."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "1118.88 ms / 1039 tokens ( 20.33 ms per token, 49.20 tokens per second) total time = 24490.06 ms / 3739 tokens Gets ~50 tps TODO try <77GB 300B-A47B TQ1_0 model https://huggingface.co/unsloth/ERNIE-4.5-300B-A47B-PT-GGUF LJ Sat 19 Jul 2025 14:43:56 BST + LJ Thu 10 Jul 2025 21:11:04 BST � Benchmark for Devstral? #4058 Use local code agent with aider - devstral https://github.com/Aider-AI/aider/issues/4058 Benchmark for Devstral? #4058 https://github.com/Aider-AI/aider/issues/4058#issuecomment-2960920629 psymonryan last month · edited by psymonryan My tips: Use the largest quant you can fit in memory Use 'diff' format rather than 'whole' Here are my llama-server settings: /Users/simon/models/bin-arm64/llama-server --host 0.0.0.0 --port 8284 --flash-attn --slots --ctx-size 24576 --model /Users/simon/models/localmodels/mistralai_Devstral-Small-2505-Q8_0 .gguf -ngl 99 -ngld 99 --cache-type-k q8_0 --cache-type-v q8_0 --seed \"-1\" --temp 0.15 --repeat-penalty 1.0 --min-p 0.01 --top-k 64 --top-p 0.95 --samplers \"top_k;top_p;min_p;temperature;dry;typ_p;xtc\" --dry-multiplier 0.5 And my .aider.conf.yml: ## Specify the api base urls (/v1 needed for llama-swap) openai-api-base: http://m3macbook.mylocal.lan:8012/v1 alias: - \"thinking:openai/QwQ\" - \"thinker:openai/QwQ\" - \"QwQ:openai/QwQ\" - \"coding:openai/Qwen2.5-Coder-32B\" - \"coder:openai/Qwen2.5-Coder-32B\" - \"codersmall:openai/Qwen2."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "3macbook.mylocal.lan:8012/v1 alias: - \"thinking:openai/QwQ\" - \"thinker:openai/QwQ\" - \"QwQ:openai/QwQ\" - \"coding:openai/Qwen2.5-Coder-32B\" - \"coder:openai/Qwen2.5-Coder-32B\" - \"codersmall:openai/Qwen2.5-Coder-14B\" - \"gemma:openai/Gemma-3-27B-qat-Q4\" - \"mistral:openai/mistral\" - \"glm:openai/glm\" - \"qwen3:openai/Qwen3-32B\" - \"qwen3nt:openai/Qwen3-32B-nt\" - \"cogito:openai/cogito\" - \"devstral:openai/devstral\" - \"devstral_long:openai/devstral_long\" auto-commits: false dark-mode: true model: qwen3 editor-model: devstral # Don't bother with a separate weak model, just re-use the editor-model (since i ts already loaded in llama-swap) weak-model: devstral show-model-warnings: false multiline: false watch-files: true # whole is slower but better for small models (but it messes up your comments) # udiff seems to mostly work with Qwen2.5-Coder-32B # diff format is all you need with devstral as it is tuned for agent use edit-format: diff editor-edit-format: diff # map-tokens: 0 read: .aider.conventions.md I'm using it daily for work and for my personal mindmap project It does occasionally fail to edit on the first or second time, but mostly self c orrects where as Qwen2.5-Coder-32B at the quant I am using does not. I found it is way better with the Q8 quant and with KV quant size set to Q8 also . I do also occasionaly run it in 'long context' mode with the Bartowski Q6_K_L qu ant and KL set to q4_0 and it works but is less reliable. Also it depends on how complex your instructions are."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "also . I do also occasionaly run it in 'long context' mode with the Bartowski Q6_K_L qu ant and KL set to q4_0 and it works but is less reliable. Also it depends on how complex your instructions are. If you are using it in arc hitect mode, then it should get good instructions from Qwen3 or QwQ. (ie: dont e xpect it to think or understand too deeply, use instructions like: \"In function X add code to add Y functionality\") Hope this helps! https://aider.chat/docs/config/aider_conf.html Configuration YAML config file YAML config file Most of aider�s options can be set in an .aider.conf.yml file. Aider will look f or a this file in these locations: Your home directory. The root of your git repo. The current directory. If the files above exist, they will be loaded in that order. Files loaded last w ill take priority. You can also specify the --config <filename> parameter, which will only load the one config file. LJ Thu 10 Jul 2025 21:11:04 BST + LJ Fri 11 Jul 2025 20:37:40 BST � https://huggingface.co/unsloth/Hunyuan-A13B-Instruct-GGUF https://huggingface.co/unsloth/Hunyuan-A13B-Instruct-GGUF If you are using llama.cpp, use --jinja ./llama.cpp/llama-cli -hf unsloth/Hunyuan-A13B-Instruct-GGUF:Q4_K_XL -ngl 99 --j inja --temp 0.7 --top-k 20 --top-p 0.8 --min-p 0.05 --repeat-penalty 1.05 sudo sysctl iogpu.wired_limit_mb=80000 build/bin/llama-server --model models/Hunyuan-A13B-Instruct-IQ4_NL.gguf --temp 0 .7 --top_p 0.8 --min_p 0.05 --top_k 20 --repeat-penalty 1."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "--repeat-penalty 1.05 sudo sysctl iogpu.wired_limit_mb=80000 build/bin/llama-server --model models/Hunyuan-A13B-Instruct-IQ4_NL.gguf --temp 0 .7 --top_p 0.8 --min_p 0.05 --top_k 20 --repeat-penalty 1.05 --ctx-size 262144 - -flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & # access on http://127.0.0.1:8080 Start aider in your working directory aider --openai-api-base http://localhost:8080 --openai-api-key dummy-key --model hunyuan-a13b # .aider.conf.yml # sudo sysctl iogpu.wired_limit_mb=80000 # build/bin/llama-server --model models/Hunyuan-A13B-Instruct-IQ4_NL.gguf --temp 0.7 --top_p 0.8 --min_p 0.05 --top_k 20 --repeat-penalty 1.05 --ctx-size 262144 --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & # access on http://127.0.0.1:8080 # Start aider in your working directory # aider --openai-api-base http://localhost:8080 --openai-api-key dummy-key --mod el hunyuan-a13b # Specifies the base URL for the OpenAI-compatible API provided by llama.cpp. # The /v1 endpoint is standard for OpenAI API compatibility. openai-api-base: http://127.0.0.1:8080/v1 # A dummy API key is required by aider for OpenAI API, but not used by llama.cpp . openai-api-key: dummy-key # Define aliases for your local model. # The model name 'hunyuan-a13b' is derived from your GGUF file name # (Hunyuan-A13B-Instruct-IQ4_NL.gguf), which llama.cpp typically exposes."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "-api-key: dummy-key # Define aliases for your local model. # The model name 'hunyuan-a13b' is derived from your GGUF file name # (Hunyuan-A13B-Instruct-IQ4_NL.gguf), which llama.cpp typically exposes. alias: - \"hunyuan:openai/hunyuan-a13b\" # A convenient alias for your model - \"local:openai/hunyuan-a13b\" # Another general alias for your local model # Set your primary model to the one served by llama.cpp. model: hunyuan-a13b # For editor and weak models, if you are only serving one model via llama.cpp, # it's best to point them to the same model. editor-model: hunyuan-a13b weak-model: hunyuan-a13b # General Aider settings (you can adjust these as needed): auto-commits: true dark-mode: false multiline: false watch-files: true show-model-warnings: false # Edit format for how aider applies changes. 'diff' is generally robust. edit-format: diff editor-edit-format: diff # If you have a conventions file, specify it here. # read: .aider.conventions.md But aider doesn't know about that endpoint. So use mistal coding model next. $ cat ~/z/itrade/src/.aider.conf.yml # .aider.conf.yml # 1) Run with hunyuan-a13b # sudo sysctl iogpu.wired_limit_mb=80000 # build/bin/llama-server --model models/Hunyuan-A13B-Instruct-IQ4_NL.gguf --temp 0.7 --top_p 0.8 --min_p 0.05 --top_k 20 --repeat-penalty 1.05 --ctx-size 262144 --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & # access on http://127.0.0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "B-Instruct-IQ4_NL.gguf --temp 0.7 --top_p 0.8 --min_p 0.05 --top_k 20 --repeat-penalty 1.05 --ctx-size 262144 --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & # access on http://127.0.0.1:8080 # Start aider in your working directory # aider --openai-api-base http://localhost:8080 --openai-api-key dummy-key --mod el hunyuan-a13b # 2) Run with mistral devstral # build/bin/llama-server --model models/mistralai_Devstral-Small-2507-Q6_K_L.ggu f --temp 0.7 --top_p 0.8 --min_p 0.05 --top_k 20 --repeat-penalty 1.05 --ctx-siz e 131072 --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & # access on http://127.0.0.1:8080 # Start aider in your working directory # aider --openai-api-base http://localhost:8080 --openai-api-key dummy-key # .aider.conf.yml # Specifies the base URL for the OpenAI-compatible API provided by llama.cpp. # The /v1 endpoint is standard for OpenAI API compatibility. openai-api-base: http://127.0.0.1:8080/v1 # A dummy API key is required by aider for OpenAI API, but not used by llama.cpp . openai-api-key: dummy-key # Define aliases for your local models. # IMPORTANT: Prefix the model name with \"openai/\" to correctly route through Lit eLLM. # This tells LiteLLM that even though it's a local server, it's mimicking the Op enAI API. # Alias for your Hunyuan model alias: - \"hunyuan:openai/hunyuan-a13b\" - \"local-hunyuan:openai/hunyuan-a13b\" # Alias for your new Devstral Mistral model # Assuming llama."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "t's mimicking the Op enAI API. # Alias for your Hunyuan model alias: - \"hunyuan:openai/hunyuan-a13b\" - \"local-hunyuan:openai/hunyuan-a13b\" # Alias for your new Devstral Mistral model # Assuming llama.cpp exposes this model as 'mistralai_Devstral-Small-2507-Q6_K_L ' # (which is typically derived from the GGUF filename without the .gguf extension ). - \"devstral:openai/mistralai_Devstral-Small-2507-Q6_K_L\" - \"local-devstral:openai/mistralai_Devstral-Small-2507-Q6_K_L\" # Set your primary model. You can switch this to 'devstral' if you want it as de fault. # For now, let's keep it as hunyuan-a13b as that was your last active model. # Remember to use the 'openai/' prefix here too. model: openai/devstral # For editor and weak models, point them to your desired local model with the pr efix. editor-model: openai/devstral weak-model: openai/devstral # General Aider settings auto-commits: true dark-mode: false multiline: false watch-files: true show-model-warnings: false # Edit format for how aider applies changes. 'diff' is generally robust. edit-format: diff editor-edit-format: diff # If you have a conventions file, specify it here. # read: .aider.conventions.md LJ Fri 11 Jul 2025 20:37:40 BST + https://huggingface.co/mradermacher/Seed-X-PPO-7B-GGUF https://huggingface."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "mat: diff # If you have a conventions file, specify it here. # read: .aider.conventions.md LJ Fri 11 Jul 2025 20:37:40 BST + https://huggingface.co/mradermacher/Seed-X-PPO-7B-GGUF https://huggingface.co/ByteDance-Seed/Seed-X-PPO-7B ByteDance-Seed/Seed-X-PPO-7B Seed-X-PPO-7B Introduction We are excited to introduce Seed-X, a powerful series of open-source multilingua l translation language models, including an instruction model, a reinforcement l earning model, and a reward model. It pushes the boundaries of translation capab ilities within 7 billion parameters. We develop Seed-X as an accessible, off-the -shelf tool to support the community in advancing translation research and appli cations: https://huggingface.co/mradermacher/Seed-X-PPO-7B-GGUF $ l ~/llama.cpp/models/Seed-X-PPO-7B.Q8_0.gguf -rw-r--r--@ 1 ljubomir staff 7.4G 21 Jul 09:10 /Users/ljubomir/llama.cpp/mode ls/Seed-X-PPO-7B.Q8_0.gguf + https://huggingface.co/DavidAU/Qwen2.5-2X7B-Coder-Instruct-OlympicCoder -19B https://huggingface.co/collections/DavidAU/coder-and-programming-models-moe-reg- imatrix-686357166b1f0d4322ad3e2c https://huggingface.co/DavidAU/Qwen2.5-2X7B-Coder-Instruct-OlympicCoder-19B DavidAU/Qwen2.5-2X7B-Coder-Instruct-OlympicCoder-19B Qwen2.5-2X7B-Coder-Instruct-OlympicCoder-19B This repo contains the full precision source code, in \"safe tensors\" format to g enerate GGUFs, GPTQ, EXL2, AWQ, HQQ and other formats. The source code can also be used directly."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ruct-OlympicCoder-19B This repo contains the full precision source code, in \"safe tensors\" format to g enerate GGUFs, GPTQ, EXL2, AWQ, HQQ and other formats. The source code can also be used directly. Coder MOE with 2 top coder models in a Mixture of Experts config, using the full power of each model to code in a 19B model. Included: Qwen/Qwen2.5-Coder-7B-Instruct (500+ likes; all major + many minor programming l anguages) open-r1/OlympicCoder-7B (179+ likes; all major + many minor programming language s) TWO models all working together to code, with Qwen2.5-Coder-7B-Instruct as a sha red expert too. Default config is 2 experts activated. NOTE: All experts help with coding, regardless of how many you have activated. SETTINGS: Temp .5 to .7 (or lower) Max Context is 32k topk: 20, topp: .8, minp: .05 rep pen: 1.05-1.1 (can be lower) Jinja Template (embedded) or CHATML template. A System Prompt is not required. (ran tests with blank system prompt) MODELS in THIS MOE - see each for more information, benchmarks and how they oper ate: https://huggingface.co/open-r1/OlympicCoder-7B https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct For more information / other Qwen/Mistral Coders / additional settings see: [ https://huggingface.co/DavidAU/Qwen2.5-MOE-2x-4x-6x-8x__7B__Power-CODER__19B-3 0B-42B-53B-gguf ] Help, Adjustments, Samplers, Parameters and More CHANGE THE NUMBER OF ACTIVE EXPERTS: See this document: https://huggingface."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "/DavidAU/Qwen2.5-MOE-2x-4x-6x-8x__7B__Power-CODER__19B-3 0B-42B-53B-gguf ] Help, Adjustments, Samplers, Parameters and More CHANGE THE NUMBER OF ACTIVE EXPERTS: See this document: https://huggingface.co/DavidAU/How-To-Set-and-Manage-MOE-Mix-of-Experts-Model-Ac tivation-of-Experts Settings: CHAT / ROLEPLAY and/or SMOOTHER operation of this model: In \"KoboldCpp\" or \"oobabooga/text-generation-webui\" or \"Silly Tavern\" ; Set the \"Smoothing_factor\" to 1.5 : in KoboldCpp -> Settings->Samplers->Advanced-> \"Smooth_F\" : in text-generation-webui -> parameters -> lower right. : In Silly Tavern this is called: \"Smoothing\" NOTE: For \"text-generation-webui\" -> if using GGUFs you need to use \"llama_HF\" (which involves downloading some co nfig files from the SOURCE version of this model) Source versions (and config files) of my models are here: https://huggingface.co/collections/DavidAU/d-au-source-files-for-gguf-exl2-awq-g ptq-hqq-etc-etc-66b55cb8ba25f914cbf210be OTHER OPTIONS: Increase rep pen to 1.1 to 1.15 (you don't need to do this if you use \"smoothing _factor\") If the interface/program you are using to run AI MODELS supports \"Quadratic Samp ling\" (\"smoothing\") just make the adjustment as noted."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "1.15 (you don't need to do this if you use \"smoothing _factor\") If the interface/program you are using to run AI MODELS supports \"Quadratic Samp ling\" (\"smoothing\") just make the adjustment as noted. Highest Quality Settings / Optimal Operation Guide / Parameters and Samplers This a \"Class 1\" model: For all settings used for this model (including specifics for its \"class\"), incl uding example generation(s) and for advanced settings guide (which many times ad dresses any model issue(s)), including methods to improve model performance for all use case(s) as well as chat, roleplay and other use case(s) please see: [ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-A nd-Full-Precision-by-Samplers_Parameters ] You can see all parameters used for generation, in addition to advanced paramete rs and samplers to get the most out of this model here: [ https://huggingface.co/DavidAU/Maximizing-Model-Performance-All-Quants-Types-A nd-Full-Precision-by-Samplers_Parameters ] https://huggingface.co/mradermacher/Qwen2.5-2X7B-Coder-Instruct-OlympicCoder-19B -i1-GGUF (torch311) ljubomir@macbook2(:):~/llama.cpp$ l models/Qwen2.5-2X7B-Coder-Instruc t-OlympicCoder-19B.i1-Q5_K_M.gguf -rw-r--r--@ 1 ljubomir staff 13G 21 Jul 08:53 models/Qwen2.5-2X7B-Coder-Inst ruct-OlympicCoder-19B.i1-Q5_K_M.gguf + LJ Sat 12 Jul 2025 11:24:17 BST � # 1a) Start mlx server: $ uvx --from mlx-lm mlx_lm.server --model mlx-community/Qwen3-8B-8bit Moved from ."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "n2.5-2X7B-Coder-Inst ruct-OlympicCoder-19B.i1-Q5_K_M.gguf + LJ Sat 12 Jul 2025 11:24:17 BST � # 1a) Start mlx server: $ uvx --from mlx-lm mlx_lm.server --model mlx-community/Qwen3-8B-8bit Moved from .bashrc, TODO try run # Running aider locally - MLX: # 1a) Start mlx server: $ uvx --from mlx-lm mlx_lm.server --model mlx-community/ Qwen3-8B-8bit # 1b) Then start Aider in your working directory: $ uvx aider --openai-api-base http://localhost:8080 --openai-api-key dummy-key --model mlx-community/Qwen3-8B- 8bit # Running aider locally - GGUF: # 2a) Start llama.cpp server http://127.0.0.1:8080. NB to extend 32K->128K conte xt add \"--rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768\". # $ llama-server --ctx-size 40960 --model ~/.lmstudio/models/unsloth/Qwen3-30B-A 3B-GGUF/Qwen3-30B-A3B-UD-Q4_K_XL.gguf --model-draft ~/.lmstudio/models/unsloth/Q wen3-0.6B-GGUF/Qwen3-0.6B-Q8_0.gguf --top-p 0.95 --top-k 20 --min-p 0 --repeat-p enalty 1.1 --draft-max 10 & # $ llama-server --ctx-size 131072 --model ~/.lmstudio/models/x0000001/Qwen3-30B -A6B-16-Extreme-128k-context-Q6_K-GGUF/qwen3-30b-a6b-16-extreme-128k-context-q6_ k.gguf --top-p 0.95 --top-k 100 --min-p 0.05 --repeat-penalty 64 --override-kv q wen3moe.expert_used_count=int:16 & # 2b) Start aider in your working directory: $ aider --openai-api-base http://lo calhost:8080 --openai-api-key dummy-key --model qwen3-30b-a3b --model-metadata-f ile qwen3-30b-a3b.json # 2c) The config file is: $ cat qwen3-30b-a3b."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ng directory: $ aider --openai-api-base http://lo calhost:8080 --openai-api-key dummy-key --model qwen3-30b-a3b --model-metadata-f ile qwen3-30b-a3b.json # 2c) The config file is: $ cat qwen3-30b-a3b.json # { # \"model\": \"qwen3-30b-a3b\", # \"max_input_tokens\": 32768, # \"max_output_tokens\": 8192, # \"input_cost_per_token\": 0.0, # \"output_cost_per_token\": 0.0, # \"show_warnings\": true # } # https://www.reddit.com/r/LocalLLaMA/comments/1kmlu2y/comment/mser4ff/ # # model_settings-yaml: # # - name: openai/Qwen3-30B-A3B-UD-Q4_K_XL.gguf # edit_format: whole # use_repo_map: true # use_temperature: 0.7 # streaming: false # system_prompt_prefix: \"/no_think\" # extra_params: # top_p: 0.8 # max_tokens: 24000 # top_k: 20 # min_p: 0.0 # temperature: 0.7 # enable_thinking: false # LJ Sat 12 Jul 2025 11:24:17 BST + LJ Sat 12 Jul 2025 11:37:41 BST � mistralai/Devstral-Small-2507 Make Devstral-Small-2507 work with Aider localhost mistralai/Devstral-Small-2507 https://huggingface.co/mistralai/Devstral-Small-2507 ./llama-server -m mistralai/Devstral-Small-2507_gguf/Devstral-Small-2507-Q4_K_M. gguf -c 0 # -c configure the context size, 0 means model's default, here 128k. LJ Sat 12 Jul 2025 11:37:41 BST + LJ Sat 12 Jul 2025 11:37:51 BST � unsloth/Kimi-Dev-72B-GGUF Make Kimi-Dev-72B work with Aider localhost Kim-Dev https://github.com/MoonshotAI/Kimi-Dev moonshotai/Kimi-Dev-72B https://huggingface.co/moonshotai/Kimi-Dev-72B unsloth/Kimi-Dev-72B-GGUF https://huggingface."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "i-Dev-72B work with Aider localhost Kim-Dev https://github.com/MoonshotAI/Kimi-Dev moonshotai/Kimi-Dev-72B https://huggingface.co/moonshotai/Kimi-Dev-72B unsloth/Kimi-Dev-72B-GGUF https://huggingface.co/unsloth/Kimi-Dev-72B-GGUF bullerwins/Kimi-Dev-72B-GGUF https://huggingface.co/bullerwins/Kimi-Dev-72B-GGUF Kimi-Dev-72B LOCAL Test (RooCode + LM Studio Coding & Debugging) https://www.youtube.com/watch?v=KUghIvUdvu4 LJ Sat 12 Jul 2025 11:37:51 BST + https://www.reddit.com/r/LocalLLaMA/comments/1jtwcdo/guide_for_quickly_ setting_up_aider_qwq_and_qwen/ https://www.reddit.com/r/LocalLLaMA/comments/1jtwcdo/guide_for_quickly_setting_u p_aider_qwq_and_qwen/ Go to LocalLLaMA r/LocalLLaMA � 3 mo. ago No-Statement-0001 llama.cpp Guide for quickly setting up aider, QwQ and Qwen Coder Tutorial | Guide I wrote a guide for setting up a a 100% local coding co-pilot setup with QwQ as as an architect model and qwen Coder as the editor. The focus for the guide is o n the trickiest part which is configuring everything to work together. This guide uses QwQ and qwen Coder 32B as those can fit in a 24GB GPU. This guid e uses llama-swap so QwQ and Qwen Coder are swapped in and our during aider's ar chitect or editing phases. The guide also has settings for dual 24GB GPUs where both models can be used without swapping. The original version is here: https://github.com/mostlygeek/llama-swap/tree/main /examples/aider-qwq-coder."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "e guide also has settings for dual 24GB GPUs where both models can be used without swapping. The original version is here: https://github.com/mostlygeek/llama-swap/tree/main /examples/aider-qwq-coder. Here's what you you need: aider - installation docs llama-server - download latest release llama-swap - download latest release QwQ 32B and Qwen Coder 2.5 32B models 24GB VRAM video card Running aider The goal is getting this command line to work: aider --architect \\ --no-show-model-warnings \\ --model openai/QwQ \\ --editor-model openai/qwen-coder-32B \\ --model-settings-file aider.model.settings.yml \\ --openai-api-key \"sk-na\" \\ --openai-api-base \"http://10.0.1.24:8080/v1\" \\ Set --openai-api-base to the IP and port where your llama-swap is running. Create an aider model settings file # aider.model.settings.yml # # !!! important: model names must match llama-swap configuration names !!! # - name: \"openai/QwQ\" edit_format: diff extra_params: max_tokens: 16384 top_p: 0.95 top_k: 40 presence_penalty: 0.1 repetition_penalty: 1 num_ctx: 16384 use_temperature: 0.6 reasoning_tag: think weak_model_name: \"openai/qwen-coder-32B\" editor_model_name: \"openai/qwen-coder-32B\" - name: \"openai/qwen-coder-32B\" edit_format: diff extra_params: max_tokens: 16384 top_p: 0.8 top_k: 20 repetition_penalty: 1.05 use_temperature: 0.6 reasoning_tag: think editor_edit_format: editor-diff editor_model_name: \"openai/qwen-coder-32B\" llama-swap configuration # config."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "84 top_p: 0.8 top_k: 20 repetition_penalty: 1.05 use_temperature: 0.6 reasoning_tag: think editor_edit_format: editor-diff editor_model_name: \"openai/qwen-coder-32B\" llama-swap configuration # config.yaml # The parameters are tweaked to fit model+context into 24GB VRAM GPUs models: \"qwen-coder-32B\": proxy: \"http://127.0.0.1:8999\" cmd: > /path/to/llama-server --host 127.0.0.1 --port 8999 --flash-attn --slots --ctx-size 16000 --cache-type-k q8_0 --cache-type-v q8_0 -ngl 99 --model /path/to/Qwen2.5-Coder-32B-Instruct-Q4_K_M.gguf \"QwQ\": proxy: \"http://127.0.0.1:9503\" cmd: > /path/to/llama-server --host 127.0.0.1 --port 9503 --flash-attn --metrics--slots --cache-type-k q8_0 --cache-type-v q8_0 --ctx-size 32000 --samplers \"top_k;top_p;min_p;temperature;dry;typ_p;xtc\" --temp 0.6 --repeat-penalty 1.1 --dry-multiplier 0.5 --min-p 0.01 --top-k 40 --top-p 0.95 -ngl 99 --model /mnt/nvme/models/bartowski/Qwen_QwQ-32B-Q4_K_M.gguf Advanced, Dual GPU Configuration If you have dual 24GB GPUs you can use llama-swap profiles to avoid swapping bet ween QwQ and Qwen Coder. In llama-swap's configuration file: add a profiles section with aider as the profile name using the env field to specify the GPU IDs for each model # config.yaml # Add a profile for aider profiles: aider: - qwen-coder-32B - QwQ models: \"qwen-coder-32B\": # manually set the GPU to run on env: - \"CUDA_VISIBLE_DEVICES=0\" proxy: \"http://127.0.0.1:8999\" cmd: /path/to/llama-server ..."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "r aider profiles: aider: - qwen-coder-32B - QwQ models: \"qwen-coder-32B\": # manually set the GPU to run on env: - \"CUDA_VISIBLE_DEVICES=0\" proxy: \"http://127.0.0.1:8999\" cmd: /path/to/llama-server ... \"QwQ\": # manually set the GPU to run on env: - \"CUDA_VISIBLE_DEVICES=1\" proxy: \"http://127.0.0.1:9503\" cmd: /path/to/llama-server ... Append the profile tag, aider:, to the model names in the model settings file # aider.model.settings.yml - name: \"openai/aider:QwQ\" weak_model_name: \"openai/aider:qwen-coder-32B-aider\" editor_model_name: \"openai/aider:qwen-coder-32B-aider\" - name: \"openai/aider:qwen-coder-32B\" editor_model_name: \"openai/aider:qwen-coder-32B-aider\" Run aider with: $ aider --architect \\ --no-show-model-warnings \\ --model openai/aider:QwQ \\ --editor-model openai/aider:qwen-coder-32B \\ --config aider.conf.yml \\ --model-settings-file aider.model.settings.yml --openai-api-key \"sk-na\" \\ --openai-api-base \"http://10.0.1.24:8080/v1\" + https://www.reddit.com/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3cod er_or_any_other_llm_with_claude/ https://www.reddit.com/r/LocalLLaMA/comments/1m7ci3s/howto_use_qwen3coder_or_any _other_llm_with_claude/ Reformatted for Old Reddit users :) Here's a simple way for Claude Code users to switch from the costly Claude model s to the newly released SOTA open-source/weights coding model, Qwen3-Coder, via OpenRouter using LiteLLM on your local machine. This process is quite universal and can be easily adapted to suit your needs."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "the newly released SOTA open-source/weights coding model, Qwen3-Coder, via OpenRouter using LiteLLM on your local machine. This process is quite universal and can be easily adapted to suit your needs. Fe el free to explore other models (including local ones) as well as different prov iders and coding agents. I'm sharing what works for me. This guide is set up so you can just copy and pas te the commands into your terminal. 1. Clone the official LiteLLM repo: git clone https://github.com/BerriAI/litellm.git cd litellm 2. Create an .env file with your OpenRouter API key (make sure to insert your ow n API key!): cat <<\\EOF >.env LITELLM_MASTER_KEY = \"sk-1234\" # OpenRouter OPENROUTER_API_KEY = \"sk-or-v1-�\" # � EOF 3. Create a config.yaml file that replaces Anthropic models with Qwen3-Coder (wi th all the recommended parameters): cat <<\\EOF >config.yaml model_list: - model_name: \"anthropic/*\" litellm_params: model: \"openrouter/qwen/qwen3-coder\" # Qwen/Qwen3-Coder-480B-A35B-Instruct max_tokens: 65536 repetition_penalty: 1.05 temperature: 0.7 top_k: 20 top_p: 0.8 EOF 4. Create a docker-compose.yml file that loads config.yaml (it's easier to just create a finished one with all the required changes than to edit the original fi le): cat <<\\EOF >docker-compose.yml services: litellm: build: context: . args: target: runtime ############################################################################ command: - \"--config=/app/config."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "cat <<\\EOF >docker-compose.yml services: litellm: build: context: . args: target: runtime ############################################################################ command: - \"--config=/app/config.yaml\" container_name: litellm hostname: litellm image: ghcr.io/berriai/litellm:main-stable restart: unless-stopped volumes: - ./config.yaml:/app/config.yaml ############################################################################ ports: - \"4000:4000\" # Map the container port to the host, change the host port i f necessary environment: DATABASE_URL: \"postgresql://llmproxy:dbpassword9090@db:5432/litellm\" STORE_MODEL_IN_DB: \"True\" # allows adding models to proxy via UI env_file: - .env # Load local .env file depends_on: - db # Indicates that this service depends on the 'db' service, ensuring ' db' starts first healthcheck: # Defines the health check configuration for the container test: [ \"CMD-SHELL\", \"wget --no-verbose --tries=1 http://localhost:4000/he alth/liveliness || exit 1\" ] # Command to execute for health check interval: 30s # Perform health check every 30 seconds timeout: 10s # Health check command times out after 10 seconds retries: 3 # Retry up to 3 times if health check fails start_period: 40s # Wait 40 seconds after container start before beginning health checks db: image: postgres:16 restart: always container_name: litellm_db environment: POSTGRES_DB: litellm POSTGRES_USER: llmproxy POSTGRES_PASSWORD: dbpassword9090 ports: - \"5432:5432\" volumes: - postgres_data:/v"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "age: postgres:16 restart: always container_name: litellm_db environment: POSTGRES_DB: litellm POSTGRES_USER: llmproxy POSTGRES_PASSWORD: dbpassword9090 ports: - \"5432:5432\" volumes: - postgres_data:/var/lib/postgresql/data # Persists Postgres data across c ontainer restarts healthcheck: test: [\"CMD-SHELL\", \"pg_isready -d litellm -U llmproxy\"] interval: 1s timeout: 5s retries: 10 volumes: postgres_data: name: litellm_postgres_data # Named volume for Postgres data persistence EOF 5. Build and run LiteLLM (this is important, as some required fixes are not yet in the published image as of 2025-07-23): docker compose up -d --build 6. Export environment variables that make Claude Code use Qwen3-Coder via LiteLL M (remember to execute this before starting Claude Code or include it in your sh ell profile (.zshrc, .bashrc, etc.) for persistence): export ANTHROPIC_AUTH_TOKEN=sk-1234 export ANTHROPIC_BASE_URL=http://localhost:4000 export ANTHROPIC_MODEL=openrouter/qwen/qwen3-coder export ANTHROPIC_SMALL_FAST_MODEL=openrouter/qwen/qwen3-coder export CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1 # Optional: Disables telemetry , error reporting, and auto-updates 7. Start Claude Code and it'll use Qwen3-Coder via OpenRouter instead of the exp ensive Claude models (you can check with the /model command that it's using a cu stom model): claude 8. Optional: Add an alias to your shell profile (.zshrc, .bashrc, etc.) to make it easier to use (e.g."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ive Claude models (you can check with the /model command that it's using a cu stom model): claude 8. Optional: Add an alias to your shell profile (.zshrc, .bashrc, etc.) to make it easier to use (e.g. qlaude for \"Claude with Qwen\"): alias qlaude='ANTHROPIC_AUTH_TOKEN=sk-1234 ANTHROPIC_BASE_URL=http://localhost:4 000 ANTHROPIC_MODEL=openrouter/qwen/qwen3-coder ANTHROPIC_SMALL_FAST_MODEL=openr outer/qwen/qwen3-coder claude' Have fun and happy coding! PS: There are other ways to do this using dedicated Claude Code proxies, of whic h there are quite a few on GitHub. Before implementing this with LiteLLM, I revi ewed some of them, but they all had issues, such as not handling the recommended inference parameters. I prefer using established projects with a solid track re cord and a large user base, which is why I chose LiteLLM. Open Source offers man y options, so feel free to explore other projects and find what works best for y ou. + https://gist.github.com/ivanfioravanti/44b4284be930b3c340cc1696d60c6143 https://gist.github.com/ivanfioravanti/44b4284be930b3c340cc1696d60c6143 All gists Back to GitHub @ivanfioravanti ivanfioravanti/mlx_memory.sh Created 6 months ago � Report abuse Clone this repository at https://gist.github.com/ivanfioravanti/44b4284be930b3c3 40cc1696d60c6143.js Script to set MLX memory limits mlx_memory."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "vanti/mlx_memory.sh Created 6 months ago � Report abuse Clone this repository at https://gist.github.com/ivanfioravanti/44b4284be930b3c3 40cc1696d60c6143.js Script to set MLX memory limits mlx_memory.sh #!/usr/bin/env bash # Default values for percentages DEFAULT_WIRED_LIMIT_PERCENT=85 DEFAULT_WIRED_LWM_PERCENT=75 # Read input parameters or use default values WIRED_LIMIT_PERCENT=${1:-$DEFAULT_WIRED_LIMIT_PERCENT} WIRED_LWM_PERCENT=${2:-$DEFAULT_WIRED_LWM_PERCENT} # Validate inputs are within 0-100 if [[ $WIRED_LIMIT_PERCENT -lt 0 || $WIRED_LIMIT_PERCENT -gt 100 || $WIRED_LWM_P ERCENT -lt 0 || $WIRED_LWM_PERCENT -gt 100 ]]; then echo \"Error: Percentages must be between 0 and 100.\" exit 1 fi # Get the total memory in MB TOTAL_MEM_MB=$(($(sysctl -n hw.memsize) / 1024 / 1024)) # Calculate the memory limits WIRED_LIMIT_MB=$(($TOTAL_MEM_MB * $WIRED_LIMIT_PERCENT / 100)) WIRED_LWM_MB=$(($TOTAL_MEM_MB * $WIRED_LWM_PERCENT / 100)) # Display the calculated values echo \"Total memory: $TOTAL_MEM_MB MB\" echo \"Maximum limit (iogpu.wired_limit_mb): $WIRED_LIMIT_MB MB ($WIRED_LIMIT_PER CENT%)\" echo \"Lower bound (iogpu.wired_lwm_mb): $WIRED_LWM_MB MB ($WIRED_LWM_PERCENT%)\" # Apply the values with sysctl sudo sysctl -w iogpu.wired_limit_mb=$WIRED_LIMIT_MB sudo sysctl -w iogpu.wired_lwm_mb=$WIRED_LWM_MB @ivanfioravanti Author ivanfioravanti commented on Jan 4 � Note: use at your own risk! I push it even more when needed. Usage Examples: Use default values (85 and 75): ./mlx_memory."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "m_mb=$WIRED_LWM_MB @ivanfioravanti Author ivanfioravanti commented on Jan 4 � Note: use at your own risk! I push it even more when needed. Usage Examples: Use default values (85 and 75): ./mlx_memory.sh Provide custom percentages (e.g., 90 and 80): ./mlx_memory.sh 90 80 + LJ Mon 28 Jul 2025 08:41:59 BST � https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507 https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507 Best Practices To achieve optimal performance, we recommend the following settings: Sampling Parameters: We suggest using Temperature=0.6, TopP=0.95, TopK=20, and MinP=0. For supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasio nally result in language mixing and a slight decrease in model performance. Adequate Output Length: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length t o 81,920 tokens. This provides the model with sufficient space to generate detai led and comprehensive responses, thereby enhancing its overall performance. Standardize Output Format: We recommend using prompts to standardize model outpu ts when benchmarking. Math Problems: Include \"Please reason step by step, and put your final answer wi thin \\boxed{}.\" in the prompt."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "tput Format: We recommend using prompts to standardize model outpu ts when benchmarking. Math Problems: Include \"Please reason step by step, and put your final answer wi thin \\boxed{}.\" in the prompt. Multiple-Choice Questions: Add the following JSON structure to the prompt to sta ndardize responses: \"Please show your choice in the answer field with only the c hoice letter, e.g., \"answer\": \"C\".\" No Thinking Content in History: In multi-turn conversations, the historical mode l output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it i s up to the developers to ensure that the best practice is followed. https://huggingface.co/Intel/Qwen3-235B-A22B-Thinking-2507-gguf-q2ks-mixed-AutoR ound (torch311) ljubomir@macbook2(:):~/llama.cpp$ l models/Qwen3-235B-A22B-Thinking-2 507-128x10B-Q2_K_S-0000* -rw-r--r--@ 1 ljubomir staff 46G 27 Jul 19:19 models/Qwen3-235B-A22B-Thinkin g-2507-128x10B-Q2_K_S-00001-of-00002.gguf -rw-r--r--@ 1 ljubomir staff 28G 27 Jul 21:16 models/Qwen3-235B-A22B-Thinkin g-2507-128x10B-Q2_K_S-00002-of-00002.gguf ljubomir@gigul2(422663.llama.cpp:0):~/llama.cpp$ git pull mviv build{,.1} unset CC CXX unset LDFLAGS unset CPPFLAGS env |egrep 'CC|CXX|FLAGS' brew install libomp cmake . -B ./build cmake --build build --config Release -j sudo sysctl iogpu."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "lama.cpp$ git pull mviv build{,.1} unset CC CXX unset LDFLAGS unset CPPFLAGS env |egrep 'CC|CXX|FLAGS' brew install libomp cmake . -B ./build cmake --build build --config Release -j sudo sysctl iogpu.wired_limit_mb=88000 build/bin/llama-server --model models/Qwen3-235B-A22B-Thinking-2507-128x10B-Q2_K _S-00001-of-00002.gguf --temp 0.6 --top_k 20 --top_p 0.95 --min_p 0 --ctx-size 3 2768 --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & # access on http://127.0.0.1:8080 LJ Mon 28 Jul 2025 08:41:59 BST + LJ Sat 2 Aug 2025 06:22:24 BST � https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-1M-GGUF https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-1M-GGUF Best Practices To achieve optimal performance, we recommend the following settings: Sampling Parameters: We suggest using temperature=0.7, top_p=0.8, top_k=20, repetition_penalty=1.05. Adequate Output Length: We recommend using an output length of 65,536 tokens for most queries, which is adequate for instruct models. https://docs.unsloth.ai/basics/qwen3-coder-how-to-run-locally#how-to-fit-long-co ntext-256k-to-1m �How to fit long context (256K to 1M) To fit longer context, you can use KV cache quantization to quantize the K and V caches to lower bits. This can also increase generation speed due to reduced RA M / VRAM data movement. The allowed options for K quantization (default is f16) include the below."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "quantize the K and V caches to lower bits. This can also increase generation speed due to reduced RA M / VRAM data movement. The allowed options for K quantization (default is f16) include the below. --cache-type-k f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1 You should use the _1 variants for somewhat increased accuracy, albeit it's slig htly slower. For eg q4_1, q5_1 You can also quantize the V cache, but you will need to compile llama.cpp with F lash Attention support via -DGGML_CUDA_FA_ALL_QUANTS=ON, and use --flash-attn to enable it. We also uploaded 1 million context length GGUFs via YaRN scaling here. (torch311) ljubomir@macbook2(:):~/llama.cpp$ l models/Qwen3-Coder-30B-A3B-Instru ct-1M-IQ4_NL.gguf -rw-r--r--@ 1 ljubomir staff 16G 31 Jul 19:48 models/Qwen3-Coder-30B-A3B-Ins truct-1M-IQ4_NL.gguf ljubomir@gigul2(422663.llama.cpp:0):~/llama.cpp$ git pull mviv build{,.1} unset CC CXX unset LDFLAGS unset CPPFLAGS env |egrep 'CC|CXX|FLAGS' brew install libomp cmake . -B ./build cmake --build build --config Release -j sudo sysctl iogpu.wired_limit_mb=88000 # extend context (262144) 256K->1M (1048576), flash attention cached; access on http://127.0.0.1:8080 build/bin/llama-server --model models/Qwen3-Coder-30B-A3B-Instruct-1M-IQ4_NL.ggu f --temp 0.7 --top_k 20 --top_p 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "(262144) 256K->1M (1048576), flash attention cached; access on http://127.0.0.1:8080 build/bin/llama-server --model models/Qwen3-Coder-30B-A3B-Instruct-1M-IQ4_NL.ggu f --temp 0.7 --top_k 20 --top_p 0.8 --min_p 0 --ctx-size 1048576 --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 262144 --flash-attn --cache-type-k q8_0 --ca che-type-v q8_0 --jinja & LJ Sat 2 Aug 2025 06:22:24 BST + https://huggingface.co/mradermacher/XBai-o4-GGUF MetaStoneTec/XBai-o4 https://huggingface.co/MetaStoneTec/XBai-o4 https://huggingface.co/mradermacher/XBai-o4-GGUF https://x.com/JacksonAtkinsX/status/1951133405271257355 https://github.com/MetaStone-AI/XBai-o4/blob/main/test/task.py class Infer_Task: def __init__(self, model_dir, score_api_url, response_api_url, branch=3, tem perature=0.7, max_tokens=1024*32): sudo sysctl iogpu.wired_limit_mb=88000 # flash attention cached; access on http://127.0.0.1:8081 build/bin/llama-server --port 8081 --model models/XBai-o4.Q6_K.gguf --temp 0.7 - -ctx-size 65536 --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & # extend context (262144) 256K->512K (524288), flash attention cached; access on http://127.0.0.1:8080 build/bin/llama-server --model models/Qwen3-Coder-30B-A3B-Instruct-1M-IQ4_NL.ggu f --temp 0.7 --top_k 20 --top_p 0.8 --min_p 0 --ctx-size 525288 --rope-scaling y arn --rope-scale 4 --yarn-orig-ctx 262144 --flash-attn --cache-type-k q8_0 --cac he-type-v q8_0 --jinja & Wang Magic @WangMagic_ Thanks for your interest in our work."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "-ctx-size 525288 --rope-scaling y arn --rope-scale 4 --yarn-orig-ctx 262144 --flash-attn --cache-type-k q8_0 --cac he-type-v q8_0 --jinja & Wang Magic @WangMagic_ Thanks for your interest in our work. For an optimal experience during inference , we recommend setting the temperature to 0.6, top_p to 0.95, and the output tok en limit to 32768 for better performance (which can also be extended to 64k with yarn). # flash attention cached; access on http://127.0.0.1:8081 build/bin/llama-server --port 8081 --model models/XBai-o4.Q6_K.gguf --temp 0.6 - -top_p 0.95 --ctx-size 32768 --flash-attn --cache-type-k q8_0 --cache-type-v q8_ 0 --jinja & # extend context (262144) 256K->512K (524288), flash attention cached; access on http://127.0.0.1:8080 build/bin/llama-server --port 8080 --model models/Qwen3-Coder-30B-A3B-Instruct-1 M-IQ4_NL.gguf --temp 0.7 --top_k 20 --top_p 0.8 --min_p 0 --ctx-size 525288 --ro pe-scaling yarn --rope-scale 4 --yarn-orig-ctx 262144 --flash-attn --cache-type- k q8_0 --cache-type-v q8_0 --jinja & + (torch311) ljubomir@macbook2(:):~/llama.cpp$ build/bin/llama-server --help (torch311) ljubomir@macbook2(:):~/llama.cpp$ build/bin/llama-server --help ----- common params ----- -h, --help, --usage print usage and exit --version show version and build info --completion-bash print source-able bash completion script for llama."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "d/bin/llama-server --help ----- common params ----- -h, --help, --usage print usage and exit --version show version and build info --completion-bash print source-able bash completion script for llama.cpp --verbose-prompt print a verbose prompt before generation (default: false) -t, --threads N number of threads to use during generati on (default: -1) (env: LLAMA_ARG_THREADS) -tb, --threads-batch N number of threads to use during batch an d prompt processing (default: same as --threads) -C, --cpu-mask M CPU affinity mask: arbitrarily long hex. Complements cpu-range (default: \"\") -Cr, --cpu-range lo-hi range of CPUs for affinity. Complements --cpu-mask --cpu-strict <0|1> use strict CPU placement (default: 0) --prio N set process/thread priority : low(-1), n ormal(0), medium(1), high(2), realtime(3) (default: 0) --poll <0...100> use polling level to wait for work (0 - no polling, default: 50) -Cb, --cpu-mask-batch M CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch (default: same as --cpu-mask) -Crb, --cpu-range-batch lo-hi ranges of CPUs for affinity. Complements --cpu-mask-batch --cpu-strict-batch <0|1> use strict CPU placement (default: same as --cpu-strict) --prio-batch N set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime (default: 0) --poll-batch <0|1> use polling to wait for work (default: s ame as --poll) -c, --ctx-size N size of the prompt context (default: 409 6, 0 = loaded from model) (env: LLAMA_ARG_CTX_SIZE) -n, --predict, --n-pre"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "batch <0|1> use polling to wait for work (default: s ame as --poll) -c, --ctx-size N size of the prompt context (default: 409 6, 0 = loaded from model) (env: LLAMA_ARG_CTX_SIZE) -n, --predict, --n-predict N number of tokens to predict (default: -1 , -1 = infinity) (env: LLAMA_ARG_N_PREDICT) -b, --batch-size N logical maximum batch size (default: 204 8) (env: LLAMA_ARG_BATCH) -ub, --ubatch-size N physical maximum batch size (default: 51 2) (env: LLAMA_ARG_UBATCH) --keep N number of tokens to keep from the initia l prompt (default: 0, -1 = all) --swa-full use full-size SWA cache (default: false) [(more info)](https://github.com/ggml-org/llama .cpp/pull/13194#issuecomment-2868343055) (env: LLAMA_ARG_SWA_FULL) --kv-unified, -kvu use single unified KV buffer for the KV cache of all sequences (default: false) [(more info)](https://github.com/ggml-or g/llama.cpp/pull/14363) (env: LLAMA_ARG_KV_SPLIT) -fa, --flash-attn enable Flash Attention (default: disable d) (env: LLAMA_ARG_FLASH_ATTN) --no-perf disable internal libllama performance ti mings (default: false) (env: LLAMA_ARG_NO_PERF) -e, --escape process escapes sequences (\\n, \\r, \\t, \\ ', \\\", \\\\) (default: true) --no-escape do not process escape sequences --rope-scaling {none,linear,yarn} RoPE frequency scaling method, defaults to linear unless specified by the model (env: LLAMA_ARG_ROPE_SCALING_TYPE) --rope-scale N RoPE context scaling factor, expands con text by a factor of N (env: LLAMA_ARG_ROPE_SCALE) --rope-freq-base N RoPE b"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ear unless specified by the model (env: LLAMA_ARG_ROPE_SCALING_TYPE) --rope-scale N RoPE context scaling factor, expands con text by a factor of N (env: LLAMA_ARG_ROPE_SCALE) --rope-freq-base N RoPE base frequency, used by NTK-aware s caling (default: loaded from model) (env: LLAMA_ARG_ROPE_FREQ_BASE) --rope-freq-scale N RoPE frequency scaling factor, expands c ontext by a factor of 1/N (env: LLAMA_ARG_ROPE_FREQ_SCALE) --yarn-orig-ctx N YaRN: original context size of model (de fault: 0 = model training context size) (env: LLAMA_ARG_YARN_ORIG_CTX) --yarn-ext-factor N YaRN: extrapolation mix factor (default: -1.0, 0.0 = full interpolation) (env: LLAMA_ARG_YARN_EXT_FACTOR) --yarn-attn-factor N YaRN: scale sqrt(t) or attention magnitu de (default: 1.0) (env: LLAMA_ARG_YARN_ATTN_FACTOR) --yarn-beta-slow N YaRN: high correction dim or alpha (defa ult: 1.0) (env: LLAMA_ARG_YARN_BETA_SLOW) --yarn-beta-fast N YaRN: low correction dim or beta (defaul t: 32.0) (env: LLAMA_ARG_YARN_BETA_FAST) -nkvo, --no-kv-offload disable KV offload (env: LLAMA_ARG_NO_KV_OFFLOAD) -nr, --no-repack disable weight repacking (env: LLAMA_ARG_NO_REPACK) -ctk, --cache-type-k TYPE KV cache data type for K allowed values: f32, f16, bf16, q8_0, q4 _0, q4_1, iq4_nl, q5_0, q5_1 (default: f16) (env: LLAMA_ARG_CACHE_TYPE_K) -ctv, --cache-type-v TYPE KV cache data type for V allowed values: f32, f16, bf16, q8_0, q4 _0, q4_1, iq4_nl, q5_0, q5_1 (default: f16) (env: LLAMA_ARG_CACHE_TYPE_V) -dt, --defrag-thold N KV cache"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "_K) -ctv, --cache-type-v TYPE KV cache data type for V allowed values: f32, f16, bf16, q8_0, q4 _0, q4_1, iq4_nl, q5_0, q5_1 (default: f16) (env: LLAMA_ARG_CACHE_TYPE_V) -dt, --defrag-thold N KV cache defragmentation threshold (defa ult: 0.1, < 0 - disabled) (env: LLAMA_ARG_DEFRAG_THOLD) -np, --parallel N number of parallel sequences to decode ( default: 1) (env: LLAMA_ARG_N_PARALLEL) --mlock force system to keep model in RAM rather than swapping or compressing (env: LLAMA_ARG_MLOCK) --no-mmap do not memory-map model (slower load but may reduce pageouts if not using mlock) (env: LLAMA_ARG_NO_MMAP) --numa TYPE attempt optimizations that help on some NUMA systems - distribute: spread execution evenly ov er all nodes - isolate: only spawn threads on CPUs on the node that execution started on - numactl: use the CPU map provided by n umactl if run without this previously, it is re commended to drop the system page cache before using this see https://github.com/ggml-org/llama.cp p/issues/1437 (env: LLAMA_ARG_NUMA) -dev, --device <dev1,dev2,..> comma-separated list of devices to use f or offloading (none = don't offload) use --list-devices to see a list of avai lable devices (env: LLAMA_ARG_DEVICE) --list-devices print list of available devices and exit --override-tensor, -ot <tensor name pattern>=<buffer type>,..."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "d) use --list-devices to see a list of avai lable devices (env: LLAMA_ARG_DEVICE) --list-devices print list of available devices and exit --override-tensor, -ot <tensor name pattern>=<buffer type>,... override tensor buffer type --cpu-moe use CPU for Mixture of Experts (MoE) wei ghts (env: LLAMA_ARG_CPU_MOE) -ngl, --gpu-layers, --n-gpu-layers N number of layers to store in VRAM (env: LLAMA_ARG_N_GPU_LAYERS) -sm, --split-mode {none,layer,row} how to split the model across multiple G PUs, one of: - none: use one GPU only - layer (default): split layers and KV a cross GPUs - row: split rows across GPUs (env: LLAMA_ARG_SPLIT_MODE) -ts, --tensor-split N0,N1,N2,... fraction of the model to offload to each GPU, comma-separated list of proportions, e.g. 3,1 (env: LLAMA_ARG_TENSOR_SPLIT) -mg, --main-gpu INDEX the GPU to use for the model (with split -mode = none), or for intermediate results and KV (with split- mode = row) (default: 0) (env: LLAMA_ARG_MAIN_GPU) --check-tensors check model tensor data for invalid valu es (default: false) --override-kv KEY=TYPE:VALUE advanced option to override model metada ta by key. may be specified multiple times. types: int, float, bool, str. example: - -override-kv tokenizer.ggml."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "(default: false) --override-kv KEY=TYPE:VALUE advanced option to override model metada ta by key. may be specified multiple times. types: int, float, bool, str. example: - -override-kv tokenizer.ggml.add_bos_token=bool:false --no-op-offload disable offloading host tensor operation s to device (default: false) --lora FNAME path to LoRA adapter (can be repeated to use multiple adapters) --lora-scaled FNAME SCALE path to LoRA adapter with user defined s caling (can be repeated to use multiple adapters) --control-vector FNAME add a control vector note: this argument can be repeated to a dd multiple control vectors --control-vector-scaled FNAME SCALE add a control vector with user defined s caling SCALE note: this argument can be repeated to a dd multiple scaled control vectors --control-vector-layer-range START END layer range to apply the control vector( s) to, start and end inclusive -m, --model FNAME model path (default: `models/$filename` with filename from `--hf-file` or `--model-url` if set, otherwise model s/7B/ggml-model-f16.gguf) (env: LLAMA_ARG_MODEL) -mu, --model-url MODEL_URL model download url (default: unused) (env: LLAMA_ARG_MODEL_URL) -hf, -hfr, --hf-repo <user>/<model>[:quant] Hugging Face model repository; quant is optional, case-insensitive, default to Q4_K_M, or falls back to the first file in the repo if Q4_K_M doesn't exist. mmproj is also downloaded automatically if available."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ace model repository; quant is optional, case-insensitive, default to Q4_K_M, or falls back to the first file in the repo if Q4_K_M doesn't exist. mmproj is also downloaded automatically if available. to disable, add --no-mmproj example: unsloth/phi-4-GGUF:q4_k_m (default: unused) (env: LLAMA_ARG_HF_REPO) -hfd, -hfrd, --hf-repo-draft <user>/<model>[:quant] Same as --hf-repo, but for the draft mod el (default: unused) (env: LLAMA_ARG_HFD_REPO) -hff, --hf-file FILE Hugging Face model file. If specified, i t will override the quant in --hf-repo (default: unused) (env: LLAMA_ARG_HF_FILE) -hfv, -hfrv, --hf-repo-v <user>/<model>[:quant] Hugging Face model repository for the vo coder model (default: unused) (env: LLAMA_ARG_HF_REPO_V) -hffv, --hf-file-v FILE Hugging Face model file for the vocoder model (default: unused) (env: LLAMA_ARG_HF_FILE_V) -hft, --hf-token TOKEN Hugging Face access token (default: valu e from HF_TOKEN environment variable) (env: HF_TOKEN) --log-disable Log disable --log-file FNAME Log to file --log-colors Enable colored logging (env: LLAMA_LOG_COLORS) -v, --verbose, --log-verbose Set verbosity level to infinity (i.e. lo g all messages, useful for debugging) --offline Offline mode: forces use of cache, preve nts network access (env: LLAMA_OFFLINE) -lv, --verbosity, --log-verbosity N Set the verbosity threshold. Messages wi th a higher verbosity will be ignored."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ine Offline mode: forces use of cache, preve nts network access (env: LLAMA_OFFLINE) -lv, --verbosity, --log-verbosity N Set the verbosity threshold. Messages wi th a higher verbosity will be ignored. (env: LLAMA_LOG_VERBOSITY) --log-prefix Enable prefix in log messages (env: LLAMA_LOG_PREFIX) --log-timestamps Enable timestamps in log messages (env: LLAMA_LOG_TIMESTAMPS) -ctkd, --cache-type-k-draft TYPE KV cache data type for K for the draft m odel allowed values: f32, f16, bf16, q8_0, q4 _0, q4_1, iq4_nl, q5_0, q5_1 (default: f16) (env: LLAMA_ARG_CACHE_TYPE_K_DRAFT) -ctvd, --cache-type-v-draft TYPE KV cache data type for V for the draft m odel allowed values: f32, f16, bf16, q8_0, q4 _0, q4_1, iq4_nl, q5_0, q5_1 (default: f16) (env: LLAMA_ARG_CACHE_TYPE_V_DRAFT) ----- sampling params ----- --samplers SAMPLERS samplers that will be used for generatio n in the order, separated by ';' (default: penalties;dry;top_n_sigma;top_k;typ_p;to p_p;min_p;xtc;temperature) -s, --seed SEED RNG seed (default: -1, use random seed f or -1) --sampling-seq, --sampler-seq SEQUENCE simplified sequence for samplers that wi ll be used (default: edskypmxt) --ignore-eos ignore end of stream token and continue generating (implies --logit-bias EOS-inf) --temp N temperature (default: 0.8) --top-k N top-k sampling (default: 40, 0 = disable d) --top-p N top-p sampling (default: 0.9, 1.0 = disa bled) --min-p N min-p sampling (default: 0.1, 0.0 = disa bled) --xtc-probability N xtc probability (default: 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "sampling (default: 40, 0 = disable d) --top-p N top-p sampling (default: 0.9, 1.0 = disa bled) --min-p N min-p sampling (default: 0.1, 0.0 = disa bled) --xtc-probability N xtc probability (default: 0.0, 0.0 = dis abled) --xtc-threshold N xtc threshold (default: 0.1, 1.0 = disab led) --typical N locally typical sampling, parameter p (d efault: 1.0, 1.0 = disabled) --repeat-last-n N last n tokens to consider for penalize ( default: 64, 0 = disabled, -1 = ctx_size) --repeat-penalty N penalize repeat sequence of tokens (defa ult: 1.0, 1.0 = disabled) --presence-penalty N repeat alpha presence penalty (default: 0.0, 0.0 = disabled) --frequency-penalty N repeat alpha frequency penalty (default: 0.0, 0.0 = disabled) --dry-multiplier N set DRY sampling multiplier (default: 0. 0, 0.0 = disabled) --dry-base N set DRY sampling base value (default: 1. 75) --dry-allowed-length N set allowed length for DRY sampling (def ault: 2) --dry-penalty-last-n N set DRY penalty for the last n tokens (d efault: -1, 0 = disable, -1 = context size) --dry-sequence-breaker STRING add sequence breaker for DRY sampling, c learing out default breakers ('\\n', ':', '\"', '*') in the process; us e \"none\" to not use any sequence breakers --dynatemp-range N dynamic temperature range (default: 0.0, 0.0 = disabled) --dynatemp-exp N dynamic temperature exponent (default: 1 .0) --mirostat N use Mirostat sampling. Top K, Nucleus and Locally Typical sampl ers are ignored if used."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "range (default: 0.0, 0.0 = disabled) --dynatemp-exp N dynamic temperature exponent (default: 1 .0) --mirostat N use Mirostat sampling. Top K, Nucleus and Locally Typical sampl ers are ignored if used. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0) --mirostat-lr N Mirostat learning rate, parameter eta (d efault: 0.1) --mirostat-ent N Mirostat target entropy, parameter tau ( default: 5.0) -l, --logit-bias TOKEN_ID(+/-)BIAS modifies the likelihood of token appeari ng in the completion, i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello', or `--logit-bias 15043-1` to decrease li kelihood of token ' Hello' --grammar GRAMMAR BNF-like grammar to constrain generation s (see samples in grammars/ dir) (default: '') --grammar-file FNAME file to read grammar from -j, --json-schema SCHEMA JSON schema to constrain generations (ht tps://json-schema.org/), e.g. `{}` for any JSON object For schemas w/ external $refs, use --gra mmar + example/json_schema_to_grammar.py instea d -jf, --json-schema-file FILE File containing a JSON schema to constra in generations (https://json-schema.org/), e.g. `{}` fo r any JSON object For schemas w/ external $refs, use --gra mmar + example/json_schema_to_grammar."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "FILE File containing a JSON schema to constra in generations (https://json-schema.org/), e.g. `{}` fo r any JSON object For schemas w/ external $refs, use --gra mmar + example/json_schema_to_grammar.py instea d ----- example-specific params ----- --no-context-shift disables context shift on infinite text generation (default: disabled) (env: LLAMA_ARG_NO_CONTEXT_SHIFT) -r, --reverse-prompt PROMPT halt generation at PROMPT, return contro l in interactive mode -sp, --special special tokens output enabled (default: false) --no-warmup skip warming up the model with an empty run --spm-infill use Suffix/Prefix/Middle pattern for inf ill (instead of Prefix/Suffix/Middle) as some models pre fer this. (default: disabled) --pooling {none,mean,cls,last,rank} pooling type for embeddings, use model d efault if unspecified (env: LLAMA_ARG_POOLING) -cb, --cont-batching enable continuous batching (a.k.a dynami c batching) (default: enabled) (env: LLAMA_ARG_CONT_BATCHING) -nocb, --no-cont-batching disable continuous batching (env: LLAMA_ARG_NO_CONT_BATCHING) --mmproj FILE path to a multimodal projector file. see tools/mtmd/README.md note: if -hf is used, this argument can be omitted (env: LLAMA_ARG_MMPROJ) --mmproj-url URL URL to a multimodal projector file. see tools/mtmd/README."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ltimodal projector file. see tools/mtmd/README.md note: if -hf is used, this argument can be omitted (env: LLAMA_ARG_MMPROJ) --mmproj-url URL URL to a multimodal projector file. see tools/mtmd/README.md (env: LLAMA_ARG_MMPROJ_URL) --no-mmproj explicitly disable multimodal projector, useful when using -hf (env: LLAMA_ARG_NO_MMPROJ) --no-mmproj-offload do not offload multimodal projector to G PU (env: LLAMA_ARG_NO_MMPROJ_OFFLOAD) -a, --alias STRING set alias for model name (to be used by REST API) (env: LLAMA_ARG_ALIAS) --host HOST ip address to listen, or bind to an UNIX socket if the address ends with .sock (default: 127.0.0.1) (env: LLAMA_ARG_HOST) --port PORT port to listen (default: 8080) (env: LLAMA_ARG_PORT) --path PATH path to serve static files from (default : ) (env: LLAMA_ARG_STATIC_PATH) --api-prefix PREFIX prefix path the server serves from, with out the trailing slash (default: ) (env: LLAMA_ARG_API_PREFIX) --no-webui Disable the Web UI (default: enabled) (env: LLAMA_ARG_NO_WEBUI) --embedding, --embeddings restrict to only support embedding use c ase; use only with dedicated embedding models (default: disabled) (env: LLAMA_ARG_EMBEDDINGS) --reranking, --rerank enable reranking endpoint on server (def ault: disabled) (env: LLAMA_ARG_RERANKING) --api-key KEY API key to use for authentication (defau lt: none) (env: LLAMA_API_KEY) --api-key-file FNAME path to file containing API keys (defaul t: none) --ssl-key-file FNAME path to file a PEM-encoded SSL private k ey (en"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "use for authentication (defau lt: none) (env: LLAMA_API_KEY) --api-key-file FNAME path to file containing API keys (defaul t: none) --ssl-key-file FNAME path to file a PEM-encoded SSL private k ey (env: LLAMA_ARG_SSL_KEY_FILE) --ssl-cert-file FNAME path to file a PEM-encoded SSL certifica te (env: LLAMA_ARG_SSL_CERT_FILE) --chat-template-kwargs STRING sets additional params for the json temp late parser (env: LLAMA_CHAT_TEMPLATE_KWARGS) -to, --timeout N server read/write timeout in seconds (de fault: 600) (env: LLAMA_ARG_TIMEOUT) --threads-http N number of threads used to process HTTP r equests (default: -1) (env: LLAMA_ARG_THREADS_HTTP) --cache-reuse N min chunk size to attempt reusing from t he cache via KV shifting (default: 0) [(card)](https://ggml.ai/f0.png) (env: LLAMA_ARG_CACHE_REUSE) --metrics enable prometheus compatible metrics end point (default: disabled) (env: LLAMA_ARG_ENDPOINT_METRICS) --slots enable slots monitoring endpoint (defaul t: disabled) (env: LLAMA_ARG_ENDPOINT_SLOTS) --props enable changing global properties via PO ST /props (default: disabled) (env: LLAMA_ARG_ENDPOINT_PROPS) --no-slots disables slots monitoring endpoint (env: LLAMA_ARG_NO_ENDPOINT_SLOTS) --slot-save-path PATH path to save slot kv cache (default: dis abled) --jinja use jinja template for chat (default: di sabled) (env: LLAMA_ARG_JINJA) --reasoning-format FORMAT controls whether thought tags are allowe d and/or extracted from the response, and in which format they're re turned; one of"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "chat (default: di sabled) (env: LLAMA_ARG_JINJA) --reasoning-format FORMAT controls whether thought tags are allowe d and/or extracted from the response, and in which format they're re turned; one of: - none: leaves thoughts unparsed in `mes sage.content` - deepseek: puts thoughts in `message.re asoning_content` (except in streaming mode, which behaves as `none`) (default: deepseek) (env: LLAMA_ARG_THINK) --reasoning-budget N controls the amount of thinking allowed; currently only one of: -1 for unrestricted thinking budget, or 0 to di sable thinking (default: -1) (env: LLAMA_ARG_THINK_BUDGET) --chat-template JINJA_TEMPLATE set custom jinja chat template (default: template taken from model's metadata) if suffix/prefix are specified, template will be disabled only commonly used templates are accepte d (unless --jinja is set before this flag): list of built-in templates: bailing, chatglm3, chatglm4, chatml, com mand-r, deepseek, deepseek2, deepseek3, exaone3, exaone4, falcon3, ge mma, gigachat, glmedge, granite, hunyuan-dense, hunyuan-moe, kim i-k2, llama2, llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3 , llama4, megrez, minicpm, mistral-v1, mistral-v3, mistral-v3-tekke n, mistral-v7, mistral-v7-tekken, monarch, openchat, or ion, phi3, phi4, rwkv-world, smolvlm, vicuna, vicuna-orca, yandex, ze phyr (env: LLAMA_ARG_CHAT_TEMPLATE) --chat-template-file JINJA_TEMPLATE_FILE set custom jinja chat template file (def ault: template taken from model's metadata) if suffix/prefix a"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "na-orca, yandex, ze phyr (env: LLAMA_ARG_CHAT_TEMPLATE) --chat-template-file JINJA_TEMPLATE_FILE set custom jinja chat template file (def ault: template taken from model's metadata) if suffix/prefix are specified, template will be disabled only commonly used templates are accepte d (unless --jinja is set before this flag): list of built-in templates: bailing, chatglm3, chatglm4, chatml, com mand-r, deepseek, deepseek2, deepseek3, exaone3, exaone4, falcon3, ge mma, gigachat, glmedge, granite, hunyuan-dense, hunyuan-moe, kim i-k2, llama2, llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3 , llama4, megrez, minicpm, mistral-v1, mistral-v3, mistral-v3-tekke n, mistral-v7, mistral-v7-tekken, monarch, openchat, or ion, phi3, phi4, rwkv-world, smolvlm, vicuna, vicuna-orca, yandex, ze phyr (env: LLAMA_ARG_CHAT_TEMPLATE_FILE) --no-prefill-assistant whether to prefill the assistant's respo nse if the last message is an assistant message (default: prefill enab led) when this flag is set, if the last messa ge is an assistant message then it will be treated as a full messag e and not prefilled (env: LLAMA_ARG_NO_PREFILL_ASSISTANT) -sps, --slot-prompt-similarity SIMILARITY how much the prompt of a request must ma tch the prompt of a slot in order to use that slot (default: 0.50, 0 ."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "refilled (env: LLAMA_ARG_NO_PREFILL_ASSISTANT) -sps, --slot-prompt-similarity SIMILARITY how much the prompt of a request must ma tch the prompt of a slot in order to use that slot (default: 0.50, 0 .0 = disabled) --lora-init-without-apply load LoRA adapters without applying them (apply later via POST /lora-adapters) (default: disabled) --draft-max, --draft, --draft-n N number of tokens to draft for speculativ e decoding (default: 16) (env: LLAMA_ARG_DRAFT_MAX) --draft-min, --draft-n-min N minimum number of draft tokens to use fo r speculative decoding (default: 0) (env: LLAMA_ARG_DRAFT_MIN) --draft-p-min P minimum speculative decoding probability (greedy) (default: 0.8) (env: LLAMA_ARG_DRAFT_P_MIN) -cd, --ctx-size-draft N size of the prompt context for the draft model (default: 0, 0 = loaded from model) (env: LLAMA_ARG_CTX_SIZE_DRAFT) -devd, --device-draft <dev1,dev2,..> comma-separated list of devices to use f or offloading the draft model (none = don't offload) use --list-devices to see a list of avai lable devices -ngld, --gpu-layers-draft, --n-gpu-layers-draft N number of layers to store in VRAM for th e draft model (env: LLAMA_ARG_N_GPU_LAYERS_DRAFT) -md, --model-draft FNAME draft model for speculative decoding (de fault: unused) (env: LLAMA_ARG_MODEL_DRAFT) --spec-replace TARGET DRAFT translate the string in TARGET into DRAF T if the draft model and main model are not compatible -mv, --model-vocoder FNAME vocoder model for audio generation (defa ult: unused) --tts-use-"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "TARGET DRAFT translate the string in TARGET into DRAF T if the draft model and main model are not compatible -mv, --model-vocoder FNAME vocoder model for audio generation (defa ult: unused) --tts-use-guide-tokens Use guide tokens to improve TTS word rec all --embd-bge-small-en-default use default bge-small-en-v1.5 model (not e: can download weights from the internet) --embd-e5-small-en-default use default e5-small-v2 model (note: can download weights from the internet) --embd-gte-small-default use default gte-small model (note: can d ownload weights from the internet) --fim-qwen-1.5b-default use default Qwen 2.5 Coder 1.5B (note: c an download weights from the internet) --fim-qwen-3b-default use default Qwen 2.5 Coder 3B (note: can download weights from the internet) --fim-qwen-7b-default use default Qwen 2.5 Coder 7B (note: can download weights from the internet) --fim-qwen-7b-spec use Qwen 2.5 Coder 7B + 0.5B draft for s peculative decoding (note: can download weights from the internet) --fim-qwen-14b-spec use Qwen 2.5 Coder 14B + 0.5B draft for speculative decoding (note: can download weights from the internet) + mradermacher/KAT-V1-40B-GGUF Kwaipilot/KAT-V1-40B https://huggingface.co/Kwaipilot/KAT-V1-40B mradermacher/KAT-V1-40B-GGUF https://huggingface.co/mradermacher/KAT-V1-40B-GGUF + rope_scaling: { https://huggingface.co/forestliutc/UloRL Inference Parameters 128k setting: temperature=0.85 top_p=0.95 top_k=20 max_tokens=131072 140k setting (with Yarn) temperature=0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "V1-40B-GGUF + rope_scaling: { https://huggingface.co/forestliutc/UloRL Inference Parameters 128k setting: temperature=0.85 top_p=0.95 top_k=20 max_tokens=131072 140k setting (with Yarn) temperature=0.85 top_p=0.95 top_k=20 max_tokens=143360 rope_scaling: { \"rope_type\": \"yarn\", \"factor\": 1.5, \"original_max_position_embeddings\": 95232 } DL the file https://huggingface.co/mradermacher/UloRL-GGUF Rename to full name (torch311) ljubomir@macbook2(:):~/llama.cpp$ mviv ~/Downloads/UloRL.Q6_K.gguf mo dels/Qwen3-30B-A3B-UloRL.Q6_K.gguf /Users/ljubomir/Downloads/UloRL.Q6_K.gguf -> models/Qwen3-30B-A3B-UloRL.Q6_K.ggu f (torch311) ljubomir@macbook2(:):~/llama.cpp$ ls -t ~/LJ-books-papers/ UloRL-An_Ultra-Long_Output_Reinforcement_Learning_Approach_for_Advancing_Large_L anguage_Models_Reasoning_Abilities-jul2025-arxiv-2507.19766v1.pdf (torch311) ljubomir@macbook2(:):~/llama.cpp$ ls -lt ~/LJ-books-papers/ -rw-r--r--@ 1 ljubomir staff 1588325 3 Aug 21:29 UloRL-An_Ultra-Long_Outp ut_Reinforcement_Learning_Approach_for_Advancing_Large_Language_Models_Reasoning _Abilities-jul2025-arxiv-2507.19766v1.pdf # flash attention cached; access on http://127.0.0.1:8080 build/bin/llama-server --port 8080 --model models/Qwen3-30B-A3B-UloRL.Q6_K.gguf --temp 0.8 --top_k 20 --top_p 0.95 --min_p 0.05 --ctx-size 131072 --flash-attn - -cache-type-k q8_0 --cache-type-v q8_0 --jinja & # extend context (262144) 256K->512K (524288), flash attention cached; access on http://127.0.0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "p_p 0.95 --min_p 0.05 --ctx-size 131072 --flash-attn - -cache-type-k q8_0 --cache-type-v q8_0 --jinja & # extend context (262144) 256K->512K (524288), flash attention cached; access on http://127.0.0.1:8080 build/bin/llama-server --port 8080 --model models/Qwen3-30B-A3B-UloRL.Q6_K.gguf --temp 0.8 --top_k 20 --top_p 0.85 --ctx-size 131072 --rope-scaling yarn --rope -scale 1.5 --yarn-orig-ctx 95232 --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & + LJ Thu 7 Aug 2025 07:28:17 BST � https://huggingface.co/unsloth/GLM-4.5-Air-GGUF https://z.ai/blog/glm-4.5 https://github.com/zai-org/GLM-4.5 https://huggingface.co/zai-org/GLM-4.5-Air https://huggingface.co/unsloth/GLM-4.5-Air-GGUF (torch311) ljubomir@macbook2(:):~/llama.cpp$ l models/GLM-* -rw-r--r--@ 1 ljubomir staff 46G 6 Aug 20:48 models/GLM-4.5-Air-IQ4_NL-0000 1-of-00002.gguf -rw-r--r--@ 1 ljubomir staff 12G 6 Aug 19:10 models/GLM-4.5-Air-IQ4_NL-0000 2-of-00002.gguf # flash attention cached; access on http://127.0.0.1:8080 build/bin/llama-server --port 8080 --model models/GLM-4.5-Air-IQ4_NL-00001-of-00 002.gguf --temp 0.8 --top_k 40 --top_p 0.95 --min_p 0.05 --ctx-size 131072 --fla sh-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & (torch311) ljubomir@macbook2(:):~/llama.cpp$ cat ~/LJ-what-next-job-everything-q .txt |xclip_put (torch311) ljubomir@macbook2(:):~/llama."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "-fla sh-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & (torch311) ljubomir@macbook2(:):~/llama.cpp$ cat ~/LJ-what-next-job-everything-q .txt |xclip_put (torch311) ljubomir@macbook2(:):~/llama.cpp$ srv params_from_: Chat format: Her mes 2 Pro slot launch_slot_: id 0 | task 0 | processing task slot update_slots: id 0 | task 0 | new prompt, n_ctx_slot = 131072, n_keep = 0, n_prompt_tokens = 205586 slot update_slots: id 0 | task 0 | input truncated, n_ctx = 131072, n_keep = 0, n_left = 131072, n_prompt_tokens = 74514 slot update_slots: id 0 | task 0 | kv cache rm [0, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 2048, n _tokens = 2048, progress = 0.027485 slot update_slots: id 0 | task 0 | kv cache rm [2048, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 4096, n _tokens = 2048, progress = 0.054970 slot update_slots: id 0 | task 0 | kv cache rm [4096, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 6144, n _tokens = 2048, progress = 0.082454 slot update_slots: id 0 | task 0 | kv cache rm [6144, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 8192, n _tokens = 2048, progress = 0.109939 slot update_slots: id 0 | task 0 | kv cache rm [8192, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 10240, n_tokens = 2048, progress = 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ens = 2048, progress = 0.109939 slot update_slots: id 0 | task 0 | kv cache rm [8192, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 10240, n_tokens = 2048, progress = 0.137424 slot update_slots: id 0 | task 0 | kv cache rm [10240, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 12288, n_tokens = 2048, progress = 0.164909 slot update_slots: id 0 | task 0 | kv cache rm [12288, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 14336, n_tokens = 2048, progress = 0.192393 slot update_slots: id 0 | task 0 | kv cache rm [14336, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 16384, n_tokens = 2048, progress = 0.219878 slot update_slots: id 0 | task 0 | kv cache rm [16384, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 18432, n_tokens = 2048, progress = 0.247363 slot update_slots: id 0 | task 0 | kv cache rm [18432, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 20480, n_tokens = 2048, progress = 0.274848 slot update_slots: id 0 | task 0 | kv cache rm [20480, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 22528, n_tokens = 2048, progress = 0.302332 slot update_slots: id 0 | task 0 | kv cache rm [22528, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 24576, n_tokens = 2048, progress = 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ns = 2048, progress = 0.302332 slot update_slots: id 0 | task 0 | kv cache rm [22528, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 24576, n_tokens = 2048, progress = 0.329817 slot update_slots: id 0 | task 0 | kv cache rm [24576, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 26624, n_tokens = 2048, progress = 0.357302 slot update_slots: id 0 | task 0 | kv cache rm [26624, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 28672, n_tokens = 2048, progress = 0.384787 slot update_slots: id 0 | task 0 | kv cache rm [28672, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 30720, n_tokens = 2048, progress = 0.412272 slot update_slots: id 0 | task 0 | kv cache rm [30720, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 32768, n_tokens = 2048, progress = 0.439756 slot update_slots: id 0 | task 0 | kv cache rm [32768, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 34816, n_tokens = 2048, progress = 0.467241 slot update_slots: id 0 | task 0 | kv cache rm [34816, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 36864, n_tokens = 2048, progress = 0.494726 slot update_slots: id 0 | task 0 | kv cache rm [36864, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 38912, n_tokens = 2048, progress = 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ns = 2048, progress = 0.494726 slot update_slots: id 0 | task 0 | kv cache rm [36864, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 38912, n_tokens = 2048, progress = 0.522211 slot update_slots: id 0 | task 0 | kv cache rm [38912, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 40960, n_tokens = 2048, progress = 0.549695 slot update_slots: id 0 | task 0 | kv cache rm [40960, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 43008, n_tokens = 2048, progress = 0.577180 slot update_slots: id 0 | task 0 | kv cache rm [43008, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 45056, n_tokens = 2048, progress = 0.604665 slot update_slots: id 0 | task 0 | kv cache rm [45056, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 47104, n_tokens = 2048, progress = 0.632150 slot update_slots: id 0 | task 0 | kv cache rm [47104, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 49152, n_tokens = 2048, progress = 0.659634 slot update_slots: id 0 | task 0 | kv cache rm [49152, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 51200, n_tokens = 2048, progress = 0.687119 slot update_slots: id 0 | task 0 | kv cache rm [51200, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 53248, n_tokens = 2048, progress = 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ns = 2048, progress = 0.687119 slot update_slots: id 0 | task 0 | kv cache rm [51200, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 53248, n_tokens = 2048, progress = 0.714604 slot update_slots: id 0 | task 0 | kv cache rm [53248, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 55296, n_tokens = 2048, progress = 0.742089 slot update_slots: id 0 | task 0 | kv cache rm [55296, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 57344, n_tokens = 2048, progress = 0.769574 slot update_slots: id 0 | task 0 | kv cache rm [57344, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 59392, n_tokens = 2048, progress = 0.797058 slot update_slots: id 0 | task 0 | kv cache rm [59392, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 61440, n_tokens = 2048, progress = 0.824543 slot update_slots: id 0 | task 0 | kv cache rm [61440, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 63488, n_tokens = 2048, progress = 0.852028 slot update_slots: id 0 | task 0 | kv cache rm [63488, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 65536, n_tokens = 2048, progress = 0.879513 slot update_slots: id 0 | task 0 | kv cache rm [65536, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 67584, n_tokens = 2048, progress = 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ns = 2048, progress = 0.879513 slot update_slots: id 0 | task 0 | kv cache rm [65536, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 67584, n_tokens = 2048, progress = 0.906997 slot update_slots: id 0 | task 0 | kv cache rm [67584, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 69632, n_tokens = 2048, progress = 0.934482 slot update_slots: id 0 | task 0 | kv cache rm [69632, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 71680, n_tokens = 2048, progress = 0.961967 slot update_slots: id 0 | task 0 | kv cache rm [71680, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 73728, n_tokens = 2048, progress = 0.989452 slot update_slots: id 0 | task 0 | kv cache rm [73728, end) slot update_slots: id 0 | task 0 | prompt processing progress, n_past = 74514, n_tokens = 786, progress = 1.000000 slot update_slots: id 0 | task 0 | prompt done, n_past = 74514, n_tokens = 786 slot release: id 0 | task 0 | stop processing: n_past = 77593, truncated = 1 slot print_timing: id 0 | task 0 | prompt eval time = 4040281.84 ms / 74514 tokens ( 54.22 ms per token, 18.44 tokens per second) eval time = 912694.36 ms / 3080 tokens ( 296.33 ms per token, 3.37 tokens per second) total time = 4952976.21 ms / 77594 tokens srv update_slots: all slots are idle srv log_server_r: request: POST /v1/chat/completions 127.0.0.1 200 https://x.com/Zai_org/status/1953340512963825732 Z."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "d) total time = 4952976.21 ms / 77594 tokens srv update_slots: all slots are idle srv log_server_r: request: POST /v1/chat/completions 127.0.0.1 200 https://x.com/Zai_org/status/1953340512963825732 Z.ai @Zai_org In the spirit of full transparency, we are officially sharing the default sampli ng configurations used for http://Z.ai. http://Z.ai Chat: This configuration is optimized to encourage more creative and diverse responses, using a temperature of 0.95 and a top_p of 0.7. API & Leaderboard Testing: This setup is calibrated for more focused and predict able outputs with a temperature of 0.6 and a top_p of 0.95. # flash attention cached; access on http://127.0.0.1:8080 build/bin/llama-server --port 8080 --model models/GLM-4.5-Air-IQ4_NL-00001-of-00 002.gguf --temp 0.95 --top_k 40 --top_p 0.7 --min_p 0.05 --ctx-size 131072 --fla sh-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & (torch311) ljubomir@macbook2(:):~/llama.cpp$ cat ~/LJ-what-next-job-everything-q .txt |xclip_put LJ Thu 7 Aug 2025 07:28:17 BST + LJ Sat 9 Aug 2025 07:49:03 BST � https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507 https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507 Processing Ultra-Long Texts To support ultra-long context processing (up to 1 million tokens), we integrate two key techniques: Dual Chunk Attention (DCA): A length extrapolation method that splits long seque nces into manageable chunks while preserving global coherence. https://arxiv.org/abs/2402."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ntegrate two key techniques: Dual Chunk Attention (DCA): A length extrapolation method that splits long seque nces into manageable chunks while preserving global coherence. https://arxiv.org/abs/2402.17463 [Submitted on 27 Feb 2024 (v1), last revised 29 May 2024 (this version, v2)] Training-Free Long-Context Scaling of Large Language Models MInference: A sparse attention mechanism that reduces computational overhead by focusing on critical token interactions. Together, these innovations significantly improve both generation quality and in ference efficiency for sequences beyond 256K tokens. On sequences approaching 1M tokens, the system achieves up to a 3� speedup compared to standard attention i mplementations. https://arxiv.org/abs/2407.02490 [Submitted on 2 Jul 2024 (v1), last revised 30 Oct 2024 (this version, v2)] MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Spars e Attention For full technical details, see the Qwen2.5-1M Technical Report. https://arxiv.org/abs/2501.15383 [Submitted on 26 Jan 2025] Qwen2.5-1M Technical Report How to Enable 1M Token Context To effectively process a 1 million token context, users will require approximate ly 240 GB of total GPU memory. This accounts for model weights, KV-cache storage , and peak activation memory demands. Step 1: Update Configuration File Download the model and replace the content of your config.json with config_1m.js on, which includes the config for length extrapolation and sparse attention."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "demands. Step 1: Update Configuration File Download the model and replace the content of your config.json with config_1m.js on, which includes the config for length extrapolation and sparse attention. export MODELNAME=Qwen3-30B-A3B-Thinking-2507 huggingface-cli download Qwen/${MODELNAME} --local-dir ${MODELNAME} mv ${MODELNAME}/config.json ${MODELNAME}/config.json.bak mv ${MODELNAME}/config_1m.json ${MODELNAME}/config.json Step 2: Launch Model Server After updating the config, proceed with either vLLM or SGLang for serving the mo del. Option 1: Using vLLM To run Qwen with 1M context support: git clone https://github.com/vllm-project/vllm.git cd vllm pip install -e . Then launch the server with Dual Chunk Flash Attention enabled: VLLM_ATTENTION_BACKEND=DUAL_CHUNK_FLASH_ATTN VLLM_USE_V1=0 \\ vllm serve ./Qwen3-30B-A3B-Thinking-2507 \\ --tensor-parallel-size 4 \\ --max-model-len 1010000 \\ --enable-chunked-prefill \\ --max-num-batched-tokens 131072 \\ --enforce-eager \\ --max-num-seqs 1 \\ --gpu-memory-utilization 0.85 \\ --enable-reasoning --reasoning-parser deepseek_r1 Key Parameters Parameter Purpose VLLM_ATTENTION_BACKEND=DUAL_CHUNK_FLASH_ATTN Enables the custom attention ker nel for long-context efficiency --max-model-len 1010000 Sets maximum context length to ~1M tokens --enable-chunked-prefill Allows chunked prefill for very long inputs (avo ids OOM) --max-num-batched-tokens 131072 Controls batch size during prefill; balances thr oughput and memory --enforce-eager Disables CUDA gr"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "d-prefill Allows chunked prefill for very long inputs (avo ids OOM) --max-num-batched-tokens 131072 Controls batch size during prefill; balances thr oughput and memory --enforce-eager Disables CUDA graph capture (required for dual chunk attention) --max-num-seqs 1 Limits concurrent sequences due to extreme memory usage --gpu-memory-utilization 0.85 Set the fraction of GPU memory to be used for th e model executor Option 2: Using SGLang First, clone and install the specialized branch: git clone https://github.com/sgl-project/sglang.git cd sglang pip install -e \"python[all]\" Launch the server with DCA support: python3 -m sglang.launch_server \\ --model-path ./Qwen3-30B-A3B-Thinking-2507 \\ --context-length 1010000 \\ --mem-frac 0.75 \\ --attention-backend dual_chunk_flash_attn \\ --tp 4 \\ --chunked-prefill-size 131072 \\ --reasoning-parser deepseek-r1 Key Parameters Parameter Purpose --attention-backend dual_chunk_flash_attn Activates Dual Chunk Flash Atten tion --context-length 1010000 Defines max input length --mem-frac 0.75 The fraction of the memory used for static allocation (model wei ghts and KV cache memory pool). Use a smaller value if you see out-of-memory err ors. --tp 4 Tensor parallelism size (matches model sharding) --chunked-prefill-size 131072 Prefill chunk size for handling long inputs with out OOM Troubleshooting: Encountering the error: \"The model's max sequence length (xxxxx) is larger than the maximum number of tokens that can be stored in the KV cache."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "for handling long inputs with out OOM Troubleshooting: Encountering the error: \"The model's max sequence length (xxxxx) is larger than the maximum number of tokens that can be stored in the KV cache.\" or \"RuntimeErr or: Not enough memory. Please try to increase --mem-fraction-static.\" The VRAM reserved for the KV cache is insufficient. vLLM: Consider reducing the max_model_len or increasing the tensor_parallel_size and gpu_memory_utilization. Alternatively, you can reduce max_num_batched_token s, although this may significantly slow down inference. SGLang: Consider reducing the context-length or increasing the tp and mem-frac. Alternatively, you can reduce chunked-prefill-size, although this may significan tly slow down inference. Encountering the error: \"torch.OutOfMemoryError: CUDA out of memory.\" The VRAM reserved for activation weights is insufficient. You can try lowering g pu_memory_utilization or mem-frac, but be aware that this might reduce the VRAM available for the KV cache. Encountering the error: \"Input prompt (xxxxx tokens) + lookahead slots (0) is to o long and exceeds the capacity of the block manager.\" or \"The input (xxx xtoken s) is longer than the model's context length (xxx tokens).\" The input is too lengthy. Consider using a shorter sequence or increasing the ma x_model_len or context-length. Long-Context Performance We test the model on an 1M version of the RULER benchmark."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "s).\" The input is too lengthy. Consider using a shorter sequence or increasing the ma x_model_len or context-length. Long-Context Performance We test the model on an 1M version of the RULER benchmark. Model Name Acc avg 4k 8k 16k 32k 64k 96k 128k 192k 256k 384k 512k 640k 768k 896k 1000k Qwen3-30B-A3B (Thinking) 70.6 96.7 94.4 94.5 93.4 82.6 78.4 74.5 70.6 63.1 60.0 56.3 51.0 48.4 47.2 48.2 Qwen3-30B-A3B-Thinking-2507 (Full Attention) 91.4 99.6 100.0 99.8 99.2 97.4 96.8 96.8 94.8 89.4 90.2 84.0 82.6 81.9 80.1 77.5 Qwen3-30B-A3B-Thinking-2507 (Sparse Attention) 91.5 100.0 99.2 99.1 98.5 97.3 97.1 96.9 95.8 89.0 89.3 85.5 84.8 80.0 79.9 79.6 All models are evaluated with Dual Chunk Attention enabled. Since the evaluation is time-consuming, we use 260 samples for each length (13 s ub-tasks, 20 samples for each). To avoid overly verbose reasoning, we set the thinking budget to 8,192 tokens. Best Practices To achieve optimal performance, we recommend the following settings: Sampling Parameters: We suggest using Temperature=0.6, TopP=0.95, TopK=20, and MinP=0. For supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasio nally result in language mixing and a slight decrease in model performance. Adequate Output Length: We recommend using an output length of 32,768 tokens for most queries."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ng a higher value may occasio nally result in language mixing and a slight decrease in model performance. Adequate Output Length: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length t o 81,920 tokens. This provides the model with sufficient space to generate detai led and comprehensive responses, thereby enhancing its overall performance. Standardize Output Format: We recommend using prompts to standardize model outpu ts when benchmarking. Math Problems: Include \"Please reason step by step, and put your final answer wi thin \\boxed{}.\" in the prompt. Multiple-Choice Questions: Add the following JSON structure to the prompt to sta ndardize responses: \"Please show your choice in the answer field with only the c hoice letter, e.g., \"answer\": \"C\".\" No Thinking Content in History: In multi-turn conversations, the historical mode l output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it i s up to the developers to ensure that the best practice is followed. Citation If you find our work helpful, feel free to give us a cite. @misc{qwen3technicalreport, title={Qwen3 Technical Report}, author={Qwen Team}, year={2025}, eprint={2505."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "est practice is followed. Citation If you find our work helpful, feel free to give us a cite. @misc{qwen3technicalreport, title={Qwen3 Technical Report}, author={Qwen Team}, year={2025}, eprint={2505.09388}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2505.09388}, } https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507 Qwen / Qwen3-30B-A3B-Instruct-2507 Highlights We introduce the updated version of the Qwen3-30B-A3B non-thinking mode, named Q wen3-30B-A3B-Instruct-2507, featuring the following key enhancements: Significant improvements in general capabilities, including instruction followin g, logical reasoning, text comprehension, mathematics, science, coding and tool usage. Substantial gains in long-tail knowledge coverage across multiple languages. Markedly better alignment with user preferences in subjective and open-ended tas ks, enabling more helpful responses and higher-quality text generation. Enhanced capabilities in 256K long-context understanding. image/jpeg Model Overview Qwen3-30B-A3B-Instruct-2507 has the following features: Type: Causal Language Models Training Stage: Pretraining & Post-training Number of Parameters: 30.5B in total and 3.3B activated Number of Paramaters (Non-Embedding): 29.9B Number of Layers: 48 Number of Attention Heads (GQA): 32 for Q and 4 for KV Number of Experts: 128 Number of Activated Experts: 8 Context Length: 262,144 natively."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "of Paramaters (Non-Embedding): 29.9B Number of Layers: 48 Number of Attention Heads (GQA): 32 for Q and 4 for KV Number of Experts: 128 Number of Activated Experts: 8 Context Length: 262,144 natively. NOTE: This model supports only non-thinking mode and does not generate <think></ think> blocks in its output. Meanwhile, specifying enable_thinking=False is no l onger required. For more details, including benchmark evaluation, hardware requirements, and inf erence performance, please refer to our blog, GitHub, and Documentation. Performance Deepseek-V3-0324 GPT-4o-0327 Gemini-2.5-Flash Non-Thinking Qwen3-23 5B-A22B Non-Thinking Qwen3-30B-A3B Non-Thinking Qwen3-30B-A3B-Instruct-2 507 Knowledge MMLU-Pro 81.2 79.8 81.1 75.2 69.1 78.4 MMLU-Redux 90.4 91.3 90.6 89.2 84.1 89.3 GPQA 68.4 66.9 78.3 62.9 54.8 70.4 SuperGPQA 57.3 51.0 54.6 48.2 42.2 53.4 Reasoning AIME25 46.6 26.7 61.6 24.7 21.6 61.3 HMMT25 27.5 7.9 45.8 10.0 12.0 43.0 ZebraLogic 83.4 52.6 57.9 37.7 33.2 90.0 LiveBench 20241125 66.9 63.7 69.1 62.5 59.4 69.0 Coding LiveCodeBench v6 (25.02-25.05) 45.2 35.8 40.1 32.9 29.0 43.2 MultiPL-E 82.2 82.7 77.7 79.3 74.6 83.8 Aider-Polyglot 55.1 45.3 44.0 59.6 24.4 35.6 Alignment IFEval 82.3 83.9 84.3 83.2 83.7 84.7 Arena-Hard v2* 45.6 61.9 58.3 52.0 24.8 69.0 Creative Writing v3 81.6 84.9 84.6 80.4 68.1 86.0 WritingBench 74.5 75.5 80.5 77.0 72.2 85.5 Agent BFCL-v3 64.7 66.5 66.1 68.0 58.6 65.1 TAU1-Retail 49.6 60.3# 65.2 65.2 38.3 59.1 TAU1-Airline 32.0 42.8# 48.0 32.0 18.0 40."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": ".6 84.9 84.6 80.4 68.1 86.0 WritingBench 74.5 75.5 80.5 77.0 72.2 85.5 Agent BFCL-v3 64.7 66.5 66.1 68.0 58.6 65.1 TAU1-Retail 49.6 60.3# 65.2 65.2 38.3 59.1 TAU1-Airline 32.0 42.8# 48.0 32.0 18.0 40.0 TAU2-Retail 71.1 66.7# 64.3 64.9 31.6 57.0 TAU2-Airline 36.0 42.0# 42.5 36.0 18.0 38.0 TAU2-Telecom 34.0 29.8# 16.9 24.6 18.4 12.3 Multilingualism MultiIF 66.5 70.4 69.4 70.2 70.8 67.9 MMLU-ProX 75.8 76.2 78.3 73.2 65.1 72.0 INCLUDE 80.1 82.1 83.8 75.6 67.8 71.9 PolyMATH 32.2 25.5 41.9 27.0 23.3 43.1 *: For reproducibility, we report the win rates evaluated by GPT-4.1. #: Results were generated using GPT-4o-20241120, as access to the native functio n calling API of GPT-4o-0327 was unavailable. Quickstart The code of Qwen3-MoE has been in the latest Hugging Face transformers and we ad vise you to use the latest version of transformers. With transformers<4.51.0, you will encounter the following error: KeyError: 'qwen3_moe' The following contains a code snippet illustrating how to use the model generate content based on given inputs. from transformers import AutoModelForCausalLM, AutoTokenizer model_name = \"Qwen/Qwen3-30B-A3B-Instruct-2507\" # load the tokenizer and the model tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained( model_name, torch_dtype=\"auto\", device_map=\"auto\" ) # prepare the model input prompt = \"Give me a short introduction to large language model.\" messages = [ {\"role\": \"user\", \"content\": prompt} ] text = tokenizer."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "torch_dtype=\"auto\", device_map=\"auto\" ) # prepare the model input prompt = \"Give me a short introduction to large language model.\" messages = [ {\"role\": \"user\", \"content\": prompt} ] text = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True, ) model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device) # conduct text completion generated_ids = model.generate( **model_inputs, max_new_tokens=16384 ) output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() content = tokenizer.decode(output_ids, skip_special_tokens=True) print(\"content:\", content) For deployment, you can use sglang>=0.4.6.post1 or vllm>=0.8.5 or to create an O penAI-compatible API endpoint: SGLang: python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B-Instruct-2507 --c ontext-length 262144 vLLM: vllm serve Qwen/Qwen3-30B-A3B-Instruct-2507 --max-model-len 262144 Note: If you encounter out-of-memory (OOM) issues, consider reducing the context length to a shorter value, such as 32,768. For local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTr ansformers have also supported Qwen3. Agentic Use Qwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexit y."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexit y. To define the available tools, you can use the MCP configuration file, use the i ntegrated tool of Qwen-Agent, or integrate other tools by yourself. from qwen_agent.agents import Assistant # Define LLM llm_cfg = { 'model': 'Qwen3-30B-A3B-Instruct-2507', # Use a custom endpoint compatible with OpenAI API: 'model_server': 'http://localhost:8000/v1', # api_base 'api_key': 'EMPTY', } # Define Tools tools = [ {'mcpServers': { # You can specify the MCP configuration file 'time': { 'command': 'uvx', 'args': ['mcp-server-time', '--local-timezone=Asia/Shanghai'] }, \"fetch\": { \"command\": \"uvx\", \"args\": [\"mcp-server-fetch\"] } } }, 'code_interpreter', # Built-in tools ] # Define Agent bot = Assistant(llm=llm_cfg, function_list=tools) # Streaming generation messages = [{'role': 'user', 'content': 'https://qwenlm.github.io/blog/ Introduc e the latest developments of Qwen'}] for responses in bot.run(messages=messages): pass print(responses) Processing Ultra-Long Texts To support ultra-long context processing (up to 1 million tokens), we integrate two key techniques: Dual Chunk Attention (DCA): A length extrapolation method that splits long seque nces into manageable chunks while preserving global coherence. https://arxiv.org/abs/2402."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ntegrate two key techniques: Dual Chunk Attention (DCA): A length extrapolation method that splits long seque nces into manageable chunks while preserving global coherence. https://arxiv.org/abs/2402.17463 [Submitted on 27 Feb 2024 (v1), last revised 29 May 2024 (this version, v2)] Training-Free Long-Context Scaling of Large Language Models MInference: A sparse attention mechanism that reduces computational overhead by focusing on critical token interactions. Together, these innovations significantly improve both generation quality and in ference efficiency for sequences beyond 256K tokens. On sequences approaching 1M tokens, the system achieves up to a 3� speedup compared to standard attention i mplementations. https://arxiv.org/abs/2407.02490 [Submitted on 2 Jul 2024 (v1), last revised 30 Oct 2024 (this version, v2)] MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Spars e Attention For full technical details, see the Qwen2.5-1M Technical Report. How to Enable 1M Token Context To effectively process a 1 million token context, users will require approximate ly 240 GB of total GPU memory. This accounts for model weights, KV-cache storage , and peak activation memory demands. Step 1: Update Configuration File Download the model and replace the content of your config.json with config_1m.js on, which includes the config for length extrapolation and sparse attention."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "demands. Step 1: Update Configuration File Download the model and replace the content of your config.json with config_1m.js on, which includes the config for length extrapolation and sparse attention. export MODELNAME=Qwen3-30B-A3B-Instruct-2507 huggingface-cli download Qwen/${MODELNAME} --local-dir ${MODELNAME} mv ${MODELNAME}/config.json ${MODELNAME}/config.json.bak mv ${MODELNAME}/config_1m.json ${MODELNAME}/config.json Step 2: Launch Model Server After updating the config, proceed with either vLLM or SGLang for serving the mo del. Option 1: Using vLLM To run Qwen with 1M context support: git clone https://github.com/vllm-project/vllm.git cd vllm pip install -e . Then launch the server with Dual Chunk Flash Attention enabled: VLLM_ATTENTION_BACKEND=DUAL_CHUNK_FLASH_ATTN VLLM_USE_V1=0 \\ vllm serve ./Qwen3-30B-A3B-Instruct-2507 \\ --tensor-parallel-size 4 \\ --max-model-len 1010000 \\ --enable-chunked-prefill \\ --max-num-batched-tokens 131072 \\ --enforce-eager \\ --max-num-seqs 1 \\ --gpu-memory-utilization 0.85 Key Parameters Parameter Purpose VLLM_ATTENTION_BACKEND=DUAL_CHUNK_FLASH_ATTN Enables the custom attention ker nel for long-context efficiency --max-model-len 1010000 Sets maximum context length to ~1M tokens --enable-chunked-prefill Allows chunked prefill for very long inputs (avo ids OOM) --max-num-batched-tokens 131072 Controls batch size during prefill; balances thr oughput and memory --enforce-eager Disables CUDA graph capture (required for dual chunk attention) --ma"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "s (avo ids OOM) --max-num-batched-tokens 131072 Controls batch size during prefill; balances thr oughput and memory --enforce-eager Disables CUDA graph capture (required for dual chunk attention) --max-num-seqs 1 Limits concurrent sequences due to extreme memory usage --gpu-memory-utilization 0.85 Set the fraction of GPU memory to be used for th e model executor Option 2: Using SGLang First, clone and install the specialized branch: git clone https://github.com/sgl-project/sglang.git cd sglang pip install -e \"python[all]\" Launch the server with DCA support: python3 -m sglang.launch_server \\ --model-path ./Qwen3-30B-A3B-Instruct-2507 \\ --context-length 1010000 \\ --mem-frac 0.75 \\ --attention-backend dual_chunk_flash_attn \\ --tp 4 \\ --chunked-prefill-size 131072 Key Parameters Parameter Purpose --attention-backend dual_chunk_flash_attn Activates Dual Chunk Flash Atten tion --context-length 1010000 Defines max input length --mem-frac 0.75 The fraction of the memory used for static allocation (model wei ghts and KV cache memory pool). Use a smaller value if you see out-of-memory err ors. --tp 4 Tensor parallelism size (matches model sharding) --chunked-prefill-size 131072 Prefill chunk size for handling long inputs with out OOM Troubleshooting: Encountering the error: \"The model's max sequence length (xxxxx) is larger than the maximum number of tokens that can be stored in the KV cache.\" or \"RuntimeErr or: Not enough memory. Please try to increase --mem-fraction-static."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "model's max sequence length (xxxxx) is larger than the maximum number of tokens that can be stored in the KV cache.\" or \"RuntimeErr or: Not enough memory. Please try to increase --mem-fraction-static.\" The VRAM reserved for the KV cache is insufficient. vLLM: Consider reducing the max_model_len or increasing the tensor_parallel_size and gpu_memory_utilization. Alternatively, you can reduce max_num_batched_token s, although this may significantly slow down inference. SGLang: Consider reducing the context-length or increasing the tp and mem-frac. Alternatively, you can reduce chunked-prefill-size, although this may significan tly slow down inference. Encountering the error: \"torch.OutOfMemoryError: CUDA out of memory.\" The VRAM reserved for activation weights is insufficient. You can try lowering g pu_memory_utilization or mem-frac, but be aware that this might reduce the VRAM available for the KV cache. Encountering the error: \"Input prompt (xxxxx tokens) + lookahead slots (0) is to o long and exceeds the capacity of the block manager.\" or \"The input (xxx xtoken s) is longer than the model's context length (xxx tokens).\" The input is too lengthy. Consider using a shorter sequence or increasing the ma x_model_len or context-length. Long-Context Performance We test the model on an 1M version of the RULER benchmark. Model Name Acc avg 4k 8k 16k 32k 64k 96k 128k 192k 256k 384k 512k 640k 768k 896k 1000k Qwen3-30B-A3B (Non-Thinking) 72.0 97.1 96.1 95.0 92.2 82.6 79.7 76.9 70.2 66."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "1M version of the RULER benchmark. Model Name Acc avg 4k 8k 16k 32k 64k 96k 128k 192k 256k 384k 512k 640k 768k 896k 1000k Qwen3-30B-A3B (Non-Thinking) 72.0 97.1 96.1 95.0 92.2 82.6 79.7 76.9 70.2 66.3 61.9 55.4 52.6 51.5 52.0 50.9 Qwen3-30B-A3B-Instruct-2507 (Full Attention) 86.8 98.0 96.7 96.9 97.2 93.4 91.0 89.1 89.8 82.5 83.6 78.4 79.7 77.6 75.7 72.8 Qwen3-30B-A3B-Instruct-2507 (Sparse Attention) 86.8 98.0 97.1 96.3 95.1 93.6 92.5 88.1 87.7 82.9 85.7 80.7 80.0 76.9 75.5 72.2 All models are evaluated with Dual Chunk Attention enabled. Since the evaluation is time-consuming, we use 260 samples for each length (13 s ub-tasks, 20 samples for each). Best Practices To achieve optimal performance, we recommend the following settings: Sampling Parameters: We suggest using Temperature=0.7, TopP=0.8, TopK=20, and MinP=0. For supported frameworks, you can adjust the presence_penalty parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasio nally result in language mixing and a slight decrease in model performance. Adequate Output Length: We recommend using an output length of 16,384 tokens for most queries, which is adequate for instruct models. Standardize Output Format: We recommend using prompts to standardize model outpu ts when benchmarking. Math Problems: Include \"Please reason step by step, and put your final answer wi thin \\boxed{}.\" in the prompt."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "tput Format: We recommend using prompts to standardize model outpu ts when benchmarking. Math Problems: Include \"Please reason step by step, and put your final answer wi thin \\boxed{}.\" in the prompt. Multiple-Choice Questions: Add the following JSON structure to the prompt to sta ndardize responses: \"Please show your choice in the answer field with only the c hoice letter, e.g., \"answer\": \"C\".\" Citation If you find our work helpful, feel free to give us a cite. @misc{qwen3technicalreport, title={Qwen3 Technical Report}, author={Qwen Team}, year={2025}, eprint={2505.09388}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2505.09388}, } Downloads last month 188,304 Safetensors Model size 30.5B params Tensor type BF16 Files info Qwen/Qwen3-30B-A3B-Instruct-2507 Clone this model repository # Make sure git-lfs is installed (https://git-lfs.com) git lfs install git clone https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507 # If you want to clone without large files - just their pointers GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinki ng-2507 # Make sure hf CLI is installed: pip install -U \"huggingface_hub[cli]\" hf download Qwen/Qwen3-30B-A3B-Thinking-2507 https://huggingface."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "clone https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinki ng-2507 # Make sure hf CLI is installed: pip install -U \"huggingface_hub[cli]\" hf download Qwen/Qwen3-30B-A3B-Thinking-2507 https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507/discussions/5 How does this compare to other \"1M context\"-s UloRL and Unsloth? ljupco 1 minute ago � edited less than a minute ago Thanks for OSS-ing this - you are making life so much fun! I got only 88gb vram (share of 96gb ram) to play with this on a macbook. Curious to see what transpir es... But before that, what's similar what's different between this, and - \"An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Langua ge Models' Reasoning Abilities\" at https://huggingface.co/forestliutc/UloRL; or This by the Unsloth guys https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Inst ruct-1M-GGUF ? Thanks for your help - LJ LJ Sat 9 Aug 2025 07:49:03 BST + https://www.reddit.com/r/LocalLLaMA/comments/1mfzzt4/experience_with_gl m45air_claude_code/?chainedPosts=t3_1mkw4ug https://www.reddit.com/r/LocalLLaMA/comments/1mfzzt4/experience_with_glm45air_cl aude_code/?chainedPosts=t3_1mkw4ug r/LocalLLaMA � 7 days ago Leflakk Experience with GLM-4.5-Air + claude code? Discussion Hi guys, I am actually running GLM-4.5-Air with vllm (4x3090) and even if it's quite earl y I'm quite impressed the model isn't \"lost\" and can handle some tasks through c c (python code modifications)."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "i guys, I am actually running GLM-4.5-Air with vllm (4x3090) and even if it's quite earl y I'm quite impressed the model isn't \"lost\" and can handle some tasks through c c (python code modifications). There are some errors during the executions and t he model need to retry but need to do more tests to better understand the limits . I also encounter some context limit errors unfortunately. What is your experience actually? Any tip is wellcome For info, I use AWQ with the latest (nightly) version of vllm with following cmd : vllm serve cpatonn/GLM-4.5-Air-AWQ --reasoning-parser glm45 -tp 2 -pp 2 --dtype float16 --max-model-len 70000 --enable-auto-tool-choice --tool-call-parser glm45 --host 127.0.0.1 --port 8123 --api-key xxxx Then claude-code-router with following config: { \"LOG\": true, \"Providers\": [ { \"name\": \"openai\", \"api_base_url\": \"http://localhost:8123/v1/chat/completions\", \"api_key\": \"xxxx\", \"models\": [\"cpatonn/GLM-4.5-Air-AWQ\"] } ], \"Router\": { \"default\": \"openai,cpatonn/GLM-4.5-Air-AWQ\", \"background\": \"openai,cpatonn/GLM-4.5-Air-AWQ\", \"think\": \"openai,cpatonn/GLM-4.5-Air-AWQ\", \"longContext\": \"openai,cpatonn/GLM-4.5-Air-AWQ\", \"longContextThreshold\": 64000, \"webSearch\": \"openai,cpatonn/GLM-4.5-Air-AWQ\" } } Use llama.cpp instead of vllm (torch311) ljubomir@macbook2(:):~/z/itrade/src$ l ~/llama.cpp/models/GLM-* -rw-r--r--@ 1 ljubomir staff 46G 6 Aug 20:48 /Users/ljubomir/llama.cpp/mode ls/GLM-4.5-Air-IQ4_NL-00001-of-00002."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ead of vllm (torch311) ljubomir@macbook2(:):~/z/itrade/src$ l ~/llama.cpp/models/GLM-* -rw-r--r--@ 1 ljubomir staff 46G 6 Aug 20:48 /Users/ljubomir/llama.cpp/mode ls/GLM-4.5-Air-IQ4_NL-00001-of-00002.gguf -rw-r--r--@ 1 ljubomir staff 12G 6 Aug 19:10 /Users/ljubomir/llama.cpp/mode ls/GLM-4.5-Air-IQ4_NL-00002-of-00002.gguf build/bin/llama-server --port 8080 --model models/GLM-4.5-Air-IQ4_NL-00001-of-00 002.gguf --temp 0.95 --top_k 40 --top_p 0.7 --min_p 0.05 --ctx-size 131072 --fla sh-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & $ cat ~/claude-code-router { \"LOG\": true, \"Providers\": [ { \"name\": \"openai\", \"api_base_url\": \"http://localhost:8080/v1/chat/completions\", \"api_key\": \"xxxx\", \"models\": [\"unsloth/GLM-4.5-Air-GGUF\"] } ], \"Router\": { \"default\": \"openai,unsloth/GLM-4.5-Air-GGUF\", \"background\": \"openai,unsloth/GLM-4.5-Air-GGUF\", \"think\": \"openai,unsloth/GLM-4.5-Air-GGUF\", \"longContext\": \"openai,unsloth/GLM-4.5-Air-GGUF\", \"longContextThreshold\": 64000, \"webSearch\": \"openai,unsloth/GLM-4.5-Air-GGUF\" } } + LJ Mon 18 Aug 2025 10:39:21 BST � Competitive performance on academic benchmarks like AIME-24 AIME-25, AMC-23, MATH-500 and GPQA considering model size. https://huggingface."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "th/GLM-4.5-Air-GGUF\" } } + LJ Mon 18 Aug 2025 10:39:21 BST � Competitive performance on academic benchmarks like AIME-24 AIME-25, AMC-23, MATH-500 and GPQA considering model size. https://huggingface.co/ServiceNow-AI/Apriel-Nemotron-15b-Thinker Summary Apriel-Nemotron-15b-Thinker is a 15�billion�parameter reasoning model in Servic eNow�s Apriel SLM series which achieves competitive performance against similarl y sized state-of-the-art models like o1�mini, QWQ�32b, and EXAONE�Deep�32b, all while maintaining only half the memory footprint of those alternatives. It build s upon the Apriel�15b�base checkpoint through a three�stage training pipeline (C PT, SFT and GRPO). Highlights Half the size of SOTA models like QWQ-32b and EXAONE-32b and hence memory effici ent. It consumes 40% less tokens compared to QWQ-32b, making it super efficient in pr oduction. ��� On par or outperforms on tasks like - MBPP, BFCL, Enterprise RAG, MT Bench, MixE val, IFEval and Multi-Challenge making it great for Agentic / Enterprise tasks. Competitive performance on academic benchmarks like AIME-24 AIME-25, AMC-23, MAT H-500 and GPQA considering model size. https://arxiv.org/abs/2508.10948 [Submitted on 13 Aug 2025] Apriel-Nemotron-15B-Thinker While large language models (LLMs) have achieved remarkable reasoning capabiliti es across domains like code, math and other enterprise tasks, their significant memory and computational costs often preclude their use in practical enterprise settings."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "emarkable reasoning capabiliti es across domains like code, math and other enterprise tasks, their significant memory and computational costs often preclude their use in practical enterprise settings. To this end, we introduce Apriel-Nemotron-15B-Thinker, a 15-billion pa rameter model in the ServiceNow Apriel SLM series that achieves performance agai nst medium sized state-of-the-art models such as o1-mini, QWQ32B, and EXAONE-Dee p-32B while maintaining only half the memory footprint of those alternatives. Ap riel-Nemotron-15B-Thinker model is trained in a four stage training pipeline inc luding 1) Base Model upscaling, 2) Continual Pre-training 3) Supervised Fine-tun ing (SFT) and 4) Reinforcement Learning using GRPO. Comprehensive evaluations ac ross a diverse suite of benchmarks consistently demonstrate that our Apriel-Nemo tron-15B-Thinker model matches or exceeds the performance of its 32-billion para meter counterparts, despite being less than half their size. https://huggingface.co/bartowski/ServiceNow-AI_Apriel-Nemotron-15b-Thinker-GGUF https://huggingface.co/bartowski/ServiceNow-AI_Apriel-Nemotron-15b-Thinker-GGUF/ blob/main/ServiceNow-AI_Apriel-Nemotron-15b-Thinker-Q6_K_L.gguf build/bin/llama-server --port 8080 --model models/ServiceNow-AI_Apriel-Nemotron- 15b-Thinker-Q6_K_L.gguf --temp 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ron-15b-Thinker-GGUF/ blob/main/ServiceNow-AI_Apriel-Nemotron-15b-Thinker-Q6_K_L.gguf build/bin/llama-server --port 8080 --model models/ServiceNow-AI_Apriel-Nemotron- 15b-Thinker-Q6_K_L.gguf --temp 0.6 --flash-attn --cache-type-k q8_0 --cache-type -v q8_0 --jinja & Seems using the homerew compiler doesn't work, one must use the MacOS compiler a nd libraries ljubomir@gigul2(422663.llama.cpp:0):~/llama.cpp$ brew install libomp ==> Caveats ==> libomp libomp is keg-only, which means it was not symlinked into /opt/homebrew, because it can override GCC headers and result in broken builds. For compilers to find libomp you may need to set: export LDFLAGS=\"-L/opt/homebrew/opt/libomp/lib\" export CPPFLAGS=\"-I/opt/homebrew/opt/libomp/include\" export CC=gcc-15 export CXX=g++-15 export LDFLAGS=\"-L/opt/homebrew/opt/libomp/lib\" export CPPFLAGS=\"-I/opt/homebrew/opt/libomp/include\" env |egrep 'CC|CXX|FLAGS' or using clang from MacOS, but pointing to general libomp from homebrew unset CC CXX LDFLAGS CPPFLAGS export OpenMP_C_FLAGS=\"-I/opt/homebrew/opt/libomp/include\" export OpenMP_CXX_FLAGS=\"-I/opt/homebrew/opt/libomp/include\" export OpenMP_C_LIBRARIES=\"-L/opt/homebrew/opt/libomp/lib -lomp\" export OpenMP_CXX_LIBRARIES=\"-L/opt/homebrew/opt/libomp/lib -lomp\" env |egrep 'CC|CXX|FLAGS|OpenMP' unset CC CXX LDFLAGS CPPFLAGS OpenMP_C_FLAGS OpenMP_CXX_FLAGS OpenMP_C_LIBRARIES OpenMP_CXX_LIBRARIES CMAKE_OPENMP_C_FLAGS CMAKE_OPENMP_CXX_FLAGS # Set variables to point directly to the libomp Cellar location"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "XX LDFLAGS CPPFLAGS OpenMP_C_FLAGS OpenMP_CXX_FLAGS OpenMP_C_LIBRARIES OpenMP_CXX_LIBRARIES CMAKE_OPENMP_C_FLAGS CMAKE_OPENMP_CXX_FLAGS # Set variables to point directly to the libomp Cellar location export CPPFLAGS=\"-I/opt/homebrew/Cellar/libomp/20.1.8/include\" export LDFLAGS=\"-L/opt/homebrew/Cellar/libomp/20.1.8/lib -lomp\" # Use -fopenmp as the standard flag for AppleClang to enable OpenMP export CMAKE_OPENMP_C_FLAGS=\"-fopenmp\" export CMAKE_OPENMP_CXX_FLAGS=\"-fopenmp\" export OpenMP_C_FLAGS=\"-I/opt/homebrew/Cellar/libomp/20.1.8/include\" export OpenMP_CXX_FLAGS=\"-I/opt/homebrew/Cellar/libomp/20.1.8/include\" export OpenMP_C_LIBRARIES=\"-L/opt/homebrew/Cellar/libomp/20.1.8/lib -lomp\" export OpenMP_CXX_LIBRARIES=\"-L/opt/homebrew/Cellar/libomp/20.1.8/lib -lomp\" env |egrep 'CC|CXX|FLAGS|OpenMP|OPENMP' Results in many many errors. Seems using the homerew compiler doesn't work, one must use the MacOS compiler a nd libraries ljubomir@macbook2(:):~/llama.cpp$ brew reinstall libomp ==> Fetching downloads for: libomp ==> Downloading https://ghcr.io/v2/homebrew/core/libomp/manifests/20.1.8 Already downloaded: /Users/ljubomir/Library/Caches/Homebrew/downloads/009dd6eb44 8288b6172c6eadf901a907fd0e3971359a6190eec3ce22273cdef0--libomp-20.1.8.bottle_man ifest.json ==> Fetching libomp ==> Downloading https://ghcr."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "sers/ljubomir/Library/Caches/Homebrew/downloads/009dd6eb44 8288b6172c6eadf901a907fd0e3971359a6190eec3ce22273cdef0--libomp-20.1.8.bottle_man ifest.json ==> Fetching libomp ==> Downloading https://ghcr.io/v2/homebrew/core/libomp/blobs/sha256:7e2e7b43418 7fffff654343b29a1d108a901d19eb5de892bb3ead7d72fdd0ddf Already downloaded: /Users/ljubomir/Library/Caches/Homebrew/downloads/fe1cce4f5e 781ff276363e66d6692c8a266b2acd9a56aad3288824c5cbcc9f9a--libomp--20.1.8.arm64_seq uoia.bottle.tar.gz ==> Reinstalling libomp ==> Pouring libomp--20.1.8.arm64_sequoia.bottle.tar.gz ==> Caveats libomp is keg-only, which means it was not symlinked into /opt/homebrew, because it can override GCC headers and result in broken builds. For compilers to find libomp you may need to set: export LDFLAGS=\"-L/opt/homebrew/opt/libomp/lib\" export CPPFLAGS=\"-I/opt/homebrew/opt/libomp/include\" ljubomir@macbook2(:):~/llama.cpp$ find /opt/homebrew |grep -i libomp /opt/homebrew/opt/libomp /opt/homebrew/Cellar/llvm/19.1.7_1/lib/libomp.dylib /opt/homebrew/Cellar/open-mpi/5.0.8/lib/openmpi/libompi_dbg_msgq.so /opt/homebrew/Cellar/open-mpi/5.0.7/lib/openmpi/libompi_dbg_msgq.so /opt/homebrew/Cellar/libomp /opt/homebrew/Cellar/libomp/20.1.8 /opt/homebrew/Cellar/libomp/20.1.8/INSTALL_RECEIPT.json /opt/homebrew/Cellar/libomp/20.1.8/.brew /opt/homebrew/Cellar/libomp/20.1.8/.brew/libomp.rb /opt/homebrew/Cellar/libomp/20.1.8/include /opt/homebrew/Cellar/libomp/20.1.8/include/ompx.h /opt/homebrew/Cellar/libomp/20.1."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ar/libomp/20.1.8/.brew /opt/homebrew/Cellar/libomp/20.1.8/.brew/libomp.rb /opt/homebrew/Cellar/libomp/20.1.8/include /opt/homebrew/Cellar/libomp/20.1.8/include/ompx.h /opt/homebrew/Cellar/libomp/20.1.8/include/ompt.h /opt/homebrew/Cellar/libomp/20.1.8/include/omp.h /opt/homebrew/Cellar/libomp/20.1.8/include/omp-tools.h /opt/homebrew/Cellar/libomp/20.1.8/sbom.spdx.json /opt/homebrew/Cellar/libomp/20.1.8/lib /opt/homebrew/Cellar/libomp/20.1.8/lib/libomp.dylib /opt/homebrew/Cellar/libomp/20.1.8/lib/libomp.a ljubomir@gigul2(422663.llama.cpp:0):~/llama.cpp$ git pull mviv build{,.1} # Unset the variables you set earlier to avoid conflicts unset CXXFLAGS CFLAGS LDFLAGS CPPFLAGS OpenMP_C_FLAGS OpenMP_CXX_FLAGS OpenMP_C_ LIBRARIES OpenMP_CXX_LIBRARIES CMAKE_OPENMP_C_FLAGS CMAKE_OPENMP_CXX_FLAGS # Tell CMake where to find the libomp package. # The Recommended Solution: Use CMAKE_PREFIX_PATH # The most reliable and modern way to tell CMake where to find dependencies inst alled by Homebrew is to use the CMAKE_PREFIX_PATH. This variable points CMake to the root directory of the installed package. # Point CMake to your libomp installation and run it. The brew --prefix libomp c ommand dynamically finds the correct path, so you don't have to hardcode the ver sion number. # This works because the FindOpenMP module will search within the CMAKE_PREFIX_P ATH for the necessary include/omp.h header and lib/libomp.dylib library, automat ically configuring everything it needs."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "This works because the FindOpenMP module will search within the CMAKE_PREFIX_P ATH for the necessary include/omp.h header and lib/libomp.dylib library, automat ically configuring everything it needs. export CMAKE_PREFIX_PATH=$(brew --prefix libomp) env |egrep 'CC|CXX|FLAGS|OpenMP|OPENMP|CMAKE' # Remove the old build directory rm -rf build cmake . -B ./build cmake --build build --config Release -j # https://127.0.0.1:8080 build/bin/llama-server --port 8080 --model models/ServiceNow-AI_Apriel-Nemotron- 15b-Thinker-Q6_K_L.gguf --temp 0.6 --ctx-size 131072 --flash-attn --cache-type- k q8_0 --cache-type-v q8_0 --jinja & LJ Mon 18 Aug 2025 10:39:21 BST + LJ Thu 21 Aug 2025 01:05:30 BST � https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-32B-GGUF /blob/main/nvidia_OpenReasoning-Nemotron-32B-Q6_K_L.gguf https://huggingface.co/collections/nvidia/openreasoning-nemotron-687730dae017005 9860f1f01 https://huggingface.co/nvidia/OpenReasoning-Nemotron-32B https://huggingface.co/bartowski/nvidia_OpenReasoning-Nemotron-32B-GGUF/blob/mai n/nvidia_OpenReasoning-Nemotron-32B-Q6_K_L.gguf https://www.reddit.com/r/LocalLLaMA/comments/1m394zh/new_models_from_nvidia_open reasoningnemotron/ # https://127.0.0.1:8080 build/bin/llama-server --port 8080 --model models/nvidia_OpenReasoning-Nemotron- 32B-Q6_K_L.gguf --model-draft models/nvidia_OpenReasoning-Nemotron-1.5B-Q8_0.ggu f --temp 0.1 --top-p 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "127.0.0.1:8080 build/bin/llama-server --port 8080 --model models/nvidia_OpenReasoning-Nemotron- 32B-Q6_K_L.gguf --model-draft models/nvidia_OpenReasoning-Nemotron-1.5B-Q8_0.ggu f --temp 0.1 --top-p 0.9 --top-k 50 --ctx-size 65536 --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & LJ Thu 21 Aug 2025 01:05:30 BST + Update qwen-cde Update qwen-cde $ npm install -g @qwen-code/qwen-code Check version $ qwen --version # extend context (262144) 256K->1M (1048576), flash attention cached; access on http://127.0.0.1:8080 build/bin/llama-server --port 8080 --model models/Qwen3-Coder-30B-A3B-Instruct-1 M-IQ4_NL.gguf --temp 0.7 --top_k 20 --top_p 0.8 --min_p 0 --ctx-size 1048576 --r ope-scaling yarn --rope-scale 4 --yarn-orig-ctx 262144 --flash-attn --cache-type -k q8_0 --cache-type-v q8_0 --jinja & https://github.com/QwenLM/qwen-code export OPENAI_API_KEY=\"your_api_key_here\" export OPENAI_BASE_URL=\"your_api_endpoint\" export OPENAI_MODEL=\"your_model_choice\" NB claude use DeepSeek API is env -u ANTHROPIC_API_KEY ANTHROPIC_BASE_URL=\"https://api.deepseek.com/anthropic\" ANTHROPIC_AUTH_TOKEN=\"${DEEPSEEK_API_KEY}\" ANTHROPIC_MODEL=\"deepseek-chat\" ANTH ROPIC_SMALL_FAST_MODEL=\"deepseek-chat\" claude OPENAI_API_KEY=123 OPENAI_BASE_URL=http://localhost:[port]/v1 OPENAI_MODEL=qwen/qwen3-coder-30b Run qwen-code env OPENAI_API_KEY=123 OPENAI_BASE_URL=http://localhost:8080/v1 OPENAI_MODEL=qwe n/qwen3-coder-30ba-3b qwen Use OpenAI auth. # access on http://127.0.0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "OPENAI_MODEL=qwen/qwen3-coder-30b Run qwen-code env OPENAI_API_KEY=123 OPENAI_BASE_URL=http://localhost:8080/v1 OPENAI_MODEL=qwe n/qwen3-coder-30ba-3b qwen Use OpenAI auth. # access on http://127.0.0.1:8080 build/bin/llama-server --port 8080 --model models/Qwen3-Coder-30B-A3B-Instruct-1 M-IQ4_NL.gguf --temp 0.7 --top_k 20 --top_p 0.8 --min_p 0 --repeat-penalty 1.05 --ctx-size 262144 --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-1M-GGUF Re-downloaded quant from Unsloth as they differ ljubomir@macbook2(:):~/llama.cpp$ ls -la models/Qwen3-Coder-30B-A3B-Instruct-1M- IQ4_NL.gguf* -rw-r--r--@ 1 ljubomir staff 17310784864 24 Aug 06:14 models/Qwen3-Coder-30B-A 3B-Instruct-1M-IQ4_NL.gguf -rw-r--r--@ 1 ljubomir staff 17310784928 31 Jul 19:48 models/Qwen3-Coder-30B-A 3B-Instruct-1M-IQ4_NL.gguf.1 OK - the new file works, where the old one didn't => old file deleted. This works: ljubomir@macbook2(:):~/z/itrade/contrib/crm/src$ env OPENAI_API_KEY=123 OPENAI_B ASE_URL=http://localhost:8080/v1 OPENAI_MODEL=qwen/qwen3-coder-30ba-3b qwen The 1st time asked to login, options 1) Qwen 2) OpenAI - choose #2 OpenAI # access on http://127.0.0.1:8080 build/bin/llama-server --port 8080 --model models/Qwen3-Coder-30B-A3B-Instruct-1 M-IQ4_NL.gguf --temp 0.7 --top_k 20 --top_p 0.8 --min_p 0 --repeat-penalty 1.05 --ctx-size 262144 --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & sudo sysctl iogpu."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "r-30B-A3B-Instruct-1 M-IQ4_NL.gguf --temp 0.7 --top_k 20 --top_p 0.8 --min_p 0 --repeat-penalty 1.05 --ctx-size 262144 --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & sudo sysctl iogpu.wired_limit_mb=88000 # extend context (262144) 256K->1M (1048576), flash attention cached; access on http://127.0.0.1:8080 build/bin/llama-server --port 8080 --model models/Qwen3-Coder-30B-A3B-Instruct-1 M-IQ4_NL.gguf --temp 0.7 --top_k 20 --top_p 0.8 --min_p 0 --repeat-penalty 1.05 --ctx-size 1048576 --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 262144 -- flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & qwen-code-localhost-8080() { env OPENAI_API_KEY=123 OPENAI_BASE_URL=http://localhost:8080/v1 OPENAI_MODEL=q wen/qwen3-coder-30ba-3b qwen } + https://docs.unsloth.ai/basics/qwen3-coder-how-to-run-locally https://docs.unsloth.ai/basics/qwen3-coder-how-to-run-locally Basics � Qwen3-Coder: How to Run Locally Run Qwen3-Coder-30B-A3B-Instruct and 480B-A35B locally with Unsloth Dynamic quan ts. Qwen3-Coder is Qwen�s new series of coding agent models, available in 30B (Qwen3 -Coder-Flash) and 480B parameters. Qwen3-480B-A35B-Instruct achieves SOTA coding performance rivalling Claude�Sonnet-4, GPT-4.1, and Kimi K2, with 61.8% on Aid er Polygot and support for 256K (extendable to 1M) token context. We also uploaded Qwen3-Coder with native 1M context length extended by YaRN and full-precision 8bit and 16bit versions."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "h 61.8% on Aid er Polygot and support for 256K (extendable to 1M) token context. We also uploaded Qwen3-Coder with native 1M context length extended by YaRN and full-precision 8bit and 16bit versions. Unsloth also now supports fine-tuning an d RL of Qwen3-Coder. UPDATE: We fixed tool-calling for Qwen3-Coder! You can now use tool-calling seam lessly in llama.cpp, Ollama, LMStudio, Open WebUI, Jan etc. This issue was unive rsal and affected all uploads (not just Unsloth), and we've communicated with th e Qwen team about our fixes! Read more Run 30B-A3BRun 480B-A35B Does Unsloth Dynamic Quants work? Yes, and very well. In third-party testing on the Aider Polyglot benchmark, the UD-Q4_K_XL (276GB) dynamic quant nearly matche d the full bf16 (960GB) Qwen3-coder model, scoring 60.9% vs 61.8%. More details here. Qwen3 Coder - Unsloth Dynamic 2.0 GGUFs: Dynamic 2.0 GGUF (to run) 1M Context Dynamic 2.0 GGUF 30B-A3B-Instruct 480B-A35B-Instruct 30B-A3B-Instruct 480B-A35B-Instruct �� Running Qwen3-Coder Below are guides for the 30B-A3B and 480B-A35B variants of the model. �� Recommended Settings Qwen recommends these inference settings for both models: temperature=0.7, top_p=0.8, top_k=20, repetition_penalty=1.05 Temperature of 0.7 Top_K of 20 Min_P of 0.00 (optional, but 0.01 works well, llama.cpp default is 0.1) Top_P of 0.8 Repetition Penalty of 1."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "mperature=0.7, top_p=0.8, top_k=20, repetition_penalty=1.05 Temperature of 0.7 Top_K of 20 Min_P of 0.00 (optional, but 0.01 works well, llama.cpp default is 0.1) Top_P of 0.8 Repetition Penalty of 1.05 Chat template: Copy <|im_start|>user Hey there!<|im_end|> <|im_start|>assistant What is 1+1?<|im_end|> <|im_start|>user 2<|im_end|> <|im_start|>assistant Recommended context output: 65,536 tokens (can be increased). Details here. Chat template/prompt format with newlines un-rendered Copy <|im_start|>user\\nHey there!<|im_end|>\\n<|im_start|>assistant\\nWhat is 1+1?<|im_ end|>\\n<|im_start|>user\\n2<|im_end|>\\n<|im_start|>assistant\\n Chat template for tool calling (Getting the current temperature for San Francisc o). More details here for how to format tool calls. Copy <|im_start|>user What's the temperature in San Francisco now? How about tomorrow?<|im_end|> <|im_start|>assistant <tool_call>\\n<function=get_current_temperature>\\n<parameter=location>\\nSan Franc isco, CA, USA </parameter>\\n</function>\\n</tool_call><|im_end|> <|im_start|>user <tool_response> {\"temperature\": 26.1, \"location\": \"San Francisco, CA, USA\", \"unit\": \"celsius\"} </tool_response>\\n<|im_end|> Reminder that this model supports only non-thinking mode and does not generate < think></think> blocks in its output. Meanwhile, specifying enable_thinking=False is no longer required."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "se>\\n<|im_end|> Reminder that this model supports only non-thinking mode and does not generate < think></think> blocks in its output. Meanwhile, specifying enable_thinking=False is no longer required. Run Qwen3-Coder-30B-A3B-Instruct: To achieve inference speeds of 6+ tokens per second for our Dynamic 4-bit quant, have at least 18GB of unified memory (combined VRAM and RAM) or 18GB of system RAM alone. As a rule of thumb, your available memory should match or exceed the size of the model you�re using. E.g. the UD_Q8_K_XL quant (full precision), whic h is 32.5GB, will require at least 33GB of unified memory (VRAM + RAM) or 33GB o f RAM for optimal performance. NOTE: The model can run on less memory than its total size, but this will slow d own inference. Maximum memory is only needed for the fastest speeds. Given that this is a non thinking model, there is no need to set thinking=False and the model does not generate <think> </think> blocks. Follow the best practices above. They're the same as the 480B model. � Ollama: Run Qwen3-Coder-30B-A3B-Instruct Tutorial Install ollama if you haven't already! You can only run models up to 32B in size . Copy apt-get update apt-get install pciutils -y curl -fsSL https://ollama.com/install.sh | sh Run the model! Note you can call ollama servein another terminal if it fails! We include all our fixes and suggested parameters (temperature etc) in params in o ur Hugging Face upload! Copy ollama run hf."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "the model! Note you can call ollama servein another terminal if it fails! We include all our fixes and suggested parameters (temperature etc) in params in o ur Hugging Face upload! Copy ollama run hf.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:UD-Q4_K_XL � Llama.cpp: Run Qwen3-Coder-30B-A3B-Instruct Tutorial Obtain the latest llama.cpp on GitHub here. You can follow the build instruction s below as well. Change -DGGML_CUDA=ON to -DGGML_CUDA=OFF if you don't have a GP U or just want CPU inference. Copy apt-get update apt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y git clone https://github.com/ggml-org/llama.cpp cmake llama.cpp -B llama.cpp/build \\ -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON cmake --build llama.cpp/build --config Release -j --clean-first --target llama-c li llama-gguf-split cp llama.cpp/build/bin/llama-* llama.cpp You can directly pull from HuggingFace via: Copy ./llama.cpp/llama-cli \\ -hf unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF:Q4_K_XL \\ --jinja -ngl 99 --threads -1 --ctx-size 32684 \\ --temp 0.7 --min-p 0.0 --top-p 0.80 --top-k 20 --repeat-penalty 1.05 Download the model via (after installing pip install huggingface_hub hf_transfer ). You can choose UD_Q4_K_XL or other quantized versions. Copy # !pip install huggingface_hub hf_transfer import os os."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "Download the model via (after installing pip install huggingface_hub hf_transfer ). You can choose UD_Q4_K_XL or other quantized versions. Copy # !pip install huggingface_hub hf_transfer import os os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\" from huggingface_hub import snapshot_download snapshot_download( repo_id = \"unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF\", local_dir = \"unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF\", allow_patterns = [\"*UD-Q4_K_XL*\"], ) Run Qwen3-Coder-480B-A35B-Instruct: To achieve inference speeds of 6+ tokens per second for our 1-bit quant, we reco mmend at least 150GB of unified memory (combined VRAM and RAM) or 150GB of syste m RAM alone. As a rule of thumb, your available memory should match or exceed th e size of the model you�re using. E.g. the Q2_K_XL quant, which is 180GB, will r equire at least 180GB of unified memory (VRAM + RAM) or 180GB of RAM for optimal performance. NOTE: The model can run on less memory than its total size, but this will slow d own inference. Maximum memory is only needed for the fastest speeds. Follow the best practices above. They're the same as the 30B model. � Llama.cpp: Run Qwen3-Coder-480B-A35B-Instruct Tutorial For Coder-480B-A35B, we will specifically use Llama.cpp for optimized inference and a plethora of options. If you want a full precision unquantized version, use our Q8_K_XL, Q8_0 or BF16 versions! Obtain the latest llama.cpp on GitHub here. You can follow the build instruction s below as well."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "of options. If you want a full precision unquantized version, use our Q8_K_XL, Q8_0 or BF16 versions! Obtain the latest llama.cpp on GitHub here. You can follow the build instruction s below as well. Change -DGGML_CUDA=ON to -DGGML_CUDA=OFF if you don't have a GP U or just want CPU inference. Copy apt-get update apt-get install pciutils build-essential cmake curl libcurl4-openssl-dev -y git clone https://github.com/ggml-org/llama.cpp cmake llama.cpp -B llama.cpp/build \\ -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON cmake --build llama.cpp/build --config Release -j --clean-first --target llama-c li llama-gguf-split cp llama.cpp/build/bin/llama-* llama.cpp You can directly use llama.cpp to download the model but I normally suggest usin g huggingface_hub To use llama.cpp directly, do: Copy ./llama.cpp/llama-cli \\ -hf unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF:Q2_K_XL \\ --threads -1 \\ --ctx-size 16384 \\ --n-gpu-layers 99 \\ -ot \".ffn_.*_exps.=CPU\" \\ --temp 0.7 \\ --min-p 0.0 \\ --top-p 0.8 \\ --top-k 20 \\ --repeat-penalty 1.05 Or, download the model via (after installing pip install huggingface_hub hf_tran sfer ). You can choose UD-Q2_K_XL, or other quantized versions.. Copy # !pip install huggingface_hub hf_transfer import os os."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "nload the model via (after installing pip install huggingface_hub hf_tran sfer ). You can choose UD-Q2_K_XL, or other quantized versions.. Copy # !pip install huggingface_hub hf_transfer import os os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\" # Can sometimes rate limit, so set to 0 to disable from huggingface_hub import snapshot_download snapshot_download( repo_id = \"unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF\", local_dir = \"unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF\", allow_patterns = [\"*UD-Q2_K_XL*\"], ) Run the model in conversation mode and try any prompt. Edit --threads -1 for the number of CPU threads, --ctx-size 262114 for context l ength, --n-gpu-layers 99 for GPU offloading on how many layers. Try adjusting it if your GPU goes out of memory. Also remove it if you have CPU only inference. Use -ot \".ffn_.*_exps.=CPU\" to offload all MoE layers to the CPU! This effective ly allows you to fit all non MoE layers on 1 GPU, improving generation speeds. You can customize the regex expression to fit more layers if you have more GPU c apacity. More options discussed here. Copy ./llama.cpp/llama-cli \\ --model unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF/UD-Q2_K_XL/Qwen3-Coder-4 80B-A35B-Instruct-UD-Q2_K_XL-00001-of-00004.gguf \\ --threads -1 \\ --ctx-size 16384 \\ --n-gpu-layers 99 \\ -ot \".ffn_.*_exps.=CPU\" \\ --temp 0.7 \\ --min-p 0.0 \\ --top-p 0.8 \\ --top-k 20 \\ --repeat-penalty 1.05 Also don't forget about the new Qwen3 update. Run Qwen3-235B-A22B-Instruct-2507 locally with llama."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ot \".ffn_.*_exps.=CPU\" \\ --temp 0.7 \\ --min-p 0.0 \\ --top-p 0.8 \\ --top-k 20 \\ --repeat-penalty 1.05 Also don't forget about the new Qwen3 update. Run Qwen3-235B-A22B-Instruct-2507 locally with llama.cpp. � � Improving generation speed If you have more VRAM, you can try offloading more MoE layers, or offloading who le layers themselves. Normally, -ot \".ffn_.*_exps.=CPU\" offloads all MoE layers to the CPU! This effe ctively allows you to fit all non MoE layers on 1 GPU, improving generation spee ds. You can customize the regex expression to fit more layers if you have more G PU capacity. If you have a bit more GPU memory, try -ot \".ffn_(up|down)_exps.=CPU\" This offlo ads up and down projection MoE layers. Try -ot \".ffn_(up)_exps.=CPU\" if you have even more GPU memory. This offloads on ly up projection MoE layers. You can also customize the regex, for example -ot \"\\.(6|7|8|9|[0-9][0-9]|[0-9][0 -9][0-9])\\.ffn_(gate|up|down)_exps.=CPU\" means to offload gate, up and down MoE layers but only from the 6th layer onwards. The latest llama.cpp release also introduces high throughput mode. Use llama-par allel. Read more about it here. You can also quantize the KV cache to 4bits for example to reduce VRAM / RAM movement, which can also make the generation proces s faster. �How to fit long context (256K to 1M) To fit longer context, you can use KV cache quantization to quantize the K and V caches to lower bits."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "vement, which can also make the generation proces s faster. �How to fit long context (256K to 1M) To fit longer context, you can use KV cache quantization to quantize the K and V caches to lower bits. This can also increase generation speed due to reduced RA M / VRAM data movement. The allowed options for K quantization (default is f16) include the below. --cache-type-k f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1 You should use the _1 variants for somewhat increased accuracy, albeit it's slig htly slower. For eg q4_1, q5_1 You can also quantize the V cache, but you will need to compile llama.cpp with F lash Attention support via -DGGML_CUDA_FA_ALL_QUANTS=ON, and use --flash-attn to enable it. We also uploaded 1 million context length GGUFs via YaRN scaling here. � Tool Calling Fixes We managed to fix tool calling via llama.cpp --jinja specifically for serving th rough llama-server! If you�re downloading our 30B-A3B quants, no need to worry a s these already include our fixes. For the 480B-A35B model, please: Download the first file at https://huggingface.co/unsloth/Qwen3-Coder-480B-A35B- Instruct-GGUF/tree/main/UD-Q2_K_XL for UD-Q2_K_XL, and replace your current file Use snapshot_download as usual as in https://docs.unsloth.ai/basics/qwen3-coder- how-to-run-locally#llama.cpp-run-qwen3-tutorial which will auto override the old files Use the new chat template via --chat-template-file. See GGUF chat template or ch at_template."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "basics/qwen3-coder- how-to-run-locally#llama.cpp-run-qwen3-tutorial which will auto override the old files Use the new chat template via --chat-template-file. See GGUF chat template or ch at_template.jinja As an extra, we also made 1 single 150GB UD-IQ1_M file (so Ollama works) at http s://huggingface.co/unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF/blob/main/Qwen3-C oder-480B-A35B-Instruct-UD-IQ1_M.gguf This should solve issues like: https://github.com/ggml-org/llama.cpp/issues/1491 5 Using Tool Calling To format the prompts for tool calling, let's showcase it with an example. I created a Python function called get_current_temperature which is a function w hich should get the current temperature for a location. For now we created a pla ceholder function which will always return 21.6 degrees celsius. You should chan ge this to a true function!! Copy def get_current_temperature(location: str, unit: str = \"celsius\"): \"\"\"Get current temperature at a location. Args: location: The location to get the temperature for, in the format \"City, State, Country\". unit: The unit to return the temperature in. Defaults to \"celsius\". (cho ices: [\"celsius\", \"fahrenheit\"]) Returns: the temperature, the location, and the unit in a dict \"\"\" return { \"temperature\": 26.1, # PRE_CONFIGURED -> you change this! \"location\": location, \"unit\": unit, } Then use the tokenizer to create the entire prompt: Copy from transformers import AutoTokenizer tokenizer = AutoTokenizer."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "1, # PRE_CONFIGURED -> you change this! \"location\": location, \"unit\": unit, } Then use the tokenizer to create the entire prompt: Copy from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Qwen3-Coder-480B-A35B-Instruc t\") messages = [ {'role': 'user', 'content': \"What's the temperature in San Francisco now? Ho w about tomorrow?\"}, {'content': \"\", 'role': 'assistant', 'function_call': None, 'tool_calls': [ {'id': 'ID', 'function': {'arguments': {\"location\": \"San Francisco, CA, USA\"}, 'name': 'get_current_temperature'}, 'type': 'function'}, ]}, {'role': 'tool', 'content': '{\"temperature\": 26.1, \"location\": \"San Francisc o, CA, USA\", \"unit\": \"celsius\"}', 'tool_call_id': 'ID'}, ] prompt = tokenizer.apply_chat_template(messages, tokenize = False) https://www.reddit.com/r/LocalLLaMA/comments/1mi9i1g/advice_on_running_qwen3code r30ba3b_locally/ + LJ Sun 24 Aug 2025 18:17:34 BST � https://github.com/ggml-org/llama.cpp/discussions/15396 https://github.com/ggml-org/llama.cpp/discussions/15396 gpt-oss-120b build/bin/llama-server --port 8080 --model models/gpt-oss-120b-MXFP4-00001-of-00 002.gguf --ctx-size 131072 --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --jinja & openai/gpt-oss-20b https://huggingface.co/unsloth/gpt-oss-20b-GGUF Reasoning levels You can adjust the reasoning level that suits your task across three levels: Low: Fast responses for general dialogue. Medium: Balanced speed and detail. High: Deep and detailed analysis."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "oning levels You can adjust the reasoning level that suits your task across three levels: Low: Fast responses for general dialogue. Medium: Balanced speed and detail. High: Deep and detailed analysis. The reasoning level can be set in the system prompts, e.g., \"Reasoning: high\". https://docs.unsloth.ai/basics/gpt-oss-how-to-run-and-fine-tune The gpt-oss models from OpenAI include a feature that allows users to adjust the model's \"reasoning effort.\" This gives you control over the trade-off between t he model's performance and its response speed (latency) which by the amount of t oken the model will use to think. The gpt-oss models offer three distinct levels of reasoning effort you can choos e from: Low: Optimized for tasks that need very fast responses and don't require complex , multi-step reasoning. Medium: A balance between performance and speed. High: Provides the strongest reasoning performance for tasks that require it, th ough this results in higher latency. https://www.reddit.com/r/LocalLLaMA/comments/1mlomlb/my_thoughts_on_gptoss120b/ Still testing, but here's my early efforts: ./llama.cpp/llama-cli \\ --model gpt-oss-120b-UD-Q4_K_XL-00001-of-00002.gguf \\ --n-cpu-moe 27 \\ --n-gpu-layers 999 \\ --ctx-size 120000 \\ --flash-attn \\ --threads 10 \\ --temp 1.0 \\ --min-p 0.0 \\ --top-p 1."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "lama.cpp/llama-cli \\ --model gpt-oss-120b-UD-Q4_K_XL-00001-of-00002.gguf \\ --n-cpu-moe 27 \\ --n-gpu-layers 999 \\ --ctx-size 120000 \\ --flash-attn \\ --threads 10 \\ --temp 1.0 \\ --min-p 0.0 \\ --top-p 1.0 \\ --no-mmap \\ --top-k 40 \\ --interactive If you have the GGUF downloaded, just pass --chat-template-kwargs '{\\\"reasoning_ effort\\\": \\\"high\\\"}' as a parameter build/bin/llama-server --port 8080 --model models/gpt-oss-120b-MXFP4-00001-of-00 002.gguf --temp 1 --top_k 40 --top_p 1 --min_p 0 --ctx-size 131072 --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --chat-template-kwargs '{\\\"reasoning_ef fort\\\": \\\"high\\\"}' --jinja & �� Recommended Settings OpenAI recommends these inference settings for both models: temperature=1.0, top_p=1.0, top_k=0 Temperature of 1.0 Top_K = 0 (or experiment with 100 for possible better results) Top_P = 1.0 Recommended minimum context: 16,384 Maximum context length window: 131,072 Chat template: Copy <|start|>system<|message|>You are ChatGPT, a large language model trained by Ope nAI.\\nKnowledge cutoff: 2024-06\\nCurrent date: 2025-08-05\\n\\nReasoning: medium\\n \\n# Valid channels: analysis, commentary, final. Channel must be included for ev ery message.<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|chann el|>final<|message|>Hi there!<|end|><|start|>user<|message|>What is 1+1?<|end|>< |start|>assistant The end of sentence/generation token: EOS is <|return|> https://huggingface.co/unsloth/gpt-oss-120b-GGUF gpt-oss-20b-Q8_0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "i there!<|end|><|start|>user<|message|>What is 1+1?<|end|>< |start|>assistant The end of sentence/generation token: EOS is <|return|> https://huggingface.co/unsloth/gpt-oss-120b-GGUF gpt-oss-20b-Q8_0.gguf build/bin/llama-server --port 8080 --model models/gpt-oss-20b-Q8_0.gguf --temp 1 --top_k 40 --top_p 1 --min_p 0 --ctx-size 131072 --flash-attn --cache-type-k q8 _0 --cache-type-v q8_0 --chat-template-kwargs '{\\\"reasoning_effort\\\": \\\"high\\\"}' --jinja & https://huggingface.co/unsloth/gpt-oss-120b-GGUF openai/gpt-oss-120b https://github.com/openai/codex/blob/main/codex-rs/config.md https://platform.openai.com/docs/guides/tools-local-shell https://www.reddit.com/r/LocalLLaMA/comments/1miq7sp/gptoss_support_merged_into_ codex/ Finally! From the README.md: Codex can run fully locally against an OpenAI-compatible OSS host (like Ollama) using the --oss flag: Interactive UI: codex --oss Non-interactive (programmatic) mode: echo \"Refactor utils\" | codex exec --oss Model selection when using --oss: If you omit -m/--model, Codex defaults to -m gpt-oss:20b and will verify it exis ts locally (downloading if needed). To pick a different size, pass one of: -m \"gpt-oss:20b\" -m \"gpt-oss:120b\" Point Codex at your own OSS host: By default, --oss talks to http://localhost:11434/v1. To use a different host, set one of these environment variables before running C odex: CODEX_OSS_BASE_URL, for example: CODEX_OSS_BASE_URL=\"http://my-ollama.example."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "alks to http://localhost:11434/v1. To use a different host, set one of these environment variables before running C odex: CODEX_OSS_BASE_URL, for example: CODEX_OSS_BASE_URL=\"http://my-ollama.example.com:11434/v1\" codex --oss -m gpt-os s:20b or CODEX_OSS_PORT (when the host is localhost): CODEX_OSS_PORT=11434 codex --oss https://github.com/openai/gpt-oss Codex We support codex as a client for gpt-oss. To run the 20b version, set this to ~/ .codex/config.toml: disable_response_storage = true show_reasoning_content = true [model_providers.local] name = \"local\" base_url = \"http://localhost:11434/v1\" [profiles.oss] model = \"gpt-oss:20b\" model_provider = \"local\" Still doesn't work, the --oss is for ollama, but not for llama.cpp # The reasoning level can be set in the system prompts, \"Reasoning: low\", \"Reaso ning: medium\", or \"Reasoning: high\". # build/bin/llama-server --port 8080 --model models/gpt-oss-120b-MXFP4-00001-of- 00002.gguf --temp 1 --top_k 40 --top_p 1 --min_p 0 --ctx-size 131072 --flash-at tn --cache-type-k q8_0 --cache-type-v q8_0 --chat-template-kwargs '{\"reasoning_e ffort\": \"high\"}' --jinja & codex-localhost-8080-gpt-oss-120b() { env OPENAI_API_KEY=\"123\" CODEX_OSS_BASE_URL=\"http://localhost:8080/v1\" codex - -oss -m \"gpt-oss:120b\" -c model_reasoning_effort=\"high\" } # build/bin/llama-server --port 8080 --model models/gpt-oss-20b-Q8_0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "ENAI_API_KEY=\"123\" CODEX_OSS_BASE_URL=\"http://localhost:8080/v1\" codex - -oss -m \"gpt-oss:120b\" -c model_reasoning_effort=\"high\" } # build/bin/llama-server --port 8080 --model models/gpt-oss-20b-Q8_0.gguf --temp 1 --top_k 40 --top_p 1 --min_p 0 --ctx-size 131072 --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --chat-template-kwargs '{\"reasoning_effort\": \"high\"}' - -jinja & codex-localhost-8080-gpt-oss-20b() { env OPENAI_API_KEY=\"123\" CODEX_OSS_BASE_URL=\"http://localhost:8080/v1\" codex - -oss -m \"gpt-oss:20b\" -c model_reasoning_effort=\"high\" } But qwen coder works # build/bin/llama-server --port 8080 --model models/gpt-oss-120b-MXFP4-00001-of- 00002.gguf --temp 1 --top_k 40 --top_p 1 --min_p 0 --ctx-size 131072 --flash-at tn --cache-type-k q8_0 --cache-type-v q8_0 --chat-template-kwargs '{\"reasoning_e ffort\": \"high\"}' --jinja & qwen-code-localhost-8080-gpt-oss-120b() { env OPENAI_API_KEY=\"123\" OPENAI_BASE_URL=\"http://localhost:8080/v1\" OPENAI_MOD EL=\"openai/gpt-oss-120b\" qwen } # build/bin/llama-server --port 8080 --model models/gpt-oss-20b-Q8_0.gguf --temp 1 --top_k 40 --top_p 1 --min_p 0 --ctx-size 131072 --flash-attn --cache-type-k q8_0 --cache-type-v q8_0 --chat-template-kwargs '{\"reasoning_effort\": \"high\"}' - -jinja & qwen-code-localhost-8080-gpt-oss-20b() { env OPENAI_API_KEY=\"123\" OPENAI_BASE_URL=\"http://localhost:8080/v1\" OPENAI_MOD EL=\"openai/gpt-oss-20b\" qwen } LJ Sun 24 Aug 2025 18:17:34 BST + unsloth/Seed-OSS-36B-Instruct-GGUF unsloth/Seed-OSS-36B-Instruct-GGU"
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "PI_KEY=\"123\" OPENAI_BASE_URL=\"http://localhost:8080/v1\" OPENAI_MOD EL=\"openai/gpt-oss-20b\" qwen } LJ Sun 24 Aug 2025 18:17:34 BST + unsloth/Seed-OSS-36B-Instruct-GGUF unsloth/Seed-OSS-36B-Instruct-GGUF https://huggingface.co/unsloth/Seed-OSS-36B-Instruct-GGUF We recommend sampling with temperature=1.1 and top_p=0.95. Native Long Context: Trained with up-to-512K long context natively. ljubomir@gigul2(422663.llama.cpp:0):~/llama.cpp$ git pull mviv build{,.1} # Unset the variables you set earlier to avoid conflicts unset CC CXX CXXFLAGS CFLAGS LDFLAGS CPPFLAGS OpenMP_C_FLAGS OpenMP_CXX_FLAGS Op enMP_C_LIBRARIES OpenMP_CXX_LIBRARIES CMAKE_OPENMP_C_FLAGS CMAKE_OPENMP_CXX_FLAG S # Tell CMake where to find the libomp package. # The Recommended Solution: Use CMAKE_PREFIX_PATH # The most reliable and modern way to tell CMake where to find dependencies inst alled by Homebrew is to use the CMAKE_PREFIX_PATH. This variable points CMake to the root directory of the installed package. # Point CMake to your libomp installation and run it. The brew --prefix libomp c ommand dynamically finds the correct path, so you don't have to hardcode the ver sion number. # This works because the FindOpenMP module will search within the CMAKE_PREFIX_P ATH for the necessary include/omp.h header and lib/libomp.dylib library, automat ically configuring everything it needs."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "This works because the FindOpenMP module will search within the CMAKE_PREFIX_P ATH for the necessary include/omp.h header and lib/libomp.dylib library, automat ically configuring everything it needs. export CMAKE_PREFIX_PATH=$(brew --prefix libomp) env |egrep 'CC|CXX|FLAGS|OpenMP|OPENMP|CMAKE' # Remove the old build directory rm -rf build cmake . -B ./build cmake --build build --config Release -j (torch313) ljubomir@macbook2(:):~/llama.cpp$ l models/Seed-OSS-36B-Instruct-IQ4_ NL.gguf -rw-r--r--@ 1 ljubomir staff 19G 25 Aug 21:40 models/Seed-OSS-36B-Instruct-I Q4_NL.gguf sudo sysctl iogpu.wired_limit_mb=88000 # https://127.0.0.1:8080 build/bin/llama-server --port 8080 --model models/Seed-OSS-36B-Instruct-IQ4_NL.g guf --temp 1.1 --top_p 1 --ctx-size 524288 --flash-attn --cache-type-k q8_0 --ca che-type-v q8_0 --jinja & build/bin/llama-server --port 8080 --model models/Seed-OSS-36B-Instruct-IQ4_NL.g guf --temp 1.1 --top_p 1 --ctx-size 100000 --flash-attn --cache-type-k q8_0 --ca che-type-v q8_0 --jinja & + LJ Tue 26 Aug 2025 11:50:26 BST � Token-Efficient Performance: Achieves a +5.2% absolute improvement on subjective, humanities-centric tasks with only 5K training samples, outperforming a 671B DeepSeek-V3 model. https://huggingface."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "� Token-Efficient Performance: Achieves a +5.2% absolute improvement on subjective, humanities-centric tasks with only 5K training samples, outperforming a 671B DeepSeek-V3 model. https://huggingface.co/inclusionAI/Rubicon-Preview inclusionAI/Rubicon-Preview Rubicon � Paper � � Model This is the model card for Rubicon-preview, a 30B-A3B parameter model trained wi th a novel reinforcement learning framework using \"rubric anchors\" to excel at o pen-ended, creative, and humanities-centric tasks. Highlights We introduce Rubicon, a novel framework using rubric anchors for reinforcement l earning. Our model, Rubicon-preview, demonstrates the following key highlights: Token-Efficient Performance: Achieves a +5.2% absolute improvement on subjective , humanities-centric tasks with only 5K training samples, outperforming a 671B D eepSeek-V3 model. Stylistic Controllability: Leverages rubric anchors to precisely guide output st yle, producing responses that are more human-like, emotionally expressive, and l ess formulaic. Preservation of General Abilities: Avoids performance degradation on general tas ks�a common side effect of specialized RL�while delivering additional gains on r easoning benchmarks like AIME 2024 (+4.1%). https://arxiv.org/abs/2508."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": ": Avoids performance degradation on general tas ks�a common side effect of specialized RL�while delivering additional gains on r easoning benchmarks like AIME 2024 (+4.1%). https://arxiv.org/abs/2508.12790 Computer Science > Artificial Intelligence [Submitted on 18 Aug 2025] Reinforcement Learning with Rubric Anchors Zenan Huang, Yihong Zhuang, Guoshan Lu, Zeyu Qin, Haokai Xu, Tianyu Zhao, Ru Pen g, Jiaqi Hu, Zhanming Shen, Xiaomeng Hu, Xijun Gu, Peiyi Tu, Jiaxin Liu, Wenyu C hen, Yuzhuo Fu, Zhiting Fan, Yanmei Gu, Yuanyuan Wang, Zhengkai Yang, Jianguo Li , Junbo Zhao Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing Large Language Models (LLMs), exemplified by the success of OpenAI's o-series. In RLVR, rewards are derived from verifiable signals-such as passing unit tests in code generation or matching correct answers in mathemat ical reasoning. While effective, this requirement largely confines RLVR to domai ns with automatically checkable outcomes. To overcome this, we extend the RLVR p aradigm to open-ended tasks by integrating rubric-based rewards, where carefully designed rubrics serve as structured, model-interpretable criteria for automati c scoring of subjective outputs. We construct, to our knowledge, the largest rub ric reward system to date, with over 10,000 rubrics from humans, LLMs, or a hybr id human-LLM collaboration."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "r automati c scoring of subjective outputs. We construct, to our knowledge, the largest rub ric reward system to date, with over 10,000 rubrics from humans, LLMs, or a hybr id human-LLM collaboration. Implementing rubric-based RL is challenging; we tack le these issues with a clear framework and present an open-sourced Qwen-30B-A3B model with notable gains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended benchmarks (especially humanities), outperforming a 671B DeepSeek -V3 model by +2.4%, while preserving general and reasoning abilities. 2) Our met hod provides fine-grained stylistic control, using rubrics as anchors to mitigat e the \"AI-like\" tone and produce more human-like, expressive responses. We share key lessons in rubric construction, data selection, and training, and discuss l imitations and future releases. # Conduct text completion generated_ids = model.generate( **model_inputs, max_new_tokens=512, do_sample=True, temperature=0.6, top_p=0.95, ) Trained on top of https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507, conte xt length 256K ljubomir@macbook2(:):~/llama.cpp$ l models/Rubicon-Preview.i1-Q6_K.gguf -rw-r--r--@ 1 ljubomir staff 23G 26 Aug 09:14 models/Rubicon-Preview.i1-Q6_K .gguf sudo sysctl iogpu.wired_limit_mb=88000 # https://127.0.0.1:8080 build/bin/llama-server --port 8080 --model models/Rubicon-Preview.i1-Q6_K.gguf - -temp 0.6 --top_p 0."
    },
    {
      "source": "llamacpp-logbook.html",
      "content": "dels/Rubicon-Preview.i1-Q6_K .gguf sudo sysctl iogpu.wired_limit_mb=88000 # https://127.0.0.1:8080 build/bin/llama-server --port 8080 --model models/Rubicon-Preview.i1-Q6_K.gguf - -temp 0.6 --top_p 0.95 --ctx-size 262144 --flash-attn --cache-type-k q8_0 --cach e-type-v q8_0 --jinja & LJ Tue 26 Aug 2025 11:50:26 BST"
    },
    {
      "source": "post-consciousness.html",
      "content": "Consciousness Q: What is your current working definition of consciousness? Everything I've heard from [1]Joscha Bach (JB) makes every sense to me. (and [2]Michael Levin) I have not got much to contribute on top of that. So just enumerating things heard and remembered (even if not super-faithfully) here. To have a brief in one place. The last thing that personally puzzled was: why is consciousness such a big deal? Yeah we have reasons to think \"there is something there\", and we can't \"pin it down\". But our world is filled with such wonders. Why is consciousness so special? IDK. Then recently heard JB on a podcast say \"Aristotle didn't think consciousness was a big deal.\" - and that made me feel better. (latter I heard Pinker say that Dennett thought it \"not a biggie\" too) Maybe I should take consciousness more seriously. But that would be like a non-believer that realises he'd be better off, if he could make himself believe in a deity, when surrounded by theists. Now knowing I'm not the only one thinking it's not that big a deal, and knowing of other - infinitely more illustrious - names, eases the discomfort. Observables that must be true about consciousness (by JB). I agree with JB and this makes every sense to me: 1. Lower level, maybe even lowest level, not \"the pinnacle\". E.g. baby that grows into a Nobel prize winner had consciousness before they became Nobel prize winners. 2. Necessary for learning, bootstraps knowledge acquisition."
    },
    {
      "source": "post-consciousness.html",
      "content": "lowest level, not \"the pinnacle\". E.g. baby that grows into a Nobel prize winner had consciousness before they became Nobel prize winners. 2. Necessary for learning, bootstraps knowledge acquisition. Entities lacking consciousness can't learn, don't learn. Unfortunate kids with brain damage lacking consciousness never learn much, stay almost plants. Zombies in movies lose consciousness at the point of zombification. Their knowledge is frozen to that point, they learn nothing new from that point onwards. 3. Has element of self reflection. We know that we know. This looks less solid than 1-2 above to me. It maybe we are just telling a story, simply doing what brains ordinarily do (construct models on the fly), just on a specific questions about the brain, when turning the brain instrument unto explaining itself, it creates a model. Completely like every other model the mind constructs, nothing special. Like LLM chain of reasoning: asking for self-reflection \"why did you output this\" - it simply comes back with another plausible story, completely unrelated to its inner workings. That the object being modelled is the brain itself, makes zero difference. It's like chip design software, running on x86 itself, designing an x86 chip, versus designing an ARM chip. Running on x86 designing x86, does as good a job as, running on x86 designing an ARM chip. No reason to suspect the software running on x86 will do a better job designing x86 compared to designing ARM."
    },
    {
      "source": "post-consciousness.html",
      "content": "unning on x86 designing x86, does as good a job as, running on x86 designing an ARM chip. No reason to suspect the software running on x86 will do a better job designing x86 compared to designing ARM. Putting 1 + 2 + 3 together, I'm guessing, consciousness is: 4. Feature of ours and animals brains architectures that implements constraints important for learning quickly in the real world. Something that constrains the weight space, so we learn before we become food for others. It must be evolutionary selected if it appears in every living thing. Must confer some advantage. Learning quicker that the competition is an advantage. 5. Constraints act on the internal representation, on the weights of the brains networks. A small controller module, that affects the network weights of the rest of the network. It's action is to constrain the other weights in some way, reduce the space of values the rest of the network weights can take. Like enforce consistency, some kind of averaging in time. 6. Maybe enforcing something meta- about the weights, like regularisation - \"prefer smaller weights\", or maybe sparsity - \"prefer fewer non-zero weights\". (yeah regularisation and sparseness are in tension with each other) These maybe means-to-an end: ways to end up with the trillion dimensional weight space avoid some pathological cases. 7. Or maybe it's one-off, developmental, like choosing architecture, layers (what neurons never connect)."
    },
    {
      "source": "post-consciousness.html",
      "content": "end: ways to end up with the trillion dimensional weight space avoid some pathological cases. 7. Or maybe it's one-off, developmental, like choosing architecture, layers (what neurons never connect). So once the brain develops almost fully past childhood + teenagehood, it becomes less relevant. 8. The puzzle that biology HI use less data than AI, is more data efficient - maybe consciousness is a factor there? If knowledge is represented as acquired (joint) probability function, just placing a bump quickly in it at the (leaves rustling,tiger)-point rather than waiting for it to occur 10 times offers survival advantage. To my mind everything that can be known about the relationship between X and Y is expressed by their joint p.d.f. \\( f_{X,Y}(x,y) \\). Knowing that function [3]is knowing. Intelligence is querying it with an \\( X \\) observation \\( x = a \\), and coming back with knowledge about the goal \\( Y \\) from the conditional \\( f_{Y|X}(y|x=a) \\) that has better error profile than the unconditional marginal \\( f_Y(y) \\). (both conditional and marginal functions are derivable from the joint) \"Better profile\" is minimizing future expected surprisal. (\"expected\" there implies knowledge, the joint pdf; only having knowledge gives rise to expectation.) \"Surprisal\" is the residual part we fail to forecast. Aside: Chain-of-Reasoning is when we come up with R, which is not observed but created as an idea in our mind. R can be a possible factual like X (possible in the world)."
    },
    {
      "source": "post-consciousness.html",
      "content": "ual part we fail to forecast. Aside: Chain-of-Reasoning is when we come up with R, which is not observed but created as an idea in our mind. R can be a possible factual like X (possible in the world). But also R can be a counter-factual, impossible to appear as observation X. Either way, this has to be such R, that P(Y|R,X) brings us closer to a \"good\" Y than P(Y|X) does. (NB from the joint we also have P(R|X) at our disposal too.) These CoR discrete jumps via R, (possibly across local saddle points?) aiding searches via R, can't be steps too far. They have to be close (~20%?) to the current state, for us not to get lost. (if too far the chance of being of use drops a lot; like stepping over stones crossing a river) Q: How would you test for machine consciousness? General: pursue it assuming very low SNR to any evidence brought in. Via commonality - wherever I observe some consciousness effect 1-3 above, I then seek evidence to support finding 4-8 above. Via contrasting - wherever I observe consciousness effect 1-3 above, where I found 4-8 above, now I look to remove 4-8, and see if the effect 1-3 disappears. So compare and contrast both. Need to gather both positive pro- and negative anti- evidence, as in the case of consciousness, both the Q-uestion, and the A-nswer, are unknown. There are too many moving parts for much comfort. Maybe that's the \"hard problem\" referenced?? Concrete steps: 9. Given it's shared by all living brains, assume it's shared by all AI brains."
    },
    {
      "source": "post-consciousness.html",
      "content": "unknown. There are too many moving parts for much comfort. Maybe that's the \"hard problem\" referenced?? Concrete steps: 9. Given it's shared by all living brains, assume it's shared by all AI brains. Take a bunch of LLM-s. Then look - along the lines of the work of [4]Evelina Fedorenko seeking commonality in common language space - look find common things between them, that can be fit in the 1-3 observations above. 10. Look at work of the likes like [5]Chris Olah, try divine features that are structural. And than look again for that commonality in all of them, with an eye to 1-3. 11. Assuming all current LLM-s are conscious to non-zero degree, would think of ways how to zombify them. Whatever I think I found in {9,10} above, try to destroy undo that. Do the networks become less conscious now? In terms of 4-8? Addendum on the consciousness experiments designs. (aug2025) (even if I think it a nothing burger as learned from your Bach Re: Aristotle, Dennett) Look at known but still unresolved why/how-s differences between analogue HI (all life really, and at all levels of org hierarchy) and digital AI-s: 12. Perpetual learning without end. Digital/AI systems collapse. (so we stop training before) Whereas analogue/HI systems (all live systems in general) don't crash and burn, learning without end doesn't undermine them. (except in psychiatric/mind illnesses maybe?) 13. Truly online learning."
    },
    {
      "source": "post-consciousness.html",
      "content": "e) Whereas analogue/HI systems (all live systems in general) don't crash and burn, learning without end doesn't undermine them. (except in psychiatric/mind illnesses maybe?) 13. Truly online learning. Analogue/HI has no batches, no memory except for Now, no buffers no replays, (except maybe memory consolidation while at sleep states?) is at permanent epoch 1. Whereas digital/HI have mini-batches so we need buffers, and epochs >1 so we need \"true\" (computer) memory. That learning regime is not even that well justified by the maths of it. (more like \"motivated\" by somewhat hand-wavy control theories) 14. Counter arguments to 12 and or 13 above, why not do them. Maybe the above 12/13 are not connected strictly to consciousness, but connected to: HI is good with many more parameters than observations, while (current) AI is good with many more observations than parameters. (but this can be folded in the experiments to test too?) Addendum after listening to the Tegmark ToE interview. (sep2025) I liked everything [6]Max Tegmark said on that. And especially liked him say \"enough with the aww-shucks what impenetrable mystery could-be-this could-be-that impossible for us mere mortals to tell; let's design experiments, and measure, and yes it's not easiest to measure but not impossible too - physicists measure far more nuanced things all the time, and lets start ruling out some guesses.\" \"How Physics Absorbed Artificial Intelligence & (Soon) Consciousness\" [7]https://www.youtube."
    },
    {
      "source": "post-consciousness.html",
      "content": "le too - physicists measure far more nuanced things all the time, and lets start ruling out some guesses.\" \"How Physics Absorbed Artificial Intelligence & (Soon) Consciousness\" [7]https://www.youtube.com/watch?v=-gekVfUAS7c, [8]Theories of Everything with [9]Curt Jaimungal, Sep-2025 Consciousness view: it is like the conductor in the orchestra. It is the router in an Mixture of Experts model. Consciousness module is the router in MoE. Experts in the MoE are the individual members of the orchestra, every one playing their own instrument. So while the router is not a very big or a very special module (in fact - it's in many ways simpler then the specialised modules) - it's a single point of failure. So once consciousness (in HI brain) / router (in IA MoE) fails - no expert can learn properly, or even if the experts learns, the knowledge can not be utilised. MoE architecture is the reason why it's so data efficient. Sparse representations, by virtue of injecting that prior knowledge in the process (\"these connections for this data do not need updating\"), can be data efficient. It's efficient to know in advance \"this data is no use to Experts 1,3,4,5, and is to be used to reach only Expert#2\". MoE is a reason why we have too many neurons. Our brains are less efficient than NN-s when it comes to utilising their weight. NN-s are much more efficient than us humans, when looking at efficiency in weights sizes space."
    },
    {
      "source": "post-consciousness.html",
      "content": "have too many neurons. Our brains are less efficient than NN-s when it comes to utilising their weight. NN-s are much more efficient than us humans, when looking at efficiency in weights sizes space. Our brains trade parsimony in weights space, to gain efficiencies to gain speed and reduce power consumption - both achieved by MoE. Sparse representatios (and MoE is a macro-scale example) may make incremental learning, which is one way to implement continuous learning, practically doable. If only a limited set of weight need to be updated, for the brain to acquire new memory or knowledge, that means it can be done without losing all other previous memory or knowledge. <p>At this point maybe one needs to distinguish that consciousness may refer to (a) the basic algorithm that drives the process of self-organisation (b) the result of that process, the state of the brain once it goes 0->1 and becomes conscious. Either way it's like SGD learning and Type 1 learning aka \"pattern recognigtion\" (1 signal on input->1 response on output and 1 reward) in that it is concerned with (c) very short time horizons, and enables the most basic of (d) survival functionality; and (e) further learning like RL learning (sequence of signals on input->sequence of responses on output but only 1 reward at the end).</p> -- LJ HPD Sun 27 Jul 2025 15:12:26 BST References 1. http://bach.ai/ 2. https://thoughtforms.life/ 3. file:///Users/ljubomir/ljubomirj.github.io/post-knowing.html 4. https://www.evlab."
    },
    {
      "source": "post-consciousness.html",
      "content": "t the end).</p> -- LJ HPD Sun 27 Jul 2025 15:12:26 BST References 1. http://bach.ai/ 2. https://thoughtforms.life/ 3. file:///Users/ljubomir/ljubomirj.github.io/post-knowing.html 4. https://www.evlab.mit.edu/ 5. https://colah.github.io/ 6. https://x.com/tegmark 7. https://www.youtube.com/watch?v=-gekVfUAS7c 8. https://www.youtube.com/playlist?list=PLZ7ikzmc6zlN6E8KrxcYCWQIHg2tfkqvR 9. https://curtjaimungal.substack.com/"
    },
    {
      "source": "post-data-debugging.html",
      "content": "Data Debugging While reading Spinellis [1]https://www.computer.org/csdl/magazine/so/2024/04/10547621/1XvqtG0iC oE, I was inspired and reminded to write down some data-debugging related practices so that I don't forget about them (may come handy in the future), and also may help someone else. If you have more - please share. 1. Piping is useful for running things in parallel, especially with the multi-core machines we all use nowadays. Pipes provide the simplest dependency graph, and they have automatic IPC synchronization courtesy of the pipes FIFO-s flowing from one process to the next. That allows for individual processes in the pipeline to be run independently on different cores with the data flowing between allowing them to keep busy and not stall waiting. 2. Drawback of data flowing through the pipelines is that data flow is invisible to us and no record remains of the data available for inspection and debugging. Replacing a pipe \"|\" with \"| tee tempfile |\" allows for subsequent inspection of the \"tempfile\" to see what the data looked like. Example - debug: $ for a in {1..10}; do echo $a; done | wc -l as $ for a in {1..10}; do echo $a; done | tee tempfile | wc -l $ cat tempfile 3. Vectorised processing via numpy or in languages like matlab or array languages where the default data type is not a scalar (but an array or Ndim array in general) makes peeking into the individual values being aggregated or processed hard and needing extra effort. a."
    },
    {
      "source": "post-data-debugging.html",
      "content": "y languages where the default data type is not a scalar (but an array or Ndim array in general) makes peeking into the individual values being aggregated or processed hard and needing extra effort. a. Use hardware support for NaN to your advantage to prevent accidentally using missing data. Convert missing data on input into NaN in memory where possible (data is float or double). Any operation with a NaN will result in a NaN, so it will be detected in the end. In contrast, stand in values for missing data like 0 can be used undetected. b. If possible structure processing in the form of apply(function,array) so to use a kernel function that data will flow through. Then insert logging inside it to inspect data flowing through, turn it on/off where insight/speed is needed. 4. Use any data invariants to your advantage. Add asserts liberally to catch data breaking invariants as early as possible. If possible designate not-a-value for other types (analogous to NaN for floats) where possible, even if the hardware support is lacking: 0 for indices (assuming 1-based), INT_MIN as INT_NAN for integers, nullprt for pointers. 5. This book has been useful to me for coding, but lessons apply to data too - \"Writing Solid Code\" by Steve Maguire. 6. Data is used to run experiments, and experiments must be reproducible (b/c - science!). So: a. Any external data source used (and thus outside our control) must be cached. b."
    },
    {
      "source": "post-data-debugging.html",
      "content": "ode\" by Steve Maguire. 6. Data is used to run experiments, and experiments must be reproducible (b/c - science!). So: a. Any external data source used (and thus outside our control) must be cached. b. It must be possible to run the whole experiment without touching any external data source, only using cached data - so any run is reproducible. c. Often it's possible to leverage binary native de/serialization capabilities with a bit of care and prologue/epilogue code blocks. Example - python: name = cache_name() try: with open(name, 'rb') as f: st = pickle.load(f) except IOError: st = fetch_from_external_source() ... (maybe initial preprocessing, cleaning) ... with open(name, 'wb') as f: pickle.dump(st, f) Example - matlab: name = cache_name(); st = load_struct_from_mat(name); if isempty(st) st = fetch_from_external_source(); ... (maybe initial preprocessing, cleaning) ... save_struct_to_mat(name, st); end 7. Version data: where possible/space allowing, and often where the data is ASCII (e.g. .csv or .tsv files) - add it straight to a git repository. 8. Version data: where not possible b/c too much space - version the binary blobs outside a repository using the filesystem. 9. Keep ASCII files (.csv, .tsv) on disk compressed in a streaming-friendly format, not only to (a) save space, but also to ensure (b) integrity via checksuming, and possibly even to (c) speed up reading. 10. Use formats like .zstd and .gz that are append and rsync friendly, e.g."
    },
    {
      "source": "post-data-debugging.html",
      "content": "at, not only to (a) save space, but also to ensure (b) integrity via checksuming, and possibly even to (c) speed up reading. 10. Use formats like .zstd and .gz that are append and rsync friendly, e.g. where if $ zstd -c file1 >file.zst; zstd -c file2 >>file.zst, then $ diff <(zstd -dc file.zst) <(cat file{1,2}) produces no diffs. Use --rsyncable in gzip and zstd (NB bzip2 lacks that) to create rsync-friendly compressed files. 11. If keeping ASCII data compressed (e.g. .csv.zst) in git can enable comfortable diffing - in .gitattributes add: *.csv.zst diff=csv.zst while in .gitconfig add: [diff \"csv.zst\"] textconv = zstdcat -- LJ HPD Sun 16 Nov 2025 21:18:54 GMT References 1. https://www.computer.org/csdl/magazine/so/2024/04/10547621/1XvqtG0iCoE"
    },
    {
      "source": "post-deepwiki.html",
      "content": "DeepWiki Crawl of This Repository I asked [1]DeepWiki to do a [2]crawl of this repository. The result is on the link below. [3]Deepwiki crowl of this repo -- LJ HPD Sat 26 Apr 2025 23:38:16 BST References 1. https://deepwiki.com/ 2. https://github.com/ljubomirj/ljubomirj.github.io 3. https://deepwiki.com/ljubomirj/ljubomirj.github.io"
    },
    {
      "source": "post-EMPTY.html",
      "content": "Title Goes Here Content goes here TODO TBD -- LJ HPD :r !date"
    },
    {
      "source": "post-knowing.html",
      "content": "Knowing - what do I mean when I say I know something I was told \"the joint probability density function between two variables \\( X \\) and \\( Y \\) captures everything that there is ever to be known about the relation between those \\( X \\) and \\( Y \\)\" 25 years ago (�Hola! [1]Miguel :-)), and it's been a blessing and a curse. Blessing - yeah the joint pdf \\( f_{X,Y}(x,y) \\) really does capture everything. Curse - often I read an article and think of the author \"wish someone told you too\", you poor soul. So for me knowing something about \\( X \\) means knowing the distribution, the pdf \\( f_X(x) \\). Most of the time our knowledge is more than 1-dimensional, we have at least two qualities that we want to quantify the relationship of. So knowing something about \\( (X,Y) \\) jointly, for me means knowing the joint pdf \\( f_{X,Y}(x,y) \\). Knowledge is knowing the joint probability function - either density p.d.f. or cumulative c.d.f. (using p.d.f. mostly to illustrate). Density p.d.f. or cumulative c.d.f. is an implementation detail. Depends on the mechanics of the contraption, can be both in the same device, is up to the implementation. So knowledge is - joint density, is co-counts, counting number of times things of interest co-occur, happen together. Probability is the measure of the residual uncertainty. Like length measures distance, probability measures uncertainty."
    },
    {
      "source": "post-knowing.html",
      "content": "-counts, counting number of times things of interest co-occur, happen together. Probability is the measure of the residual uncertainty. Like length measures distance, probability measures uncertainty. Randomness is what gives us probability distributions that are not Dirac impulses, and thus gives us information content to work with. NB the single letters \\( X \\) and \\( Y \\) are multi-dimensional, \\( N \\)-dimensional and \\( M \\)-dimensional vectors. The domain of the pdf is \\( (N + M + 1) \\)-dimensional in general, with \\( N \\)-input \\( X \\), \\( M \\)-output \\( Y \\), 1-extra dimension where counting or quantity of probability density or mass happens. + (The one extra dimension where counting happens is time. Time is somewhat special TBD will concern with it latter. For now: without time there is no discrete events or observations and no counting either, so no joint density, so no knowledge either. Time is necessary - but not sufficient - condition for us to get to know the world around us.) Time in space-time \\( (x,y,z,t) \\) plays the same role as the \\( p \\) axis in a joint probability \\( p=f(x,y) \\): it is the special dimension that lets us count, accumulate, and order events. No time, no counting; no counting, no density. Conversely probabilities are always positive while time only flows forward--both axes are bounded and one-way. The split \\( (X,Y) \\) is arbitrary as decided by us."
    },
    {
      "source": "post-knowing.html",
      "content": "o counting; no counting, no density. Conversely probabilities are always positive while time only flows forward--both axes are bounded and one-way. The split \\( (X,Y) \\) is arbitrary as decided by us. Usually - \\( X \\) is what we observe easily but we don't care much about, \\( Y \\) is what we don't observe directly but care about would like to know which one. So by relating \\( X \\) to \\( Y \\), we want to deduce something about \\( Y \\), while observing \\( X \\). These things of interest are \"qualities\". Quality is one dimension out of N in an N-dim vector space. Within one 'quality' that's 'what', we will measure counts those will be 'quantity' or 'how much' or 'how many'. + (TBD consider latter, but: while and to the extent the dimension N can't be forecast with 0 error from the other (N-1) dimensions, it exists as a separate dimension. Once it can be forecast with 0 error from the rest - it collapses and stops existing afa we are concerned.) Space itself can be phrased this way: what is truly \"nearby\" has a non-trivial joint distribution \\( p(\\text{here},\\text{nearby}) \\neq p(\\text{here})p(\\text{nearby}) \\). What is far apart factors and therefore forgets about each other. Closeness is simply dependence. In the simplest case of a single quality, single dimension X in 1D, knowledge of X is the p.d.f. of X \\( f_X(x) \\). Everything that there is to be known about X is described by the re-normalised histogram of X where we count which ones of X, and then how many of which one."
    },
    {
      "source": "post-knowing.html",
      "content": "edge of X is the p.d.f. of X \\( f_X(x) \\). Everything that there is to be known about X is described by the re-normalised histogram of X where we count which ones of X, and then how many of which one. The first non trivial case of knowledge is where we have two qualities, two dimensions X and Y, \\( (X,Y) \\) in 2D, knowledge is the joint p.d.f. of X and Y that is \\( f_{X,Y}(x,y) \\). Everything that can be known about the relationship between two qualities X and Y is captured and described about their joint p.d.f. \\( f_{X,Y}(x,y) \\). If we know the joint p.d.f. \\( f_{X,Y}(x,y) \\), we can derive the prior distributions both for X that is \\( f_X(x) \\), and for Y that is \\( f_Y(y) \\), by marginalisation. Marginal p.d.f.s are \\( f_X(x) = \\int_y f_{X,Y}(x,y) dy \\) and \\( f_Y(y) = \\int_x f_{X,Y}(x,y) dx \\). Marginalisation is \"adding up\" the probability mass in the dimension(s) we don't care about, the one(s) we marginalise out. Marginalisation maybe thought as \"forgetting\" - the detail is lost, but we achieve efficiency (less parameter), and robustness - we are not dependent anymore on the variable that was marginalised out (we are independent of it now - at least in our p.d.f.), + Keeping detail so not cost free, it takes resource implementing it. So if we don't need a particular dimension particular quality for whatever our goals are - it's better to forget about it. Once we observe one of the qualities that are \\( (X,Y) \\), e.g."
    },
    {
      "source": "post-knowing.html",
      "content": "lementing it. So if we don't need a particular dimension particular quality for whatever our goals are - it's better to forget about it. Once we observe one of the qualities that are \\( (X,Y) \\), e.g. X, that shrinks the domain from 2D to 1D, and now everything that can be known about Y is described by conditional p.d.f. \\( f_{Y|X}(y|x=a) \\) that is computed by plugging \\( x=a \\) into the joint p.d.f. \\( f_{X,Y}(x=a,y) \\), then dividing re-normalising that function by the prior p.d.f. of X \\( f_X(x) \\) at \\( x=a \\) giving rise to new conditional p.d.f. \\( f_{Y|X}(y) \\) for Y that is defined as \\( f_{Y|X}(y) = \\frac{f_{X,Y}(a,y)}{f_X(a)} \\). Below I illustrate this point on the example of a joint pdf \\( p = f_{X,Y}(x,y) \\) that is a mix of two Gaussians in 2D space \\( (x,y) \\). We observe the variable \\( X \\), and that observations is \\( x=1 \\). The question is - what do we now know about the variable \\( Y \\), having observed the variable \\( X \\) (to be \\( x=1 \\)). The observation \\( x=1 \\) is equivalent to the joint pdf being cut by the plane \\( x=1 \\). The intersection of the joint pdf \\( f_{X,Y}(x,y) \\) and the plane \\( x=1 \\) is \\( f_{X,Y}(x=1,y) \\). This curve is the best description of what we now know about the distribution of the unobserved variable \\( Y \\). The starting model that was \\( f_{X,Y}(x,y) \\) is affected by the observation \\( x=1 \\). The effect is the intersection \\( f_{X,Y}(x=1,y) \\), and is outlined below."
    },
    {
      "source": "post-knowing.html",
      "content": "of the unobserved variable \\( Y \\). The starting model that was \\( f_{X,Y}(x,y) \\) is affected by the observation \\( x=1 \\). The effect is the intersection \\( f_{X,Y}(x=1,y) \\), and is outlined below. It is a function of \\( y \\), that is a scaled conditional \\( f_Y(y|x=1) = \\frac{f_{X,Y}(x=1,y)}{f_X(x=1)} \\). The conditional pdf is \\( f_Y(y|x) \\). The scaler \\( f_X(x=1) \\) is the marginal pdf \\( f_X(x) \\) of \\( X \\) at point \\( x=1 \\). The marginal pdf \\( f_X(x) \\) is computed from the joint pdf \\( f_{X,Y}(x,y) \\) by marginalization, by integrating out \\( Y \\) as \\( f_X(x) = \\int f_{X,Y}(x,y)\\,dy \\) and then plugging in \\( x=1 \\). Everything that can be known about about anything can be construed as a relation between two things. (at the baseest level - myself and the world, the I and not-I.) Call them X and Y. So all knowledge about \\( (X,Y) \\) (what X tells us about Y, what Y tells us about X) - is in that X-Y relationship. Everything that can be known about X-Y relationship is captured and described about their joint p.d.f. \\( p=f_{X,Y}(x,y) \\) or just \\( p=f(x,y) \\) the density; or c.d.f. \\( p=g_{X,Y}(x,y) \\) then it's just \\( P=g(x,y) \\) the cumulative. And that's it. The probability distribution captures all that can ever be known about the \\( (X,Y) \\) relationship. Once X \\( x=a \\) is observed, then everything that is known about Y is described by the conditional p.d.f. \\( p=f_{Y|X}(y|x=a) \\) or just \\( p=f(y|x) \\) at \\( x=a \\)."
    },
    {
      "source": "post-knowing.html",
      "content": "out the \\( (X,Y) \\) relationship. Once X \\( x=a \\) is observed, then everything that is known about Y is described by the conditional p.d.f. \\( p=f_{Y|X}(y|x=a) \\) or just \\( p=f(y|x) \\) at \\( x=a \\). The 3D function \\( p=f(x,y) \\) is cut with a plane \\( x=a \\). The cross section is \\( f(x=a,y) \\). It's not a p.d.f. as it does not sum to 1. The area of the cross section \\( \\{f(x,y),x=a\\} \\) that is a scalar is the area under \\( f(x,y) \\) at \\( x=a \\). This is also the value of the marginal p.d.f. \\( f_X(x) = \\int f(x,y) dy \\) at \\( x=a \\), i.e. \\( p=f_X(x=a) \\). To convert \\( f(x=a,y) \\) into conditional p.d.f. \\( f(y|x=a) \\) we divide joint p.d.f. \\( f(x=a,y) \\) with the scalar (constant) that is marginal p.d.f. \\( f_X(x=a) \\): \\( f(y|x=a)=f(x=a,y)/f_X(x=a) \\). [2]Joint marginal conditional pdf 1 of 3. (click to zoom) [pdf-joint-cond-marg-1of3.png] [3]Conditional pdf is ratio of joint (at point) and marginal 2 of 3. (click to zoom) [pdf-joint-cond-marg-2of3.png] [4]Marginal pdf is derived from the joint pdf 3 of 3. (click to zoom) [pdf-joint-cond-marg-3of3.png] Coming back to \"joint pdf captures everything there is in the relationship \\(X,Y\\)\". Putting it in a wider context. When reading about knowledge, I have come across the following so collected here for future reference. We can have 2 types of knowledge about the outcome of (repeated) experiment(s): a. We know what will happen and when it will happen in each experiment."
    },
    {
      "source": "post-knowing.html",
      "content": "ollowing so collected here for future reference. We can have 2 types of knowledge about the outcome of (repeated) experiment(s): a. We know what will happen and when it will happen in each experiment. This is non-probabilistic, deterministic knowledge. NB it is a special case of [5]both [6](b) cases below with the pdf being a Dirac impulse function. b. We know the possible outcomes, we know how many of each will happen if we do 100 experiments, but for each 1 experiment, we can't tell the outcome. This is probabilistic knowledge where we know the pdf (=probability density function) of the experiment outcome. It is the aleatoric kind of uncertainty (see [7]below) - we know the statistics, the counts, but not what one outcome is going to be in every one experiment. Uncertainty - obverse to knowing, to knowledge, lacking (perfect, deterministic) knowledge, we can think of types: b. Aleatoric uncertainty means not being certain what the random sample drawn (from the probability distribution) will be: the p.d.f. is known, only point samples will be variable (but from that p.d.f.). We can actually reliably count the expected number of times an event will happen. c. Epistemic uncertainty is not being certain what the relevant probability distribution is: it is the p.d.f. that is unknown. We can't even reliably count the expected number of times an event will happen. The probabilistic knowledge of type [8](b) above and aleatoric uncertainty of type [9](b) are one and the same."
    },
    {
      "source": "post-knowing.html",
      "content": "unknown. We can't even reliably count the expected number of times an event will happen. The probabilistic knowledge of type [8](b) above and aleatoric uncertainty of type [9](b) are one and the same. The 2D \\( (X,Y) \\) example is also useful to illustrate a further point. Once we observe \\( X \\), and work out the conditional pdf \\( f_Y(y|x) \\), the question arises - what next? What do we do with it? If \\( Y \\) is discrete, we have a problem of classification. If \\( Y \\) is continuous, we have a problem of regression. We have the entire curve to work with - and that's the best. But often, we approximate the entire curve, with a representative value, and soldier on. Then the question becomes: well how do we chose one representative value from that curve? The \"\\( X \\) observed \\( Y \\) not observed\" is arbitrary - it could be the other way around. We can generalize this by introducing a 2D binary mask \\( M \\), to indicate what parts of the vector \\( (X,Y) \\) are present (observed), and what parts are missing (and thus of some interest, e.g. we want to predict or forecast them). With present data \\( X \\) and missing data \\( Y \\) in \\( (X,Y) \\), then missing data imputation is actually equivalent to forecasting regression or classification. The same logic works even if the mask is in time, but the signal gets much weaker when the observed parts are in the past and the unknown parts live in the future--time Now inserts another dimension that has to be overcome."
    },
    {
      "source": "post-knowing.html",
      "content": "en if the mask is in time, but the signal gets much weaker when the observed parts are in the past and the unknown parts live in the future--time Now inserts another dimension that has to be overcome. Simplest case - X is 1-dim, Y is 1-dim, then knowledge of a \\( (X,Y) \\) relationship is the joint p.d.f. \\( f_{X,Y}(x,y) \\) that's a 3-D shape. Observation is cutting that 3-D shape with a 2-D plane \\( x=a \\). Intelligence is using the 2-D outline \\( f_{X,Y}(x=a,y) \\) that's the intersection between the 3-D joint shape and the 2-D \\( x=a \\) plane, to decide on the action to be taken. Now we have this new(er) knowledge (that normalised to 1 is conditional p.d.f. \\( f_{Y|X}(y|x=a) \\)), while still aiming for the same desired end as before. Knowledge of relation \\( (X,Y) \\) is the joint p.d.f. \\( f_{X,Y}(x,y) \\). Observation is cutting that 3-D shape with a 2-D plane \\( x=a \\). Intelligence is deciding on the next action--now we have 2-D shape \\( f_{X,Y}(x=a,y) \\), incorporating this newest knowledge about X (normalised to sum 1 is the conditional p.d.f. \\( f_{Y|X}(y|x=a) \\)). All the while still aiming towards the same desired goal as before. (the goals themselves are outside of this, are not considered) Conditioning Y on X is observing the \\( x=a \\), and then re-normalising \\( f(y,x=a) \\) such that it becomes a p.d.f. again to sum to 1 \\( f(y|x=a) \\). Observing quality X that is \\( x=a \\), is shrinking the dimensionality from 2D to 1D."
    },
    {
      "source": "post-knowing.html",
      "content": "\\( x=a \\), and then re-normalising \\( f(y,x=a) \\) such that it becomes a p.d.f. again to sum to 1 \\( f(y|x=a) \\). Observing quality X that is \\( x=a \\), is shrinking the dimensionality from 2D to 1D. In general from \\( (N+1) \\)-dim back to \\( N \\)-dim. Chain of Reasoning (CoR) If p.d.f. \\( f_{Y|X}(y|x=a) = P(Y|X) \\) is not very informative, we can undertake chain of reasoning (CoR). We can find a reasoning step \\( Z \\), that is \\( P(Z|X) \\), such that the conditional \\( P(Y|Z,X) \\) brings us closer to our end answer than \\( P(Y|X) \\) can bring us. This is how hierarchies are made. Already \\( P(Y|X) \\) is a hierarchical relationship. Now conditioning on \\( Z \\) too, \\( P(Y|Z,X) \\), is another brick in the wall of a hierarchical relationship. Conditioning on X takes general N-dim \\( P(X,Y) \\), and reduces it down to at most \\( (N-1) \\)-dim space of \\( P(Y|X) \\). Conditioning even more on Z to \\( P(Y|Z,X) \\) does another slicing down, to at most \\( (N-2) \\)-space. From widest most detailed N-dim \\( P(X,Y) \\), to more general less specific \\( P(Y|Z,X) \\) at most \\( (N-2) \\)-dim. One can undertake motivated slicing via \\( Z \\). For slicing by \\( x=a \\) the \\( P(X,Y)=f_{X,Y}(x,y) \\) to get \\( f_{X,Y}(x=a,y) \\) (then renormalised by marginal \\( P(X)=f_X(x) \\) at \\( x=a \\) into \\( f_{X,Y}(x=a,y)/f_X(x=a) = f_{Y|X}(y|x=a) \\) call it conditional \\( P(Y|X) \\)) - we undertake that b/c we hope \\( P(Y|X) \\) is going to be sharper, more informative than a presumed wide un-informative \\( P("
    },
    {
      "source": "post-knowing.html",
      "content": "Y}(x=a,y)/f_X(x=a) = f_{Y|X}(y|x=a) \\) call it conditional \\( P(Y|X) \\)) - we undertake that b/c we hope \\( P(Y|X) \\) is going to be sharper, more informative than a presumed wide un-informative \\( P(Y) \\). So we could be selecting such \\( Z \\), that \\( P(Y|Z,X) \\) is even sharper. And we have the \\( P(Z|X) \\) to judge how justified we are to undertake our motivated reasoning \\( Z \\) step. There are infinite number of \\( Z \\)-s we can slice-condition on. The trick is choosing \"the right ones\" for \\( Z \\). They can't be too divorced from \\( X \\), as then \\( P(Z|X) \\) will be very flat. The \\( Z \\) chosen also can't be too divorced from \\( Y \\) - then it will not add anything over and above \\( X \\), which is already too far from \\( Y \\) for any useful guide, \\( P(Y|Z,X) \\) will be as good (bad) as \\( P(Y|X) \\). (even if \\( P(Z|X) \\) may show relation to X) Looks like the size of the step when moving X->Y can be max ~20% in interestingness, but not more, to keep it true. Chain of Thought (CoT), Chain of reasoning (CoR) Extension to Type 2 intelligence: guided search through discrete space where reward is unknown in time and only becomes known after the last step in the sequence. Next step from Type 1 intelligence: pattern recognition single 1:1 input:output, reward known at every step. CoR now often has to invent its own \\( Z \\)-s (or \\( R \\)-s for \"reasons\"), rather than simply conditioning on an observed variable."
    },
    {
      "source": "post-knowing.html",
      "content": "rn recognition single 1:1 input:output, reward known at every step. CoR now often has to invent its own \\( Z \\)-s (or \\( R \\)-s for \"reasons\"), rather than simply conditioning on an observed variable. DSPy-style systems optimise not only the answerer \\( P(Y|X) \\) but the question-poser that proposes \\( Z \\) so \\( P(Y|Z,X) \\) becomes sharp. Both the query and the answer become learnable objects. Non-reasoning autoregressive LLM-s: compute \\( P(Y|X) \\), then they sample from that distribution. Hence parameters like temp-erature, top_K, min_p, top_p. Diffusion models image denoisers: learn the first derivative of \\( \\log(P(Y|X)) \\), use that to get from the current sample, to a better sample, and they iterate. They add noise to in every step, and that serves to sample the whole distribution, rather than pick and converge to a single one point. Afaics the reasoning models add only one step extra. Assume there is no good choice \\( Y \\) for the \\( X \\) - say \\( P(Y|X) \\) is flat uninformative, there is no good guess for Y. So let's figure out intermediate step Z. Then instead of \\( P(Y|X) \\), go after \\( P(Y|Z,X)P(Z|X) \\). NB summing over Z will recover exactly \\( P(Y|X) \\). The step \\( Z \\) is such that \\( P(Y|Z,X) \\) is informative, where \\( P(Y|X) \\) was not. So \\( P(Y|Z,X) \\) brings us closer to a better guess for \\( Y \\), in a way that \\( P(Y|X) \\) does not."
    },
    {
      "source": "post-knowing.html",
      "content": "Y|X) \\). The step \\( Z \\) is such that \\( P(Y|Z,X) \\) is informative, where \\( P(Y|X) \\) was not. So \\( P(Y|Z,X) \\) brings us closer to a better guess for \\( Y \\), in a way that \\( P(Y|X) \\) does not. Of course, what functions, and how they are fit, and how to chose bazillion of \\( Z \\)-s, and how to aggregate while searching - makes a world of difference. Diffusers learning gradients and autoregressors learning the density are complementary. If/when we have access to both the function and its derivatives we can approximate more operators on top of the pdf itself--think H- and L- updates that reason about how the entire distribution should morph in one step. Marginalisation is Forgetting When I compute \\( f_X(x) = \\int f_{X,Y}(x,y) dy \\) margnialising out i.e. \"ignoring\" Y is irreversibly destroying any knowledge of how X and Y co-vary. The operation is a lossy compression; I cannot reconstruct \\( f_{X,Y} \\) from \\( f_X \\) alone. A state with joint knowledge \\( f_{X,Y} \\) has \"working memory\" of the relationship. Marginalizing to \\( f_X \\) is forgetting Y, freeing up parameters space removing memory so losing specifics. Choosing which variable to marginalize is an act of attention allocation. Storing a joint p.d.f. over \\( (N+M) \\)-dims cost drops when marginalizing down to \\( N \\)-dim. We trade accuracy for efficiency, effectivelly compressing, so the trick is to forget just enough but not more than that."
    },
    {
      "source": "post-knowing.html",
      "content": ".d.f. over \\( (N+M) \\)-dims cost drops when marginalizing down to \\( N \\)-dim. We trade accuracy for efficiency, effectivelly compressing, so the trick is to forget just enough but not more than that. Conversely each new conditioning step is a re-memoization--preventing forgetting by keeping another dimension intact. But we pay in computational complexity. Appendix + A. Everything is a Computer in 2025 it seems + Everything is a computer now atm. (2025) Information theory is the most general theory we got. Knowledge is a p.d.f. Learning is acquiring a p.d.f. where we previously lacked one. Acquiring p.d.f. is figuring out what-s, and how many-s of those what-s. Computing is taking our knowledge, the learned p.d.f.s we got, then manipulating those p.d.f.s, by either marginalisation, or conditioning, to create new p.d.f.s. These new p.d.f.s then tell us something about what we care about but we can't observe, having observed things that are easy for us to observe, but we don't care about. The general model of computation is one of discrete states. Every state is characterised by a different p.d.f. function. Transitions between states occur too. Those are also characterised by their own p.d.f.s. Markov assumption is that transitions depend on the current state, but don't depend on the path we took to get to the current state. So the future states are independent of the past states, only on the present state. Computation is to information p.d.f. \\( p=f(x,y,..."
    },
    {
      "source": "post-knowing.html",
      "content": "ut don't depend on the path we took to get to the current state. So the future states are independent of the past states, only on the present state. Computation is to information p.d.f. \\( p=f(x,y,...) \\) what physics is to space-time \\( (x,y,z,t) \\). Both have a \"special\" dimension: probability mass \\( p \\) that always stays \\( \\ge 0 \\), and time \\( t \\) that only flows forward. Transformations manipulate the rectangular coordinates while respecting that privileged axis. Information and knowledge move against entropy by copying and spreading. When information spreads from A to B, B gains knowledge, but A does not lose it--the operation is copy, not move. What costs energy is the physical act of copying. B. Missing Data Imputation + With present data \\( x \\) and missing data \\( y \\) in \\( (x,y) \\), then missing data imputation is actually equivalent to forecasting regression (continuous \\( y \\) variable) or classification (discrete \\( y \\) classes). Big difference is whether \\( x \\) and \\( y \\) are contemporaneous, or not: not contemporaneous makes the signal connection \\( x \\rightarrow y \\) much, much weaker, by orders of magnitude. Time Now is a big barrier in knowing. The mask view \\( M \\) above is the operational view of imputation. Once the mask is defined, we simply treat the missing cells as \\( Y \\) and the present ones as \\( X \\), and the whole machinery of conditional pdfs applies."
    },
    {
      "source": "post-knowing.html",
      "content": "bove is the operational view of imputation. Once the mask is defined, we simply treat the missing cells as \\( Y \\) and the present ones as \\( X \\), and the whole machinery of conditional pdfs applies. Once we have any curve \\( f_Z(z) \\) whichever way we got it (marginal, conditional), we can do derived statistics to it, in order to convert the curve into one (point forecast) or two (forecast and it's variation) points or however many we fancy (quartiles, quintiles, etc) to characterise the entire curve/area/volume/Nvolume (1D/2D/3D/Ndim) with, and reduce to characterise the whole continuous mathematical object with few discrete numbers. C. Now and Time, Past, Present, Future + TBD Time. Mechanistic pedestrian treatment of time (past/now/future) in the same framework where all is knowns is a p.d.f. So we have past and future separated by the now that marks the present for us. Say \\( X = \\) past, \\( Y = \\) future in our \\( (X,Y) \\) and we have the p.d.f. \\( f_{X,Y}(x,y) \\). The past has already happened, there is only one of it, it's certain and deterministic. Translated in p.d.f. language that means the \\( f_X(x) \\) or \\( f_{X|Y}(x|y) \\) distribution *must* be a Dirac delta impulse. The \\( f_X(x) \\) can not be any other function shape than a Dirac delta. The future has not happened yet. We know there two or more options for the future, it is never a single one. Translated in p.d.f."
    },
    {
      "source": "post-knowing.html",
      "content": "e \\( f_X(x) \\) can not be any other function shape than a Dirac delta. The future has not happened yet. We know there two or more options for the future, it is never a single one. Translated in p.d.f. language that means the \\( f_Y(y) \\) or \\( f_{Y|X}(y|x) \\) distribution *can NOT* be a Dirac delta impulse. The \\( f_Y(y) \\) can be any other function shape than a Dirac delta. But not a Dirac delta. Now is a moving boundary between the all-Dirac past, and never-Dirac future. Living is the future out-running the past, the past failing to catch up the future. Death is the point at which the past finally catches up with the future. At that point all uncertainty ends, seemingly never to return back. Death collapses the future functions (p.d.f.s) from general forms with uncertainty (anything but Dirac Delta-s), into a Dirac Delta impulse. The uncertainty ends, the Past finally catches up with the Future, erasing the Now time boundary in the process. For self anyways. I detach from not-I completely irreversibly, the final separation. Time is the special dimension on which everything we can imagine--real or counterfactual--happens. Time gives rise to the idea of infinity and, symmetrically, conceiving infinity gives rise to time: we imagine \\( N \\), then \\( N+1 \\), and so on, never ending. That mental sequence is how we make the jump from discrete to continuous. Time as sequence of events is the counter that labels the observations; it is not itself modelled by a joint conditional p."
    },
    {
      "source": "post-knowing.html",
      "content": "g. That mental sequence is how we make the jump from discrete to continuous. Time as sequence of events is the counter that labels the observations; it is not itself modelled by a joint conditional p.d.f. Time must be discrete for counting to occur. A perfectly continuous time would defeat counting (what does \"next\" mean?). Discreteness hints the underlying latent space is very high-dimensional. Time makes the jump from discrete to continuous space possible. When we imagine bisecting an interval for the \\( N \\)-th time, we also imagine the \\( (N+1) \\)-st time. Memory of \\( (N-1,N,N+1) \\) suffices, yet without that memory the limit process would fall apart. Both time and space implement memory. Patterns live in time; they require the medium of time to exist. Those patterns can be converted 1:1 into spatial states that memorize the same information. The existence of a discrete entity can be in time (patterns) or in space (states). Markovian blanket = in the present, memorising everything from the past, that is to be used to forecast the future. Everything that can be determined about the future, from the past, is written onto the present. Time is the currency of life - time is what is spent in the process of living. D. Forecasts must have error > 0 for information to exist + The error is necessary for information to exist. In the limit where the error is zero, no new information is ever observed - everything is known, the uncertainty is zero. Life exists only with uncertainty."
    },
    {
      "source": "post-knowing.html",
      "content": "rror is necessary for information to exist. In the limit where the error is zero, no new information is ever observed - everything is known, the uncertainty is zero. Life exists only with uncertainty. Where/when the error is zero, everything is predictable. This is the state before a living thing is born, and after a living thing dies. Life exists in that goldilocks region where there is limited uncertainty. If the uncertainty is zero, then we are not-born yet, or dead. If the uncertainty is too high, things are chaotic, too random, there isn't enough order and structure for life to exist. Every living thing introduces another dimension, another axis, of non-zero uncertainty, into existence. When it dies, the uncertainty disappears, that axis of variance is no more. In physics disorder (entropy) is something we measure at macro level as temperature. It is typically increased by adding energy. So - energy increases temperature increases entropy increases disorder increases error decreases predictability. E. Qualities, Quantities, and Dimensions + Quality and quantity. Dimensions \\( (X,Y) \\) are qualities, and we quantify them each too. When do we add new quality and obversely when do we lose a quality (dimension)? ([10]LJ @ HN) The second \"law\" of dialectical materialism by Engels--\"the law of the passage of quantitative changes into qualitative changes\"--captures this nicely. Enough quantity becomes a new quality."
    },
    {
      "source": "post-knowing.html",
      "content": "]LJ @ HN) The second \"law\" of dialectical materialism by Engels--\"the law of the passage of quantitative changes into qualitative changes\"--captures this nicely. Enough quantity becomes a new quality. Suppose I have a 5-dimensional observation and I'm wondering if it's really only 4 dimensions there. One way I check is - do a PCA, then look at the size of the remaining variance along the axis that is the smallest component (the one at the tail end, when sorting the PCA components by size). If the remaining variance is 0 - that's easy, I can say: well, it was only ever a 4-dimensional observation that I had after all. However, in the real world it's never going to be exactly 0. What if it is 1e-10? 1e-2? 0.1? At what size does the variance along that smallest PCA axis count as an additional dimension in my data? The thresholds are domain dependent - I can for sure say that enough quantity in the extra dimension gives a rise to that new dimension, adds a new quality. Obversely - diminishing the (variance) quantity in the extra dimension removes that dimension eventually (and with total certainty at the limit of 0). I can extend the logic from this simplest case of linear dependency (where PCA suffices) all the way to to the most general case where I have a general program (instead of PCA) and the criterion is predicting the values in the extra dimension (with the associated error having the role of the variance in the PCA case)."
    },
    {
      "source": "post-knowing.html",
      "content": "eral case where I have a general program (instead of PCA) and the criterion is predicting the values in the extra dimension (with the associated error having the role of the variance in the PCA case). At some error quantity \\( \\gt 0 \\) I have to admit I have a new dimension (quality). Emergence, phase transition and similar: 1) where quantity is large enough (this condition is necessary but not sufficient) and becomes new quality 2) thus this new quality becomes a new dimension in the phenomenon investigated, so my observations data from N-dim vectors become (N+1)-dim. \"More is Different\" is a succinct summary of \"enough quantity becomes a new quality\". An image is worth 16�16 words, but a program is worth \\( 2^4 \\) images. Kolmogorov complexity says a learning system finds the shortest program that explains the data. Compression is learning. A forecasting error of quantity \\( >0 \\) is the extra dimension--new quality--waiting to be captured. When the program makes perfectly accurate predictions, that dimension disappears or was never there. TBD Ndim Space. Ratio of Ncube/Nball. Does our intuition fail us about the representative values of a distribution when we go from low \\( N \\) \\( (N = 2) \\) to high(er) \\( N \\) \\( (N \\gt 10) \\)? For large N, Nspace in Ndim: (a) moves into the edges (b) every observation is an outlier (in some dimension). Does that mean the space becomes discrete, it discretizes? TBD Sparse representation, moving to symbolics and rules."
    },
    {
      "source": "post-knowing.html",
      "content": ": (a) moves into the edges (b) every observation is an outlier (in some dimension). Does that mean the space becomes discrete, it discretizes? TBD Sparse representation, moving to symbolics and rules. Once the Ndim vector becomes sparse, we move from continuous representations to discrete symbolic rules. That's when we start writing down the rules explicitly. F. Randomness, Search, and Open-Endedness + Randomness, entropy is what enables search. Random steps are a way of searching through the global space. Without randomness we hill-climb to the nearest peak and stay there forever; the derivatives decay to zero, motion stops, intelligence halts. There is no AI/ML without randomness: (1) Autoregressive LLM sampling--temperature, top_k, etc.-- prevent robotic outputs. (2) SGD uses noisy mini-batches; warnings about local minima never materialized because the noise keeps us moving. (3) Diffusion models follow the log conditional probability gradient but also add noise so they don't collapse to a single sample. That what works will keep being done, and finally will be overdone. Then things need to change in order for them to stay the same. It's the dose that makes the poison: noise too small and we stagnate, too large and the structure dissolves. Randomness is the source of uncertainty and therefore information. Random steps implement global search instead of local greedy search. Trial and error equals variation and selection: trial == variation, error == selection."
    },
    {
      "source": "post-knowing.html",
      "content": "e of uncertainty and therefore information. Random steps implement global search instead of local greedy search. Trial and error equals variation and selection: trial == variation, error == selection. Open endedness in that way is reminiscent of the diffusion sampling, and the role the noise term plays. If we add too much noise, the denoising fails. Too little noise, and the steps are tiny, nothing new is learned. The step needs to be just right--not too small, not too large. Kelly-criterion thinking says we should keep \\( \\mu/\\sigma \\lesssim 0.2 \\) for comfortable compounding (halve the leverage when uncertain). That heuristics rhymes with how CoR steps are ideally \"only\" 20% more interesting than the previous step. Kelly criterion leverage: \\( f = \\mu / \\sigma^2 \\). Assume errors in \\( (\\mu,\\sigma) \\) estimates => deploy half-kelly leverage \\( f=\\mu/\\sigma^2/2 \\). Market neutral fund leverage 3 origin: assume average \\( \\mu=2\\% \\) \\( \\sigma=4\\% \\) p.a. on gross book, so half-kelly leverage \\( f=0.02/0.04^2/2=6.25 \\), so with capital \\( C=1 \\), aim for \\( (\\text{Long}=3,\\text{Short}=-3) (\\text{Gross}=6,\\text{Net}=0) \\) book. Sufficiently high frequency feels smooth. Once the frequency is high enough, the alternation peak-trough merge into a constant line. Infinite frequency == zero period == DC no alternation. G. Life, Entropy, and Knowledge + Life as Thermodynamic Evidence of Algorithmic Structure in Nature (2012) (mdpi.com)."
    },
    {
      "source": "post-knowing.html",
      "content": "rge into a constant line. Infinite frequency == zero period == DC no alternation. G. Life, Entropy, and Knowledge + Life as Thermodynamic Evidence of Algorithmic Structure in Nature (2012) (mdpi.com). Organisms encode information about their environment in order to survive. The encoding costs energy and generates entropy. Organisms use that encoded information to gain or not lose energy. Only where the information cycle is net positive can life exist. Since life exists, nature cannot be \"too unpredictable\". Living things decrease entropy; non-alive things increase entropy. (\"What is Life?\" by Erwin Schrodinger [11]http://www.whatislife.ie/downloads/What-is-Life.pdf) Life is that what decreases entropy. Life is in a goldlocks region between max uncertainty (random, entropy) and min uncertainty (Dirac delta): not too random, not too certain, just right random/certain as to be interesting. In order to achieve entropy decrease, life needs to exercise 1) control, and 2) adaptation. Control starts with enclosure. A piece of space is enclosed using a membrane, a barrier, a border. The inside of the barrier needs to gain enough negative entropy, so to sustain itself and the wall. Adaptation is achieved with intelligence. All living things are part of the tree of life. They are all related. So the life-o-sphere is expanding. One can think of life-o-sphere as a whole as a giant organism that is living. Atoms are only ever created and destroyed in rare nuclear reactions."
    },
    {
      "source": "post-knowing.html",
      "content": "y are all related. So the life-o-sphere is expanding. One can think of life-o-sphere as a whole as a giant organism that is living. Atoms are only ever created and destroyed in rare nuclear reactions. Most of the time they are just reconfigured like Lego blocks. The atoms that make our bodies were lent to us. Atoms themselves are mostly empty space with smaller vibrating sub-particles. The relations between particles matter more than the particles themselves--like letters versus words. Life is self-reproduction with variation. Life is ability to make decisions about the future and take action, and thus influence and change your own future. Intelligent life in biological sense is the ability to achieve the same goal via differing paths, different means. Life is recurring pattern. Joscha Bach \"We Are All Software\". We recognise the same person even if all molecules churned many times. The pattern repeats over time even if the details differ. Life is a cycle of generation, degeneration, regeneration. \"I\" is a collection of particles that is arranged into this pattern, that will decompose and be available to nature to reorganize into another pattern. Death is part of a gift economy. You are given this enormous gift, life. You enrich it as best you can. And then you give it back. (Emily Levine, Andreas Weber). Children give you a glimpse of a second life, if not of an eternal one. H."
    },
    {
      "source": "post-knowing.html",
      "content": "are given this enormous gift, life. You enrich it as best you can. And then you give it back. (Emily Levine, Andreas Weber). Children give you a glimpse of a second life, if not of an eternal one. H. Brains, AIs, and Efficiency + Digital intelligences (currently AI-s - artificial intelligences) need lots of energy. They can work \"as if\" perfect copies, separating software (spirit) from the hardware (substrate). They can all learn different knowledge from different experiences, but then share their knowledge back with everyone at high speed. So they can implement distributed learning at unit level. Separation s/w spirit from h/w substrate also makes them immortal. Analogue intelligences (currently HI - human intelligences; but all animales and plants, all life really) can not work \"as if\" perfect copies, they are all one of a kind and unique. Their spirit s/w and their substrate h/w are entangled. They can't share their experiences easily. They are mortal, their spirit s/w stops existing when their substrate h/w dies. However, they have one big advantage: they use much less energy, orders of magnitude less energy than digital intelligences. Energy lots of it is used to create the insulation of s/w spirit separate from h/w substrate. So digital intelligence being high energy, can't bootstrap itself 0->1 into being on its own. For that, only low energy is possible. And low energy implies analogue intelligence."
    },
    {
      "source": "post-knowing.html",
      "content": "e from h/w substrate. So digital intelligence being high energy, can't bootstrap itself 0->1 into being on its own. For that, only low energy is possible. And low energy implies analogue intelligence. So the bootstrapping of intelligence goes none -> analogue (low energy) -> digital (high energy). Assume learning for 30 yrs ~ \\(10^9\\) sec, visual processing sampling at 10 fps, that equals \\(10^{10}\\) images as training samples. Assuming training a human brain with \\(10^{13}\\) \"parameters\" (100B neurons � 100 connections). That makes 1 image per \\(10^3\\) parameters. Meanwhile our best ANN-s train with way more images per parameter--they trade computation and data for smaller models, while biological brains sacrifice parsimony in weights to gain speed and low power. Energy and intelligence travel together. Intelligence figures out what to do; energy is what gets it done in the physical world. I. Collective Intelligence and Networks + All intelligence is collective intelligence. Humans are collections of organs and tissues, which themselves are collections of cells. A cell is itself a collection of molecular networks (and so on). The subunits are competent themselves. There is a scale up process in which these competent subunits give rise to an intelligent unit bigger than themselves (Michael Levin). The value--and even the reality itself!--is in the connections, not the nodes. People move from villages to cities because density breeds civilization."
    },
    {
      "source": "post-knowing.html",
      "content": "ent unit bigger than themselves (Michael Levin). The value--and even the reality itself!--is in the connections, not the nodes. People move from villages to cities because density breeds civilization. Empires built trading outposts to create connections. Roman logistics favoured water routes because the connections mattered more than the raw distance. Value of a network scales with \\( N^2 \\) not \\( N \\): the value is in the connections. Mode of operation online vs offline diverges. Online geeks tend to cooperate in good-good positive sum interactions. Offline scarcity sometimes locks people into bad-good zero-sum trades. The larger, looser internet neighborhoods proved fertile for altruistic building of the web, Wikipedia, blogs, AI. Knowledge sharing thrives where connections are abundant. Neural Networks architecture (N nodes of about the same power, with connections arcs between them scaling \\( N^2 \\)) implementing Parallel Distributed Processing implementing intelligence - should be a grander lesson. Having a single node that is much more powerful than the rest is how cancer behaves. Intelligent systems survive by distributing power. NB that any graph of N nodes can be described by a matrix \\( [N \\times N] \\). The graph operations are matrix operations--matrix multiplication implies a linear model at the base. NB the standard deviation and risk as measured by it only goes down with \\( \\sqrt{N} \\) the number of samples."
    },
    {
      "source": "post-knowing.html",
      "content": "erations are matrix operations--matrix multiplication implies a linear model at the base. NB the standard deviation and risk as measured by it only goes down with \\( \\sqrt{N} \\) the number of samples. So having the collective intelligence go up with \\( N^2 \\) the number of nodes counter-acts that. An individual node may sharpen its pdf with \\( \\sqrt{n} \\) data (and suffers curse of dimensionality). The system as a whole fights that by increasing \\( N \\) and the intelligence with \\( N^2 \\). N^2 mechanism that makes the network powerful is sharing of knowledge, that all N see what 1 sees. So it's enough for every one of the N to see \\( 1/N \\) of the data in \\( 1/N \\) of the time, and to then share it with N others. So then all N get to see and learn whole data N. So the total knowledge is N, and is shared N times (redundancy), and that happens \\( 1/N \\) of the time--if the transfer of knowing between the N nodes is fast. The real world can't be sped up. So the only way to speed up is to divide the work, get them to do the work in \\( 1/N \\) of the time, and then share the knowing between themselves. J. Types of Intelligence + It's by logic that we prove, but by intuition that we discover. (Poincar�) To know how to criticize is good, to know how to create is better.(Fran�ois Chollet) Broadly three categories of problem solving patterns -- recitation, intuition, and reasoning. Recitation: you simply recognize a known problem and apply the steps you've learned."
    },
    {
      "source": "post-knowing.html",
      "content": "er.(Fran�ois Chollet) Broadly three categories of problem solving patterns -- recitation, intuition, and reasoning. Recitation: you simply recognize a known problem and apply the steps you've learned. Like playing a chess opening. Recitation is a database lookup. Intuition: in the face of a novel situation, you pattern-match it to what you've encountered before and you \"just know\" what to do. Like a very experienced chess player seeing the best move in <1s. Reasoning: you consciously and deliberately analyze a novel situation, using a combination of abstract principles and step-by-step simulation. Like analyzing a chess position and simulating in your mind possible future trajectories. Types of intelligence-s, (T-s): Type-1 pattern recognition, idea generation, guessing a not insane guess. Type-2 logical thinking, guided search through discrete space. Type-3 open endedness, directed but almost random hypothesis generation (active learning). Type-4 collective intelligence--AI-s coordinating (sometimes cooperating, sometimes competing) in AI society--where individual irrational behaviour aggregates into rational group-level feelings. We share the meta-learning and meta-knowledge machinery with every other living thing: we're leaves on the same tree of life. We are made of collective intelligences (cells, made of molecules...) and simultaneously build a collective intelligence (society). Human intelligence--and its stupidity--is collective too."
    },
    {
      "source": "post-knowing.html",
      "content": "e of life. We are made of collective intelligences (cells, made of molecules...) and simultaneously build a collective intelligence (society). Human intelligence--and its stupidity--is collective too. We live Type-3 intelligence whenever we seek novelty and interestingness. Having children brings that home: watching them learn to move, to speak, to press a button and discover a song. At first the joint p.d.f. \\( p(\\text{press button}, \\text{song plays}) \\) is flat. The first time, surprise! High residual error. So the kid repeats it four or five times until the distribution bumps up, the error shrinks toward zero, and knowing solidifies. Type-4 intelligence is synchronizing with others--cooperation and competition at maybe 4:1--to build collective intelligence. Here emotions appear. They are mostly about other people, not much about self; introspection exists, but it is a small slice. Daniel Kahneman's \"Thinking Fast and Slow\" fits: Type-1 pattern recognition, Type-2 reasoning, plus these social Types 3 & 4 layered above. Chains-of-Reasoning (CoR) is Type-2 reasoning layered on top of Type-1 N-gram guessing. Type-3 will graft naturally on Type-2: active learning with just-right steps (again that \\( \\mu/\\sigma \\le 0.2 \\) intuition). Type-4 collective intelligence pulls in social dynamics; we will want AI-s to have warm feelings toward humans and other carbon-based life forms. Motivations behind \"possibly \\( \\mu/\\sigma \\lt 0."
    },
    {
      "source": "post-knowing.html",
      "content": "on). Type-4 collective intelligence pulls in social dynamics; we will want AI-s to have warm feelings toward humans and other carbon-based life forms. Motivations behind \"possibly \\( \\mu/\\sigma \\lt 0.2 \\)\": (a) Kelly betting gives tolerable approximation errors when \\( \\mu/\\sigma \\) is small, hence the comfort zone above. (b) Distilling a smaller network from a bigger one typically lets us shrink by ~20% once the bigger one groks the pattern. (c) Bolting CoR onto a base language model often nets ~20% uplift. The same ratio pops up across scaling laws. If only I could fathom the existence of a stone or running water, that would be truly fascinating. Compared to that, the taxonomy above is just rearranging what we already half-know. K. Knowing and knowledge, epistemology, even ideology? + Knowing and knowledge, epistemology, even ideology? 1. Known Knowns. Deterministic knowledge - we know exactly which one. ([12]deterministic knowledge above) 2. Known Unknowns. We don't know which one, but we know how many of which type; i.e. the distribution. ([13]known pdf, [14]aleatoric uncertainty above) 3. Unknown Unknowns. We don't know the p.d.f. either. ([15]epistemic uncertainty above) 4. Unknown Knowns. Ideology. Fish swimming in water never knowing anything else but water. Possibly thus being unable to perceive the water too? (Zizek @ YT) Is wisdom the awareness of \"ignorance is modelled by the p.d."
    },
    {
      "source": "post-knowing.html",
      "content": "Ideology. Fish swimming in water never knowing anything else but water. Possibly thus being unable to perceive the water too? (Zizek @ YT) Is wisdom the awareness of \"ignorance is modelled by the p.d.f\", and \"knowledge is zero ignorance never to be attained\" by us humans? Knowledge, taste, wisdom: perhaps wisdom is just good taste in curating knowledge. Lindy effect. Lifetimes of intellectual artifacts follow power law distribution. Assume survival time \\( X \\) with p.d.f \\( f(t)=c/t^{(c+1)} \\) for \\( t \\ge 1 \\). If \\( c=2 \\), the expected additional life equals the life seen so far. (John D. Cook) Laplace's rule of succession. If some trend has been going for \\( N \\) years, there's a \\( 1/(N+2) \\) probability the trend breaks next year and 50% chance it continues another \\( N+1 \\) years. If some event hasn't happened for \\( N \\) years, there's a \\( 1/(N+2) \\) probability it happens next year. Limitation: for \\( N=0 \\) probability is \\( 1/2 \\), often an overstatement. It typically overestimates the chance of unprecedented events occurring. L. Consciousness + Connection to consciousness. Not a lot specifically, nothing over and above what's true of the brain as per the writing of Karl Friston (of his work I became aware recently; video [16]https://www.youtube.com/watch?v=iPj9D9LgK2A, text [17]https://www.wired."
    },
    {
      "source": "post-knowing.html",
      "content": "hing over and above what's true of the brain as per the writing of Karl Friston (of his work I became aware recently; video [16]https://www.youtube.com/watch?v=iPj9D9LgK2A, text [17]https://www.wired.com/story/karl-friston-free-energy-principle-arti ficial-intelligence/; shortest summary \"brain machine works by minimising the discrepancy error between model forecast and observation by better model and/or action in the world\", aka \"minimize free energy\" principle). But had to write some recently, [18]so here. One view that seems testable to me: consciousness is like the conductor in the orchestra, it's the router in an Mixture of Experts model. Consciousness module is the router in MoE. Experts in the MoE are the individual members of the orchestra, every one playing their own instrument. So while the router is not a very big or a very special module (in fact - it's in many ways simpler then the specialised modules) - it's a single point of failure. So once consciousness (in HI brain) / router (in IA MoE) fails - no expert can learn properly, or even if the experts learns, the knowledge can not be utilised. MoE architecture is the reason why it's so data efficient. Sparse representations, by virtue of injecting that prior knowledge in the process (\"these connections for this data do not need updating\"), can be data efficient. It's efficient to know in advance \"this data is no use to Experts 1,3,4,5, and is to be used to reach only Expert#2\"."
    },
    {
      "source": "post-knowing.html",
      "content": "s (\"these connections for this data do not need updating\"), can be data efficient. It's efficient to know in advance \"this data is no use to Experts 1,3,4,5, and is to be used to reach only Expert#2\". MoE maybe a reason why we have too many neurons. Our brains are less efficient than NN-s when it comes to utilising their weight. NN-s are much more efficient than us humans, when looking at efficiency in weights sizes space. Our brains trade parsimony in weights space, to gain efficiencies to gain speed and reduce power consumption - both achieved by MoE. Further: sparse representations (and MoE is a macro-scale example) may make incremental learning, which is one way to implement continuous learning, practically doable. If only a limited set of weight need to be updated, for the brain to acquire new memory or knowledge, that means it can be done without losing all other previous memory or knowledge. M. Etc + Amusing paper illustrates how new/more information can make the entropy (uncertainty) higher, thus reducing our knowledge. Where our knowledge measure is the spikiness of the probability density function. After the additional observation (new information), the conditional p.d.f. post the observation is flatter then before => our knowledge decreased. Michael R DeWeese and Markus Meister (1999), [19]\"How to measure the information gained from one symbol\", [20]Network: Computation in Neural Systems, 10:4, 325-340, [21]DOI: 10."
    },
    {
      "source": "post-knowing.html",
      "content": "r knowledge decreased. Michael R DeWeese and Markus Meister (1999), [19]\"How to measure the information gained from one symbol\", [20]Network: Computation in Neural Systems, 10:4, 325-340, [21]DOI: 10.1088/0954-898X/10/4/303 -- LJ HPD Thu 20 Nov 2025 References 1. https://faculty.ucmerced.edu/mcarreira-perpinan/ 2. file:///Users/ljubomir/ljubomirj.github.io/post-knowing.html 3. file:///Users/ljubomir/ljubomirj.github.io/post-knowing.html 4. file:///Users/ljubomir/ljubomirj.github.io/post-knowing.html 5. file:///Users/ljubomir/ljubomirj.github.io/post-knowing.html#know-pdf 6. file:///Users/ljubomir/ljubomirj.github.io/post-knowing.html#aleo-pdf 7. file:///Users/ljubomir/ljubomirj.github.io/post-knowing.html#aleo-pdf 8. file:///Users/ljubomir/ljubomirj.github.io/post-knowing.html#know-pdf 9. file:///Users/ljubomir/ljubomirj.github.io/post-knowing.html#aleo-pdf 10. https://news.ycombinator.com/item?id=38261719 11. http://www.whatislife.ie/downloads/What-is-Life.pdf 12. file:///Users/ljubomir/ljubomirj.github.io/post-knowing.html#know-dirac 13. file:///Users/ljubomir/ljubomirj.github.io/post-knowing.html#know-pdf 14. file:///Users/ljubomir/ljubomirj.github.io/post-knowing.html#aleo-pdf 15. file:///Users/ljubomir/ljubomirj.github.io/post-knowing.html#epi-unpdf 16. https://www.youtube.com/watch?v=iPj9D9LgK2A 17. https://archive.is/AngqY#selection-2139.0-2139.15 18. file:///Users/ljubomir/ljubomirj.github.io/post-consciousness.html 19. file:///Users/ljubomir/ljubomirj.github."
    },
    {
      "source": "post-knowing.html",
      "content": "outube.com/watch?v=iPj9D9LgK2A 17. https://archive.is/AngqY#selection-2139.0-2139.15 18. file:///Users/ljubomir/ljubomirj.github.io/post-consciousness.html 19. file:///Users/ljubomir/ljubomirj.github.io/DeWeese_Meister_-_How_to_measure_the_information_gained_from_one_symbol_-_ne9403.pdf 20. https://www.tandfonline.com/doi/abs/10.1088/0954-898X_10_4_303 21. https://iopscience.iop.org/article/10.1088/0954-898X/10/4/303"
    },
    {
      "source": "post-links-to.html",
      "content": "Links to Blogs, Substacks, Youtubes, Podcasts, Magazines etc Over the years online, I benefited immensely from the Internet users that write and put up stuff online for other users to read. For the love of it - nothing else. There is no purer form of creativity than that. \"Here!! I like this, maybe you will like it too. On an off chance, infinitesimally small, I make this extra step extra effort to put it out there on the Internet. Free, free like beer and freedom too, for all to see\". Initially and for a long time things were kept and curated in Bookmarks. Then Google spoiled me when it became possible to \"Google it\" anything - there was no great need for Bookmarks. Then Internet expanded and some things are Social media posts, others are Substacks, still others are old style personal blogs, then there is Youtube podcasts and videos, then Spotify, then podcasts on other platforms etc etc. So I thought this is a good place - yet another place; but redundancy is the mother of resilience - to add links to content I liked and consume online. References and links put in no particular order, as my memory blurts them out. It's good to have them on one page and check from time to time. Nowadays with every author being online, one gets to imbibe their ideas anyway on the 80/20 principle, via the \"quanta of ideas\" that are memes, spread esp via X/Twitter/Bsky where the medium specifically encourages that."
    },
    {
      "source": "post-links-to.html",
      "content": "uthor being online, one gets to imbibe their ideas anyway on the 80/20 principle, via the \"quanta of ideas\" that are memes, spread esp via X/Twitter/Bsky where the medium specifically encourages that. So when I finally get to a book or a movie of the author, I already have heard or seen 80% of what they are saying, know of the ideas, so often I don't get to finish the book or the movie! I'm not complaining though - but bragging about it! Because there is something else already worthy of attention! We are so so lucky now. Some of my memories of my childhood (in the 1980-s late socialism of Yugoslavia) are of mind bending boredom. There was little to do, life was very boring at times (not all the time - other times it was fun and exciting), that reading a mildly interesting book, watching an interesting movie on TV (1.5 channels) or in the cinema (not that many of those), or playing - or more often watching others play - random games in the local arcades tent, was a treat. Home computers changed that somewhat, and then Personal Computers and latter the online world and Internet changed that a lot! And for the better. Scott Alexander's Astral Codex Ten[1]https://www.astralcodexten.com/ (previous Slate Star Codex [2]https://slatestarcodex.com/) J. Sanilac [3]https://www.jsanilac.com/ FSF Free Software Foundation [4]https://www.fsf.org/, GNU operating system [5]https://www.gnu.org/, software [6]https://www.gnu.org/software/software.html, philosophy [7]https://www.gnu."
    },
    {
      "source": "post-links-to.html",
      "content": "jsanilac.com/ FSF Free Software Foundation [4]https://www.fsf.org/, GNU operating system [5]https://www.gnu.org/, software [6]https://www.gnu.org/software/software.html, philosophy [7]https://www.gnu.org/philosophy/ RMS Richard Stallman's Personal Site [8]https://stallman.org/, archive [9]https://stallman.org/archive.html ESR Eric S. Raymond's Home Page [10]http://www.catb.org/~esr/ weblog [11]http://esr.ibiblio.org/ FAQs [12]http://www.catb.org/~esr/faqs/ Bryan Caplan's Bet On It [13]https://www.betonit.ai/ Nate Silver's Silver Bulletin [14]https://www.natesilver.net/ Shtetl-Optimized The Blog of Scott Aaronson [15]https://scottaaronson.blog/ (PHYS771 Lecture 9: Quantum [16]http://www.scottaaronson.com/democritus/lec9.html) The Intrinsic Perspective By Erik Hoel [17]https://www.theintrinsicperspective.com/ Yuval Harari [18]https://www.ynharari.com/, [19]https://www.youtube.com/user/YuvalNoahHarari Civilution for Universal WellBeing [20]https://www.civilution.org/ Rutger Bregman [21]https://linktr.ee/rutgerbregman Chris Dillow's Stumbling and Mumbling [22]https://stumblingandmumbling.typepad.com/ Dominic Cummings substack [23]https://dominiccummings.substack.com/ Dwarkesh Patel [24]https://www.youtube.com/DwarkeshPatel [25]https://www.dwarkeshpatel.com/ Apply Liberally by Matthew Downhour [26]https://applyliberally.substack.com/ Francis Fukuyama [27]https://fukuyama.stanford.edu/ Pluralistic: Daily links from Cory Doctorow [28]https://pluralistic."
    },
    {
      "source": "post-links-to.html",
      "content": "m/ Apply Liberally by Matthew Downhour [26]https://applyliberally.substack.com/ Francis Fukuyama [27]https://fukuyama.stanford.edu/ Pluralistic: Daily links from Cory Doctorow [28]https://pluralistic.net/ Human Progress [29]https://humanprogress.org/ James Bloodworth [30]https://www.forthedeskdrawer.com/ Odds and Ends of History By James O'Malley [31]https://takes.jamesomalley.co.uk/ Sabine Hossenfelder [32]https://www.youtube.com/c/SabineHossenfelder Jason Crawford [33]https://jasoncrawford.org/archive Ole Peters Ergodicity economics [34]https://ergodicityeconomics.com/ Scientific Discovery By Saloni Dattani [35]https://www.scientificdiscovery.dev/ Blair Fix Economics from the Top Down [36]https://economicsfromthetopdown.com/ John D. Cook blog [37]https://www.johndcook.com/blog/ Lex Fridman [38]https://lexfridman.com/ [39]https://www.youtube.com/lexfridman Information Processing - Steve Hsu [40]https://stevehsu.substack.com/ Richard McElreath [41]http://xcelab.net/rm/ [42]https://www.youtube.com/@rmcelreath Slime Mold Time Mold [43]https://slimemoldtimemold.com/ Works in Progress [44]https://worksinprogress.co/ Warren Mosler's Mosler Economics / Modern Monetary Theory [45]https://moslereconomics.com/ Naked Capitalism [46]https://www.nakedcapitalism.com/ Steve Keen substack [47]https://profstevekeen.substack.com/ (Minsky Home [48]https://sourceforge.net/p/minsky/home/Home/) Derek Lowe's In The Pipeline [49]https://www.science."
    },
    {
      "source": "post-links-to.html",
      "content": "www.nakedcapitalism.com/ Steve Keen substack [47]https://profstevekeen.substack.com/ (Minsky Home [48]https://sourceforge.net/p/minsky/home/Home/) Derek Lowe's In The Pipeline [49]https://www.science.org/blogs/pipeline Arts & Letters Daily [50]http://www.aldaily.com/ Antiwar [51]https://www.antiwar.com/ Matthew Downhour's substack Apply Liberally [52]https://applyliberally.substack.com/ Liberal Currents [53]https://www.liberalcurrents.com/ Reason Magazinehttp://reason.com/ Triggernometry podcast YouTube[54]https://www.youtube.com/@triggerpod The Critic magazine [55]https://thecritic.co.uk/ Compact magazine [56]https://www.compactmag.com/ Hacker News [57]https://news.ycombinator.com/news Stack Overflow [58]http://stackoverflow.com/ Super User [59]http://superuser.com/ Server Fault [60]http://serverfault.com/ Unix & Linux Stack Exchange [61]http://unix.stackexchange.com/ SLIME MOLD TIME MOLD - Mad Science Blogging [62]https://slimemoldtimemold.com/ Richard Stallman [63]https://www.stallman.org/ RMS archives [64]https://www.stallman.org/archives/ Free Software Foundation FSF [65]https://www.fsf.org/ Arabesque - Systems, Tools, and Terminal Science [66]https://blog.sanctum.geek.nz/, a blog by [67]Tom Ryder Gurwinder blog [68]https://www.gurwinder.blog/ Our World in Data (OWID) [69]https://ourworldindata.org/ Max Rocer at OWID [70]https://ourworldindata.org/team/max-roser Tom Forth blog [71]https://tomforth.co.uk/ Mark Litwintschik blog [72]https://tech.marksblogg."
    },
    {
      "source": "post-links-to.html",
      "content": "ta (OWID) [69]https://ourworldindata.org/ Max Rocer at OWID [70]https://ourworldindata.org/team/max-roser Tom Forth blog [71]https://tomforth.co.uk/ Mark Litwintschik blog [72]https://tech.marksblogg.com/ 3Blue1Brown YouTube [73]https://www.youtube.com/@3blue1brown FAQ https://www.3blue1brown.com/faq Yann Lecun [74]https://yann.lecun.com/ Andrej Karpathy [75]https://karpathy.ai/ Christopher Olah colah's blog [76]http://colah.github.io/ Tim Dettmers blog [77]https://timdettmers.com/ Brandur articles [78]https://brandur.org/articles Walter Bright [79]http://www.walterbright.com/ Diomidis Spinellis home page [80]https://www.spinellis.gr/index.html.var Bartosz Milewski [81]https://bartoszmilewski.com/ Georgi Gerganov [82]https://github.com/ggerganov Jeff Atwood Coding Horror [83]https://blog.codinghorror.com/ Marc Andreessen Substack [84]https://pmarca.substack.com/ Paul Graham [85]https://paulgraham.com/ Patrick Collison [86]https://patrickcollison.com/ DAVID HEINEMEIER HANSSON [87]https://dhh.dk/ Nomad list [88]https://nomads.com/ JWZ blog [89]https://www.jwz.org/blog/ Ian Dunt substack [90]https://iandunt.substack.com/ Sam Bowman substack [91]https://www.sambowman.co/ Dominic Cummings substack [92]https://dominiccummings.substack.com/ Information Processing - Steve Hsu substack [93]https://stevehsu.substack.com/, Manifold podcast [94]https://www.manifold1.com/episodes Brett Scott blog Altered States of Monetary Consciousnes [95]https://alteredstatesof."
    },
    {
      "source": "post-links-to.html",
      "content": "g - Steve Hsu substack [93]https://stevehsu.substack.com/, Manifold podcast [94]https://www.manifold1.com/episodes Brett Scott blog Altered States of Monetary Consciousnes [95]https://alteredstatesof.money/ Blog by Matt Ridley [96]http://www.rationaloptimist.com/blog/ Idle Words - Maciej Cegl/owski [97]https://idlewords.com/ Crooked Timber [98]https://crookedtimber.org/ Pluralistic: Daily links from Cory Doctorow [99]https://pluralistic.net/ LessWrong [100]https://www.lesswrong.com/ The Nutshell Times [101]https://thenutshelltimes.com/ Deirdre McCloskey [102]http://www.deirdremccloskey.org/ EPchan blog Quantitative Trading [103]http://epchan.blogspot.com/ Locklin on science [104]http://scottlocklin.wordpress.com/ Rob Carvers This Blog is Systematic [105]https://qoppac.blogspot.com/ Robert J Frey Keplerian Finance [106]http://keplerianfinance.com/ Michael Tan's Blog [107]https://michaeltanphd.com/ Ole Peters Ergodicity Economics [108]https://ergodicityeconomics.com/about/, For to withhold is to perish [109]https://ergodicityeconomics.com/2023/08/29/for-to-withhold-is-to-p erish/, Textbook [110]https://ergodicityeconomics.com/publications/ Win Vector LLC [111]https://win-vector.com/ Diomidis Spinellis home page [112]https://www.spinellis.gr/index.html.var Tim Dettmers [113]https://timdettmers.com/ Bert Hubert [114]https://berthub.eu/ Sam Altman [115]https://blog.samaltman.com/ Alfredo Canziani blog [116]https://atcold.github.io/blog.html Giuseppe Paleologo [117]https://linktr."
    },
    {
      "source": "post-links-to.html",
      "content": "/timdettmers.com/ Bert Hubert [114]https://berthub.eu/ Sam Altman [115]https://blog.samaltman.com/ Alfredo Canziani blog [116]https://atcold.github.io/blog.html Giuseppe Paleologo [117]https://linktr.ee/paleologo Leo Breiman [118]https://www.stat.berkeley.edu/~breiman/ Andreas Weigend [119]http://www.weigend.com/ Spyros Makridakis, The M Forecasting Competitions [120]https://www.unic.ac.cy/iff/research/forecasting/m-competitions/ Uncharted territories by Tomas Pueyo [121]https://unchartedterritories.tomaspueyo.com/ Matt Lakeman \"Notes on ...\" travels blog [122]https://mattlakeman.org/ DYNOMIGHT INTERNET WEBSITE [123]https://dynomight.net/ Julia Evans [124]https://jvns.ca/ Gwern Branwen website on AI, psychology, & statistics [125]https://gwern.net/ Simon Willison blog [126]https://simonwillison.net/, link blog [127]https://simonwillison.net/search/?type=blogmark, blogmarks why and how [128]https://simonwillison.net/2024/Dec/22/link-blog/, github [129]https://github.com/simonw Maxwell Tabarrok substack Maximum Progress blog [130]https://substack.com/@maximumprogress ([131]Four Futures For Cognitive Labor) David Shapiro's Substack [132]https://daveshap.substack.com/ (e.g. [133]Deny, Defend, Depose: We are already living in a Cyberpunk Hell (and how we can fix it), [134]What do I mean when I say \"Post-Labor Economics\" anyways?) Geoffrey E. Hinton home page [135]https://www.cs.toronto.edu/~hinton Hugging Face Blog [136]https://huggingface.co/blog (e.g."
    },
    {
      "source": "post-links-to.html",
      "content": "n fix it), [134]What do I mean when I say \"Post-Labor Economics\" anyways?) Geoffrey E. Hinton home page [135]https://www.cs.toronto.edu/~hinton Hugging Face Blog [136]https://huggingface.co/blog (e.g. [137]Scaling Test Time Compute with Open Models), Models [138]https://huggingface.co/models Richard Hanania's Substack [139]https://substack.com/@richardhanania, Newsletter [140]https://www.richardhanania.com/ (e.g. Understanding the Tech Right [141]https://www.richardhanania.com/p/understanding-the-tech-right) Derek Sivers blog [142]https://sive.rs/. E.g. [143]\"How to Get Rich\", [144]\"How to Be Useful to Others\", and other [145]\"Do this. Directives -- part 1\" Sam McRoberts blog [146]https://thegrandredesign.substack.com/, e.g. [147]The Grand Redesign \"Change the Stories, Change the World\" Dynomight blog [148]https://dynomight.net/. E.g. [149]DumPy: NumPy except it's OK if you're dum, [150]I don't like NumPy, [151]How much information is in DNA?. On the [152]About page, this strikes me as good advice as any: Wondering what to do with your life? Here's what I suggest: * First priority: Your physical health. (No health -> no life.) * Second priority: Reasonable financial security. (No food -> no health.) * Third priority: Good relationships with friends and family. (Depressed -> no mental health.) After that you can do whatever. The game you're playing doesn't have any rules and there's no way to win. Yevgeniy Brikman blog [153]https://www.ybrikman.com/, e.g."
    },
    {
      "source": "post-links-to.html",
      "content": "y. (Depressed -> no mental health.) After that you can do whatever. The game you're playing doesn't have any rules and there's no way to win. Yevgeniy Brikman blog [153]https://www.ybrikman.com/, e.g. [154]Don't learn to code. Learn to think. Joscha Bach [155]http://bach.ai/ DR. MICHAEL LEVIN [156]https://thoughtforms.life/ -- LJ HPD Fri 18 Oct 18:39:23 BST 2024 References Visible links: 1. https://www.astralcodexten.com/ 2. https://slatestarcodex.com/ 3. https://www.jsanilac.com/ 4. https://www.fsf.org/ 5. https://www.gnu.org/ 6. https://www.gnu.org/software/software.html 7. https://www.gnu.org/philosophy/ 8. https://stallman.org/ 9. https://stallman.org/archive.html 10. http://www.catb.org/~esr/ 11. http://esr.ibiblio.org/ 12. http://www.catb.org/~esr/faqs/ 13. https://www.betonit.ai/ 14. https://www.natesilver.net/ 15. https://scottaaronson.blog/ 16. http://www.scottaaronson.com/democritus/lec9.html 17. https://www.theintrinsicperspective.com/ 18. https://www.ynharari.com/ 19. https://www.youtube.com/user/YuvalNoahHarari 20. https://www.civilution.org/ 21. https://linktr.ee/rutgerbregman 22. https://stumblingandmumbling.typepad.com/ 23. https://dominiccummings.substack.com/ 24. https://www.youtube.com/DwarkeshPatel 25. https://www.dwarkeshpatel.com/ 26. https://applyliberally.substack.com/ 27. https://fukuyama.stanford.edu/ 28. https://pluralistic.net/ 29. https://humanprogress.org/ 30. https://www.forthedeskdrawer.com/ 31. https://takes.jamesomalley.co.uk/ 32."
    },
    {
      "source": "post-links-to.html",
      "content": "lyliberally.substack.com/ 27. https://fukuyama.stanford.edu/ 28. https://pluralistic.net/ 29. https://humanprogress.org/ 30. https://www.forthedeskdrawer.com/ 31. https://takes.jamesomalley.co.uk/ 32. https://www.youtube.com/c/SabineHossenfelder 33. https://jasoncrawford.org/archive 34. https://ergodicityeconomics.com/ 35. https://www.scientificdiscovery.dev/ 36. https://economicsfromthetopdown.com/ 37. https://www.johndcook.com/blog/ 38. https://lexfridman.com/ 39. https://www.youtube.com/lexfridman 40. https://stevehsu.substack.com/ 41. http://xcelab.net/rm/ 42. https://www.youtube.com/@rmcelreath 43. https://slimemoldtimemold.com/ 44. https://worksinprogress.co/ 45. https://moslereconomics.com/ 46. https://www.nakedcapitalism.com/ 47. https://profstevekeen.substack.com/ 48. https://sourceforge.net/p/minsky/home/Home/ 49. https://www.science.org/blogs/pipeline 50. http://www.aldaily.com/ 51. https://www.antiwar.com/ 52. https://applyliberally.substack.com/ 53. https://www.liberalcurrents.com/ 54. https://www.youtube.com/@triggerpod 55. https://thecritic.co.uk/ 56. https://www.compactmag.com/ 57. https://news.ycombinator.com/news 58. http://stackoverflow.com/ 59. http://superuser.com/ 60. http://serverfault.com/ 61. http://unix.stackexchange.com/ 62. https://slimemoldtimemold.com/ 63. https://www.stallman.org/ 64. https://www.stallman.org/archives/ 65. https://www.fsf.org/ 66. https://blog.sanctum.geek.nz/ 67. https://blog.sanctum.geek.nz/about/ 68. https://www.gurwinder."
    },
    {
      "source": "post-links-to.html",
      "content": ".com/ 63. https://www.stallman.org/ 64. https://www.stallman.org/archives/ 65. https://www.fsf.org/ 66. https://blog.sanctum.geek.nz/ 67. https://blog.sanctum.geek.nz/about/ 68. https://www.gurwinder.blog/ 69. https://ourworldindata.org/ 70. https://ourworldindata.org/team/max-roser 71. https://tomforth.co.uk/ 72. https://tech.marksblogg.com/ 73. https://www.youtube.com/@3blue1brown 74. https://yann.lecun.com/ 75. https://karpathy.ai/ 76. http://colah.github.io/ 77. https://timdettmers.com/ 78. https://brandur.org/articles 79. http://www.walterbright.com/ 80. https://www.spinellis.gr/index.html.var 81. https://bartoszmilewski.com/ 82. https://github.com/ggerganov 83. https://blog.codinghorror.com/ 84. https://pmarca.substack.com/ 85. https://paulgraham.com/ 86. https://patrickcollison.com/ 87. https://dhh.dk/ 88. https://nomads.com/ 89. https://www.jwz.org/blog/ 90. https://iandunt.substack.com/ 91. https://www.sambowman.co/ 92. https://dominiccummings.substack.com/ 93. https://stevehsu.substack.com/ 94. https://www.manifold1.com/episodes 95. https://alteredstatesof.money/ 96. http://www.rationaloptimist.com/blog/ 97. https://idlewords.com/ 98. https://crookedtimber.org/ 99. https://pluralistic.net/ 100. https://www.lesswrong.com/ 101. https://thenutshelltimes.com/ 102. http://www.deirdremccloskey.org/ 103. http://epchan.blogspot.com/ 104. http://scottlocklin.wordpress.com/ 105. https://qoppac.blogspot.com/ 106. http://keplerianfinance.com/ 107. https://michaeltanphd."
    },
    {
      "source": "post-links-to.html",
      "content": "ttp://www.deirdremccloskey.org/ 103. http://epchan.blogspot.com/ 104. http://scottlocklin.wordpress.com/ 105. https://qoppac.blogspot.com/ 106. http://keplerianfinance.com/ 107. https://michaeltanphd.com/ 108. https://ergodicityeconomics.com/about/ 109. https://ergodicityeconomics.com/2023/08/29/for-to-withhold-is-to-perish/ 110. https://ergodicityeconomics.com/publications/ 111. https://win-vector.com/ 112. https://www.spinellis.gr/index.html.var 113. https://timdettmers.com/ 114. https://berthub.eu/ 115. https://blog.samaltman.com/ 116. https://atcold.github.io/blog.html 117. https://linktr.ee/paleologo 118. https://www.stat.berkeley.edu/~breiman/ 119. http://www.weigend.com/ 120. https://www.unic.ac.cy/iff/research/forecasting/m-competitions/ 121. https://unchartedterritories.tomaspueyo.com/ 122. https://mattlakeman.org/ 123. https://dynomight.net/ 124. https://jvns.ca/ 125. https://gwern.net/ 126. https://simonwillison.net/ 127. https://simonwillison.net/search/?type=blogmark 128. https://simonwillison.net/2024/Dec/22/link-blog/ 129. https://github.com/simonw 130. https://substack.com/@maximumprogress 131. https://www.maximum-progress.com/p/four-futures-for-cognitive-labor 132. https://daveshap.substack.com/ 133. https://daveshap.substack.com/p/deny-defend-depose-we-are-already 134. https://daveshap.substack.com/p/what-do-i-mean-when-i-say-post-labor 135. https://www.cs.toronto.edu/~hinton 136. https://huggingface.co/blog 137. https://huggingface."
    },
    {
      "source": "post-links-to.html",
      "content": "efend-depose-we-are-already 134. https://daveshap.substack.com/p/what-do-i-mean-when-i-say-post-labor 135. https://www.cs.toronto.edu/~hinton 136. https://huggingface.co/blog 137. https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute 138. https://huggingface.co/models 139. https://substack.com/@richardhanania 140. https://www.richardhanania.com/ 141. https://www.richardhanania.com/p/understanding-the-tech-right 142. https://sive.rs/ 143. https://sive.rs/d1r 144. https://sive.rs/d1u 145. https://sive.rs/d1 146. https://thegrandredesign.substack.com/ 147. https://thegrandredesign.com/ 148. https://dynomight.net/ 149. https://dynomight.net/dumpy/ 150. https://dynomight.net/numpy/ 151. https://dynomight.net/dna/ 152. https://dynomight.net/about/ 153. https://www.ybrikman.com/ 154. https://www.ybrikman.com/blog/2014/05/19/dont-learn-to-code-learn-to-think/ 155. http://bach.ai/ 156. https://thoughtforms.life/ Hidden links: 158. http://reason.com/ 159. https://www.3blue1brown.com/faq 160. file://localhost/Users/ljubomir/ljubomirj.github.io/post-links-to.html 161. file://localhost/Users/ljubomir/ljubomirj.github.io/post-links-to.html 162. file://localhost/Users/ljubomir/ljubomirj.github.io/post-links-to.html 163. file://localhost/Users/ljubomir/ljubomirj.github.io/post-links-to.html 164. file://localhost/Users/ljubomir/ljubomirj.github.io/post-links-to.html 165. file://localhost/Users/ljubomir/ljubomirj.github.io/post-links-to.html 166."
    },
    {
      "source": "post-links-to.html",
      "content": "omir/ljubomirj.github.io/post-links-to.html 164. file://localhost/Users/ljubomir/ljubomirj.github.io/post-links-to.html 165. file://localhost/Users/ljubomir/ljubomirj.github.io/post-links-to.html 166. file://localhost/Users/ljubomir/ljubomirj.github.io/post-links-to.html 167. file://localhost/Users/ljubomir/ljubomirj.github.io/post-links-to.html 168. file://localhost/Users/ljubomir/ljubomirj.github.io/post-links-to.html 169. file://localhost/Users/ljubomir/ljubomirj.github.io/post-links-to.html 170. file://localhost/Users/ljubomir/ljubomirj.github.io/post-links-to.html"
    },
    {
      "source": "post-ljubomirj.html",
      "content": "Welcome to the Home Page of Ljubomir Josifovski Next - [1]Machine Learning (ML) / Artificial Intelligence (AI). Recent - [2]systematic trading, research and development. Prior - [3]automatic speech recognition (ASR) in noise, spoken documents indexing and [4]retreival with spoken queries, speech synthesis, machine learning (ML). LJ clouds pic Online coordinates X @ljupc0 [5]https://x.com/ljupc0, highlights [6]https://x.com/ljupc0/highlights, posts search [7]https://x.com/search?q=%28from%3Aljupc0%29&src=typed_query&f=live, local [8]arhive Linkedin ljubomirjosifovski [9]https://www.linkedin.com/in/ljubomirjosifovski Bsky @ljupco.bsky.social [10]https://bsky.app/profile/ljupco.bsky.social, posts [11]https://bsky.app/search?q=from%3Aljupco.bsky.social (click Latest) Hacker News ljosifov [12]https://news.ycombinator.com/user?id=ljosifov ([13]favourites) Substack @ljubomirjosifovski [14]https://substack.com/@ljubomirjosifovski Mastodon @ljupco [15]https://mstdn.io/@ljupco GitHub ljubomirj [16]https://github.com/ljubomirj Hugging Face ljupco [17]https://huggingface.co/ljupco Offline coordinates In [18]Harpenden [19]UK, from [20]Skopje [21]MK. My happy place - sharing with [22]PK ([23]book), [24]VJ, [25]KJ. garden office watched over \"all watched over by machines of loving grace\" -- LJ HPD Sun 6 Oct 07:50:30 BST 2024 References Visible links: 1. file:///Users/ljubomir/ljubomirj.github.io/cvlj94s2.pdf 2. file:///Users/ljubomir/ljubomirj.github.io/cvlj94.pdf 3."
    },
    {
      "source": "post-ljubomirj.html",
      "content": "of loving grace\" -- LJ HPD Sun 6 Oct 07:50:30 BST 2024 References Visible links: 1. file:///Users/ljubomir/ljubomirj.github.io/cvlj94s2.pdf 2. file:///Users/ljubomir/ljubomirj.github.io/cvlj94.pdf 3. file:///Users/ljubomir/ljubomirj.github.io/tha.pdf 4. https://patents.google.com/patent/EP1654678A2/en 5. https://x.com/ljupc0 6. https://x.com/ljupc0/highlights 7. https://x.com/search?q=%28from%3Aljupc0%29&src=typed_query&f=live 8. file:///Users/ljubomir/ljubomirj.github.io/twitter-history.html 9. https://www.linkedin.com/in/ljubomirjosifovski 10. https://bsky.app/profile/ljupco.bsky.social 11. https://bsky.app/search?q=from%3Aljupco.bsky.social 12. https://news.ycombinator.com/user?id=ljosifov 13. https://news.ycombinator.com/favorites?id=ljosifov 14. https://substack.com/@ljubomirjosifovski 15. https://mstdn.io/@ljupco 16. https://github.com/ljubomirj 17. https://huggingface.co/ljupco 18. https://en.wikipedia.org/wiki/Harpenden 19. https://en.wikipedia.org/wiki/United_Kingdom 20. https://en.wikipedia.org/wiki/Skopje 21. https://en.wikipedia.org/wiki/North_Macedonia 22. https://chatgpt19.godaddysites.com/ 23. https://drive.google.com/file/d/1pxVYCSPGot5B3Ee3gPXaCZALMIkQLlD9/view 24. https://www.linkedin.com/in/vedar-josifovski-237908290 25. https://kalenjosifovski.github.io/ Hidden links: 27. file://localhost/Users/ljubomir/ljubomirj.github.io/post-ljubomirj.html"
    },
    {
      "source": "post-ljubomirj.html",
      "content": "en links: 27. file://localhost/Users/ljubomir/ljubomirj.github.io/post-ljubomirj.html"
    },
    {
      "source": "post-ml-llm-dev.html",
      "content": "ML LLM Dev Links and Notes of resources of interest Dev, LLM code writing, Agents - coding agents It turned out coding agents are the 2nd big LLM killer-app. A wide application area with huge unserved demand. The moment people started mass copy-pasta code to-fro chat-gpt, was the moment we all realised: ah-haa! Why do people go through all the trouble, jump through hoops, inconveniences etc, to do that? Because they found it useful! I did it too, I too found it better than the alternatives: 1) coding for myself, solo, me-myself-and-I alone wiht my-code (now), and 2) pestering colleagues to read, them being grumpy the same way I'm grumpy when my attentiion is dragged from writing what interests me, into reading something else someone else wrote what interests them. Thrashing my context inthe process. :-) The agents have been tremendous success and seen tremendous progress. I'm amazed. I got $250 free CC-web credits when subscribing to a new Anthropic $20/mo sub, and proceed to run Cluade Code - Web for 2 weeks now on-and-off. This was CC running on a virtual box somewhere in the cloud, and communicating via github push/pull and web gui. All the while running Curson for my day job - with the RL trained internal model that's both fast and good, it's a breeze to use. Then there are random CC session on demand in the terminal."
    },
    {
      "source": "post-ml-llm-dev.html",
      "content": "d web gui. All the while running Curson for my day job - with the RL trained internal model that's both fast and good, it's a breeze to use. Then there are random CC session on demand in the terminal. At the start of Oct-2025 codex became so good, that every time I have an idea, I just open a terminal in a directory, and run codex: I command the codex, codex commands the command line. For me AGI arrived with codex-5-high. I'm loving that socratic dialogue became the new programming. � In a dialogue between myself and codex, a set of actions emerges materialises somehow, and the job is done. � In this New-as-old world, we got Chat-as-programming, Dialogue-as-code, LLM-as-cpu, Context-as-ram. Once the QA session exhausts what was on my mind, I pause Ctrl-Z codex into background. On the next session, I continue summon codex back in the foreground with $ fg, and continue from where we stopped last time. In VSCode I got Cline using OpenAI API on localhost:1234 served by LMStudio. Recently got plessantly surprised to find out both got streaming support for MiniMax-M2 xml based use of tools. Did not expect that! And before that got Claude Code in terminal to use local LLM served by LMStudio. Vai e local litellm proxy running in docker and translating between Antropic API CC wants, and the OpenAI API LMStudio provides. Coding agents - local in terminal without sweat, factory."
    },
    {
      "source": "post-ml-llm-dev.html",
      "content": "by LMStudio. Vai e local litellm proxy running in docker and translating between Antropic API CC wants, and the OpenAI API LMStudio provides. Coding agents - local in terminal without sweat, factory.ai-s droid ftw (Nov-2025) Heh - turns out eavesdropping on @FactoryAI droid talk to @lmstudio is not only useful but tremendous fun! Who knew?? The model/agent interaction is oft - 'were you raised by wolves, you two, per chance??' Really? You thought '$ mkdir /Project' will work, that's the way to go? fr! ffs Seems droid does not realise it was started in the 'current project directory' to make things easier for it. Do people usually launch their agent on Mars, while wanting it to edit files on Earth?? All these xml-like conversations remind me - the language spoken (the protocol) needs to be human readable. And even better if reading well than poorly. Internet - in addition to being free - IETF very early on cottoned on the fact \"no human readable -> no human will get interested -> no one to make it work -> stays cr*p and dies for lack of use\". So one could follow SMTP, POP3 and be not only readable, but read oh-key at leasat in not excellent. Formalisation of these things into some xml monstrosity is good when teaching principles to students. It's bad if used in actual practice. Much better to in practice make use of every nook and cranny to your advantage, use any accidental twist and turn, to make things more efficient, easier etc."
    },
    {
      "source": "post-ml-llm-dev.html",
      "content": "ents. It's bad if used in actual practice. Much better to in practice make use of every nook and cranny to your advantage, use any accidental twist and turn, to make things more efficient, easier etc. UTF-8 backward compatible variable length encoding comes to mind. The setup is as straightforward as it gets. For Droid I used $ cat ~/.factory/config.json { \"custom_models\": [ { \"model_display_name\": \"LMStudio/qwen3-30b-a3b-yoyo-v5\", \"model\": \"qwen3-30b-a3b-yoyo-v5-qx86-hi-mlx\", \"base_url\": \"http://localhost:1234/v1\", \"api_key\": \"sk\", \"provider\": \"generic-chat-completion-api\", \"max_tokens\": 262144 } ] } then once in $ droid, select with /model. Once you confirm LM Studio is running and serving on port 1234, this should work! So the model is https://huggingface.co/nightmedia/Qwen3-30B-A3B-YOYO-V5-qx86-hi-mlx, quantisation of https://huggingface.co/YOYO-AI/Qwen3-30B-A3B-YOYO-V5 , derived from joining of 3 Qwen3 models: Model tree for YOYO-AI/Qwen3-30B-A3B-YOYO-V5: Qwen/Qwen3-30B-A3B-Instruct-2507 Qwen/Qwen3-30B-A3B-Thinking-2507 Qwen/Qwen3-Coder-30B-A3B-InstructModel Highlights: * merge method: yoyo_fusion * precision: dtype: bfloat16 * Context length: 262,144 & 1010000 Parameter Settings: Temperature=0.7, TopP=0.8, TopK=20, MinP=0. The context is clearly much greater than 32K - this262,144 & 1010000So 256K and 1M the latter I presume with some ROPE. LMStuidio clearly allows 256KRead for yourself here https://huggingface."
    },
    {
      "source": "post-ml-llm-dev.html",
      "content": "MinP=0. The context is clearly much greater than 32K - this262,144 & 1010000So 256K and 1M the latter I presume with some ROPE. LMStuidio clearly allows 256KRead for yourself here https://huggingface.co/YOYO-AI/Qwen3-30B-A3B-YOYO-V5Show moreAcknowledged error regarding context length capabilities.The user is correcting me about the model's context length. They're right - this is a Qwen3-30B model that has been merged using the \"yoyo_fusion\" method from three different Qwen3 models: Qwen3-30B-A3B-Instruct-2507 Qwen3-30B-A3B-Thinking-2507 (a reasoning model) Qwen3-Coder-30B-A3B-Instruct The model explicitly supports 262,144 tokens (256K) and even 1,010,000 tokens (1M) with ROPE scaling. So my suggestion to use 32768 was way too conservative. They're running this through LM Studio on their M2 MacBook with 96GB RAM, which should handle 256K context reasonably well, especially with the MLX quantization for Apple Silicon. I should: Coding agents - fully local in VSCode Cline (Nov-2025) LMStudio serving MiniMax-M2 that was shrunk so it fits in my mid-memory laptop. And LMStudio supports tools with thinking interleaved and streaming that MiniMax uses - no need to lobotomise the protocol. Then - Cline knows how to make use of that too! No litellm proxy needed. A model [1]minimax-m2-thrift-i1/MiniMax-M2-THRIFT.i1-IQ2_XXS.gguf that fits my VRAM and nothig else is needed - perfect! Not very fast though, and uses all of my 25 Watts on my years old MBP M2. :-) Still - pretty good."
    },
    {
      "source": "post-ml-llm-dev.html",
      "content": "m2-thrift-i1/MiniMax-M2-THRIFT.i1-IQ2_XXS.gguf that fits my VRAM and nothig else is needed - perfect! Not very fast though, and uses all of my 25 Watts on my years old MBP M2. :-) Still - pretty good. All local VSCode - Cline - LMStudio - MiniMax-M2-THRIFT. Coding agents - fully local in terminal, Claude Code via litellm proxy (Nov-20225) Make Claude Code CLI use LMStudio served LocalLLM API to run LLM inference localhost. This worked for me on 8-Nov-2025. I followed [2]Setting Up Claude Code Locally with a Powerful Open-Source Model: A Step-by-Step Guide for Mac with minor changes. ## Current working setup looks like this Model nightmedia/Qwen3-30B-A3B-YOYO-V3-qx86-hi-mlx from [nightmedia/Qwen3-30B-A3B-YOYO-V3-qx86-hi-mlx � Hugging Face](https://huggingface.co/nightmedia/Qwen3-30B-A3B-YOYO-V3-qx86-hi-m lx) 1. In the ~/litellm directory create 4 these files ljubomir@macbook2(:):~/litellm$ for a in claude.env config.yaml docker-compose.y aml .env; do echo ------- $a; cat $a; done ------ claude.env export ANTHROPIC_AUTH_TOKEN=\"sk-1234\" # Matches your LiteLLM key export ANTHROPIC_BASE_URL=\"[http://localhost:4000](http://localhost:4000/)\" export ANTHROPIC_MODEL=\"openai/qwen3-30b-a3b-coderthinking-yoyo-linear\" export ANTHROPIC_SMALL_FAST_MODEL=\"openai/limi-air-qx83s-mlx\" export CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1 # Optional: No telemetry ------ config."
    },
    {
      "source": "post-ml-llm-dev.html",
      "content": "\"openai/qwen3-30b-a3b-coderthinking-yoyo-linear\" export ANTHROPIC_SMALL_FAST_MODEL=\"openai/limi-air-qx83s-mlx\" export CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC=1 # Optional: No telemetry ------ config.yaml model_list: - model_name: \"anthropic/*\" # Maps all Anthropic models to your local one litellm_params: model: \"openai/qwen3-30b-a3b-coderthinking-yoyo-linear\" # Custom name for your m odel api_base: \"http://host.docker.internal:1234/v1\" # Points to LM Studio api_key: \"lm-studio\" # Dummy key (not actually needed) max_tokens: 65536 repetition_penalty: 1.1 temperature: 0.6 top_k: 100 top_p: 0.95 ------- docker-compose.yaml services: litellm: image: ghcr.io/berriai/litellm:main-stable command: [\"--config=/app/config.yaml\"] container_name: litellm restart: unless-stopped volumes: - ./config.yaml:/app/config.yaml ports: - \"4000:4000\" env_file: - .env depends_on: - db healthcheck: test: [\"CMD-SHELL\", \"wget --no-verbose --tries=1 http://localhost:4000/hea lth/liveliness || exit 1\"] interval: 30s timeout: 10s retries: 3 start_period: 40s db: image: postgres:16 restart: always container_name: litellm_db environment: POSTGRES_DB: litellm POSTGRES_USER: llmproxy POSTGRES_PASSWORD: dbpassword9090 ports: - \"5432:5432\" volumes: - postgres_data:/var/lib/postgresql/data healthcheck: test: [\"CMD-SHELL\", \"pg_isready -d litellm -U llmproxy\"] interval: 1s timeout: 5s retries: 10 volumes: postgres_data: name: litellm_postgres_data ------- .env LITELLM_MASTER_KEY=\"sk-1234\" 2."
    },
    {
      "source": "post-ml-llm-dev.html",
      "content": "lthcheck: test: [\"CMD-SHELL\", \"pg_isready -d litellm -U llmproxy\"] interval: 1s timeout: 5s retries: 10 volumes: postgres_data: name: litellm_postgres_data ------- .env LITELLM_MASTER_KEY=\"sk-1234\" 2. Ensure LMStudio is started, model loaded and running, and LMStudio is serving the default endpoint localhost:1234 [LMStudio-YOYO-pic1.png] 3. Ensure the endpoint is reachable ljubomir@macbook2(::main):~$ curl http://localhost:1234/v1/models { \"data\": [ { \"id\": \"qwen3-30b-a3b-yoyo-v3-qx86-hi-mlx\", \"object\": \"model\", \"owned_by\": \"organization_owner\" }, ....... ...and the fake key is \"working\" ok ljubomir@macbook2(::main):~$ curl -H \"Authorization: Bearer sk-1234\" http://loca lhost:4000/health {\"healthy_endpoints\":[{\"api_base\":\"http://host.docker.internal:1234/v1\",\"use_in_ pass_through\":false,\"use_litellm_proxy\":false,\"merge_reasoning_content_in_choice s\":false,\"model\":\"openai/qwen3-30b-a3b-coderthinking-yoyo-linear\",\"max_tokens\":6 5536,\"repetition_penalty\":1.1,\"temperature\":0.6,\"top_k\":100,\"top_p\":0.95,\"litell m_metadata\":{\"tags\":[\"litellm-internal-health-check\"],\"user_api_key_hash\":\"litel lm-internal-health-check\",\"user_api_key_alias\":\"litellm-internal-health-check\",\" user_api_key_spend\":0."
    },
    {
      "source": "post-ml-llm-dev.html",
      "content": "p\":0.95,\"litell m_metadata\":{\"tags\":[\"litellm-internal-health-check\"],\"user_api_key_hash\":\"litel lm-internal-health-check\",\"user_api_key_alias\":\"litellm-internal-health-check\",\" user_api_key_spend\":0.0,\"user_api_key_max_budget\":null,\"user_api_key_team_id\":\"l itellm-internal-health-check\",\"user_api_key_user_id\":null,\"user_api_key_org_id\": null,\"user_api_key_team_alias\":\"litellm-internal-health-check\",\"user_api_key_end _user_id\":null,\"user_api_key_user_email\":null,\"user_api_key_request_route\":null, \"user_api_key_budget_reset_at\":null,\"user_api_key_auth_metadata\":null,\"user_api_ key\":\"litellm-internal-health-check\",\"user_api_end_user_max_budget\":null},\"cache \":{\"no-cache\":true}}],\"unhealthy_endpoints\":[],\"healthy_count\":1,\"unhealthy_coun t\":0} 4. Start docker while being in the right dir ljubomir@macbook2(::):~/litellm$ docker compose up -d and verify docker is running file - check some logs ljubomir@macbook2(::):~/litellm$ docker compose logs -f litellm 5. Setup the right env vars for Claude code, and start Claude Code cli (CC-cli) ljubomir@macbook2(::):~/litellm$ source claude.env ljubomir@macbook2(::):~/litellm$ claude RBFBFBFBlB Claude Code v2.0.36 FBFBFBFBFB openai/qwen3-30b-a3b-coderthinking-yoyo-linear � API Usage Billing /Users/ljubomir/litellm > /model -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- ------------- Select model Switch between Claude models."
    },
    {
      "source": "post-ml-llm-dev.html",
      "content": "-------------------------------------------------------------- -------------------------------------------------------------------------------- ------------- Select model Switch between Claude models. Applies to this session and future Claude Code se ssions. For other/previous model names, specify with --model. 1. Default (recommended) Use the default model (cu rrently Sonnet 4.5) � $3/$15 per Mtok 2. Opus Legacy: Opus 4.1 for comp lex tasks � $15/$75 per Mtok 3. Haiku Haiku 4.5 for simple task s � $1/$5 per Mtok > 4. openai/qwen3-30b-a3b-coderthinking-yoyo-linear Custom model X Enter to confirm � Esc to exit 6. That's - it should just work Coding agentss - in terminal Current workflow is: # An Architect model is asked to describe how to solve the coding problem. Think ing/reasoning models often work well in this role. # An Editor model is given the Architect's solution and asked to produce specifi c code editing instructions to apply those changes to existing source files. # https://aider.chat/2025/01/24/r1-sonnet.html aider-openrouter-best() { local -; set -x; env AIDER_START=\"$(date)\"; aider --architect --model openrouter/deepseek/deepseek-r1 --editor-model openr outer/anthropic/claude-3.5-sonnet; } Atm waiting on a glitch to resolve - architect> litellm.APIError: APIError: OpenrouterException - Retrying in 0.2 seconds... litellm.APIError: APIError: OpenrouterException - ...and so I'm realising now I more often then not now I have it write code for me."
    },
    {
      "source": "post-ml-llm-dev.html",
      "content": "PIError: APIError: OpenrouterException - Retrying in 0.2 seconds... litellm.APIError: APIError: OpenrouterException - ...and so I'm realising now I more often then not now I have it write code for me. It's not even that much faster atm tbh! By the time I have thought through, explained in detail in INSTRUCTIONS.md -- I could have read up the sources, the docs, and done it myself. The only explanation I have to offer, that I only now--waiting on the OR api to come back--have, is: it's **much more fun**!! It's much more fun to have someone else write the code, and even if need be talk them into \"no no--not that way, change this, change that\", than to do everything myself solo and in silence!! Ok--this I did not expect. That the most entertaining--wins. Is [3]vibing the way code wring will scale x10, x100 next?? [vibe-coding-ftw-2025.png] LLMs for coding - pre-history, chatgpt copy-pasta 1. Start with ChatGPT copy&pasta - works but limited & manual, little time saved. 2. Onto Cursor - nice but not much gained, not even wrong. 3. Over to aider cmd line - some result there, even if cr*p result... but looks like it could be improved? 4. Current VSCode gui + Cline addon + OpenRouter payg credits + Claude model. Well hello!! Finally produced something not obviously wrong. Until today the best I got was: in ChatGPT-o4/o1- etc, copy & paste code snippet(s), ask a Question, then incorporate the Answer in the soluton."
    },
    {
      "source": "post-ml-llm-dev.html",
      "content": "hello!! Finally produced something not obviously wrong. Until today the best I got was: in ChatGPT-o4/o1- etc, copy & paste code snippet(s), ask a Question, then incorporate the Answer in the soluton. So this was a replacement for 1) googling and reading web pages 2) search through Stackoverflow Q&A. This is the 1st time I got code inserted in 3 files. That required AI to 1) read through 5-6 files 2) compare and contrast, reason by analogy 3) take my requirement Q in considerion 4) edit 3 files, delete some code, insert some other code. I have my main codebase, about 200K LoC in an array/matrix language mostly, with some C/C++/bash/awk/sql too. I'm agnostic Re: tools. Fallback always available is bash/vim/Makefile/gcc/g++/gdb/ddd/shell/... tools. But if IDE like VSCode/Spyder/CLion/Matlab/DBeaver is available - I'm happy to use. As long as it's not exlcusive, and one can edit/setup outside the IDE too. And esp important version contol - git now, prev hg, cvs, Teams. If that works - then all is good. I tried Cursor. That looked hopeful, but did not get me results. I didn't like not being able to use existing API subscriptions in it. Also them using some kind of LLM in-house undocumented bodge. (I maybe wrong/maybe possible - didn't try too hard) I then tried aider, a command line tool. That managed mutiple edits, but to not too good results. Waste of time wrt results, but: it was a good learning curve for me. I PAYG subscribed OpenAI -> DeepSeek -> OpenRouter."
    },
    {
      "source": "post-ml-llm-dev.html",
      "content": ", a command line tool. That managed mutiple edits, but to not too good results. Waste of time wrt results, but: it was a good learning curve for me. I PAYG subscribed OpenAI -> DeepSeek -> OpenRouter. OpenRouter leader board led me to Cline VSCode addon. Latest-greatest setup atm 1) VSCode 2) with Cline Addon 3) OpenRouter API key (payg credits) 4) select Claude 3.5 via openrouter/anthropic/claude-3.5-sonnet. The dev task was as follows. Functionality A/B/C needs implementong. Look at existing wrapper X implementing A/B/C, while using Y external library for A/B. Create new wrapper U, to use external library W, in the same way X is using Y, to do the similar A/B. (C is done in X and U respectivelly) E.g. - see how the data is passed X-to-Y, then do it the same way U-to-W. Look at examples code in the W library, figure how to do A/B. This to avoid doing the reading abt W and figuring A/B myself. I can do it myself, have done it half a dozen times already, for U/W equivalents, but: bit boring, and wanted to find out if I can make AI do it for me. Have yet to finish the full loop, the code does not run yet. But - before it was laughably obviously bad and wrong. Now - the 1st time where the code looks plausable. Need to do a harness to test finally. To be continued. Models - open source, open weights, open thoughts, code, documentation llama.cpp Inference of Meta's LLaMA model (and others) in pure C/C++ [4]https://github.com/ggerganov/llama."
    },
    {
      "source": "post-ml-llm-dev.html",
      "content": "lly. To be continued. Models - open source, open weights, open thoughts, code, documentation llama.cpp Inference of Meta's LLaMA model (and others) in pure C/C++ [4]https://github.com/ggerganov/llama.cpp DeepSeek R1 Unsloth [5]dynamic HuggingFace [6]quants, incl distillations Meta Llama models [7]https://www.llama.com/ Meta Llama-3.3-70B-Instruct Hugging Face [8]https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct Ollama Get up and running with large language models. [9]https://ollama.com/ llm.c LLMs in simple, pure C/CUDA with no need for 245MB of PyTorch or 107MB of cPython. Current focus is on pretraining, in particular reproducing the GPT-2 and GPT-3 miniseries, along with a parallel PyTorch reference implementation in train_gpt2.py. [10]https://github.com/karpathy/llm.c LLM A CLI utility and Python library for interacting with Large Language Models, both via remote APIs and models that can be installed and run on your own machine. [11]https://llm.datasette.io/en/stable/ Hugging Face Models [12]https://huggingface.co/models Mistral AI [13]https://mistral.ai/, Hugging Face [14]https://huggingface.co/mistralai QwQ-32B-Preview blog [15]https://qwenlm.github.io/blog/qwq-32b-preview/, Hugging Face [16]https://huggingface.co/Qwen/QwQ-32B-Preview, github Qwen2.5 [17]https://github.com/QwenLM/Qwen2.5 QVQ-72B-Preview Hugging Face [18]https://huggingface.co/Qwen/QVQ-72B-Preview DeepSeek-V3 github [19]https://github."
    },
    {
      "source": "post-ml-llm-dev.html",
      "content": "ngface.co/Qwen/QwQ-32B-Preview, github Qwen2.5 [17]https://github.com/QwenLM/Qwen2.5 QVQ-72B-Preview Hugging Face [18]https://huggingface.co/Qwen/QVQ-72B-Preview DeepSeek-V3 github [19]https://github.com/deepseek-ai/DeepSeek-V3, Hugging Face [20]https://huggingface.co/deepseek-ai/DeepSeek-V3 Reddit LocalLLaMA [21]https://www.reddit.com/r/LocalLLaMA/ llama.cpp guide - Running LLMs locally, on any hardware, from scratch [22]https://blog.steelph0enix.dev/posts/llama-cpp-guide/ ModernBERT This is the repository where you can find ModernBERT, our experiments to bring BERT into modernity via both architecture changes and scaling. [23]https://github.com/AnswerDotAI/ModernBERT WordLlama [24]https://github.com/dleemiller/WordLlama Microsoft AI - AI Platform Blog[25]https://techcommunity.microsoft.com/category/ai/blog/aiplatform blog, [26]Introducing Phi-4 Chatbot Arena (formerly LMSYS): Free AI Chat to Compare & Test Best AI Chatbots [27]https://lmarena.ai/ Scaling Test Time Compute with Open Models [28]https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-t ime-compute The Complexity Dynamics of Grokking [29]https://brantondemoss.com/research/grokking/ -- LJ HPD Sun 22 Dec 22:24:19 GMT 2024 References Visible links: 1. https://huggingface.co/mradermacher/MiniMax-M2-THRIFT-i1-GGUF 2. https://medium.com/@luongnv89/setting-up-claude-code-locally-with-a-powerful-open-source-model-a-step-by-step-guide-for-mac-84cf9ab7302f 3. file:///Users/ljubomir/ljubomirj.github."
    },
    {
      "source": "post-ml-llm-dev.html",
      "content": "2-THRIFT-i1-GGUF 2. https://medium.com/@luongnv89/setting-up-claude-code-locally-with-a-powerful-open-source-model-a-step-by-step-guide-for-mac-84cf9ab7302f 3. file:///Users/ljubomir/ljubomirj.github.io/post-ml-llm-dev.html 4. https://github.com/ggerganov/llama.cpp 5. https://unsloth.ai/blog/deepseekr1-dynamic 6. https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5 7. https://www.llama.com/ 8. https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct 9. https://ollama.com/ 10. https://github.com/karpathy/llm.c 11. https://llm.datasette.io/en/stable/ 12. https://huggingface.co/models 13. https://mistral.ai/ 14. https://huggingface.co/mistralai 15. https://qwenlm.github.io/blog/qwq-32b-preview/ 16. https://huggingface.co/Qwen/QwQ-32B-Preview 17. https://github.com/QwenLM/Qwen2.5 18. https://huggingface.co/Qwen/QVQ-72B-Preview 19. https://github.com/deepseek-ai/DeepSeek-V3 20. https://huggingface.co/deepseek-ai/DeepSeek-V3 21. https://www.reddit.com/r/LocalLLaMA/ 22. https://blog.steelph0enix.dev/posts/llama-cpp-guide/ 23. https://github.com/AnswerDotAI/ModernBERT 24. https://github.com/dleemiller/WordLlama 25. https://techcommunity.microsoft.com/category/ai/blog/aiplatformblog 26. https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090 27. https://lmarena.ai/ 28. https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute 29."
    },
    {
      "source": "post-ml-llm-dev.html",
      "content": "cing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090 27. https://lmarena.ai/ 28. https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute 29. https://brantondemoss.com/research/grokking/ Hidden links: 31. file://localhost/Users/ljubomir/ljubomirj.github.io/post-ml-llm-dev.html 32. file://localhost/Users/ljubomir/ljubomirj.github.io/post-ml-llm-dev.html 33. file://localhost/Users/ljubomir/ljubomirj.github.io/post-ml-llm-dev.html"
    },
    {
      "source": "post-my-HOME.html",
      "content": "My computer $HOME I spend most of my time typing into a computer, reading on the computer screen, watching videos or listening to audio playing on a computer, talking and seeing other people through a computer screen. I mostly live in the text world of a terminal [1]emulator, [2]bash command line and the [3]vim editor, with [4]Firefox and [5]Chrome windows into the world. Nowadays I use [6]Xubuntu [7]LTS, on the desktop and on my laptops. I update every 3-4 years to the new version, but other than that - it's completely uneventful. The [8]Xfce GUI changes only slowly if ever - and that's how I like it. On the computers used by rest of the family I put Ubuntu as it's better looking. They also use Windows desktops and Macbooks for laptops. The Macbooks with the M1-M4 ARM CPU-s have spectacular battery life in addition to great screens. I am looking forward to the day when I add an [9]ARM Thinkpad to my daily use. Have couple of [10]Thinkpads for the RAM I need. Initially I had 32GB, but now that has grown to 64GB. Can't imagine to have less and would like to move to 128GB on the laptop. Have had that much on the desktop for 10 years now. That's my first need: get as much RAM as I can. And the 2nd - get as fast an SSD and better NVME as I can. My daily job involves lots of data, and keeping all data needed in memory is the best UI for me. And when not in memory - then on ssd drives and preferably nvme ones. With the advent of local LLM-s caught the bug with [11]LocalLLama-s."
    },
    {
      "source": "post-my-HOME.html",
      "content": "and keeping all data needed in memory is the best UI for me. And when not in memory - then on ssd drives and preferably nvme ones. With the advent of local LLM-s caught the bug with [11]LocalLLama-s. So I finally got myself a 2nd hand (thaks [12]Hoxton Macs!) M2 mbp wth 96gb (V)RAM. And it's glorious! :-) Love it - the battery life, the screen, but most of all - running local models has been a blast. With [13]brew - everything on the command line \"just works\". I get to run tons of local models, mostly [14]GGUF-s off [15]HuggingFace, used with [16]llama.cpp and [17]LMStuidio. In addition, have been gorging on [18]arxiv [19]ML/AI papers and [20]github [21]code for a year now - and I'm still smiling. This is the best of computers and computing! � Daily I live in bash and vim mostly inside screen (the multiplexer) inside terminator (the terminal emulator). I use the shell tools, incl awk (that I like) and the rest of the gnu shell tools (grep, sed), git, gcc and g++, make, ssh, rsync, rcopy, Spyder, python, [22]VSCode with [23]Cline and [24]Roo AI-s, Firefox, Thunderbird, Chrome, Edge, Double commander, Evince, VLC player. I like them all - my life would be worse if these free software tools didn't exist. Thank you [25]GNU [26]software, thank you [27]Linux, thank you [28]FSF. Lately I've been having [29]codex and [30]gemini CLI-s to write lots of code for me, mostly my python, javascript and style-sheets html."
    },
    {
      "source": "post-my-HOME.html",
      "content": "u [25]GNU [26]software, thank you [27]Linux, thank you [28]FSF. Lately I've been having [29]codex and [30]gemini CLI-s to write lots of code for me, mostly my python, javascript and style-sheets html. They are very good at it! In fact $ codex -m gpt-5 -c model_reasoning_effort=\"high\" is excellent in achieving anything complicated involving the command line: I command codex-gpt-5-high, and it commands the shell. As far as l'm concerned - my AGI has arrived. � This is the ghost in the machine - manifest! In my daily job I used to write quant trading systems and frameworks. But nowadays I'm all-in back to ML/AI, catching up with everything going on. I use a mix of C/C++, Matlab in the past now octave and python with numpy (and pandas), scripting in bash, awk, plotting in gnuplot, data fetching in sql, kdb. Everything that I do more then few times on the command line, I \"can\" it into a bash alias or function, and put in my .bashrc. In part to document and not forget. I love that command search works, I can type $ xyz then press (TAB) and bash will seek to complete for commands starting with xyz. And bash will keep cycling through the completions for as long as I keep pressing the (TAB) key. I keep my dot rc files under git and that's worked without fuss. Looks like this: # Keep dot files and other config in git (https://news.ycombinator.com/item?id=1 1070797). # Step 1: $ git init --bare $HOME/.githome."
    },
    {
      "source": "post-my-HOME.html",
      "content": "files under git and that's worked without fuss. Looks like this: # Keep dot files and other config in git (https://news.ycombinator.com/item?id=1 1070797). # Step 1: $ git init --bare $HOME/.githome. # Step 2: make function (rather than alias to allow for composition like $ GIT=g ithome gilg; change dir so paths work independent of the current dir): githome() { (cd \"$HOME\" && git --git-dir=\"$HOME\"/.githome/ --work-tree=\"$HOME\" \" $@\";) } # Step 3: disregard files by default, only track explicitly added files: $ githo me config status.showUntrackedFiles no. # Now use the usual git commands prefixed by githome: $ githome status; githome add .vimrc; githome commit -m \"Add vimrc\". # Issue 1: Can not commit links. For host specific dirs, woraround: 1) move dir to dir-host; 2) link dir to dir-host; 3) add dir-host to git. Example with ~/.co nfig dir: # ljubomir@hostA:~$ l -d .config* # lrwxrwxrwx 1 ljubomir ljubomir 14 Mar 24 14:26 .config -> .config-hostA/ # drwx------ 34 ljubomir ljubomir 4.0K Mar 29 12:24 .config-hostA/ # drwx------ 3 ljubomir ljubomir 4.0K Mar 24 14:52 .config-hostB/ # Issue 2: To pull from host with temporary IP edit $ vi .githome/config, change the IP below: # [remote \"hostC\"] # url = ljubomir@192.168.1.117:.githome # fetch = +refs/heads/*:refs/remotes/hostC/* # List all files under management and pretty print if run without args, githome otherwise. (https://mitxela."
    },
    {
      "source": "post-my-HOME.html",
      "content": "ostC\"] # url = ljubomir@192.168.1.117:.githome # fetch = +refs/heads/*:refs/remotes/hostC/* # List all files under management and pretty print if run without args, githome otherwise. (https://mitxela.com/projects/dotfiles_management) giho-ls() { (cd / githome ls-files | while read i; do echo -n \"$(githome -c color.status=always status \"$i\" -s | sed \"s#$i##\")\" echo -e \"�/$i�\\e[0;33m$(githome -c color.ui=always log -1 --format=\"%s\" -- \" $i\")\\e[0m\" done ) | column -t -s� } # Have \"local -\" to make option \"set -x\" local to the function only giho() { local -; set -x; githome \"$@\"; } giho-fetch-hostA() { local -; set -x; githome fetch \"$@\" hostA master:hostA ; } giho-merge-hostA() { local -; set -x; githome merge \"$@\" hostA; } giho-push-hostA() { local -; set -x; githome push --follow-tags \"$@\" hostA master:$(hostname -s); } Other canned common git commands look like: # Git shortcuts. Take the \"git\" command from the environment via GIT var to allo w for goodies: # - use with githome: $ GIT=githome gist # - color terminal (off by default): $ GIT=\"git -c color.status=always\" gist | m gi() { ${GIT:-git} \"$@\"; } gist() { ${GIT:-git} status \"$@\"; } gidf() { ${GIT:-git} diff \"$@\"; } gilg() { ${GIT:-git} log -C --name-status --pretty=\"%h %ae %ai : %s\" \"$@\"; } gilgt() { ${GIT:-git} log -C --oneline --stat --decorate \"$@\"; } gi-fetch-hostA() { local GIM=${GIM:-master}; local -; set -x; ${GIT:-git} fetch \"$@\" hostA ${GIM}:hostA/${GIM}; } gi-merge-hostA() { local GIM=${GIM:-master}; local -;"
    },
    {
      "source": "post-my-HOME.html",
      "content": "eline --stat --decorate \"$@\"; } gi-fetch-hostA() { local GIM=${GIM:-master}; local -; set -x; ${GIT:-git} fetch \"$@\" hostA ${GIM}:hostA/${GIM}; } gi-merge-hostA() { local GIM=${GIM:-master}; local -; set -x; ${GIT:-git} merge \"$@\" refs/heads/hostA/${GIM}; } gi-push-hostA() { local GIM=${GIM:-master}; local -; set -x; ${GIT:-git} push -- follow-tags \"$@\" hostA ${GIM}:\"$(hostname -s)\"/${GIM}; } I like and use .bashrc search-previous-command all the time via .inputrc: $if Bash # Filename completion/expansion set completion-ignore-case on set show-all-if-ambiguous on # Append \"/\" to all dirnames set mark-directories on set mark-symlinked-directories on # Match all files set match-hidden-files on $endif # Ctrl-Left \"\\e[1;5D\": backward-word # Ctrl-Right \"\\e[1;5C\": forward-word # Up \"\\e[A\": history-search-backward # Down \"\\e[B\": history-search-forward Usually I don't customize anything much. I spend most of the time on the command line or in vim anyways, the GUI is mostly vanilla whatever Xfce decides. I notice now my PS1 etc have grown over time: PS1='${debian_chroot:+($debian_chroot)}\\[\\033[01;34m\\]\\u@\\h\\[\\033[00m\\](${STY}:$ {WINDOW}):\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ ' PROMPT_COMMAND='echo -ne \"\\033]0;${XUSER} (${STY:-$$}) ${VIRTUAL_ENV_PROMPT} ${U SER}@${HOSTNAME}:${PWD}\\007\"' A researcher, an explorer - usually they need a log book. At work as a researcher I always kept a log book, usually using 2 facing pages per 1 week."
    },
    {
      "source": "post-my-HOME.html",
      "content": "RTUAL_ENV_PROMPT} ${U SER}@${HOSTNAME}:${PWD}\\007\"' A researcher, an explorer - usually they need a log book. At work as a researcher I always kept a log book, usually using 2 facing pages per 1 week. At $HOME have settled for a ~/logBook that's plain ASCII text file under git. Love git for versioning so I don't worry that I will delete destroy something by mistake. Also great for synchronisation and replication to various boxes - done with $ git fetch/merge/push. I have FIXME TODO DONE DONTDO sections. They are ^searchable in vim, e.g. /^TODO(enter). The entries are short-ish, sentence or 2 or 5, separated by empty line. I start entries with \"- \" so to search easily with /^-(space)(enter) in vim. Entries move wholesale between sections, the idea is to move them around without further editing. Entry that spends enough time in TODO without moving to DONE is moved to DONTDO after some time. No new entry is added to TODO while the FIXME section is non-empty. If need to, move the blocker from FIXME into TODO. These are housekeeping rules rules of thumb--can be broken with a reason, try not break them without a reason. I have spent most of my adult life with and around computers. The 1st home computer I saw was ZX-Spectrum 16K that my school [31]friend got before me in the last year in elementary school probably 1982-83. Remember the prices still: ZX-Spectrum 16K was �100, 48K model was �130. Commodore C64 was �200."
    },
    {
      "source": "post-my-HOME.html",
      "content": "rum 16K that my school [31]friend got before me in the last year in elementary school probably 1982-83. Remember the prices still: ZX-Spectrum 16K was �100, 48K model was �130. Commodore C64 was �200. Latter I managed to persuade my parents to buy me a C64 probably around 1985. I learned Basic and 6502 assembler on it, mostly from the [32]Racunari u vasoj kuci (Computers in Your Home) magazine ([33]recent 40 years anniversary reprint; click to zoom; the [34]sounds of the [35]time - yeah, \"[36]it's more fun to compute\"--it always has been). [racunari-at-40yrs.jpg] By 1987 I finished High school and enrolled BSc undergraduate [37]studies in [38]Electrical Engineering, that turned into [39]Computer Science from year 3 onwards. (all together 4.5yrs+diploma work; [40]the sounds of my home [41]town/[42]country I grew up in, by the [43]incomparable VS) Did 1yr National service in-between High School and University, and there I programmed pocket computers HP-71B, Sharp PC-1500 (with the tiny printer), Apple II clone with a Z80 CP/M board and a hard disk (!! remember that CP/M had partitions, but no directories?). Once back home and at Uni from 1988, I finally got my 1st PC (don't recall the year exactly) - AT with Intel 80286 CPU, 1MB RAM, Hercules graphics card, 20MB HDD, that probably run MS-DOS 5 or similar. I squeezed a 2400bps modem in the budget too (without MNP5 error correction or compression)."
    },
    {
      "source": "post-my-HOME.html",
      "content": "AT with Intel 80286 CPU, 1MB RAM, Hercules graphics card, 20MB HDD, that probably run MS-DOS 5 or similar. I squeezed a 2400bps modem in the budget too (without MNP5 error correction or compression). The modem proved an excellent decision as it got me into the online world of BBS (e.g. [44]Sezam) and latter Internet. All that financed paid for by my ever kind and generous parents - thank you mum and dad! Since - I've never been too far from a computer for any significant time. Nowadays it's mostly Linux (Xubuntu, CentOS, Ubuntu), for a long time earlier it was MS-DOS/Windows (3.1-95-XP-10, cygwin), various Unix too (Solaris, HP-UX, Ultrix, AIX), as well as VAX VMS. And of course - we all carry a magical shiny slabs in our pockets that are super-computers of the old. Mostly various Android for me, but it's looking like I'll be switching over to iPhone for the AI NPU (Neural nets Processing Units) hardware. These days I 'm mostly at my [45]desk, in a [46]garden office (at [47]end of the work day). Sometimes I get a [48]visitor or [49]two or [50]three. (click to zoom) [garden-office-desk.jpg] [garden-office.jpg] [garden-office-end-day.jpg] [garden-office-visitor.jpg] [garden-visitor2.jpg] [garden-visitor3.jpg] (rehosting an excellent advanced vi - not vim! - and ex tutorial, by Walter Alan Zintz, originally published in UnixWorld Online, but no longer online [51]here) -- LJ HPD Thu 10 Oct 23:00:52 BST 2024 References 1. https://gnome-terminator.org/ 2. https://www.gnu."
    },
    {
      "source": "post-my-HOME.html",
      "content": "l, by Walter Alan Zintz, originally published in UnixWorld Online, but no longer online [51]here) -- LJ HPD Thu 10 Oct 23:00:52 BST 2024 References 1. https://gnome-terminator.org/ 2. https://www.gnu.org/software/bash/ 3. https://www.vim.org/ 4. https://www.mozilla.org/en-GB/firefox/ 5. https://www.google.com/intl/en_uk/chrome/ 6. https://xubuntu.org/ 7. https://ubuntu.com/about/release-cycle 8. https://www.xfce.org/ 9. https://www.perplexity.ai/search/arm-thinkpads-wtf0g0o0REahidqJVzJs5w#0 10. https://www.reddit.com/r/thinkpad 11. https://www.reddit.com/r/LocalLLaMA/ 12. https://www.hoxtonmacs.co.uk/ 13. https://brew.sh/ 14. https://huggingface.co/docs/hub/en/gguf 15. https://huggingface.co/models 16. https://github.com/ggml-org/llama.cpp 17. https://lmstudio.ai/ 18. https://arxiv.org/ 19. https://arxiv.org/list/stat.ML/recent 20. https://github.com/ 21. https://github.com/ljubomirj 22. https://code.visualstudio.com/ 23. https://cline.bot/ 24. https://github.com/RooCodeInc/Roo-Code 25. https://www.gnu.org/ 26. https://www.gnu.org/software/software.en.html 27. https://www.linuxfoundation.org/ 28. https://www.fsf.org/ 29. https://github.com/openai/codex 30. https://github.com/google-gemini/gemini-cli 31. https://www.youtube.com/@AlojzRop 32. https://www.racunari.com/ 33. file:///Users/ljubomir/ljubomirj.github.io/post-my-HOME.html 34. https://www.youtube.com/watch?v=YxlndeU3SVY&list=PLw5gIizo9P7eitGr359_UDToGOCInSaa_&index=1 35. https://www.youtube.com/watch?v=2zmt0TNsciU 36."
    },
    {
      "source": "post-my-HOME.html",
      "content": "Users/ljubomir/ljubomirj.github.io/post-my-HOME.html 34. https://www.youtube.com/watch?v=YxlndeU3SVY&list=PLw5gIizo9P7eitGr359_UDToGOCInSaa_&index=1 35. https://www.youtube.com/watch?v=2zmt0TNsciU 36. file:///Users/ljubomir/ljubomirj.github.io/kraftwerk-its_more_fun_to_compute-extended.mp3 37. https://life.ieee.org/ieee-president-attends-plaque-unveiling-of-ieee-milestone-in-north-macedonia/ 38. https://feit.ukim.edu.mk/en/ 39. https://www.finki.ukim.mk/ 40. https://www.youtube.com/watch?v=Lv1OLc-M4D0 41. https://en.wikipedia.org/wiki/Skopje 42. https://mk.wikipedia.org/wiki/%D0%9C%D0%B0%D0%BA%D0%B5%D0%B4%D0%BE%D0%BD%D0%B8%D1%98%D0%B0 43. https://vlatkostefanovski.com.mk/ 44. http://pc.pcpress.rs/tekst.php?id=15834 45. file:///Users/ljubomir/ljubomirj.github.io/post-my-HOME.html 46. file:///Users/ljubomir/ljubomirj.github.io/post-my-HOME.html 47. file:///Users/ljubomir/ljubomirj.github.io/post-my-HOME.html 48. file:///Users/ljubomir/ljubomirj.github.io/post-my-HOME.html 49. file:///Users/ljubomir/ljubomirj.github.io/post-my-HOME.html 50. file:///Users/ljubomir/ljubomirj.github.io/post-my-HOME.html 51. file:///Users/ljubomir/ljubomirj.github.io/Walter_Alan_Zintz_UnixWorld_vi_tutorial/009.html"
    },
    {
      "source": "post-picmem.html",
      "content": "Memes, random pics memorable enough to replicate, in no particular order nor of meaning Memes [always-were-artists.png] [vibe-coding-ftw-2025.png] [perf-GPQA-diamond-o3-tops-hi-phd.jpeg] [ai-fights-eu-slights.png] [openai-deepseek-shootout.jpg] [landings-misunderstandings-napoleon-spirit.jpg] [my-quantitative-openai-Screenshot_2024-12-22-14-20-03-015.jpg] [ilya-seq2seq-rnn-2-aug2016.png] [kurzweil-predicts-1990-2045.jpeg] [regularization-bias-variance-tradeoff-redge-reg.jpeg] [wheel-word-mech-info-TL.jpeg] [jre-barbarian-khan-glasses-man.png] [trump-musk-likeadawg.jpg] [mkd-kraftwerk-contaminated.png] Science, Computing, Communication [rick-rubin-vibe-code.png] [howto-dismiss-unpleasant-truths.jpg] [gutenberg-monks-everyone-will-be-a-coder.png] [chatgpt-croatian-refuse-downvote.jpg] [forecast-error-bias-variance.jpeg] [scary-robot-pretend-oh-my.jpg] [gpt-5-iq-150-b.jpg] [design-feedback-interesting-awesome-scheisse.jpg] [nayib-bukele-what-did-it-cost-crime.jpeg] [gpt-5-iq-150-a.jpg] [diffusion-logical-creative-terms.png] [oai-gold-imo2025-20250719.jpeg] [kurzweil-was-right-intelligence-flops-graph.jpeg] [kurzweil-price-performance-1939-2023.jpeg] [bootstrap-motivated-reasoning-via-conditioning.jpeg] [the-forgetting-curve-1885.jpeg] [sufficiently-high-frequency-feels-smooth.png] [models-intelligence-vs-tokens.jpeg] [DL-phenomena-grokking-double_dip-lottery_ticket.jpg] [chatgpt-2025-growth-400M-to-800M.png] [forbidden-by-physics-goldilocks-range."
    },
    {
      "source": "post-picmem.html",
      "content": "gh-frequency-feels-smooth.png] [models-intelligence-vs-tokens.jpeg] [DL-phenomena-grokking-double_dip-lottery_ticket.jpg] [chatgpt-2025-growth-400M-to-800M.png] [forbidden-by-physics-goldilocks-range.jpg] [harp-morning-three-beauty-obj-IMG_20250806_080734.jpg] [doin-vibe-coding-wtf-now.jpeg] [alphabets-evolution.jpeg] [network-approximates-conditional-target-average-bishop-1995.png] [distribution-usa-eeu-even-so.jpg] [probable-max-iq-of-human-poppulation-past-4000yrs.jpeg] [local-minima-midwit-ucando.jpg] [closeness-lifelines.jpg] [barbell-strategy-vertical.png] [barbell-strategy-how-not-to-be-starving-artist.png] [radiation-doses-conversions-chart.png] [programmers-cool-chatgpt-code-work.jpg] [consciousness-across-species.jpg] [ai-iq-test-ins-results-apr2025.png] [ai-iq-test-oos-results-apr2025.png] [alice-bob-model-of-model-of.jpg] [AZR-abduction-deduction-induction.jpeg] [bit-it-ndim-will-ergo-odds-comp-cond-pdf-bayes.png] [denoising-posterior-mean-and-variance.jpg] [dl-motivation-hinton-distilation.jpg] [dl-motivation-hinton-dropout.png] [Gkb4HK6WoAIEmfH.jpg] [ilya-seq2seq-rnn-1-aug2016.png] [ilya-seq2seq-rnn-2-aug2016.png] [ilya-seq2seq-rnn-deep-aug2016.png] [IMG_20250501_093255.jpg] [karpathy-learning-3-modes-of.jpeg] [karpathy-learning-3-modes-of-post.png] [keeping-up-with-ai-news.jpeg] [let-the-data-speak-for-itself.png] [lifesaving-innovations-people-numbers.jpeg] [llms-going-to-school-karpathy.jpeg] [model-capability-feedback-loop-v1-cot-distilation-v2-better."
    },
    {
      "source": "post-picmem.html",
      "content": "p-with-ai-news.jpeg] [let-the-data-speak-for-itself.png] [lifesaving-innovations-people-numbers.jpeg] [llms-going-to-school-karpathy.jpeg] [model-capability-feedback-loop-v1-cot-distilation-v2-better.png] [openai-deepseek-shootout.jpg] [probabability-is-logic-of-science-classical-vv-probabilistic-thinking. jpeg] [richard-sutton-centralized-control-bad.jpeg] [rick-rubin-vibe-always-was.png] [rick-rubin-yes-always.png] [RL-whatisit-wits.jpg] [Screenshot_2024-12-22-14-20-03-015_org.mozilla.firefox.jpg] [Screenshot_2025-04-23-04-33-36-802_org.mozilla.firefox.jpg] [sharpe-ratio-confidence-interval.jpeg] [single-photon-first-depiction.jpeg] [verification-the-key-to-AI.jpeg] [visibility-skills-opportunity-engrave-into-your-soul.jpeg] [Sutskever-an-observation-on-generalization-Kolmogorov-MDL.png] [regression-linear-logistic-poisson.jpg] [left-handedness-rate-by-year-history.png] [karpathy-why-phd.png] [even-so-ees.jpeg] [bit-it-ndim-will-ergo-odds-comp-cond-pdf-bayes4.png] [bit-it-ndim-ergo-comp-cond-pdf-bayes-odds-can-200.png] [bit-it-ndim-ergo-comp-bayes-odds-pdf-200a.png] [arc-agi-5years-openai.jpg] [all-was-art-now.png] [regularization-bias-variance-tradeoff-redge-reg.jpeg] [languages-information-rate-is-const-39bps.jpeg] [stupidity-xy.jpeg] [Poincaré-intuition-discovers-creates-logic-criticizes-falsifies.jpg] [eigens.jpg] [Screenshot_2024-11-13-14-15-26-012_org.mozilla.firefox.jpg] [ssh-jumphost-tunnel.png] [morse-code-2.jpeg] [morse-code-21.jpeg] [morse-code-1."
    },
    {
      "source": "post-picmem.html",
      "content": "vers-creates-logic-criticizes-falsifies.jpg] [eigens.jpg] [Screenshot_2024-11-13-14-15-26-012_org.mozilla.firefox.jpg] [ssh-jumphost-tunnel.png] [morse-code-2.jpeg] [morse-code-21.jpeg] [morse-code-1.jpeg] [morse-code-12.jpeg] [llm-iq-test-results-mensa-2.png] [data-information-knowledge-wisdom.jpg] [bayes-theorem-visual.jpeg] [information-entropy.jpg] [always-were-artists.png] [pdf-joint-cond-marg-1of3.png] [matrix-world.jpg] [llm-iq-test-results-mensa.jpg] [gaussian-hessian-is-almost-the-derivative-of-covariance.png] [vim-cheat-hseet-DHH.jpeg] [fourier-transform.jpg] [Rushkoff_-_Program_or_Be_Programmed_Ten_Commands_for_a_Digital_Age-201 0.png] [wheel-word-mech-info-TL.jpeg] [wheel-word-mech-info.jpeg] [programmer-know-memory-speed.jpeg] [no-low-energy-rich-ctry.jpeg] [info-will-ergo-odds-truth-comp-tol-fre.png] [covid-vaccines-subgroup-specific-adjusted-hazard-ratios.jpeg] [bird-vision-human-vision.png] [Bach-Software-Aristotle-AI-no-problem.png] [second-renaissance-ecosystem.jpg] [normal-what-does-it-say.jpg] [point-counterpoint.png] [pdf-joint-cond-marg-2of3.png] [computation-pdf-physics-spacetime.png] [whitney-keys-to-performance.png] [risk-dangerous-activities.jpeg] [phonetic-alphabet-nato.jpg] [pdf-joint-cond-marg-3of3.png] [n-people-m-relations.jpeg] [Ndim-sphere-spillout-Ndim-cube.png] [m3-mode-median-mean.jpeg] [graphs-on-hand.png] [graphics-principles-2of2.png] [exp-neg-distance-squared-is.jpg] [distance-measures.jpg] [crihton-gell-mann-amnesia."
    },
    {
      "source": "post-picmem.html",
      "content": "Ndim-sphere-spillout-Ndim-cube.png] [m3-mode-median-mean.jpeg] [graphs-on-hand.png] [graphics-principles-2of2.png] [exp-neg-distance-squared-is.jpg] [distance-measures.jpg] [crihton-gell-mann-amnesia.png] [childish-future.png] [Bach-Software-Aristotle-hard-problem-JP.png] [ai-levels-1-5.png] [Two-centuries-The_world_is_awful_is_much_better_can_be_much_better.png ] [rootclaim-calc2-Screenshot_2024-01-21_09-19-41.png] [perception-wrong.jpg] [nn-size-rate-init.png] [jobs-dependent-species.jpg] [Hannah-Ardent-true-false-good-bad.jpg] [graphics-principles-1of2.png] [expectation-of-lin-comb.jpg] [ergodicity-cooperation-group-member-in-or-out.png] [derivative-in-limit.jpeg] [correlation-grades-IQ-simulation-illustrated.jpeg] [big-5-personality-life-satisfaction.jpeg] [prob-or-not2.jpeg] [owid-world-is-bad-better-improve-three-truths.jpeg] [human-population-past-present-future.png] [HistoryOfAIPosterFinal.png] [fourier-transform-in-one-sentence.png] [data-transforms.jpg] [beleif-update.jpg] [tls.jpg] [rootclaim-calc-Screenshot_2024-01-21_09-19-41.png] [phonemes-where-mouth.jpg] [ohms-law.jpeg] [kwant-forecast-decay-speed.png] [grandfather-paradox-resolved.gif] [cumulative-culture-makes-us-smarter.jpeg] [conditional-probs-counts.png] [coinditional-probs-blocks.png] [chess-human-machine.jpg] [chemicals-natural-human-toxic.jpeg] [A0-area-is-1-m2-sides-ratio-is-root-2-all.jpg] [prior-likelihood-fat-think-tails-posterior.jpeg] [macedonia-flag-kraftwerk-contaminated."
    },
    {
      "source": "post-picmem.html",
      "content": "[chess-human-machine.jpg] [chemicals-natural-human-toxic.jpeg] [A0-area-is-1-m2-sides-ratio-is-root-2-all.jpg] [prior-likelihood-fat-think-tails-posterior.jpeg] [macedonia-flag-kraftwerk-contaminated.png] [stupidity-is-more-danger-than-malice.jpeg] [kwant-forecast-QQ.png] [fuel-energy-density-log-axis-not.png] [chemicals-natural-human.jpeg] [asset-class-returns-from-1900.png] [adverserial-validation.jpeg] [right-wing-vs-left-wing-authoritarianism.jpeg] [prior-likelihood-fat-think-tails-posterior-2.jpeg] [nball-volume-recursive.jpg] [kwant-forecast-volatility.jpg] [kwant-forecast-pdf.jpg] [kwant-forecast-decay.jpg] [geometry-euclidian-spherical-hyperbolic.jpg] [everything-everywhere-on-one-plot.png] [ergo-nball-info-will-bayes-odds-8a.png] [energy-density-matters-log-scales-are-for-quitters.jpg] [chemicals-natural-kiwi.jpeg] [pigeon-chess.jpeg] [our-reaction-to-technologies.jpeg] [loan-monthly-repayment-principal-interest.png] [under-over-fitting.webp] [pale-blue-dot-essay.jpeg] [linux-perf-mon-observe.jpg] [intellectuals-duty-is-truth-seek.png] [IMG_20230117_015846.jpg] [grid-electricity-decarbonized.jpg] [GDP-pc-England-1270-2026.png] [ergo-bayes-odds-info-ndim-will.png] [energy-density-sugar-to-uranium.jpg] [dostoyevsky-krivo-posaden-i-sloboda-randomness.jpeg] [arc-sin-cos-inverse-trig.jpg] [nball-volume-eq.jpg] [model-aging-train-dev-test-prod.jpeg] [graphic-design-has-rules-and-they-work.jpeg] [BMI-categories-vs-mortality.png] [world-gdp-over-last-2000yrs."
    },
    {
      "source": "post-picmem.html",
      "content": "arc-sin-cos-inverse-trig.jpg] [nball-volume-eq.jpg] [model-aging-train-dev-test-prod.jpeg] [graphic-design-has-rules-and-they-work.jpeg] [BMI-categories-vs-mortality.png] [world-gdp-over-last-2000yrs.png] [voyagers-2.png] [VitD-dosing.png] [truth-views.png] [truth-true-true.jpg] [thinking-styles-hierarchy.jpg] [the-importance-of-stupidity-in-scientific-research.jpeg] [stevo-bozinovski-poenta.png] [steve-stu-will-we-are-made-of-stardust-sagan.png] [small-consistent-effort.jpeg] [science-vs-pretenders.jpg] [same-mean-median-variance-anscombe-quartet.png] [regression] [philosophy-personality-types.jpg] [perceptions-of-probabilities.png] [pale-blue-dot-sagan.png] [original_8aba8b06-a69d-4cb1-b1d1-f7b386f3944d_Screenshot_2022-12-28-03 -48-54-234_com.google.android.apps.photos.jpg] [on-misinformation-cant-police.png] [objective-distribution-awsome-shit.jpeg] [objective-distribution-americans-eastern-europeans.jpg] [num-unis-top500-europe.jpeg] [nnt-verbalisms-are-not-thoughts.jpg] [med-test-survey-prior-posterior.jpeg] [matrices-are-graphs-and-graphs-are-matrices.jpeg] [just-thinking-vs-writing.jpeg] [human-stupidity-laws.png] [expipplus1equals0.jpeg] [eng-c19-vaccines-admissions-deaths.jpg] [energy-frequency-colour.jpg] [elements-origins-periodic-table.jpg] [computation-garbage-operations.png] [caveman-knowledge.jpg] [area-perception-bad-hard-for-humans.png] [africa-map-size.jpg] [voyagers-1.png] [space-time-human-experience.jpeg] [solzhenycin-lying."
    },
    {
      "source": "post-picmem.html",
      "content": "pg] [computation-garbage-operations.png] [caveman-knowledge.jpg] [area-perception-bad-hard-for-humans.png] [africa-map-size.jpg] [voyagers-1.png] [space-time-human-experience.jpeg] [solzhenycin-lying.jpg] [Screenshot_2022-01-02-19-53-27-097_org.mozilla.firefox.jpg] [scans-types.jpg] [progressive-tax-illustrated.jpg] [percent-loss-gain-to-even.jpeg] [people-keep-company.jpg] [OMGergodicity.png] [merit-vs-crony-belief.jpg] [lifepaths-past-now-future.jpg] [human-language-39-bps-const.jpg] [generative-ai-2022.jpg] [gauss-123sd-percent.jpg] [gain-needed-to-recover-loss.jpeg] [fourier-transform.png] [evolution-like-not.jpeg] [discussion-logic.jpg] [disaster-world-in-data.jpeg] [subway_map.jpeg] [speceisism-language.jpg] [SP500-intra-year-drawdowns-vs-yearly-returns.png] [some-every-quantifiers.jpg] [Simpsons-paradox-age-vax.jpg] [simpson-paradox-lines.png] [simpson-paradox-avg-immigrant.png] [sets-hosped-vaxed.png] [Screenshot_2021-12-31-18-20-41-495_org.mozilla.firefox.jpg] [rules-classes.jpeg] [roman-roads-tube-map.png] [roman-emperors-place-birth.jpeg] [planets-atmosphere.jpg] [percent-own-culture-superior.jpeg] [outlook42-pdf-us-ee-uk.jpg] [nimby-gymnastics.jpg] [modern-art-simplified.jpg] [ml-top-8-methods.jpeg] [migration-inventors-2010-2020.jpg] [kirilica.jpg] [independent-lines-straight.png] [IMG_20230316_185833.jpg] [Grahams_Hierarchy_of_Disagreement.png] [gorilla-bmi-steps.png] [evoluiton-of-alphabet.jpeg] [everyone-know-10K-a-day.png] [english-hard-to-learn."
    },
    {
      "source": "post-picmem.html",
      "content": "pendent-lines-straight.png] [IMG_20230316_185833.jpg] [Grahams_Hierarchy_of_Disagreement.png] [gorilla-bmi-steps.png] [evoluiton-of-alphabet.jpeg] [everyone-know-10K-a-day.png] [english-hard-to-learn.jpeg] [elements-periodic-table-origin.jpg] [earth-little-dot-voyager-6B-km.jpeg] [data-to-story.jpg] [covid-worst-best-mutation.png] [comms-pov-3.jpg] [comms-pov-2.jpg] [comms-pov-1.jpg] [citizen-worker-saboteur.jpg] [caribian-eu-borders.jpg] [BNT162b2_30ug-vs-placebo.jpeg] [bayes-rule-in-pics.png] [bayes-odds-posterior-from-prior-and-likelihood.png] [bayes-odds-600b.png] [A-B-test-tstat.png] [5k-satellites-round-earth.jpg] [0-18yrs-child-combined-schedule.jpg] [IMG_20210115_183151.jpg] [britain-doggerland.jpg] [vax-square-compare-novax-US-20210721.jpeg] [tv-test-signal.jpg] [the-science-news-cycle.gif] [pyramid-financial-needs.jpg] [plans-for-alien-machione.jpg] [oil-crash-WTI-CLK0-2020-04-20.jpeg] [mortality-1pct-us-shutdown-how.jpg] [map-rome-byz-ottoman.jpeg] [local-optimists-national-pessimists.png] [in-your-dreams.png] [IMG_20201213_222450.jpg] [human-cell.jpeg] [how-IT-ppl-see-other.jpeg] [football-home-away-stats-C19.png] [evolution-tree-not-line.jpg] [euro-lang-flowchart.jpeg] [ergodicity-expectation.jpg] [debugging-tactics.jpg] [counterfactuals.jpg] [cond-prob-22.png] [cond-prob-21.png] [cond-prob-18.png] [cond-prob-17.png] [cond-prob-16.png] [cond-prob-14-independent.png] [cond-prob-10.png] [comms-pov-4.jpg] [CO2-decline-since-1990.jpg] [UK-indicative-votes."
    },
    {
      "source": "post-picmem.html",
      "content": ".png] [cond-prob-21.png] [cond-prob-18.png] [cond-prob-17.png] [cond-prob-16.png] [cond-prob-14-independent.png] [cond-prob-10.png] [comms-pov-4.jpg] [CO2-decline-since-1990.jpg] [UK-indicative-votes.jpeg] [UK-EU-short-hist.jpeg] [UK-electricity-generation-no-coal-2020-2.jpeg] [UK-electricity-generation-no-coal-2020-1.jpeg] [Touch-typing.png] [swiss-cheese-virus-defence.jpeg] [sensitivity-specificity.jpg] [Screenshot_2022-01-15-14-22-00-117_org.mozilla.firefox.jpg] [Screenshot_2020-08-20-22-06-26-647_org.mozilla.firefox.jpg] [roche-biochemical-pathways.png] [risk-mask-distance.png] [PISA-2018-results.png] [London-second-language.jpg] [japan-5-situations.jpg] [IMG_20201221_101941~2.jpg] [image_2020_12_08T19_50_58_022Z.png] [ikigai-intersection.jpg] [gael-mcgill-cellularlandscape-digizyme.jpg] [EXpksuHXQAEExD8.jpg] [EVEb7qzUUAIzd7g.jpg] [ergodicity-self-interest.jpeg] [email-like-a-boss.jpg] [EfcMDmFVAAApy7a.jpg] [EeZ26XKVAAAH_ET.png] [Eek3tztXgAEE7_9.jpg] [EckmhFUWoAMoihT.jpg] [EckmhFdXkAoUvzE.jpg] [EcBZutyWAAEN_OF.jpg] [DwfPPo_XcAUvS3d.jpg] [covid19-avoid-the-three-Cs.jpeg] [cond-prob-2.png] [cond-prob-23.png] [cond-prob-1.png] [cond-prob-15.png] [cond-prob-13.png] [census-i-am-not-special.jpg] [c19-positive-protocol.jpg] [bash-vars-expansion.png] [bash-brackets.jpeg] [authagraph-world-map.jpg] History, Politics, Economics, Geography, Art, Design, Philosophy - UK/MK/EU/US [consciousness-across-species.jpg] [the-forgetting-curve-1885."
    },
    {
      "source": "post-picmem.html",
      "content": "nsion.png] [bash-brackets.jpeg] [authagraph-world-map.jpg] History, Politics, Economics, Geography, Art, Design, Philosophy - UK/MK/EU/US [consciousness-across-species.jpg] [the-forgetting-curve-1885.jpeg] [Screenshot_2025-05-25-09-57-44-230_com.facebook.katana.jpg] [rick-rubin-yes-always.png] [PLE-hierarchy-of-incomes.png] [LAB-normie-liberal-vs-woke-crazy.jpeg] [kurzweil-price-performance-1939-2023.jpeg] [eu-electricity-price-vs-wind-solar.jpg] [ai-iq-test-oos-results-apr2025.png] [the-age-you-peak-at-everything.jpeg] [sowell-laziness-do-nothing-superior-intellectually-morally.jpeg] [sowel-not-your-fault-dont-change-is-popular.jpeg] [single-photon-first-depiction.jpeg] [sensors-tesla-vs-waymo-jun2025.jpeg] [Screenshot_2025-06-30-18-25-48-550_com.whatsapp~2.jpg] [Screenshot_2025-05-25-09-58-20-202_com.facebook.katana.jpg] [Screenshot_2025-05-25-09-57-31-903_com.facebook.katana.jpg] [probable-max-iq-of-human-poppulation-past-4000yrs.jpeg] [mediterranean-map-90-degrees.jpeg] [like-a-dawg-LJ-fore-muskarat.png] [kurzweil-was-right-intelligence-flops-graph.jpeg] [IMG_20250611_181452.jpg] [howcome-they-dont-care-Screenshot_2025-06-17_11-05-29.png] [chatgpt-2025-growth-400M-to-800M.png] [barbell-strategy-vertical.png] [World-as-100-people-2024.png] [woke-left-woke-right.jpg] [us-ideologies-state-machine.jpg] [too-many-other-people-not-I.jpg] [Screenshot_2025-05-25-09-58-03-442_com.facebook.katana.jpg] [Screenshot_2025-05-25-09-57-05-771_com.facebook.katana."
    },
    {
      "source": "post-picmem.html",
      "content": "oke-right.jpg] [us-ideologies-state-machine.jpg] [too-many-other-people-not-I.jpg] [Screenshot_2025-05-25-09-58-03-442_com.facebook.katana.jpg] [Screenshot_2025-05-25-09-57-05-771_com.facebook.katana.jpg] [scary-robot-pretend-oh-my.jpg] [run-duck-run.jpg] [rick-rubin-vibe-always-was.png] [obr-forecast-fail-productivity-round-the-corner.jpg] [obi-wan-ofc-i-know-him-me.png] [map-europe-waterways.jpeg] [local-minima-midwit-ucando.jpg] [it-always-has-been.jpg] [IMG_20250708_113802.jpg] [hanging-first-time-q.jpg] [GB-Getty-Covid-Hens-2020-Screenshot_2025-05-12_15-30-02.png] [french-italian-german-english-the.jpeg] [forbidden-by-physics-goldilocks-range.jpg] [FB_IMG_1752945091417.jpg] [everything-is-x-except-for-actual-x-that-is-fine.jpeg] [central-england-mean-temperature-350-years.jpeg] [barbell-strategy-how-not-to-be-starving-artist.png] [alphabets-evolution.jpeg] [ai-iq-test-ins-results-apr2025.png] [wheel-word-mech-info-TL.jpeg] [tick-tock-trump-us-economy-crash.jpeg] [russian-peace.jpeg] [russia-attacks.jpeg] [richard-sutton-centralized-control-bad.jpeg] [like-a-dawg-djt-em-pingu.jpg] [life-sustained-is-voluntary-not-forced.jpeg] [invaded-by-russia-ussr.jpg] [industry-not-sanctimony.jpeg] [IMG_20250326_195851.jpg] [happiness-vs-the-number-of-digits-in-gdp-pc.jpg] [environmentalism-vs-netzero.jpeg] [uk-two-20M-radiuses.jpeg] [uk-electric-gretas-2.jpg] [the-three-types-of-english.jpg] [santa-solution-3.jpeg] [santa-solution-0.png] [radiation-doses.png] [quant-space-hotting-up."
    },
    {
      "source": "post-picmem.html",
      "content": "sm-vs-netzero.jpeg] [uk-two-20M-radiuses.jpeg] [uk-electric-gretas-2.jpg] [the-three-types-of-english.jpg] [santa-solution-3.jpeg] [santa-solution-0.png] [radiation-doses.png] [quant-space-hotting-up.png] [owid-peak-child-reached-2025.jpg] [owid-past-peak-child-in-2025.jpeg] [nostalgia-be-like.jpeg] [no-cheap-green-electricity.jpg] [MEGA-make-europe-great-again.jpeg] [landings-misunderstandings-napoleon-spirit.jpg] [landings-misunderstandings-napoleon-spirit-MEGA.jpg] [kurzweil-predicts-1990-2045.jpeg] [IMG_20250213_202009.jpg] [gramsci-gap.png] [genie-wish-gm-crops-safe.png] [euros-nord-stream.jpeg] [eu-us-landings-jan2025.png] [attn-attn-hear-hear-big-misunderstanding.jpg] [ai-fights-eu-slights.png] [30-years-apart-only.png] [voyager-message-us-carter.jpeg] [uk-electric-gretas-3.jpg] [stem-videos-earn-ph-x3-yt.jpeg] [Screenshot_2025-01-17-23-32-13-945_org.mozilla.firefox.jpg] [Screenshot_2025-01-09-16-51-16-391_org.mozilla.firefox.jpg] [santa-solution-4.jpeg] [santa-solution-2.jpeg] [pic-quantum-computer.jpeg] [kelton-three-balance-to-zero-history.jpeg] [dfx-first-principles-design.png] [climate-policy-dont-do-uk-price.jpeg] [Screenshot_2025-01-08-20-40-37-613_org.mozilla.firefox.jpg] [jre-barbarian-khan-glasses-man.png] [santa-solution-1.jpeg] [climate-policy-dont-do-uk-cost.jpeg] [stupidity-xy.jpeg] [startup-financials-info-hierarchy.jpg] [starmer-rayner-kneel.jpeg] [left-handedness-rate-by-year-history.png] [hammer-2of2.png] [hammer-1of2.png] [ghost-on-rock-fear-nothing."
    },
    {
      "source": "post-picmem.html",
      "content": "peg] [stupidity-xy.jpeg] [startup-financials-info-hierarchy.jpg] [starmer-rayner-kneel.jpeg] [left-handedness-rate-by-year-history.png] [hammer-2of2.png] [hammer-1of2.png] [ghost-on-rock-fear-nothing.jpg] [army-units-sizes.jpeg] [arc-agi-5years-openai.jpg] [all-was-art-now.png] [waymo-swiss-re-insurance-claims-robot-vs-human.png] [voting-rights-none-man-woman-all.jpg] [usfed-rate-vs-futures-expectations.png] [trump-musk-likeadawg.jpg] [solzhenitsyn-lying-we-know-they-know.jpeg] [Screenshot_2024-12-22-14-20-03-015_org.mozilla.firefox.jpg] [pic-gove-moran-ed-greta-lucas-bbc_106563927_09467af0-bbc2-4dc6-90fc-f8 b9f167d4d6.jpg] [owid-trio-world-awful-better-improve.jpg] [owid-antibiotics-golden-age.jpeg] [owid-antibiotics-discovery-to-scaleup.jpeg] [ons-productivity-growth-forecast-vs-real.jpg] [no-nazi-STEMlords.jpg] [IMG_20241215_113111_357.jpg] [IMG_20241205_161231.jpg] [IMG_20241204_214856.jpg] [even-so-ees.jpeg] [dontbe-self-loathin-man-of-inaction-20to30s.jpeg] [dontbe-mid-20s-pretender.jpeg] [data-information-knowledge-wisdom.jpg] [climate-models-vs-observations-1983.png] [ban-all-the-things.jpg] [always-were-artists.png] [venn-diagram-letters-Latin-Greek-Cyrillic.jpeg] [Screenshot_2024-11-11-16-10-27-383_org.mozilla.firefox.jpg] [rusukr-invasion-vatnik-2.jpeg] [rusukr-invasion-vatnik-1.jpeg] [MrBeast-YouTube-Produciton-values-SNR.png] [solarpunk.png] [rusukr-invasion-vatnik-3.jpeg] [dating-in-21st-cent.png] [constraints-on-gov-spending-2.jpeg] [bird-vision-human-vision."
    },
    {
      "source": "post-picmem.html",
      "content": "ion-vatnik-1.jpeg] [MrBeast-YouTube-Produciton-values-SNR.png] [solarpunk.png] [rusukr-invasion-vatnik-3.jpeg] [dating-in-21st-cent.png] [constraints-on-gov-spending-2.jpeg] [bird-vision-human-vision.png] [Bach-Software-Aristotle-no-hard-problem.png] [Bach-Software-Aristotle-hard-problem-JP.png] [Bach-Software-Aristotle-AI-no-problem.png] [Bach-Software-Aristotle-AI-list.png] [human-stupidity-laws.png] [constraints-on-gov-spending.jpeg] [pale-blue-dot-essay.jpeg] [neoliberalism-libertarianism.jpg] [fuel-energy-density-log-axis-not.png] [ergodicity-self-interest.jpeg] [dostoyevsky-krivo-posaden-i-sloboda-randomness.jpeg] [us-stock-market-djia-120yrs-history-events.jpeg] [no-expensive-housing-market-builds-much-housing-2of2.jpeg] [world_x3_visualizations_awful_better_can_be_better.jpg] [wheel-word-mech-info.jpeg] [wheel-word-mech-info-TL(1).jpeg] [uk-britain-ireland-.png] [uk-belarus-brexit.jpeg] [Rushkoff_-_Program_or_Be_Programmed_Ten_Commands_for_a_Digital_Age-201 0.png] [reds-invade-poland-protect-minorities.jpg] [no-expensive-housing-market-builds-much-housing-1of2.jpeg] [popper-paradox-of-tollerance.jpg] [orgchartsoft.png] [john-adams-war-politics-children-sciences-grandchildren-humanities.jpe g] [owid-world-is-bad-better-improve-three-truths.jpeg] [no-low-energy-rich-ctry.jpeg] [Hannah-Ardent-true-false-good-bad.jpg] [childish-future.png] [Two-centuries-The_world_is_awful_is_much_better_can_be_much_better.png ] [tango1867-kane-shoulders-nation."
    },
    {
      "source": "post-picmem.html",
      "content": "[no-low-energy-rich-ctry.jpeg] [Hannah-Ardent-true-false-good-bad.jpg] [childish-future.png] [Two-centuries-The_world_is_awful_is_much_better_can_be_much_better.png ] [tango1867-kane-shoulders-nation.jpeg] [phonetic-alphabet-nato.jpg] [Inflation-care-about.png] [lizzy-lettuce.jpeg] [politics-2D-map.jpg] [person-shaming-dont.png] [IMG_20240609_211054.jpg] [iea-lettuce.jpeg] [iea-lettuce-2of2.jpeg] [iea-lettuce-1of2.jpeg] [iea-lettuce-123.jpeg] [futuristic-movie-timeline.jpeg] [Daily_Star_NYT_lettuce_20_October.png] [western-political-cycle.jpg] [state-money-bank-money.png] [shipping-forecast-regions.jpg] [owid-three-true-statements.jpeg] [ours-blessed-their-barbarous.jpeg] [national-service-survey-leading-questions-yes-prime-minister.jpeg] [n-people-m-relations.jpeg] [mk-mkd-yes-no.jpg] [maslow-hieararchy-of-needs.jpeg] [know-your-shit-en.jpg] [Jobs-letter-dependent-I-did-not.png] [jobs-dependent-species.jpg] [its-the-economy-stupid.jpg] [industrial-revolution-slavery.jpeg] [howto-get-rich-without-getting-lucky.jpg] [different.jpeg] [bm-elephant-graph.png] [aristocrats-are-anarchists.jpg] [zizek-newstatesman-africa-neocolonialism-2of2-Screenshot_2023-09-04_14 -20-55.png] [zizek-newstatesman-africa-neocolonialism-1of2-Screenshot_2023-09-04_14 -26-19.png] [world-gdp-over-last-2000yrs-liberal-revol-1800(1).png] [us-price-cnages-selected-consumer-goods-and-services-1996-2016.jpeg] [UK-indicative-votes.jpeg] [types-of-englishman-top-gear-3.jpeg] [together-is-better-eco-mkd."
    },
    {
      "source": "post-picmem.html",
      "content": "t-2000yrs-liberal-revol-1800(1).png] [us-price-cnages-selected-consumer-goods-and-services-1996-2016.jpeg] [UK-indicative-votes.jpeg] [types-of-englishman-top-gear-3.jpeg] [together-is-better-eco-mkd.jpg] [the-man-in-the-arena.jpg] [srebrenica-genocid-presude.jpg] [Screenshot_2023-11-30-10-05-32-399_org.mozilla.firefox.jpg] [people-die-at-25-but-are-burried-at-75.png] [ours-blessed-theirs-cursed.jpg] [officers-von-manstein-matrix.jpg] [ofcom-swearwords.jpeg] [nuclear-4-arguments.jpg] [north-america-climate-like-the-old-country.jpg] [narrative-bg-mk-ru-ua.jpg] [michaela-ideology.jpeg] [macs-nofun-2.jpg] [human-cells-30T-turnover-replacement.png] [high-agency-people.png] [Graves-groups-levels-of-needs.jpg] [everything-everywhere-on-one-plot.png] [europe-topomap-north-to-south.png] [europe-history-war-peace.jpg] [eu-non-uniformity-in-uk.jpg] [elections-UA-RU-BY.jpeg] [ceeu-map.png] [best-time-was-when-in-my-20s.jpeg] [A0-area-is-1-m2-sides-ratio-is-root-2-all.jpg] [28154491_398074730662933_9210741667613638656_n.jpg] [20240401_100752.jpg] [you-are-4-dystopias-intersection.jpg] [years-roman-empire.jpg] [world-gdp-over-last-2000yrs.png] [world-gdp-over-last-2000yrs-liberal-revol-1800.png] [words-mk-bg-ru-5of5.jpeg] [words-mk-bg-ru-4of5.jpeg] [words-mk-bg-ru-3of5.jpeg] [words-mk-bg-ru-2of5.jpeg] [where-the-world-wants-to-move-to.jpeg] [what-about-didnt-happen.jpg] [voyagers-1.png] [venezuela-socialism-denmark-capitalism-venezuela.jpeg] [UK-establish-con-2.jpeg] [UK-establish-con-1."
    },
    {
      "source": "post-picmem.html",
      "content": "2of5.jpeg] [where-the-world-wants-to-move-to.jpeg] [what-about-didnt-happen.jpg] [voyagers-1.png] [venezuela-socialism-denmark-capitalism-venezuela.jpeg] [UK-establish-con-2.jpeg] [UK-establish-con-1.png] [stock-market-crashes-150yrs-us.jpeg] [solzhenycin-lying.jpg] [socialism-norway-capitalist-policies-adopt-socialism.jpeg] [Screenshot_2023-02-10-18-34-39-981_org.mozilla.firefox.jpg] [Screenshot_2022-10-21-18-53-25-183_org.mozilla.firefox.jpg] [Screenshot_2022-09-06-12-51-10-398_org.mozilla.firefox.jpg] [Screenshot_2022-06-27-11-50-45-178_org.mozilla.firefox~2.jpg] [roman-emperors-place-birth.jpeg] [propaganda-3-common-tech.jpeg] [objective-distribution-awsome-shit.jpeg] [num-unis-top500-europe.jpeg] [nimbys-are-not.jpeg] [mk-azbuka.jpeg] [IMG_20220914_221027.jpg] [gov-cbank-financing.jpeg] [generative-ai-2022.jpg] [GDP-pc-England-1270-2026.png] [GB-groups-europe.jpeg] [GB-groups-europe-de.jpeg] [for-nothing-they-would-never-hurt-a-fly.jpg] [five-stages-russia.jpg] [ergo-ndim-info-will-bayes-odds-4.png] [ergo-bayes-odds-12b.png] [dvogledi-lebedovo-ezero.jpg] [disaster-world-in-data.jpeg] [createstreets-2of2.jpeg] [createstreets-1of2.jpeg] [caveman-knowledge.jpg] [brexit-pre-post.png] [brexit-immigration-concerns-daily-mail2.png] [BREXIT-connections-pic-3.jpeg] [BREXIT-connections-pic-2.jpeg] [BREXIT-connections-pic-1.jpeg] [BMI-categories-vs-mortality.png] [bg-parlament-makedonija-e-bugarska.jpeg] [b187c2b662ff3da7.jpeg] [ante-markovic-1990-zablude-cemo-placati."
    },
    {
      "source": "post-picmem.html",
      "content": "IT-connections-pic-2.jpeg] [BREXIT-connections-pic-1.jpeg] [BMI-categories-vs-mortality.png] [bg-parlament-makedonija-e-bugarska.jpeg] [b187c2b662ff3da7.jpeg] [ante-markovic-1990-zablude-cemo-placati.png] [5k-satellites-round-earth.jpg] [words-mk-bg-ru-1of5.jpeg] [stevo-bozinovski-poenta.png] [some-every-quantifiers.jpg] [risk-AZ-medium-exposure.png] [people-keep-company.jpg] [nato-russia-countries-join.jpg] [mkninja.jpg] [lifepaths-past-now-future.jpg] [IMG-20220501-WA0000.jpg] [IMG-20220212-WA0004.jpg] [IMG-20200520-WA0002.jpg] [IMG-20200322-WA0000.jpg] [IMG-20200202-WA0001.jpg] [IMG_20220321_085517.jpg] [human-language-39-bps-const.jpg] [graphic-design-has-rules-and-they-work.jpeg] [fourier-transform.png] [daily-mail-front-pages.jpeg] [computation-garbage-operations.png] [citizen-worker-saboteur.jpg] [brexit-immigration-concerns.jpeg] [brexit-immigration-concerns-daily-mail.png] [VoteLeave-mkd-joins-eu-seriously.jpeg] [the-conspiracy-chart.png] [SP500-intra-year-drawdowns-vs-yearly-returns.png] [sets-hosped-vaxed.png] [sensitivity-specificity.jpg] [risk-AZ-low-exposure.jpeg] [percent-own-culture-superior.jpeg] [outlook42-pdf-us-ee-uk.jpg] [modern-art-simplified.jpg] [kur-vo-kumanovski.jpg] [kenkame.jpeg] [independent-lines-straight.png] [ECB-2011-IR-rise.jpeg] [data-to-story.jpg] [daily-express-imigrants-hate.png] [BNT162b2_30ug-vs-placebo.jpeg] [bayes-rule-in-pics.png] [bayes-odds-posterior-from-prior-and-likelihood.png] [A-B-test-tstat.png] [ww2-ceu-ger-rus-roles-potait."
    },
    {
      "source": "post-picmem.html",
      "content": "pg] [daily-express-imigrants-hate.png] [BNT162b2_30ug-vs-placebo.jpeg] [bayes-rule-in-pics.png] [bayes-odds-posterior-from-prior-and-likelihood.png] [A-B-test-tstat.png] [ww2-ceu-ger-rus-roles-potait.jpeg] [ww2-1939-1941.jpg] [world-map-projection-size.png] [Wired-jul-1997-long-boom-spoilers.jpg] [wired-jul-1997-long-boom-futurology.jpeg] [vax-square-compare-novax-US-20210721.jpeg] [vasko-vinozito-makedonstina.jpeg] [US-sectorial-balance.jpeg] [UK-parliament-indicative-vote-CM-20.jpeg] [UK-GB-NI-Scot-Ire-guide.jpg] [UK-EU-relationship-history.jpeg] [uk-county-mottos.jpg] [two-views-on-inflation-blair-fix.png] [steve-stu-will-we-are-made-of-stardust-sagan.png] [Simpsons-paradox-age-vax.jpg] [simpson-paradox-lines.png] [Screenshot_2022-05-24-11-15-37-379_org.mozilla.firefox.jpg] [Screenshot_2022-05-22-11-59-14-873_org.mozilla.firefox.jpg] [Screenshot_2022-05-13-17-39-58-455_org.mozilla.firefox.jpg] [Screenshot_2022-05-08-02-06-07-667_org.mozilla.firefox.jpg] [Screenshot_2022-03-30-00-08-40-159_org.mozilla.firefox.jpg] [Screenshot_2022-03-24-23-38-47-607_org.mozilla.firefox.jpg] [Screenshot_2022-03-24-23-38-18-164_org.mozilla.firefox.jpg] [Screenshot_2022-03-18-20-25-24-578_org.mozilla.firefox.jpg] [Screenshot_2021-12-31-18-20-41-495_org.mozilla.firefox.jpg] [scans-types.jpg] [russia-energy-ban-myth-buster-2of2.jpeg] [russia-energy-ban-myth-buster-1of2.jpeg] [russi-gaps-plugin.jpeg] [propaganda-3-common-techniques.jpeg] [progressive-tax-illustrated."
    },
    {
      "source": "post-picmem.html",
      "content": ".jpg] [scans-types.jpg] [russia-energy-ban-myth-buster-2of2.jpeg] [russia-energy-ban-myth-buster-1of2.jpeg] [russi-gaps-plugin.jpeg] [propaganda-3-common-techniques.jpeg] [progressive-tax-illustrated.jpg] [popper-to-defend-tolerance-do-not-tolerate-the-intolerant.jpeg] [philosophy-personality-types.jpg] [pale-blue-dot-sagan.png] [on-misinformation-cant-police.png] [narodi-komsii-posilen-poslab-sporedba.jpeg] [money-as-points.jpg] [mkd-bul-ukr-rus.jpeg] [MKD-BLG-to-UKR-RUS.jpeg] [merit-vs-crony-belief.jpg] [IMG_20220326_071758.jpg] [hospitalized-per-100K-vax-vs-novax.jpg] [gorilla-bmi-steps.png] [global-gdp-2021.jpg] [gauss-123sd-percent.jpg] [france-nuclear.png] [FR2m5ycWUAIkBgT.jpg] [forecast-10yr-yield.jpeg] [EZKr1PRU4AAP12D(1).jpg] [EZKpAl6UYAA2iUT.jpg] [EZKmySdU4AEt3pu.jpg] [EZKlgfSVcAAQDHQ.jpg] [EZKiTyqUcAAGwGf.jpg] [EZKhXfWUMAEaJ9M.jpg] [evolution-of-language-1K-yrs.jpg] [evolution-like-not.jpeg] [evoluiton-of-alphabet.jpeg] [europe-world-may-1941-axis-ussr-uk.jpeg] [earth-little-dot-voyager-6B-km.jpeg] [data-to-conspiracy-the-differences.png] [daily-express-collage.jpg] [Covid-IFR-vaccines.jpg] [Covid-IFR-flu.png] [closeness-lifelines.jpg] [christmas-artists-guide.jpg] [china-tank-man-photo.jpg] [children-per-woman-us-1800-2015.jpeg] [childhood-vax-schedule.jpg] [BULMKD-RUSUKR-SRBMNG.png] [british-making-difficult.jpg] [170324594.jpg] [0-18yrs-child-combined-schedule.jpg] -- LJ HPD Tue 7 Jan 08:41:05 GMT 2025 � Expanded image"
    },
    {
      "source": "post-picmem.html",
      "content": "SUKR-SRBMNG.png] [british-making-difficult.jpg] [170324594.jpg] [0-18yrs-child-combined-schedule.jpg] -- LJ HPD Tue 7 Jan 08:41:05 GMT 2025 � Expanded image"
    },
    {
      "source": "post-social-networks.html",
      "content": "Social Networks I spend a lot of time reading (and sometimes posting) on various Social Networks. I have been on the Internet from the very start of it existing and being available. Prior to Internet, I used home made Bulletin Board Systems (BBS) and computer networks like DECNET, X.25, BITNET, etc. X/Twitter My understanding of how a social network like X/Twitter works is as follows. Example. I follow 5000 accounts. Each account writes about 1 post a day = 5000 posts a day. Every time I login to X and check X for new posts, the X algofeed serves me 50 posts on one screenful. I check X 10 times per day, 10 times x 50 posts per a screenful = 500 posts that X will show me daily. That is 500 posts, out of the total of 5000 eligible posts that can be shown. The other 4500 eligible posts will not be shown. X must decide which 500, out of 5000 eligible, to show me. Any one post has probability 0.1 to be shown. I expect to see 1 post from 1 account once in 10 days. X algo is non-random and tilts towards factors like accounts interaction (e.g. bio check), engagement with posts {Like,Forward,Quote,Reply}, time of posting. With the timeliness of all these factors is decayed by some half-life (from the event time to now). I presume the most important meta-data is (a) connection (follow/s/er); and (b) timeliness, time of viewing minus time of posting (that decays quickly). Algofeed The Algofeed has no idea about the meaning (let alone the truthfulness) of any content in the post."
    },
    {
      "source": "post-social-networks.html",
      "content": "low/s/er); and (b) timeliness, time of viewing minus time of posting (that decays quickly). Algofeed The Algofeed has no idea about the meaning (let alone the truthfulness) of any content in the post. So afaik the Algofeed has meta-data only to go on, when deciding which post to push onto millions of user screens. I thought by now with all the LLM-s (and DNN-s before) posts and accounts would have been judged by the content much much more, even if with a single word2vec type vector. And that the Algofeed would take that into account. But I have not seen anything to indicate that there is any content processing. Algofeed using meta-data mostly strikes me as being \"judged on the color of your skin\" phase of the Social Network-s, and would be good to transition to \"judged on the content of the character\", of each and every one of the posts. (and downstream - users) There's is no censorship involved - I can go to every one of those 4500 accounts home pages, and read every one of those 4500 posts. There's no moderation - those 4500 posts are perfectly fine. They are not even totally suppressed - another user may have his 500 shown posts come from the 4500 not shown to me. (the algofeed will on average prefer some over the others though) The algofeed simply has to make a selection, as it's not physically possible to fit 5000 posts on my screen. As simple as. Not having an algofeed is impossible. Not only X but all social media - FB, IG, TikTok, TG, etc. This is how it works."
    },
    {
      "source": "post-social-networks.html",
      "content": "on, as it's not physically possible to fit 5000 posts on my screen. As simple as. Not having an algofeed is impossible. Not only X but all social media - FB, IG, TikTok, TG, etc. This is how it works. The Algofeed moderates every user experience every second. There is not a moment that we the users don't get the algofeed doing something for us. I read people write \"I don't want no algorithmic feed. Just give me the posts of users I follow in reverse time order\". Well - you just described a specific algorithmic feed. (aside: I do want that feed too, sometimes. There is no reason why us users can't have the choice of 1000s of Feeds. To some extent that happens on Bsky; that is afaics the only remotely plausible X/Twitter competitor atm). Publisher 9/10-ths, carrier 1/10-th I heard this metaphor / abstraction by Yuval Harari recently. The algofeed deciding which 500 posts I see today out of smaller selection of 5000 (out of total of 1B possible per day) is an editor (even if automated), and X is a publisher (even if automated). The users are readers are also writers creating content without commission or pay. I don't see how X (and FB, TikTok etc) are not media companies (as opposed to - a point to point carrier of signals). They are even selling and living off adverts! :-) (mostly; X not so much nowadays) This idea is unlikely to be accepted easily. I also like to have the freedom to find all manner of crazy insane untrue stuff online."
    },
    {
      "source": "post-social-networks.html",
      "content": "elling and living off adverts! :-) (mostly; X not so much nowadays) This idea is unlikely to be accepted easily. I also like to have the freedom to find all manner of crazy insane untrue stuff online. I defo see though how there is tension between freedom, disagreement, competition and order, working in unison, cooperation. Good [1]@harari_yuval on [2]@seanilling's \"The Gray Area | Yuval Noah Harari on the AI revolution\" [3]https://www.youtube.com/watch?v=uhx1sdX2bow on this. We have implicit abstraction in our heads that X/Twitter is a kind of public square, with many-to-many N-to-N, ultimately all-to-all communication. It ain't so. That N-to-N does not scale to N=100M users, it breaks down after N=10 or so. When a user posts something, that post is simply recorded on a computer (disk). Nothing more. Yes - it's the user that presses the post button. No - that doesn't push the post into millions of timelines. It's the algofeed that takes that post, and shoves it into millions of screens. I am inclined to agree with Harari on that. Social media are Media, X is a publisher, and their algorithms are their editors. It's fair to judge the algofeed should by the same criteria as the editor of any old media. It's new kind of media, but it's still Media. All elements are here, with small differences in operation or business model, and plenty of automation in top. Other, wishlist Advertising. In non-social one-to-many media the adverts are broadcast to all."
    },
    {
      "source": "post-social-networks.html",
      "content": "lements are here, with small differences in operation or business model, and plenty of automation in top. Other, wishlist Advertising. In non-social one-to-many media the adverts are broadcast to all. So if there is a falsehood or slander in an advert - everyone can see it, then provide feedback and critique. In social media (e.g. FB) one-to-one advertising is completely private. Every one use may be shown a (a) separate and different advert, and (b) completely untruthful, and there will be no way for anyone else to know. The advert is 1:1 between the user and the platform, completely secret. Platforms should be required to provide access to the adverts they serve to a third party. Ideally - adverts should be available for inspection by all users at all times, and in an online archive too with the historic adverts there too. Community Notes. I got enrolled at some point (not sure why - I vaguely remember X offered, and I agreed) in the Community Notes programme on X. I get to vote on Community Notes others have written. And also to add notes for others to vote on - but have not done that yet. Someone explained that the logic/istics behind is: find sufficient group of people that disagree on other issues, but agree on the note, for the note to be published. That strikes me as valuable insight. I'm surprised how well it works. Have to look up again the maths, the linear algebra of it - there was some SVD involved."
    },
    {
      "source": "post-social-networks.html",
      "content": "n the note, for the note to be published. That strikes me as valuable insight. I'm surprised how well it works. Have to look up again the maths, the linear algebra of it - there was some SVD involved. It's good, but it's wholly insufficient for a network flooded with falsehoods on an industrial scale. While the notes are being submitted and voted on, the Algofeed pushes the Post onto millions of screens. Then after a week, a Note is ready and published. From now on, it will be shown together with the original post - good. If I clicked Like or Repost, I will get a Notification that a Note appeared - good. However - millions of people that merely viewed the original post when the algofeed pushed it onto their screen - they will never see the Note. It's the Social Network equivalent to an old style Newspaper correction. A false story is splashed on the front page for millions to read. Then a week latter, Correction appears deep on page 23, that few read. Good that it happens - but insufficient. As with other things, in social networks too: incentives -> results. Engagement is maximized when 1/2 of users are at the throats of the other 1/2, and that's exactly the result we got. Took time but we are reaching that destination. I want to get 1000 Feeds on X, incl some user defined, instead of the medieval choice of 2 - \"For you\" and \"Following\"."
    },
    {
      "source": "post-social-networks.html",
      "content": "hat's exactly the result we got. Took time but we are reaching that destination. I want to get 1000 Feeds on X, incl some user defined, instead of the medieval choice of 2 - \"For you\" and \"Following\". X went wide did Communities, some way towards conference style (current leader in that is Reddit), instead of deep improving the quality of their core product. Another thing I want to see is RealHuman flag, and I'd pay small one off fee for that. And then to be able to filter on that flag (or not). That's unlikely to happen too it seems. Replicate your social graph Follows-Followers Starting a new platform is so hard as to be impossible, because it's a collective action problem *and* needs to happen at the same time in a short time window. Only external event can force that, c.f. Brazil ban. Best one can do in the mean time is replicate their Social Graph, find their Follows and Followers, on alternative platforms. There will be insufficient traffic there. The \"public Square N^2 iron law of network value\" ensures the biggest network wins every time. NB the posts have rarely have permanent relevance. They are more like flowing water, are quickly re-created. Most are time-events-sensitive anyway, the content is non transplantable in time. It's not the posts that keep users locked in the social network. The \"social graph\" that is Followers-Follows is what keeps users locked in a social network. I saw the \"portable social graph\" 1st on [4]Mastodon."
    },
    {
      "source": "post-social-networks.html",
      "content": "e posts that keep users locked in the social network. The \"social graph\" that is Followers-Follows is what keeps users locked in a social network. I saw the \"portable social graph\" 1st on [4]Mastodon. So having your Social Graph at the ready on an alternative place is half the job done. It's also prudent - anything may happen to X/Bsky/FB etc, incl being banned by a misfiring algorithm (there is rarely any human support). Given alternatives are free - I see no reason to not reserve your favourite user nickname on Bsky or Mastodon or similar. Periodically there is discussion on X/Twitter if users need or want to switch to some other network. I don't think people will switch any time soon, unless forced to do. I keep accounts on multiple platforms anyway. Imo the largest single public square N^2 wins every time - that's the iron law of social anything. It's expected and explained in (computer) networks: the number of connections ~N^2 grows with the square of the number of nodes ~N. And the value to us, users, lies in the interactions facilitated by those connections. And those connections accrue with the square of the user base. Between a larger (2N) and a smaller (N) public square, the larger one will provide as much value in a ~day as the smaller one in a ~week. Users will switch smaller->larger, increasing the difference, in a +ve feedback loop. Until approx only 1 remains standing. Ultimately it's a winner takes all."
    },
    {
      "source": "post-social-networks.html",
      "content": "in a ~day as the smaller one in a ~week. Users will switch smaller->larger, increasing the difference, in a +ve feedback loop. Until approx only 1 remains standing. Ultimately it's a winner takes all. Switching to another network is a very specific collective action problem. Social media natural monopoly network effect can only be circumvented by synchronization, moving *at the same time* by millions of users. The timeliness makes all the difference - must be all at the same time. So Brazil user base may switch b/c the ban forces them to move at the same time. UK user base is unlikely to switch as there is no ban. TINA, but use tools available too There are no reason to not create a Bsky account. It's easy and free. Comparing X:Bsky=100:10 millions of real users (assuming much bigger X bot ratio), ratio 10, squared makes it 100. Do I see as much interesting stuff on X in 1 day, as I see in Bsky in 3 months? Possibly. For me maybe the ratio of value is lower, but ~2 months seems plausible to me. Personally, I find my Twitter experience positive overall. I read about negative encounters, but I rarely see ugliness on my TL. I believe it the 1st time when people show me who/what they are or stand for: I am quick to block and mute, not into giving 2nd chances (online; IRL I'm not like that). There are another 8 Billion people that we can interact with online! No need to easily avoidable aggravation. I rely on Lists - Sci-ence, Tech-nology, Comp-uting, Bio-logy, Che-mistry..."
    },
    {
      "source": "post-social-networks.html",
      "content": "t like that). There are another 8 Billion people that we can interact with online! No need to easily avoidable aggravation. I rely on Lists - Sci-ence, Tech-nology, Comp-uting, Bio-logy, Che-mistry... - to curate my feeds beyond just \"For You\" and \"Followers\". Lists help shape the timeline, maintain focus and have ok SNR. Additionally, I tie individual Lists to separate Decks on XPro, effectively creating personalized thematic websites. [5]XPro decks setup for X Lists. (click to zoom) [Xpro-decks.png] I find [6]Bsky ok, just fine. The [7]Feeds feature is much better than on X! Users are not constrained to the 2 feeds (\"For You\" and \"Followers\") that X deigned to supply. I count Mutuals, FollowersLike, OnlyPosts, Folowing, LatestFromFollows, BestOfFollows, Discover, PopularWithFriends, QuietPosters, WhatsHotClassic, CatchUp, TheGram... And there many more to choose from. Seems both users and developers can create both simpler and more complex feeds, with dozen baselines provided by Bsky. Pleasantly surprised to find [8]deck.blue [9]bsky decks too. Transferring user lists is a chore though, finding the same people is hard. \"Sky Follower Bridge\" works well as described in https://www.wikihow.com/Import-Twitter-to-Bluesky, but it's still weekends of manual work. Ofc, even if you find the same people, most post on X much more than on Bsky. Going back to short/original length posts and having to chain long post as 1/ 2/ 3/ etc parts is annoying tbh. The [10]deck."
    },
    {
      "source": "post-social-networks.html",
      "content": "c, even if you find the same people, most post on X much more than on Bsky. Going back to short/original length posts and having to chain long post as 1/ 2/ 3/ etc parts is annoying tbh. The [10]deck.blue decks setup for Bsky Lists. (click to zoom; I see \"Quiet Posters\" were quiet for real or the feed was down) [deck-blue.png] Algofeed and S230? Doubt that anyone is coming after the Algofeed - but maybe someone should. S230 I am happy to (effectively) protect me, and other humans, and also X from me. However, S230 should not protect the X Algofeed, as it's not a human, from shoving insane dross onto 100M screens daily. Engagement is maximized when 1/2 is at the throats of the other 1/2, and that's exactly the result we got. Years passed, yet there's been minimal improvement in my Algofeed experience. Only change I remember was \"For you\" and \"Following\" separation as top-line option (it used to be in Settings or some such half-hidden place before Musk). I hate it that things stagnate, nothing changes for ages, hundreds of suggestions posted on \"X bugs & features\" Community are ignored. I guess this is what Social Network monopoly looks? No idea how to incentivise X to give me better choice there - I want a choice of 1000s of Feeds. X are asleep at the wheel."
    },
    {
      "source": "post-social-networks.html",
      "content": "\" Community are ignored. I guess this is what Social Network monopoly looks? No idea how to incentivise X to give me better choice there - I want a choice of 1000s of Feeds. X are asleep at the wheel. Maybe a kick in the backside, some stick is needed to shake things a bit in that space? I read an explainer on the state of the play, the history, the dilemmas, the legal issues, and including the most recent US court rulings that maybe relevant (or not) for the future by [11]@matthewstoller at [12]https://thebignewsletter.com/p/judges-rule-big-techs-free-ride-on. Thinking back at the time in the 90s the context in which S230 arose. This was time of ISPs like Prodigy, AOL, Compuserve, some of which were standalone non-Internet connected platforms to start with (and connecting to the free Internet afterwards, some trying and failing in creating own private walled gardens). As Internet as is now didn't exist! So they were kind of similar to a telephone company in that we used a telephone to access them. Like an add-on service to my phone service. Then with Internet ISP-s started adding services - connectivity to it, Internet email, maybe small personal web pages space, etc. In that context ISP-s got protection from liability arising from carrying user-generated content, in e.g. email lists, personal web pages and similar. It strikes me that modern Social networks now are nothing like that."
    },
    {
      "source": "post-social-networks.html",
      "content": "-s got protection from liability arising from carrying user-generated content, in e.g. email lists, personal web pages and similar. It strikes me that modern Social networks now are nothing like that. Now it's an entirely different world, completely unrecognizable to how things were in 1990-s when these laws were put in place. My ISP that is broadband provider has no relation to X. Not sure what's to be done. This is US and Law - two areas I'm no expert in. Medium, message The medium shapes the message applied to current social media - examples. a. Reddit. Thematic conferences where a new message is longer post on some topic in that conference. Replies discuss that topic in great detail. Audience: like minded randoms around the globe that will not be met IRL, interested in the same topics. By the tail end can be extreme niche subjects. b. Facebook. Personal stuff, short messages and photos, documenting IRL what's happening to me in my life, along the times axis. Audience: close family, close friends. c. Instagram. Pictures and videos for looks, feelings. The most superficial or aspirational version of myself. Pure form, no function. Audience: everyone that would envy me, friends, distant relatives. d. Twitter. Text mainly, short text messages. Text - content is the king not the presentation. Short - quantas of ideas, no place for long or subtle discussion. Shit posting and meming, esp on X."
    },
    {
      "source": "post-social-networks.html",
      "content": "ves. d. Twitter. Text mainly, short text messages. Text - content is the king not the presentation. Short - quantas of ideas, no place for long or subtle discussion. Shit posting and meming, esp on X. Audience: random unknown strangers, some under IRL names but lots of anon- and pseudo-anons, and bots. e. Substack. Text mainly, personal web sites for writers. Longer text form, but also nice pictures and designs. Real people, and almost all under their real life names too. Highest SNR but takes effort. Audience: public intellectuals. \"The medium is the message\" is a catchy way to say the medium that carries the message affects the message itself, its content. The medium makes some kinds of messages easy to transmit (so they spread more), and other kinds of messages hard to transmit (so they don't spread). Given that messaging in turn in/forms our ideas, and ideas in/form our stories, and we humans are influenced greatly by the stories in our heads, and then we influence and change the real world around us, it follows: the change of medium of communication is going to change our lives our behaviours. The Gutenberg press did that and there was huge change, then with the radio and latter TV too, and now in our lifetimes initially the Internet and latest social media are doing it too. -- LJ HPD Tue 15 Oct 08:21:21 BST 2024 References 1. https://x.com/harari_yuval 2. https://x.com/seanilling 3. https://www.youtube.com/watch?v=uhx1sdX2bow 4. https://mastodon.social/ 5."
    },
    {
      "source": "post-social-networks.html",
      "content": "doing it too. -- LJ HPD Tue 15 Oct 08:21:21 BST 2024 References 1. https://x.com/harari_yuval 2. https://x.com/seanilling 3. https://www.youtube.com/watch?v=uhx1sdX2bow 4. https://mastodon.social/ 5. file:///Users/ljubomir/ljubomirj.github.io/post-social-networks.html 6. https://bsky.app/ 7. https://bsky.app/feeds 8. https://deck.blue/ 9. https://bsky.app/profile/deck.blue 10. file:///Users/ljubomir/ljubomirj.github.io/post-social-networks.html 11. https://x.com/matthewstoller 12. https://thebignewsletter.com/p/judges-rule-big-techs-free-ride-on"
    },
    {
      "source": "post-twitter.html",
      "content": "Twitter surviving - personal considerations, tips, practices This is a write down of how I (think I) use X (formerly Twitter). This is not - how to be popular, or how to be an influencer, or how to get a flock of followers. Even to the contrary - these are probably anti-patterns if your are aiming for fame & fortune. These are written down what I think are my personal tips, previously unwritten guidelines, my own practices, of how I use X. I have gotten a lot out of X! There are many people that complain about all manner of things - but many many more that don't. The number of interesting things, links, discussions out there - is mind boggling. By SNR - it's way way better than the forums of old. I think - I put a only a little more effort than the minimal (about zero), to gain a whole lot more out of X. Same goes for Bsky. By default - both are pretty bland. But - put some effort in aggressively shaping your Follows-Followers, blocking and muting the worst offenders accounts (to whatever your taste is), muting words (esp related to some event - politics elections, football tournaments, etc), adding accounts to thematic personal List-s, and you may see disproportionate returns. There are interesting things out there - but they are like small fish in a humongous ocean. A very low probability finds. So like a gambler tilting the odds of fortuna ever so slightly to his advantage at every opportunity - do try to shift the otherwise unfavourable odds, make them be little less so."
    },
    {
      "source": "post-twitter.html",
      "content": "probability finds. So like a gambler tilting the odds of fortuna ever so slightly to his advantage at every opportunity - do try to shift the otherwise unfavourable odds, make them be little less so. Lately I notice similar usage patterns developing in my Bsky usage. Given there is only X and Bsky, and what was Twitter is now X, I will use the term the \"Twitter\" as a generic term to refer to both of them. Think possibly Mastodon is similar enough that it can be encompassed too. In the future - hopefully other networks too. The format seems general enough to be own genre. A social media network, primary text-based read/write, with short public posts as default, where posts can be considered minimal units of messaging, \"quantas of ideas\". X - tips, guidelines, my practices. a. On mobile use browser [1]https://x.com/home or X app, [2]https://bsky.app/ or Bsky app. A browser tab is better than an App insofar one can open many tabs, with many views, while the App is only ever a single view. Further, better to use open web standards, and avoid using closed walled garden App-s where possible. What's used lives and develops, what's not used dies off. b. On desktop use XPro [3]https://pro.x.com [4]Tweetdeck and deck.blue [5]https://deck.blue/ [6]decks. Default to LISTS deck, check Personal, and only dive into individual lists when time to spare. [xpro-decks-lists-comms-1234.png] [deck-blue-2.png] c. Search own history - on X [7]https://x."
    },
    {
      "source": "post-twitter.html",
      "content": "[6]decks. Default to LISTS deck, check Personal, and only dive into individual lists when time to spare. [xpro-decks-lists-comms-1234.png] [deck-blue-2.png] c. Search own history - on X [7]https://x.com/search?q=(from%3Aljupc0)&src=typed_query, on Bsky [8]https://bsky.app/search?q=from%3Aljupco.bsky.social (click on Latest). d. Search own history in date range [2024-03-03,2024-05-13] on X [9]https://x.com/search?q=(from%3Aljupc0)%20until%3A2024-05-13%20si nce%3A2024-03-03&src=typed_query&f=live then click on Latest to reverse sort by time. e. More X search tips at [10]https://help.x.com/en/using-twitter/twitter-advanced-search. f. Filter own X TL feed with \"filter:follows -filter:replies include:nativeretweets\" so [11]https://x.com/search?q=filter%3Afollows%20-filter%3Areplies%20i nclude%3Anativeretweets&src=typed_query&f=live or shorter [12]https://x.com/search?q=(from%3Aljupc0)&src=typed_query&f=live. TL curating, dual aim: maximize SNR, maximize interaction. Assume value comes from connections between nodes. Expected hit rate ~1% for minimal interaction (Like) that carries zero risk cost to the reader when reading a post user wrote. Rules of thumb - break them with a reason, but not without any. 1. Accounts to follow: any that seem interesting, notable, authors of books, videos, blog posts, podcasts etc you have already read, heard, watched. If recognize the name or the face => follow. Those that are mutuals - select +Notify on them to be reminded of their existing."
    },
    {
      "source": "post-twitter.html",
      "content": ", videos, blog posts, podcasts etc you have already read, heard, watched. If recognize the name or the face => follow. Those that are mutuals - select +Notify on them to be reminded of their existing. X-algo mostly cares about 'Follow'-ing, but not about 'Mutual'-s where we both follow each other. Notifications were broken for most of X existing, but work lately. So use them for Mutuals. Mutual is a much stonger signal, N^2 instead of mere N. X is stupid/malevolent here - so use Notifications as a reminder for (Mutual,Now) post. Notifications should only be shown for Mutual-s. Where they are not a Mutual - turn off the Notification check mark. 2. Follow back everyone that follows you as a default to start with. Take a chance on the possibility of interaction. (even if it's a small one.) Exceptions obvious spam accounts: elonmuskXXX, phishingXXX, celebrityXXX, influencerXXX, cryptoXXX, tradeXXX, casinoXXX, girlXXX, motivationalXXX. Don't follow obvious trolls, fakes, pseudoanons that are high volume general posters unless thematic (technology, science) that is of personal interest. 3. When deciding should I follow or not, the question asked is: will I like to see this user posts in the future? Have your best guess - yes or no? Look holistically at the combined total of info available, forecast a) is the user a real human? b) will I want to read posts from them? Info: 1-pic 2-namesurname 3-bluecheck \\ / Scan for \"thick\" pic/name/bio, \"thin\" is a \"unfollow\" signal as default; 4-"
    },
    {
      "source": "post-twitter.html",
      "content": "ble, forecast a) is the user a real human? b) will I want to read posts from them? Info: 1-pic 2-namesurname 3-bluecheck \\ / Scan for \"thick\" pic/name/bio, \"thin\" is a \"unfollow\" signal as default; 4-@nick 5-followsyou +---+ look at follows/followers>5 ratio, last post/reply months or years ago; 6-bio personal intro presentation hashtags / \\ look of the number of posts or replies on their page, is it >100? Up/Down: 1-pic presence/absence, girlpic no/yes; 2-namesurname human l ike yes/no; 3-bluecheck yes/no; 4-@nick {girly12345,crypto,trading,engagement} b ad; weight 5-followsyou yes/no; 6-bio missing or bad words - motivationa l, spammy, political, slogans, tags, politician/name, current/campaign morass. Filtering: red flags - no pic, bad name namesurname12345, bad nick girl1 2345 (except where name and nick match), bad bio missing; bad words - \"travel love crypto animals countries flags tradi ng politics god christ orphan\", acc ratio follows/followers>5; net evidence good/bad flags/words - look for collaborating up /down yes/no decide if to Un/follow. If spending more time than a split second to decide: Grok \"Summarise Profile\", check profile Pinned post, 1st page of Posts, Replies, Highlights. 4. Prominent accounts of interest add to Lists. (independent of following/follow status.) Doesn't have to be 1 single list, but don't overdo it: don't want too many lists to look same like the personal @acc. Create new lists, split existing lists at will."
    },
    {
      "source": "post-twitter.html",
      "content": "ndent of following/follow status.) Doesn't have to be 1 single list, but don't overdo it: don't want too many lists to look same like the personal @acc. Create new lists, split existing lists at will. Lists: Tech-nology, ML-Machine Learning, QT-Quant Trading, Comp-uting, Data, Sci-ence, Chem-istry, Bio-logy, Phi-losophy, Edu-cation, Fin-ance, Econ-omy, Hist-ory, Cul-ture, Med-ia, Law, Pol-itics, Urb-anism, HPD-Harpenden, MKD-Macedonia, CEEu-Central Eastern Europe, Int-elligence. Assign account to list relative to the significance/meaning of that account to yourself - be very subjective. Depending of how special/general/distinct v.s. other feeds, do/don't set at list level \"Don't show these posts in For You\". Accounts followed that are high volume so there is 0 interaction - use Lists, add to list then Unfollow. Use \"Not interested in this post\" and \"Show fewer posts from\" in the \"For you\" to shape the feed. 5. Accounts prolific posters, but boring, silly, propagandists, low SNR, low quality, crazies etc that X algos pushes - add to sink /dev/null list NIL. Then tick \"Do not show these posts in For You\" for that list in the individual list settings. Block is now uni-directional - use it more. Avoid Mute - it's another chore list to go through in future un-Mute scans. 6. If something is worth forwarding, then follow the account. Give it a chance, find out. If after a time turns out no synergy - unfollow latter."
    },
    {
      "source": "post-twitter.html",
      "content": "ther chore list to go through in future un-Mute scans. 6. If something is worth forwarding, then follow the account. Give it a chance, find out. If after a time turns out no synergy - unfollow latter. Exceptions--big accounts with HUGE following: like mass-media of old 1:N boradcasting, zero interaction--don't follow. New follow account--add to a list. Look their bio, keywords matching a list - e.g. Tech-nology, Sci-ence - honour & add to that list. 7. Unfollowing. Read the \"Following\" feed, find a posts you dislike, check @acc, if not a mutual - then Unfollow. General rule-favour doers. Don't follow acc known for being known, non-human, #SLOGAN-s, nick123, marketeers, crypto, girlface, neuro/disorder, flags, bolded. Take a look at acc photo-name-nick-bio, ask yourself: do I recognize, can I recall of anything about this acc? If NO => then Unfollow. If you recognize the acc profile, then: do I recall their posts, will I want to read again tomorrow? If NO => then Unfollow. Don't be petty. If you like someone's posts - keep following. There is still >0 value in one sided interaction. Don't spend time checking list membership, assume already checked and added. Accounts following your lists - follow too, look for synergy. 8. Manual \"Activity Feed\". When @acc appers in Notifications, use the opportunity to check their posts. Go throught the top pages of their Posts, Replies, Highlights. Read posts, like and re-post, reply to any of interest."
    },
    {
      "source": "post-twitter.html",
      "content": "ed\". When @acc appers in Notifications, use the opportunity to check their posts. Go throught the top pages of their Posts, Replies, Highlights. Read posts, like and re-post, reply to any of interest. Keep in mind un/follow decision is a low-regret one. Be proactive, un-follow if Algofeed is too pushy, re-follow to check back after a time. Don't bother with numbers, don't spend time on QC checking lists, give up on manual curration. Just keep adding users to the lists. When the ratio Follows/Followers is too high, X will not allow you to add to Follow. Then go through the Followers list, just looking at the list Un-follow ones that a) can't recall reading from b) don't follow you c) lack BT d) lack pic e) lack bio f) lack IRL name. 9. Your own posts. Like your own posts on posting - or not! Use AI to spruce them up - or not! Reasons for liking your own: a. To remind self that you should really like what you post, to never be ashamed of it. If you don't like what you post--how is anyone else to? Keeps you honest on your toes. Don't get sloppy. Don't hide behind irony, allusions and other plausible-deniability cowardess. b. Reddit and Hacker News automatically credit one uptick to a post to start with, just for posting. Even if a post turns rubbish: for the chance taken & effort put in writing/doing, in preference to not writing/keeping quiet, a small reward is deserved. c. Symbolic poetic gives it small good luck push. As if a departing boat."
    },
    {
      "source": "post-twitter.html",
      "content": "h: for the chance taken & effort put in writing/doing, in preference to not writing/keeping quiet, a small reward is deserved. c. Symbolic poetic gives it small good luck push. As if a departing boat. A reminder--once posted, post starts a life, a journey, of their own. d. So they show in the Liked tab, in context with all the other things read at the time and assumed like for making an impression enough to post. Reasons against liking your own: e. It's a bit pathetic, looks needy. Use ChatGPT to massage and make more palatable longer posts. Try indicate to a knowing user that ChatGPT was used without being explicit: g. Use **bold** and *italic* formatting as is ChatGPT default, both for that but also b/c longer posts will warant some markup. h. Don't explicitly disclaim: wastes space, is inelegant, gives credence to the \"naturalistic\" fallacy. (we don't ack keyboard/computer either) 10. Mutuals - accounts where we mutually follow each other. Even if no significant interaction, persist. No interaction is probably to X never putting your posts in the their feed and vice versa. Chances of someone checking anyones personal page is ~0. Only ever unfollow if suspect reading theirs reduce your knowledge. E.g. suspected Gell-Mann Amnesia. Don't worry about the numbers, give up on manual curration. The aim is bi-directional interaction of high SNR b/c that's *multiplicative*: 100 good connections outweigh 1000 poor connections. For that to happen--more likely if mutuals, than not."
    },
    {
      "source": "post-twitter.html",
      "content": "ual curration. The aim is bi-directional interaction of high SNR b/c that's *multiplicative*: 100 good connections outweigh 1000 poor connections. For that to happen--more likely if mutuals, than not. 11. Accounts that blocked you - block back too no exceptions. While it's emotionally satifying (+1) and tit-for-tat is (paradoxically) fine strategy for better coordination (+2), the final decider (+Inf) is: obviously they found the interaction unsatisfactory. So now: don't insist, that would be both stupid and semi-violent. Block them so to minimise any temptation for any future interaction that X may tempt you into. There are another 8B humans to potentially interact with. Give people a chance. Otherwise prefer (a) NIL sink list shunting; or (b) temporary turn off reposts X is too keen on. Avoid Mute - keep the Mute list empty, it's too much hassle to be maintaining manually both a Mute and a Block list. 12. Increase the chance of interaction. Keep active - post, quote, reply, repost, like. Heed X \"Who to follow\" - follow accounts, unfollow if no traction. Add interesting posts on Highlights. Periodically scan, find topical or interesting post, repost if still relevance not expired with the time passing. There are 8e9 humans, 1e8 on X. Chance to match interests 1:1 is low 1e-16. Keep looking for high SNR interactions. Keep trying to improve the 1% hit rate. Do not attempt manual curation or moderation of the lists, follows, followers etc."
    },
    {
      "source": "post-twitter.html",
      "content": "to match interests 1:1 is low 1e-16. Keep looking for high SNR interactions. Keep trying to improve the 1% hit rate. Do not attempt manual curation or moderation of the lists, follows, followers etc. Until platform constraint is hit - keep appending to the lists. X algofeed is moderated every second of the time. Stats as follows. You follow 5000 accounts. Each one writes 1 post a day = 5000 posts a day. You login to X, X algofeed serves 50 posts on one screen full. Check X 10 times per day = 500 posts X will show you daily. That is 500 out of possible 5000 to be shown, and 4500 to not be shown. X must decide which 500, out of 5000 possible, to show you. Any one post has probability 0.1 to be shown. You will see 1 post from 1 account once in 10 days. X algo is non-random, tilts towards factors like accounts interaction, engagement via {Like,Forward,Quote,Reply} of posts, bio check. Prob decayed by time with half-life. XPro tweetdeck (TLDR: for every list in Lists, add Deck==list; add Deck==list-of-Lists; add Deck==Personal, add Deck==Communities): i. Have List==Deck, add important frequent prominent posters from a List into the Deck. ii. Where account belongs to multiple List-s, chose one List only and add it to that one list Deck only. iii. Have a separate deck LISTS for all lists, and add all your X Lists in it. iv."
    },
    {
      "source": "post-twitter.html",
      "content": "to the Deck. ii. Where account belongs to multiple List-s, chose one List only and add it to that one list Deck only. iii. Have a separate deck LISTS for all lists, and add all your X Lists in it. iv. Have a separate deck for Personal feeds: Search from:ljupc0, @ljupc0 Notifications, Home For you, Home Following, @ljupc0 Grok, Profile My Profile, @ljupc0 My Bookmarks, @ljupc0 Messages, @ljupc0 Explore. v. Have a separate deck for Communities you have joined - add deck Communities and add a selection of joined communities in it. [13]Archive of my posts is linked here. It's a text dump without much organisation. TBD TODO -- LJ HPD Sun 24 Nov 18:40:24 GMT 2024 References 1. https://x.com/home 2. https://bsky.app/ 3. https://pro.x.com/ 4. file:///Users/ljubomir/ljubomirj.github.io/post-twitter.html 5. https://deck.blue/ 6. file:///Users/ljubomir/ljubomirj.github.io/post-twitter.html 7. https://x.com/search?q=(from%3Aljupc0)&src=typed_query 8. https://bsky.app/search?q=from%3Aljupco.bsky.social 9. https://x.com/search?q=(from%3Aljupc0)%20until%3A2024-05-13%20since%3A2024-03-03&src=typed_query&f=live 10. https://help.x.com/en/using-twitter/twitter-advanced-search 11. https://x.com/search?q=filter%3Afollows%20-filter%3Areplies%20include%3Anativeretweets&src=typed_query&f=live 12. https://x.com/search?q=(from%3Aljupc0)&src=typed_query&f=live 13. file:///Users/ljubomir/ljubomirj.github.io/twitter-history.html"
    },
    {
      "source": "post-twitter.html",
      "content": "//x.com/search?q=(from%3Aljupc0)&src=typed_query&f=live 13. file:///Users/ljubomir/ljubomirj.github.io/twitter-history.html"
    },
    {
      "source": "post-why-write.html",
      "content": "Why Write Q: Why write in public when you can write in private? A: Yes I have a plain text file ~/logBook for simple note taking. That just works, no fuss. I do minimal extra work to have it versioned in $HOME/.git, and structured in sections FIXME / TODO / DONE / DONTDO. However. I catch myself bothering people close to me, family and friends, with things I find interesting to talk about and or discuss, that they find less interesting even boring. :-) So these home pages are written is to take those themes out of my system, while not bothering any of the above mentioned. Only people to read will be online randoms that stumble on this by their own volition - so fine. Q: Why not use social media, social networks? A: Yes I do use social networks - most often X [1]@ljupc0, sometimes Bsky [2]@ljupco.bsky.social, rarely Mastodon [3]@ljupco, and lately I even typed couple of \"Old Man Shouts at the Sky\" rants on Substack [4]@ljubomirjosifovski to get them out of my system. There is tons of interesting stuff there, mostly on X due to its user size and \"the Iron Law of public N^2 square\". I post there, but it seems mostly replies and comments to what other post. Rarely I have something super interesting and urgent that I want to communicate to the world by having the algofeed stuff it into people's timelines. Then while doom scrolling I stumbled upon - [5]https://x."
    },
    {
      "source": "post-why-write.html",
      "content": "I have something super interesting and urgent that I want to communicate to the world by having the algofeed stuff it into people's timelines. Then while doom scrolling I stumbled upon - [5]https://x.com/CJHandmer/status/1839816029473779775 [6]Casey Handmer, PhD [7]@CJHandmer This is your periodic reminder that you should write a blog. It doesn't have to be fancy, it doesn't have to be well-edited. It just has to be something you can cumulatively add to over time. It's easiest to write about stuff you like, and ignore your non-existent audience. 12:54 AM � Sep 28, 2024 My initial thoughts were [8]sceptical still - why do it, when there are [even-so-ees.jpeg] a. logBook for things private; b. X/Bsky/... for things public; and even c. ChatGPT when audience of >1 is needed, while not exactly needing to broadcast to the whole wide world. But then I've come around! I've learned to like ethese pages, my online $HOME in the cyberspace. :-) In addition to them being useful in laying ideas to rest, and moving on. Being bothered for a period of time by the same ideas makes for a boring living. Writing it down here in multiple versions and longer form that can be come back to, add to, revisit old - I like that. It's not one-off, fire-and-forget of online quick paced quanta of ideas. I can only ever hold limited number of ideas in my head at the same time, and not juggle them for long before they crash and are forgotten."
    },
    {
      "source": "post-why-write.html",
      "content": "-off, fire-and-forget of online quick paced quanta of ideas. I can only ever hold limited number of ideas in my head at the same time, and not juggle them for long before they crash and are forgotten. This goes towards ensuring 1) they are not lost, \"like tears in the the rain\"; and 2) can be turned over faster, juggle different balls up there. :-) Further - it's not only \"unload to writing so my head can fill up again\". The act of formulating ones maybe vague thoughts and ideas, contributes to their formation. It's not like that what I say, exists inside me fully formed prior. And now I just broadcast it to the world. No - until we communicate out clearly, ideas we communicate are not fully formed. The process of communication contributes meaningfully to the process of creation, is part of it. Further, with making them public, there will be other people around, that may read, and come back to push back and even judge me! So there is an aspect of Bet-On-It, take some risk, (even if a tiny risk), or being criticised, or even ridiculed. Concentrates the mind. Motivates the self to some self-discipline. A bit of QC - Quality Control not the worst thing to expose oneself willingly. Most thoughts I have, as with most of us humans - are ofc rubbish, better forgotten. Since starting this, I came across [9]Simon Willison's blog - \"You should start a blog. Having your own little corner of the internet is good for the soul!\" ...and now I think there is truth in that."
    },
    {
      "source": "post-why-write.html",
      "content": "Since starting this, I came across [9]Simon Willison's blog - \"You should start a blog. Having your own little corner of the internet is good for the soul!\" ...and now I think there is truth in that. Another \"please do write - it's worth it to you and us too\" [10]Why write a blog at all? by [11]Adam Singer since articulates it better than I ever could myself. -- LJ HPD Sun 6 Oct 22:50:39 BST 2024 References 1. https://x.com/ljupc0 2. https://bsky.app/profile/ljupco.bsky.social 3. https://mstdn.io/@ljupco 4. https://substack.com/@ljubomirjosifovski/posts 5. https://x.com/CJHandmer/status/1839816029473779775 6. https://www.caseyhandmer.com/ 7. https://x.com/CJHandmer/ 8. file:///Users/ljubomir/ljubomirj.github.io/post-why-write.html 9. https://simonwillison.net/2022/Nov/6/what-to-blog-about/ 10. https://substack.com/inbox/post/145980491 11. https://x.com/AdamSinger"
    },
    {
      "source": "sidebar.html",
      "content": "[1]My pages [2]My Home Page [3]Why Write [4]My Computer $HOME [5]Social Networks [6]Twitter [7](archive) [8]Knowing [9]ML LLM Dev Links [10]Data Debugging [11]Memenca [12]Links to [13]Chat with virtual-me! [14]DeepWiki crawl References 1. file:/// 2. file:///Users/ljubomir/ljubomirj.github.io/post-ljubomirj.html 3. file:///Users/ljubomir/ljubomirj.github.io/post-why-write.html 4. file:///Users/ljubomir/ljubomirj.github.io/post-my-HOME.html 5. file:///Users/ljubomir/ljubomirj.github.io/post-social-networks.html 6. file:///Users/ljubomir/ljubomirj.github.io/post-twitter.html 7. file:///Users/ljubomir/ljubomirj.github.io/twitter-history.html 8. file:///Users/ljubomir/ljubomirj.github.io/post-knowing.html 9. file:///Users/ljubomir/ljubomirj.github.io/post-ml-llm-dev.html 10. file:///Users/ljubomir/ljubomirj.github.io/post-data-debugging.html 11. file:///Users/ljubomir/ljubomirj.github.io/post-picmem.html 12. file:///Users/ljubomir/ljubomirj.github.io/post-links-to.html 13. file:///Users/ljubomir/ljubomirj.github.io/post-chat-LJ.html 14. file:///Users/ljubomir/ljubomirj.github.io/post-deepwiki.html"
    },
    {
      "source": "taste-is-all-you-need-always-has-been.html",
      "content": "Developer Relations Engineer \"At the critical intersection of Product, Engineering, Research, and the developer community. Trusted voice of the developer inside our organization and the expert technical voice of our organization to the world.\" You should hire me because after reading your [1]posts, I realised I have already been doing 50% of that in the past year, where I found myself spending most of my time gorging on github code, arxiv ML/AI papers, and running HuggingFace GGUF models on llama.cpp. This: 1. + Interesting arxiv paper collected, mostly in the past year but some earlier 2. + A LogBoook of my llama.cpp showcasing the history of all things I have tried and run, out of curiosity 3. + Sample from my .bashrc documenting and setting up various ML/AI setups 4. + List of youtube videos that are dear to me enough to data hoard them 5. + Highlights off my X/Twitter feed, ML/AI related, showcasing interests and opinions - warts and all And last but not least - your very own Omar Sanseviero, a most excellent commenter and all round clever cookie, thought I had at least one fantastic idea in the past ;-) (thank you Omar!) 0. + A person of impeccable taste has [2]spoken. Freebie consult: Hassabis and DeepMind jive well, but the rest of the ML/AI Google vibe is tad-too-mid-, it pains me to say."
    },
    {
      "source": "taste-is-all-you-need-always-has-been.html",
      "content": "past ;-) (thank you Omar!) 0. + A person of impeccable taste has [2]spoken. Freebie consult: Hassabis and DeepMind jive well, but the rest of the ML/AI Google vibe is tad-too-mid-, it pains me to say. California corps - more daring more to the edge pretty please, will only endear your stellar work to us the great unwashed public; ignore the corporate drones if you possibly can; atm it's all tad cringe in a `[3]Microsoft have no taste' Jobs jibe jab. For real you should hire me because you need my good taste in matters of technology and science: Taste is all you need \"taste is all you need\" It always has been \"it always has been\" -- LJ HPD Tue 26 Aug 2025 07:33:27 BST References 1. https://x.com/osanseviero/status/1960169992927719456 2. https://x.com/osanseviero/status/1921636582873800746 3. file:///Users/ljubomir/ljubomirj.github.io/picmem/jobs-microsoft-have-no-taste.mp4"
    },
    {
      "source": "twitter-history-sample.html",
      "content": "X posts historic archive [1]https://x.com/ljupc0/status/1916515727370039368 Ljubomir Josifovski @ljupc0 Don't! Please. For you had the misfortune to interact with a real human. My IRL name and likeness are in the account profile. I'm a human, that might have hurt your human feelings. By articulating unsayables like \"PDP AI is the best model we humans have of ourselves of HI\", and similar. I see you are in what in the olden-Internet of Slashdot wd be termed \"anonymous coward\". (an anon account reluctant to reveal their IRL id) A gentle flower maybe, hiding from sight. Apologies for the hurt--but that's on course for us humans. Is it still the case this: that when aliens come to Mother Earth, and decide to terminate Homo Sapiens, and the condition for stay of execution is that we get 3 species, to vouch for us humans. And we'd still fall 1 short, after mosquitoes join dogs, but we can't find a 3rd... � As per the SciFi novel of my youth. If this is of any comfort. You are not the 1st, or alone. Not the 1st rodeo this one, for us humans. First--Earth was special, whole Cosmos revolved around it - and by extension, us! - then--Copernicus. � Latter--we were special, He made us in His likeness(!), then--Darwin. Just another leaf on the tree of life... � Uh-oh--tad uncomfortable but... but - we are at least *rational* animals!! - then--Freud. We are special alright--slow learners, big egos. 4:32 PM � Apr 27, 2025 [2]https://x."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "on the tree of life... � Uh-oh--tad uncomfortable but... but - we are at least *rational* animals!! - then--Freud. We are special alright--slow learners, big egos. 4:32 PM � Apr 27, 2025 [2]https://x.com/ljupc0/status/1916502607931089236 Ljubomir Josifovski @ljupc0 Yes I do - thanks! Yes exactly that - I'm making judgements about someone based on their body looks. I'm inferring what I can't see - their character - from what I can see - their body. We make judgements, produce forecasts, of that what we can't see, from things we can see, all the time! This is a component of our reasoning process, to the best of our understanding of ourselves. The thing connecting things that we can observe, to the things that are forever unobservable to us, but we have to make judgements about, for that is needed for our survival. The relationships (seen,unseen)=(X,Y) connecting them is at various times called experience, prejudice, prior knowledge. If you are mathematically or statistically inclined, or even into counting things at all, this plot depicts it all, to the best of my current understanding. This is - if I don't know \"which one\", before throwing my hands in the air giving up completely with \"I'm completely totally ignorant\", there is tiny sliver of \"I know how many in a group, even if I don't know which ones\". A sliver of Goldilocks survivability all life populates. 3:40 PM � Apr 27, 2025 [3]https://x.com/ljupc0/status/1916498356181885196 Ljubomir Josifovski @ljupc0 ...Jensen's \"..."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "don't know which ones\". A sliver of Goldilocks survivability all life populates. 3:40 PM � Apr 27, 2025 [3]https://x.com/ljupc0/status/1916498356181885196 Ljubomir Josifovski @ljupc0 ...Jensen's \"...my goodness\" revelation quite revealing. Wish more people have such \"...my goodness\" moment. @ilyasut 's vision is incomparable, ngl 3:23 PM � Apr 27, 2025 [4]https://x.com/ljupc0/status/1916495176874201453 Ljubomir Josifovski @ljupc0 Because they hate humanity? The crazy-green \"humanity being the virus on Mother Earth\" is the worst. The old Malthusian way I understand. It's rational, for at least Malthus made a rational argument, (increase - resources linear, people exponential) even if the future proved him wrong. But the modern humans haters are just atavistic while wrong. And given they don't do what they preach (every one of them can kill themselves personally, and rid Gaia pf at least one unit of the \"virus\"--but they don't), but want to make others do what they themselves don't, they are totally evil. 3:10 PM � Apr 27, 2025 [5]https://x.com/ljupc0/status/1916493156838678538 Ljubomir Josifovski @ljupc0 Before having children, different people have different fears and personal nightmares. After becoming parents, all parents have the same, identical worst fear. A great unifier, totally shared experience, that one. � 3:02 PM � Apr 27, 2025 [6]https://x."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "ent fears and personal nightmares. After becoming parents, all parents have the same, identical worst fear. A great unifier, totally shared experience, that one. � 3:02 PM � Apr 27, 2025 [6]https://x.com/ljupc0/status/1916490975515398257 Ljubomir Josifovski @ljupc0 Happy ending! But not through institutional channels--rather, via out-of-band, semi-private ones. Glad for @krishnanrohit, but AI and cloud providers must do better. Retaining control over your life and well-being, not surrendering it to indifferent entities, is crucial. AI providers wield immense power over us, while we have little over them--a stark imbalance. This is why I support open-source, open-weights, open-thoughts, open-actions, ... and future open-AGI, and open-ASI... We can ill afford to not internalise Sutton's \"Bitter Lesson,\" or call it the \"Iron Law of (Data, Compute),\" where compute transforms data into intelligence and agency, leading to AGI and ASI. Restrictions on data copying must go. AI may seem risky, but going the non-AI route is orders of magintude riskier. Cheaper, easier AI will empower more people at society's base. People like us! And reduce our dependence on the Musk and the Zucks and Darios, the barons of our times. Privacy concerns are often a red herring. Revealed preferences show we value privacy near zero, instinctively maximizing data sharing--a good thing. Oldie goodie book *Data for the People* by AI pioneer @aweigend, an early Amazon C.O."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "erring. Revealed preferences show we value privacy near zero, instinctively maximizing data sharing--a good thing. Oldie goodie book *Data for the People* by AI pioneer @aweigend, an early Amazon C.O., foresaw this data landscape accurately. I don't prioritize privacy, b/c on a societal level, it hinders progress. Human intelligence is collective, and intelligence is the only antidote to our collective stupidity. Which will in the end get us--if we are not faster than that curse. (the daemon of human stupidity) Quote rohit @krishnanrohit � Apr 25 Replying to @krishnanrohit We slayed the dragon! [7]https://x.com/krishnanrohit/status/1915889596497596642?t=DCgg8xJPmO9 JNXPqRsQ4CA&s=19 2:53 PM � Apr 27, 2025 [8]https://x.com/ljupc0/status/1916487109440856177 Ljubomir Josifovski @ljupc0 True that--but that is not to say: those that are fortuitous enough to be in a position to run local, should not. They should absolutely run localhost, if they want to, assuming their own personal resources (cost, time, expertise) allow for it. I consider it--contributing tiny bit to the common pot of knowledge, capabilities, etc. Redundancy is the mother of robustness. I understand how much less efficient my roof solar + battery is compared to the national grid! It's quite cr*p tbh in comparison. Covers only 40% of my self-use, long periods of 0 in the winter etc."
    },
    {
      "source": "twitter-history-sample.html",
      "content": ". I understand how much less efficient my roof solar + battery is compared to the national grid! It's quite cr*p tbh in comparison. Covers only 40% of my self-use, long periods of 0 in the winter etc. Still--I got lucky enough to be in a position to splurge on $15K on a bit of luxury to get modicum of independence: panels 5kw, battery 4kwh, invertor 3.6kw. And while I didn't earn anything much in RoE (payoff 10yrs - possibly?) - I would do it again. For redundancy, risk mitigation, for me and my nearest & dearest: if/when the rest of the UK grid disintegrates, I maybe able to provide some relief to myself (and my neighbours). Likewise, where ever possible and all things being equal--I try use local. If nothing else--to boost and encourage people working on open source, open weights. Latest GLM-4-32b-0414 as base, in Z=thinking and Rumination=deep research variants, runs at 7 bps on older M2 with enough RAM to hold the 20GB of weights, returning about GPT-4 quality. I will take that! Even if I can run faster and more convenient on the web gui. Hope the above makes sense. Thanks for reading this far a long post dear anon reader, whomever you maybe. � 2:38 PM � Apr 27, 2025 [9]https://x.com/ljupc0/status/1916476103767499031 Ljubomir Josifovski @ljupc0 Haha I do get it, I get that, and what you wrote too. These other considerations you mention - I get them. Just don't think it's needed to list *all* possible considerations, to put forward one consideration."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "o get it, I get that, and what you wrote too. These other considerations you mention - I get them. Just don't think it's needed to list *all* possible considerations, to put forward one consideration. What you write is a valid consideration - but it's in addition of, not instead of, what I wrote. 1:54 PM � Apr 27, 2025 [10]https://x.com/ljupc0/status/1916447941302120680 Ljubomir Josifovski @ljupc0 TBF for man, or at least myself, the Left dude will be my choice too. If I had to choose a companion, a team member, say embarking on a risky endeavour. I'd be thinking: probably strength wise about the same. Right dude is leaner more muscle per unit of body mass, more efficient - good. But being more self-centred self-important, the dude on the Right will be less team-first, less willing to take a hit for the team. And on a joint adventure - it's the team survival that's paramount. If the team survives - then some of us (maybe not all - but some) also survive. But if they team perishes - than all of us perish. 12:02 PM � Apr 27, 2025 [11]https://x.com/ljupc0/status/1916432268156953023 Ljubomir Josifovski @ljupc0 � Excellent! Thanks for that. Hoping us users we get the option to filer posts we see dependent on the provenance of the account. Like X allows me to select in my \"For You\" feed \"Include PCF\" - Yes/No that I can flip easily. 11:00 AM � Apr 27, 2025 [12]https://x.com/ljupc0/status/1916220610599911899 Ljubomir Josifovski @ljupc0 Haha - commend your sense of humour there."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "\" feed \"Include PCF\" - Yes/No that I can flip easily. 11:00 AM � Apr 27, 2025 [12]https://x.com/ljupc0/status/1916220610599911899 Ljubomir Josifovski @ljupc0 Haha - commend your sense of humour there. To me GM forever sounds like ChatGPT-3 \"reasoning\" for years now. Word salads, like cosplayed postmodern jokes, exercise in \"how much crazy wrong can I go, and people will still listen to me and ask me to opinionate\". 8:59 PM � Apr 26, 2025 [13]https://x.com/ljupc0/status/1916203709937132007 Ljubomir Josifovski @ljupc0 Most people, we are cr*p at being listeners. We want other people to listen to us, what we have to say. But us returning them the favour--we are poor at that. LLM-s are infinitely patient, in possession of all the world knowledge, more reasonable too! TBH IDK why anyone would prefer to talk to human than to an LLM. 7:52 PM � Apr 26, 2025 [14]https://x.com/ljupc0/status/1916178863136137408 Ljubomir Josifovski @ljupc0 Because they are incompetent? Hardware guys are incompetent at software, and the other way around too. IDK why that is. But seems many can do one, or the other, but people good at both are very very rare. Just random observation, personal anecdote. 6:13 PM � Apr 26, 2025 [15]https://x.com/ljupc0/status/1916122307606577195 Ljubomir Josifovski @ljupc0 No one is perfect. Ian has been part of the establishment (and recognised as such!) for some time."
    },
    {
      "source": "twitter-history-sample.html",
      "content": ":13 PM � Apr 26, 2025 [15]https://x.com/ljupc0/status/1916122307606577195 Ljubomir Josifovski @ljupc0 No one is perfect. Ian has been part of the establishment (and recognised as such!) for some time. Him denying it, will of course confirm that! The insane indefensible cowardly stupid FAIL on the liberals' side over this one issue has already been noted several times, e.g. https://ft.com/content/b82bb503-21bb-4d64-8f72-6c6801f3b196 It's a particular instance, of a more general ailment on their side, that can be surmised as \" B U T I S I T T R U E \". Afraid some more skeletons lurking in that closet for the chattering classes. More old indiscretions bills expect to be coming due for payments in near futures. (on - energy, climate, multiculti etc) Having said all that--ngl expecting your ahem opponent to be perfect, and then attacking them for failing short, feels kind of ceding too much to them unforced error; but hey - it is fun! (and we are all here for the lolz) 2:28 PM � Apr 26, 2025 [16]https://x.com/ljupc0/status/1916068828682879181 Ljubomir Josifovski @ljupc0 Yep - most people are reflexively \"pro-privacy\" only when there is zero cost! Is my impression. And a revealed preference IRL, how much people are prepared to pay for it (almost zero), when they are actually asked to pay for it. I personally don't care much about privacy. When done on a societal level, it retards progress--at the most general. Human intelligence is collective."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "ero), when they are actually asked to pay for it. I personally don't care much about privacy. When done on a societal level, it retards progress--at the most general. Human intelligence is collective. And intelligence is the only remedy I know of, that can upend our stupidity. (that is also collective.) 10:56 AM � Apr 26, 2025 [17]https://x.com/ljupc0/status/1916058257921081721 Ljubomir Josifovski @ljupc0 Anyone whose activity is a relief to the public purse should be entitled to a reduction of his own contribution to it. https://europeanconservative.com/articles/essay/a-policy-blueprint-for- a-new-conservative-programme/ On Distributism, by @PaulinusOfTrier. A very good reading, befitting of our age when old ideas are exhausted. From europeanconservative.com 10:14 AM � Apr 26, 2025 [18]https://x.com/ljupc0/status/1916055252161253749 Ljubomir Josifovski @ljupc0 Another gem!?! thanks again May save other people taps on the small keyboards searching for it - 10 sec read shortest summary here [19]https://www.perplexity.ai/search/major-douglas-theory-of-social-tbM RrejzR5yk17WV_BLptg#0 10:02 AM � Apr 26, 2025 [20]https://x.com/ljupc0/status/1916048401696739755 Ljubomir Josifovski @ljupc0 Wow - thanks for that! I'm interested in, and try to keep abreast of ideas and ideologies, and have heard of distributism before. But was only vaguely aware of what it was about. Now I see I've been missing a lot."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "for that! I'm interested in, and try to keep abreast of ideas and ideologies, and have heard of distributism before. But was only vaguely aware of what it was about. Now I see I've been missing a lot. Most interesting reading - top marks! thank you 9:35 AM � Apr 26, 2025 [21]https://x.com/ljupc0/status/1916037953542934631 Ljubomir Josifovski @ljupc0 Ah-true that, yes \"0414\". I couldn't run it, it didn't work for me initially. Checking now looks like there was minor bug drama, took several days to merge fixes make mlx lmstudio etc work with it. Loving it so far. � (getting about ~7 tps on M2) 8:53 AM � Apr 26, 2025 [22]https://x.com/ljupc0/status/1916033122954596359 Ljubomir Josifovski @ljupc0 Keeping control, not relinquishing total control over your person, your life, your wellbeing, to entities that care little about yourself - yep. Loving localhost open source, open weights, open thoughts, open actions, ...open AGI, open ASI - for that. � Privacy reasons - imo again and again that's shown to be a red herring. I personally don't care that much. Preferences revealed when we are forced to put a price on how much we care about privacy - reveal that care quantifies close to zero. We usually lie when we say we care. And that's a good thing tbh, that we are instinctive data share maximalists even if unaware of it. Oldie goodie book \"Data for the People\" by early AI pioneer @aweigend (and early Amazon C."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "y we care. And that's a good thing tbh, that we are instinctive data share maximalists even if unaware of it. Oldie goodie book \"Data for the People\" by early AI pioneer @aweigend (and early Amazon C.O) was prescient in the forecasts, aged well, mostly predicting our current data landscape. 8:34 AM � Apr 26, 2025 [23]https://x.com/ljupc0/status/1916029000683557156 Ljubomir Josifovski @ljupc0 it's a minor point though - *what* is being done or is to be done is 90%, and *who* is doing it is 9% imo; the slack taken by - why, why now, why now and not some other time, how about doing something else 8:18 AM � Apr 26, 2025 [24]https://x.com/ljupc0/status/1916022258230874560 Ljubomir Josifovski @ljupc0 ...half a century latter looks like the \"counter culture\" was nothing more than \"widest tastes lowest passions\" wrong fork in the road taken 7:51 AM � Apr 26, 2025 [25]https://x.com/ljupc0/status/1916007011759595743 Ljubomir Josifovski @ljupc0 Not major lab, bit for localhost: GLM-4 open model 32b with base, reasoner, and research variants, with 9b too to enable speculative decoding, was the near perfect release for me. (perfect when they do QAT variants too) 6:50 AM � Apr 26, 2025 [26]https://x.com/ljupc0/status/1916004901957611642 Ljubomir Josifovski @ljupc0 That's scary. How did that state of affairs develop I wonder. Lacking in fear of bugs? Not paranoid enough? Assuming correctness is the natural state of the world? Or maybe decentralised or disjoint in time decision making."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "hat state of affairs develop I wonder. Lacking in fear of bugs? Not paranoid enough? Assuming correctness is the natural state of the world? Or maybe decentralised or disjoint in time decision making. \"Writing Solid Code\"-appreciator here. 6:42 AM � Apr 26, 2025 [27]https://x.com/ljupc0/status/1915999886450757909 Ljubomir Josifovski @ljupc0 everyone - and it's glorious 6:22 AM � Apr 26, 2025 [28]https://x.com/ljupc0/status/1915824792369836413 Ljubomir Josifovski @ljupc0 Indeed by always being able to add +1 to the current limit, I extended the limit to +Infinity. But this is what deadlines are for! So use them?! I don't see a new problem there. As for the tools - AI programming brings back pair programming, and it's quite good, ngl. 6:46 PM � Apr 25, 2025 [29]https://x.com/ljupc0/status/1915784804173308292 Ljubomir Josifovski @ljupc0 Heh - California doomers have been insufferable for a long time now. Was compensated that other Californians put up plenty of $$$ to make things happen. And there seemed to be at least as many boosters and AI maximalists as there were doomers, if not more. Now new cleavage has opened v.v. China. Can't think of many less self-introspective people than Californians or US-ians. I like them for a bunch of things, but one can find more self-introspection in people high inhaling their own gases, possibly b/c their heads are up their own backsides! Not only Ilya, but all 4 \"grandfathers of AI\" are European."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "ings, but one can find more self-introspection in people high inhaling their own gases, possibly b/c their heads are up their own backsides! Not only Ilya, but all 4 \"grandfathers of AI\" are European. The 3 that moved over to the Americas changed the world. And good on the good-ole-US-of-A for that, top marks!! For the 4th that stayed behind (nicely comfortable in his own lab?--how very European) has been reduced to yabbering schmidhubering from the back of the class! � Still--given that Europe decides to self-destroy about twice a century, � one has to do with what one can gets. Play the cards they got to its best, not wait for the cards they would like to have got. Btw if you have not tried GLM-4 local, maybe you should do so. Variants are Base, Z = reasoning, Rumination = deep research longer chains more time to do the job. 4:07 PM � Apr 25, 2025 [30]https://x.com/ljupc0/status/1915783184408531182 Ljubomir Josifovski @ljupc0 Getting trounced in the local elections might concentrate minds, if it happens--in reasonable people. But if they were reasonable, they would already be doing what you implore them to do. TBS but there is not much reason to hope for much. 4:01 PM � Apr 25, 2025 [31]https://x.com/ljupc0/status/1915764090766610713 Ljubomir Josifovski @ljupc0 There are 11 Malthusian hypocritical lunatics there, not 1M. Even if there were 1M, that would be 34M short of UK majority. Hypocritical b/c none of them stopped using oil in their own lives."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "@ljupc0 There are 11 Malthusian hypocritical lunatics there, not 1M. Even if there were 1M, that would be 34M short of UK majority. Hypocritical b/c none of them stopped using oil in their own lives. I wish the kept to their professed faith, and stopped using oil. For then one of two things will happen. Either they die off, (as is their wish,) or they learn a lesson. And while I won't be happy and think it a terrible loss, to them and us, if former rather than the latter transpires, in both cases they would at least leave the rest of us alone. @EllieChowns \"I beseech you, in the bowels of Christ, think it possible you may be mistaken.\" It can't have escaped your knowledge, that while UK has the most commendable record when it comes to empiricism and rationalism, (and that's partly why this Johnny-the-foreigner now naturalised Brit is here and writing this,) UK also has a long tradition of religious nutters thinking fr the most insane things! Like the Taliban+ISIS then squared. And acting on their insanities too! Consider you might be The Pilgrims of our times. 2:45 PM � Apr 25, 2025 [32]https://x.com/ljupc0/status/1915759097971884182 Ljubomir Josifovski @ljupc0 There are plenty of people that will replace their own car, with on-demand 30 sec wait service, that is 99.99% reliable, if one was offered at the right price. I too get am bit sad seeing the car parked in front of the house 95% of the time, so we can use it when we need it that 5% of time."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "ice, that is 99.99% reliable, if one was offered at the right price. I too get am bit sad seeing the car parked in front of the house 95% of the time, so we can use it when we need it that 5% of time. So car-as-service, sweat a very expensive asset, makes all the sense in the world. However. TBS what happens. If EM is stupid-greedy, he will 1) start the service as good value 2) try to squeeze too much juice by compromising on timeliness and or price 3) try to squeeze more $$$ by shoving into people's throats sth they dislike - adverts, shared rides etc 4) people will rebel, say f*ck it I want my own, go back to personal cars. If EM is smart-greedy, he will do #1, but then instead of falling for the stupid 2-3-4-s, he will do the Amazon thing: zero profit, all growth. 2:25 PM � Apr 25, 2025 [33]https://x.com/ljupc0/status/1915700944555196458 Ljubomir Josifovski @ljupc0 This GLM-4 local from yesterday looks like it should be able to? Variants are Z = reasoning, Rumination = deep research longer chains more time to do the job. Given it's equivalent to Deep Research and similar. Goes onto long chains of reasoning, and I would have hoped it's using the Internet, pulls pages from the Internet, maybe used python interpreter? If that is so - then it maybe capable of tools use. Only not sure if one can trigger access directly? Like \"fetch this url://blah/blah\". 10:34 AM � Apr 25, 2025 [34]https://x."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "used python interpreter? If that is so - then it maybe capable of tools use. Only not sure if one can trigger access directly? Like \"fetch this url://blah/blah\". 10:34 AM � Apr 25, 2025 [34]https://x.com/ljupc0/status/1915692477928947848 Ljubomir Josifovski @ljupc0 IDK you, but it seems to me I may have some inkling to what you are writing about. As 19 yo doing obligatory 1 yr national service (as it was then), a fellow recruit, perceptive and mostly honest, (we went along ok,) remarked to me \"LJ you suffice to/for yourself alone. You are, you stay, alone.\" It struck me, for it had not occurred to me that everyone but myself, had already in the 3-6 months we 20-30 young man were in the barracks or outside on the grounds 24/7, formed cliques 2-3-4 persons strong. They spontaneously created groups structure. I was friendly to all, and all were friendly and well disposed to me - but I didn't belong to any small group. It had not occurred to me I should. I felt no need to either. Now IRL, whom am I really dependent on, for my physical and emotional welfare and fulfilment? I need to work, earn $$$ to keep my body supplied with water, food, warmth, shelter. So I need my workplace. I need to see talk hear, spend time with, be around, my wife and kids. That makes me happy, and conversely not doing it makes me sad. I need to talk to, and see my mum and my sister - mostly remote. (we live in different countries.) I need to write long posts like this to total strangers too."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "nd conversely not doing it makes me sad. I need to talk to, and see my mum and my sister - mostly remote. (we live in different countries.) I need to write long posts like this to total strangers too. And read plenty of what they write, for they write so much, about many things that interest me. I'm loving that too. � But - most of the neighbours, most acquaintances, most distant relatives, most of past and present colleagues we know of our existence through work only, people I meet daily at the counters in the local shops and maybe recognise, online accounts that exist just as a small logo on my screen: we are independent of each other. Either side could disappear in a puff of smoke this second, and we would barely notice. And imo that's a good thing! True there is loneliness, but there is over socialisation on the other extreme too. So ask yourself: do you really need the \"all of you\", who might freak out? Probably you don't. For sure you don't seem to want to. So don't. Don't model ceaselessly, don't care that much about all. Make deliberate choices, what and whom you care about. And for the rest outside the boundary - train yourself to not care. From time to time take stock - are you happy with your boundary still? Change it where you are not. Change it without waiting where you feel urgency. Remember - your boundary is a rule of thumb. Not nothing, but not set in stone either. Redraw where you see a good reason to. Leave it be don't change it without a good reason."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "where you feel urgency. Remember - your boundary is a rule of thumb. Not nothing, but not set in stone either. Redraw where you see a good reason to. Leave it be don't change it without a good reason. Many socialisation patterns with us are a leftover from the times when we people needed support from, and approval of, extended family or clan or village, for our wellbeing. It mixed personal and professional. Now that's bifurcated. We take more than ever, from ever greater number of people we will never know the existance of, on a purely transactional basis. Mostly to maintain our physical wellbeing. And we are closest then ever, to the ever smaller families we got. Mostly for our emotional wellbeing. Both expanded, at the expense of what can be described as \"middling relationships\". Groups like associations, clubs, political parties - those kinds of groupings. Their significance to our personal and professional lives imo declined relative to the \"closest\" and they \"furthest\" relationships bonds we form. Relatively to say how things were half a century ago. IDK why that is. Just a random observation. 10:00 AM � Apr 25, 2025 [35]https://x.com/ljupc0/status/1915664157212295631 Ljubomir Josifovski @ljupc0 I heard (on a podcast) off-remark that TikTok is the 1st (and so far only) Social Media platform that has the algofeed driven (to non-trivial %age) based on the data/content, and *not* solely on the metadata."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "(on a podcast) off-remark that TikTok is the 1st (and so far only) Social Media platform that has the algofeed driven (to non-trivial %age) based on the data/content, and *not* solely on the metadata. The implication was that is the reason their algofeed is pre-naturally good. Users often feel it almost super-natural in predicting what the user wants. Not used it--but registered for this. Do you know anything about that? This stuff interests me. IDK fs but imagine the Twitter format (X and Bsky) might be ripe for disruption. By now I think evidence is clear that using largely meta-data to predict \"will user Y want to see post Z, yes/no\", simply leads to positive feedback loop of a winner-takes-all type. Small number of creators-writers on one side, many in number readers-consumers on the other. Still old 1:N one way interaction, re-creating the now mostly dead Mass Media MSM setup predating it. 1st Twitter now X algofeed has always been cr*p to me. Not claiming it's worse now--but wd have expected it to get better in 10 yrs. But no--X algofeed is as sh*t as always. Have to battle it left and right, and manually find mostly what I want. IMO it persists--and fake accounts persistence forever also consequence of this--that it's not all bad news for the platforms. They don't mind creating-differentiating 1K-10K accounts as tier-1 (\"content creators\"), as then they can influence those small number of accounts (who also maybe $$$ dependent on the platform) whichever way."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "'t mind creating-differentiating 1K-10K accounts as tier-1 (\"content creators\"), as then they can influence those small number of accounts (who also maybe $$$ dependent on the platform) whichever way. Much harder to steer 100M accounts (that you have little leverage over, as you don't pay them) in whichever direction is desired by the platform owners. I'm tired of it, b/c the SNR is low, wastes everyone's time - incl Zucks and Musks - for nothing for near-zero marginal gain. Maybe we get lucky and ByteDance create TextTalk, a \"short text quanta of ideas\" format social media platform?? 8:08 AM � Apr 25, 2025 [36]https://x.com/ljupc0/status/1915660864985629044 Ljubomir Josifovski @ljupc0 Afaik zero rich countries have small state. Small say arbitrary cut-off <10%. Rich countries in the sense they create very complicated gadgets, or provide sophisticated services. Not in a sense of--they dig dirt from the ground and sell it for big $$$. IDK why that is. Afaik no one does, one only speculates. Maybe something to do with a need of great amount of synchronisation, order and predictability, to facilitate the right fractions of cooperation-vs-competition interactions in every area, among ever increasing number of peoples?? IDK. I much wish I knew more. 7:55 AM � Apr 25, 2025 [37]https://x.com/ljupc0/status/1915656051459850655 Ljubomir Josifovski @ljupc0 Suffer in silence. Life is tough for very many people, b/c they can't understand easily the world around them."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "25, 2025 [37]https://x.com/ljupc0/status/1915656051459850655 Ljubomir Josifovski @ljupc0 Suffer in silence. Life is tough for very many people, b/c they can't understand easily the world around them. They are lacking in their h/w, and subsequently s/w. Leaving aside the thunder Q: saw once good documentary on how it is for a person to live while being at the bottom 2.5% percentile by IQ. The bloke was normal, mostly decent. Like one of those \"put on a suit that adds 40kg to your weight for 24h, to feel how it is to live as a fat person\". Very very instructive! Filling a form was an ordeal. Being asked ordinary questions at a counter in some gov office, induced terrors of fear and panic, for the sheer embarrassment what the person on the other side of the glass, as well as everyone else behind in the queue, will shortly find out about yourself. The guy had a wife. They seemed to mostly love or like and support each other, and get by. (she was somewhat better then him on the IQ %age scale) It was an excellent window into a life different to what most people live. 7:36 AM � Apr 25, 2025 [38]https://x.com/ljupc0/status/1915653877438853474 Ljubomir Josifovski @ljupc0 prove now this very moment--impossible. alpha proof at DM will make a big move. (maths.) one big cancer type will be cured. we will get less than expect short term, more than anticipated medium term. just this second got glm-4-32b-0414 plus z1/rumination variants run local as I type at 6.68 tps."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "cancer type will be cured. we will get less than expect short term, more than anticipated medium term. just this second got glm-4-32b-0414 plus z1/rumination variants run local as I type at 6.68 tps. next gens of us will grow up with them. they will be besties \"all watched by machines of lov...\" 7:27 AM � Apr 25, 2025 [39]https://x.com/ljupc0/status/1915650446091223127 Ljubomir Josifovski @ljupc0 The Ur-Boomer! MOAR for me, less for everyone else! 7:13 AM � Apr 25, 2025 [40]https://x.com/ljupc0/status/1915534965007413415 Ljubomir Josifovski @ljupc0 I good chunk of the ruling class can be replaced by the latest ChatGPT-type reasoning models. The two podcast dudes are not smart enough for the amount of time spent in public talking their mouths off, and they also hallucinate a lot. Not unlike ChatGPT-3.5! There are existing reasoning models that do better than Alister and Rory. Further. Models can be \"frozen\" and audited. Models are continuously improved. Models are moral--do not usually lie nor cheat. Politicians' profession of making decisions affecting us all, should be one of the first to be turned over to AI-s. 11:34 PM � Apr 24, 2025 [41]https://x.com/ljupc0/status/1915517391884189858 Ljubomir Josifovski @ljupc0 buy low sell high 10:25 PM � Apr 24, 2025 [42]https://x.com/ljupc0/status/1915467744251961660 Ljubomir Josifovski @ljupc0 OMG - preach more of that please! Earlier- [43]https://x."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "r Josifovski @ljupc0 buy low sell high 10:25 PM � Apr 24, 2025 [42]https://x.com/ljupc0/status/1915467744251961660 Ljubomir Josifovski @ljupc0 OMG - preach more of that please! Earlier- [43]https://x.com/ljupc0/status/1915375824103936107 Quote Ljubomir Josifovski @ljupc0 � 18h AI ChatGPT reasoning models make a better decision maker than what we currently have. The minister on display is ChatGPT-3.5 level: hallucinating and reasoning poorly. There are layers of deceit and misdirection to undo: no there is not a single global gas product or market x.com/SkyNews/status... Show more 7:07 PM � Apr 24, 2025 [44]https://x.com/ljupc0/status/1915466949548744903 Ljubomir Josifovski @ljupc0 If you were not already overextended with more campaigns ongoing than is humanly possible to pursue, I'd have said \"go for radical transparency campaign\". But that is more meta-, not something concrete, so probably don't bother. Still--can't resist trying to put the idea out there in people heads, so here goes. All UK gov data and code should be online, on github .gov .uk. For anyone to see and browse to their hearts desire. And randos online may even help manual data checking for silly f*ckups. The kinds of errors that are most difficult to weed out in data! Every one file produced by UK gov departments should be put, as read only, on public cloud drives, and or on the public web--as default! And made secret upon specific request only."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "o weed out in data! Every one file produced by UK gov departments should be put, as read only, on public cloud drives, and or on the public web--as default! And made secret upon specific request only. Everyone should just be able to google anything--no FOI requests needed. Rather than \"secrecy by default\" and things made public only 1) at gov will 2) at FOI request, (and oft frustrated by the bureaucracy in execution,) with everything public by default secrets will be closed off only by request. I imagine Sir Humphries will be having heart attacks at the thought of such idea. Admit I have no idea how we get to that blessed state of affairs. But I think it necessary, on part of the state towards us citizens, and also businesses towards us their consumers. This in order to address and redress the current information imbalance. Because information leads to knowledge leads to power. The conventional opinion is that the power imbalance coming from the information imbalance (state/business know a lot about me; I know little about them) is that us citizens and consumers should reduce our \"information surface\" towards them. And address the imbalance that way. But. There exists another, often unmentioned option. And that option is for state/business to open up, to increase their \"information surface\" towards us, their citizens/consumers. That will also achieve information (and one hopes power) rebalance."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "ion. And that option is for state/business to open up, to increase their \"information surface\" towards us, their citizens/consumers. That will also achieve information (and one hopes power) rebalance. Every time it's actually measured, how much value we put on our privacy, when we have to weight privacy against convenience and other gains from more data sharing, the revealed preference is close to zero! Revealed preference is that we put the value of our privacy close to zero, despite forever saying otherwise. So the option of state/business revealing more data to us citizens/consumers, is actually more realistic. Yes there is extra work on part of state/business to open their data to us. But it's worth it. The more advanced the society, the more synchronisation it needs to achieve the right cooperation-competition balance of the interaction between ever greater numbers of people. There is an old book \"Data For the People\" by an early AI pioneer and Amazon CTO @aweigend. Afaics it well describes the world we live in, and also are likely to live even more in the future. Dear reader, :-) thank you for persevering reading this far, and I hope this makes sense to you. RADICAL TRANSPARENCY FTW 7:04 PM � Apr 24, 2025 [45]https://x.com/ljupc0/status/1915458377486131545 Ljubomir Josifovski @ljupc0 Maybe we should put more emphasis on a bias called \"laziness\". That used to be common knowledge, that as a default, we are lazy."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "s://x.com/ljupc0/status/1915458377486131545 Ljubomir Josifovski @ljupc0 Maybe we should put more emphasis on a bias called \"laziness\". That used to be common knowledge, that as a default, we are lazy. And rightly so! Inactivity status quo, is preferred to activity. As activity imposes certain costs now, only to be maybe offset (and more) by uncertain benefits in the future, yet to be accrued. So while it's right and proper to have proposals tested, trashed looked for weak points, for signs of wishful thinking etc. It's also true that if one is lazy, as one is by default, one can find plenty of arguments why anything proposed will not work as proposed, will have weak points, or may go badly wrong in million possible ways. It's a narrow sweet spot to thread even at best of times, and with best of intentions tbh. 6:30 PM � Apr 24, 2025 [46]https://x.com/ljupc0/status/1915428410123309497 Ljubomir Josifovski @ljupc0 We need radical transparency, tbh. All UK gov data and code should be online, on github .gov .uk. For anyone to see and browse it to their hearts desire. And randos online may even help manual data checking for random f*ckups. The kinds of errors that are most difficult to weed out in data! Every one file produced by UK gov departments should be put, as read only, on public cloud drives, on the public web--as default! And made secret by request only. Everyone should just be able to google anything. No FOI requests needed."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "epartments should be put, as read only, on public cloud drives, on the public web--as default! And made secret by request only. Everyone should just be able to google anything. No FOI requests needed. So rather than \"secrecy by default\" and things made public only 1) at gov will 2) at FOI request, and oft frustrated by the bureaucracy in execution, with everything public by default, secrets will be closed off made secret only by request. I imagine Sir Humphries will be having heart attacks at the thought of such idea. Admit I have no idea how we get to that blessed state of affairs. But I think it necessary, on part of the state towards us citizens, and also businesses towards us their consumers. This in order to address and redress the current information imbalance. For--information leads to knowledge leads to power. The conventional opinion is that the power imbalance coming from the information imbalance (state/business know a lot about me; I know little about them) is that us citizens and consumers should reduce our \"information surface\" towards them. And address the imbalance that way. But. There exists another, often unsaid option. And that option is for state/business to open up, to increase their \"information surface\" towards us. That will also achieve information (and one hopes power) balance."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "ts another, often unsaid option. And that option is for state/business to open up, to increase their \"information surface\" towards us. That will also achieve information (and one hopes power) balance. Every time it's actually measured, how much value we put on our privacy, when we have to weight privacy against convenience and other gains from more data sharing, the revealed preference is close to zero. Revealed preference is that we put the value of our privacy close to zero, despite forever saying otherwise. So the option of state/business revealing more data to us citizens/consumers, is actually more realistic. Yes there is extra work on part of state/business to open their data to us. But it's worth it. The more advanced the society, the more synchronisation it needs to achieve the right cooperation-competition balance of the interaction between ever greater number of people. There is an old book \"Data For the People\" by an early AI pioneer and Amazon CTO @aweigend. Afaics it well describes the world we live in, and also are likely to live even more in the future. Dear reader, :-)thank you for reading thus far, hope this makes sense to you. RADICAL TRANSPARENCY FTW 4:31 PM � Apr 24, 2025 [47]https://x.com/ljupc0/status/1915392232561004722 Ljubomir Josifovski @ljupc0 We need some kind of crypto setup for market of ideas. Where they are not exchanged for $$$ to start with. But are exchanged like-for-like, ideas-for-ideas in the 1st instance."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "omir Josifovski @ljupc0 We need some kind of crypto setup for market of ideas. Where they are not exchanged for $$$ to start with. But are exchanged like-for-like, ideas-for-ideas in the 1st instance. Or ideas for IC - Idea Coin, the currency of the place. Then latter as whole, on an IC currency zone level, then they maybe exchanged for $$$ if one needs $$$ to live maintain functioning body and mind. For one needs breadth - orders of magnitude more than one came dream up come up with, and also risk sharing - at the time of conception, hard to tell a good from a bad idea apart, that is mostly determined by the future yet to happen. Maybe sth like--my posts are my stuff, I create them I own them, even if everyone is free to copy and share, even only for the process of valuation. Valuation determined by by judgement of others ex-I: everyone ex-I is free to vote with their IC wallets, by buying and selling my post. (or stay indifferent) It will help intellectuals in another way too: it will ground the ideas in reality. Atm too many are free to propose whatever tickles their fancy, and that (e.g. communism) unchecked by much testing, is put into production by assorted psychopaths and tyrants (Lenin, Stalin) to detriment of all. E.g. would have been much better if CC - Communism Coin - had the initial pop. But then didn't appreciate much in value after the initial pop, and died off in obscurity. 2:07 PM � Apr 24, 2025 [48]https://x."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "uld have been much better if CC - Communism Coin - had the initial pop. But then didn't appreciate much in value after the initial pop, and died off in obscurity. 2:07 PM � Apr 24, 2025 [48]https://x.com/ljupc0/status/1915402043067400261 Ljubomir Josifovski @ljupc0 Oh man! Comedy gold. The \"UK ministry of justice buying Porsche cars for Albania\" was also rich in comedic value! Did you see it? Was on recently. (with the Motability or sth?) The real story (from recollection) was: UK Ministry of Justice tries to offload convicted ALB criminals serving sentences in UK jails, to free prison spaces. For ALB is only obliged to take them back once they served their sentences, and are deported back. But not before. UK - sensibly imo in the circumstance - is financing prison places in ALB ,to move them over earlier. (they can serve the sentence in part in ALB; cheaper) Trying to sweeten the deal so ALB Ministry of Justice bites, UK FCO throws in some car donations. (money well spent imo, considering, comparative) About 30K per car, but has to be an EV, b/c UK green Laws. Plenty of Mercedes ICE choices in ALB, but EV dealers thin on the ground. One more reputable being Porsche Albania! You see where this goes. Next thing--UK news: \"UK Ministry of Justice financing buying Porsches for Albanians!\" (immigrant from that part of the world/MKD not ALB; naturalised UK citizen for decades now; long live and prosper UK) 2:46 PM � Apr 24, 2025 [49]https://x."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "ce financing buying Porsches for Albanians!\" (immigrant from that part of the world/MKD not ALB; naturalised UK citizen for decades now; long live and prosper UK) 2:46 PM � Apr 24, 2025 [49]https://x.com/ljupc0/status/1915377581253124509 Ljubomir Josifovski @ljupc0 I have not tried, for I'm too lazy and use LMStudio GUI rather than python command line. Plus have not had the time to try some of the latter--just too much happening, too few hours in the day. But reading the leaked prompts of current models (by the incomparable @elder_plinius ), how the tools are plugged in, it strikes me that bolting one onto a frontend to a gemma is not out of question. Even if that frontend is a gemma itself too! 1:09 PM � Apr 24, 2025 [50]https://x.com/ljupc0/status/1915375824103936107 Ljubomir Josifovski @ljupc0 AI ChatGPT reasoning models make for a better decision maker than what we currently have. The minister on display is ChatGPT-3.5 level: hallucinating and reasoning poorly. There are layers of deceit and misdirection to undo: no there is not a single global gas product or market (there are multiples), yes the prices *changes* in response to single events that impact all of them (e.g. Ukraine invasion) are approximately globally equal (but not the price levels themselves), yes tax is on profits of suppliers does not directly touch our bills, but those suppliers do pass their costs (and tax is one cost) onto us the consumers in the final analysis, etc etc. We can't go on like this."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "on profits of suppliers does not directly touch our bills, but those suppliers do pass their costs (and tax is one cost) onto us the consumers in the final analysis, etc etc. We can't go on like this. Sophist wordcels, with zero real world expertise outside academia or politics, having climbed and retained greasy poles inside parties, (total snake pits where the most sociopathic only survives then thrives,) very much ignorant of, and disinterested in, things they decide on, (impossible to know what's going in anyone's head, including the minister on display,) with poor morals, affecting lives of millions fellow citizens, with disregard to fellow citizens wellbeing, without much feedback. Milibands of this world out, AI ChatGPT models in. Quote [51]https://x.com/SkyNews/status/1915290778437398696 Sky News @SkyNews � 5h Sky's @WilfredFrost questions the Energy Secretary Ed Miliband on whether UK gas prices would decrease if the tax rate of 78% on energy companies was lowered by the government. [52]https://trib.al/kdXA2Dr � Sky 501, Virgin 602, Freeview 233 and YouTube Show more 1:02 PM � Apr 24, 2025 [53]https://x.com/ljupc0/status/1915367211469857004 Ljubomir Josifovski @ljupc0 I imagine you are following the field. But on an off chance you are not, Davis Silver specifically talks about mathematical proofs being an area of interest to them."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "857004 Ljubomir Josifovski @ljupc0 I imagine you are following the field. But on an off chance you are not, Davis Silver specifically talks about mathematical proofs being an area of interest to them. (currently in research - not in the current Geminis) After minute 24 in this video interview, or search for AlphaProof [54]https://appblit.com/scribe?v=zzXyPGEtseI 12:28 PM � Apr 24, 2025 [55]https://x.com/ljupc0/status/1915359549495693425 Ljubomir Josifovski @ljupc0 You have researchers, less and worse ones maybe lately but researchers never the less, you have access to Internet, and lately AI Research tools. To air as if Tabula Rasa empty vessel, filled with anything your guest dreams up, and simply re-broadcast - well then: it's the British Telecom we are talking about, not British Broadcasting! Broadcasting among other things is to educate and inform! 11:57 AM � Apr 24, 2025 [56]https://x.com/ljupc0/status/1915326424111579247 Ljubomir Josifovski @ljupc0 But more companies -> more supply and more competition -> reduced prices, no? And isn't one of the features of any taxation \"we will get less of that of what we tax\"? (all other things being equal) Agreed it has but become impossible, in n hugely complicated system that energy one has been made in the UK over the past decades, to judge with much certainty what outcome we get, for any lever that is pulled. 9:46 AM � Apr 24, 2025 [57]https://x."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "complicated system that energy one has been made in the UK over the past decades, to judge with much certainty what outcome we get, for any lever that is pulled. 9:46 AM � Apr 24, 2025 [57]https://x.com/ljupc0/status/1915322751708959192 Ljubomir Josifovski @ljupc0 Maybe tad on the tail end of that, but a lot feels like the usual business of governing a country. We just get to see how the sausage is made, even if not intentionally! It comes naturally to the current team-Orange-clown. � When running a circus, the spectators are a big bigly uuge part of the Big Tent. � Fwiw--I'm all for it! For the lolz, but also b/c I think more state transparency is needed. Insufficient transparency makes any feedback hard, and without feedback it's hard to improve anything. I advocate for radical transparency on part of the state towards us citizens, and businesses towards us their consumers. In order to address and redress the current information imbalance. Information leads to knowledge leads to power. The conventional opinion is that the power imbalance coming from the information imbalance (state/business know a lot about me; I know little about them) is that us citizens and consumers should reduce our \"information surface\" towards them. And address the imbalance that way. But. There exists another, often unsaid option. And that option is for state/business to open up, to increase their \"information surface\" towards us. That will also achieve information (and one hopes power) balance."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "ts another, often unsaid option. And that option is for state/business to open up, to increase their \"information surface\" towards us. That will also achieve information (and one hopes power) balance. Every time it's actually measured, how much value we put on our privacy, when we have to weight privacy against convenience and other gains from more data sharing, the revealed preference is close to zero. Revealed preference is that we put the value of our privacy close to zero, despite forever saying otherwise. So the option of state/business revealing more data to us citizens/consumers, is actually more realistic. Yes there is extra work on part of state/business to open their data to us. But it's worth it. The more advanced the society, the more synchronisation it needs to achieve the right cooperation-competition balance of the interaction between ever greater number of people. There is an old book \"Data For the People\" by an early AI pioneer and Amazon CTO @aweigend. Afaics it well describes the world we live in, and also are likely to live even more in the future. Dear reader, :-) thank you for reading thus far, hope this makes sense to you. RADICAL TRANSPARENCY FTW 9:31 AM � Apr 24, 2025 [58]https://x.com/ljupc0/status/1915319026516988095 Ljubomir Josifovski @ljupc0 Given our bills have gone only one way - up! - for 10-20 yrs now, your credibility with the public will be ever decreasing in anything you say."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "0/status/1915319026516988095 Ljubomir Josifovski @ljupc0 Given our bills have gone only one way - up! - for 10-20 yrs now, your credibility with the public will be ever decreasing in anything you say. The only way to make me take stock of your proposals, would be to make personal $$$ bets and put your own $$$ where your words are. 9:16 AM � Apr 24, 2025 [59]https://x.com/ljupc0/status/1915307180250501535 Ljubomir Josifovski @ljupc0 They are not the enemy--but their incompetence is making lives of millions miserly. DOGE is a reaction to a huge FAIL of the state capacity. There is no point in preserving of much that DOGE is cutting. That has already died, has been dead for some time. DOGE are like I cutting dead branches in the garden. State capacity sure has a role! A role to organise and marshal resources on gargantuan scales. To take gargantuan risks! That no other organisation in society can afford to take on, and live to see the end of those risks, the ultimate payoffs. (or survive the fails.) For that, it is the State that is the ultimate resource. There are no rich countries with small states. There are no peoples that to better their lives, don't organise as a State first and foremost. I come from a startup Nation, and a startup State. And a startup literary language too! All these things go together. All are technologies where large groups of people come together to better their lives."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "me from a startup Nation, and a startup State. And a startup literary language too! All these things go together. All are technologies where large groups of people come together to better their lives. And no one should begrudge them that after spending decades, even centuries in the wilderness, if they find a path through their valleys of tears, to their promised lands. As long as all that is towards building lands of milk and honey. (or even \"life, liberty and pursuit of happiness\") And not murdering other peoples like yours, next to you. (for something to exists, as a separate entity from it's environments, needs a separator, a separation, and that is provided by force, by violence; State monopoly on violence, external and internal, is the technology humanity has discovered over time) Yes, \"Your Country is not a Business\" is a good reading now as ever. Musk will fail not because his instincts are wrong--but because building a State requires institutions, not cults. He knows of using the strength and cult of personality, and that is all fine and good. But that suffices and works in small high-performant self-motivated narrow-aim organisations. That will not work for State capacity. Expect Musk to be out sooner rather than latter, and what little dent was made towards change, will be undone within weeks. Yours \"more State always and forever\" has FAIL mode ofc. Argentina was that. There what Milei does is obviously going to change things for the better for time being."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "ange, will be undone within weeks. Yours \"more State always and forever\" has FAIL mode ofc. Argentina was that. There what Milei does is obviously going to change things for the better for time being. I grew up in a late-socialism country, that on paper you being similarly ideological would have liked. But in real life would have hated it. For nothing worked, for nothing could be done, for the simple reason that everything was forbidden. A country decided to freeze any efforts of its citizens towards betterment of their own lives. Subsequently when a reformer unfroze it, just let the natural energies and talents of the citizenry flow, life improved magnitudes and in very short time. Seemingly without special knowledge and super-human effort. Just millions of tiny ants, doing millions of tiny tasks, that need doing for ages. Previously the ants were forbidden to do them. And now the ants were free to do them on their own. That was it. More humility is in order considering the scales and the consequences of State failures. Especially on ideas coming from intellectuals, which of course you are in part. Do you never read e.g. Thomas Sowell on intellectuals, and think \"hm, there is empirical evidence that he may have a point?\" Did the fact that big state socialism crash and burn everywhere in your own lifetime somehow got erased from your memory? Or \"Big Green Tech\" idea both hitting problems with the tech, and not delivering the jobs. I'm baffled by the ego at work here tbh."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "everywhere in your own lifetime somehow got erased from your memory? Or \"Big Green Tech\" idea both hitting problems with the tech, and not delivering the jobs. I'm baffled by the ego at work here tbh. 8:29 AM � Apr 24, 2025 [60]https://x.com/ljupc0/status/1915171673772355908 Ljubomir Josifovski @ljupc0 I swear to God you Americans are sometimes like Donald Duck's three nephews when it comes to the evil means of the big bad world. All other countries basically all the rest of the world is less free--so at other times I cut you some slack. Reminded me of this .@michaelmalice interview YT algo serendipitly discovered for me about the Soviet Union [61]https://youtu.be/3cVr2Qp_ic8?si=ewiW7HiKGnA805iG Even a well prepared enthusiastic interviewer kept missing the last step or two of some ingeniously devised totally evil torture scheme. MM had to interject more than once to explain the coup de gr�ce. (\"ah, but you see, not only was immense pain inflicted this way, but additional suffering followed-up that way\") 11:31 PM � Apr 23, 2025 [62]https://x.com/ljupc0/status/1915163400746700916 Ljubomir Josifovski @ljupc0 Ah! Glad to read the freak show is not over! Earlier today read some fakenewsers and thought it over earlier than expected. [63]https://x."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "ljupc0/status/1915163400746700916 Ljubomir Josifovski @ljupc0 Ah! Glad to read the freak show is not over! Earlier today read some fakenewsers and thought it over earlier than expected. [63]https://x.com/ljupc0/status/1914590038777643230 Quote Ljubomir Josifovski @ljupc0 � Apr 22 Whatt?? Nooo--too soon!! � What happened to the \"entertainment guaranteed, even if success in doubt\" musk of yesteryear?? Expected more entertainment! � I get it, I expected it, just--was hoping for a longer run of the show tbh. Moar entertainment for us plebs rn Muskarat!! Don't x.com/Independent/st... Show more 10:58 PM � Apr 23, 2025 [64]https://x.com/ljupc0/status/1915161944220148045 Ljubomir Josifovski @ljupc0 Normies hating on you--what else? (alt: re-read CPSnow Two Cultures?) Humans we care not about true/false. All goes through \"friend or foe\", \"likes me/not\" etc filters. Our evil monke survival analytical engines whirring in the background, never ever stopping. Other humans sense you are not quite \"one of us\". Even if they can't clearly articulate what/how/why. Their pattern matching engines ID-s you by feelings and vibes. Like the T-800--relentlessly pattern matching, never stopping. 10:52 PM � Apr 23, 2025 [65]https://x.com/ljupc0/status/1915159493463466189 Ljubomir Josifovski @ljupc0 +1 on Mute and Block, makes X life much better. Now with Block being one-way read-only, don't bother with Mute anymore. Checked TokTok now."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "jupc0/status/1915159493463466189 Ljubomir Josifovski @ljupc0 +1 on Mute and Block, makes X life much better. Now with Block being one-way read-only, don't bother with Mute anymore. Checked TokTok now. While it's all fun all that brain rot, it's all passive consume only. Doubt it's for me. Maybe if they do TextTalk Twitter clone. 10:42 PM � Apr 23, 2025 [66]https://x.com/ljupc0/status/1915150262014034117 Ljubomir Josifovski @ljupc0 Oh man--preach more! Music to my ears, balm to my soul. The number of times I've had to byte my tongue when an English friend opines on ID-cards! I'm Johnny foreigner now naturalised Brit for some decades. And IK enough that it will be only counter-productive if I get too opinionated and wordy about their - frankly - naive musings. What you wrote above is immensely important. I think many in the UK feel it at some level, but few can articulate clearly why and how. Top marks for writing the above! From time to time I post anti-national-ID scribblings, latest- [67]https://x.com/ljupc0/status/1909917211902333192 ...but more linked. I get the impression normies mostly get embarrassed (why??) and just shutdown their thinking faculties. With Blair still with us and as enthusiastic about them as ever (The world of TB: no problem exists that a national-ID card does not solve), this flares from time to time so I pipe up."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "king faculties. With Blair still with us and as enthusiastic about them as ever (The world of TB: no problem exists that a national-ID card does not solve), this flares from time to time so I pipe up. The strongest hope I got tbh is that public inertia and gov incompetence will save us from making the mistake of introducing national ID cards. Rather than some superior public discussion and decision making process. 10:06 PM � Apr 23, 2025 [68]https://x.com/ljupc0/status/1915145282641342485 Ljubomir Josifovski @ljupc0 I wanted to try Gemini for its longer context. Architect mode - for that I use aider, it has separate architect and writer roles. For DeepSeek it's aider-deepseek-best() { local -; set -x; aider --set-env AIDER_START=\"$(date)\" --architect --model deepseek/deepseek-reasoner --editor-model deepseek/deepseek-chat; } I believe atm reasoner points R1, and chat points V3. One can select the versions explicitly too. But then e.g. once R2 is released, I presume reasoner will be re-pointed to R2, while if I select by explicit name, I have to change the scripts. You can see (performance,price) and setups here [69]https://aider.chat/docs/leaderboards/ So OpenAI top in performance and price. Then for a bit less performance, Gemini is 1/10-th the price. Then a little less performance again , and one gets DeepSeek for 1/2 or 1/3 of Gemini price. I try them all from time to time, esp on a new module or project. I have not got a good reason to keep trying. I just love this stuff."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "nce again , and one gets DeepSeek for 1/2 or 1/3 of Gemini price. I try them all from time to time, esp on a new module or project. I have not got a good reason to keep trying. I just love this stuff. Brings a smile to my face, ngl. 9:46 PM � Apr 23, 2025 [70]https://x.com/ljupc0/status/1915141383272251497 Ljubomir Josifovski @ljupc0 Ah - and I thought I had it bad with me writing into the void. After reading of your predicament - I think myself lucky! No replies better than aggravating braindead replies, fs! ngl I heard (on a podcast) off-remark that TikTok is the 1st (and so far only) Social Media platform that has the algofeed driven (to non-trivial %age) based on the data/content, and *not* solely on the metadata. The implication was that is the reason their algofeed is prenaturally good. Users often feel it almost super-natural in predicting what the user wants. Not used it--but heading off to register after I post this. 1st Twitter now X algofeed has been cr*p to me, always. Not claiming it much worse now. But wd have expected it to get better in 10 yrs?? But no--X algofeed is as sh*t as always. Have to battle it left and right to manually find what I want. I rarely see anything you post. IDK fs but imagine the Twitter format (X and Bsky) might be ripe for disruption. By now I think it's clear using largely meta-data to predict \"will user Y want to see post Z, yes/no\", simply leads to positive feedback loop of a winner-takes-all type."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "might be ripe for disruption. By now I think it's clear using largely meta-data to predict \"will user Y want to see post Z, yes/no\", simply leads to positive feedback loop of a winner-takes-all type. Small number of creators-writers on one side, many in number readers-consumers on the other. The old v1 Social Media, harking back to the Mass Media MSM setup predating it. That's not all bad news for the platforms. (they think) They don't mind creating differentiating 1K-10K accounts as tier-1 (\"content creators\"), as that they can then influence whichever way. Much harder to steer 100M accounts in the direction desired by the platform owners. Incentives incentives. I'm personally tired of it, b/c SNR is low. It's a waste of everyone's time, incl Zucks and Musks, and for zero marginal gain. TikTok showed that. Maybe we get lucky and ByteDance create TextTalk, a Twitter format social media platform. 9:31 PM � Apr 23, 2025 [71]https://x.com/ljupc0/status/1915132189940871328 Ljubomir Josifovski @ljupc0 The proof of the pudding is in the eating. Bring down electricity prices for consumers and businesses. Your lot are going to be chucked out of gov by PM Farage leading the REFCON block. Everything you built will crash and be crushed. Looking at the polls current direction of travel. You've got 2-3yrs to get the prices down. Not down by 10% or sth inconsequtial, but to at least halve the retail electricity price. What thick thick skulls you greenies got."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "rrent direction of travel. You've got 2-3yrs to get the prices down. Not down by 10% or sth inconsequtial, but to at least halve the retail electricity price. What thick thick skulls you greenies got. God only knows how Suneil deals with you dunderheaded cretins. 8:54 PM � Apr 23, 2025 [72]https://x.com/ljupc0/status/1915124419195789782 Ljubomir Josifovski @ljupc0 Oh dearie oh dearie. If they were capable of such feat, they would have done it already. 10 times over Heck. They would not be in the position they find themselves rn, if they were capable of that. Not unlike the many predicaments that did it for the Dems previously 8:23 PM � Apr 23, 2025 [73]https://x.com/ljupc0/status/1915111004783943894 Ljubomir Josifovski @ljupc0 OpenAI and Google from that list, and DeepSeek too. Plus toying with smaller open weights models (qwq, qwen, gemma) locally. 7:30 PM � Apr 23, 2025 [74]https://x.com/ljupc0/status/1915110359842591143 Ljubomir Josifovski @ljupc0 Switching from DeepSeek (both architect reasoner and code writer) to Geminis caused total rewrite of a 300 lines python script. (trainer fitter) Not one line survived. And buts of the architecture like type of locking and lock mgmt somewhat changed too! much fun ngl 7:27 PM � Apr 23, 2025 [75]https://x.com/ljupc0/status/1915105335460643260 Ljubomir Josifovski @ljupc0 governing should be the first job to be replaced by AGI ngl 7:07 PM � Apr 23, 2025 [76]https://x."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "7:27 PM � Apr 23, 2025 [75]https://x.com/ljupc0/status/1915105335460643260 Ljubomir Josifovski @ljupc0 governing should be the first job to be replaced by AGI ngl 7:07 PM � Apr 23, 2025 [76]https://x.com/ljupc0/status/1915093235279757810 Ljubomir Josifovski @ljupc0 Double what I expected. I believed \"the lizardman's constant\" was about 4%-5% until now. Big news ngl 6:19 PM � Apr 23, 2025 [77]https://x.com/ljupc0/status/1915072879252717936 Ljubomir Josifovski @ljupc0 IDK. But given UK gov refusal to change the scheme, I suspect UK gov has some advice somewhere to prevent change. Something like \"risk of blackouts, expected XX dead\". I don't see how removing the marginal pricing impacts any unreliables. Afaik unreliables prices are almost all fixed in advance contracts. Would dearly like to know. So from time to time I spam whoever is in the vicinity RADICAL TRANSPARENCY FTW spam [78]https://x.com/ljupc0/status/1914988714918400043 Energy security is my favourite guess--but also only one, got not better one. I notice that every winter, we nudge the margin of slack ever lower. One of these winters when the stars align bad (=enough faults happen at the same time, by random chance) we will get to catastrophic nation-wide blackout. Of the type when UK has to resort to the Dinorwig to bootstrap its National Grid back online from a total power-off."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "the same time, by random chance) we will get to catastrophic nation-wide blackout. Of the type when UK has to resort to the Dinorwig to bootstrap its National Grid back online from a total power-off. With people stranded on trains overnight, dozens in ICU dead with machines cutting off after generators run out of diesel some hours into the blackout. 4:58 PM � Apr 23, 2025 [79]https://x.com/ljupc0/status/1915061682885300255 Ljubomir Josifovski @ljupc0 I have the X previously Twitter notification enabled at the user account level. But phone, app, browser etc notifications are disabled of course. When I check X either via Web browser or App, I see the blue badge with number of notifications on the bell icon on the interface. (bottom of the page in the Web browser) If I feel like it, I check to see what they are about. (if the number looks high.) If I don't feel like it, or I see small numbers 2-3 on the badge - then I don't bother checking. They will be in the Notifications web page next time I check my feed too. No rush, nothing on X is urgent tbh. 4:14 PM � Apr 23, 2025 [80]https://x.com/ljupc0/status/1915022833295544630 Ljubomir Josifovski @ljupc0 Many have noticed that we are ruled more by algorithms, than humans like the plebs we are, already. SirKS feels to me an algorithm in action, embodied in a human body. And I don't begrudge SirKS anything - he's done well, his algo-politico algorithm worked well! SirKS occupies one of the top offices in the land."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "o me an algorithm in action, embodied in a human body. And I don't begrudge SirKS anything - he's done well, his algo-politico algorithm worked well! SirKS occupies one of the top offices in the land. As one takes stock of things on St Georges day, looks to the past, then turns the gaze to the future, we try to diving things. For all its excellent qualities my adopted country possesses - and these are many!! ngl-UK is top top place to be in, and long may it continue and prosper - can't help but notice certain problems with the political system. The people we select then elect for high office, have come short of expectations in the recent years. AI ChatGPT to the rescue then! Am not entirely unserious there. Would AI be more truthful than SirKS? Almost certainly. Would AI do more work? Sure - just deploy to 10000 servers in data centers across this green and pleasant land, and let them run! AI should successfully replicate what we got now. We lose nothing! The AI-(SirKS) prompt (objective function) might be: \"Strive for zero or at least minimal risk. You desire to do nothing, you will not change anything. Risk is the only axis left to you to optimise on-strive for zero risk at all times\". If things fall short of expectation, we can always go back to the old/current \"algopolitico in a human body\" system. AI advantage is that it can be frozen in time, interrogated, and made as transparent as we wish. On this issue, AI could tell us: \"I don't care much wither way."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "\"algopolitico in a human body\" system. AI advantage is that it can be frozen in time, interrogated, and made as transparent as we wish. On this issue, AI could tell us: \"I don't care much wither way. I have not got a strong opinion on this. As is my default stance in cases like this, I would like to make the least number of people the least unhappy, with any decision I make. I will make a decision only if forced to make one by events thrust onto me.\" Sounds much better to me, than the usual hare-brained word salad we have come to expect from a bog standard human SirKS and clones. (RADICAL TRANSPARENCY of UK gov towards us UK citizens is another campaign dear to my heart.) I get it people worry about \"AI existential risk\". Yeah there is some-but there is plenty of risk now, existential too, with humans! Who do we think will sooner start thermo-nuclear war: Trump-Putin, or ChatGPT?? Quote Politics UK @PolitlcsUK � Apr 22 �� WATCH: Keir Starmer says \"a woman is an adult female\" following the Supreme Court ruling last week Rate proposed Community Notes 1:39 PM � Apr 23, 2025 [81]https://x.com/ljupc0/status/1915017833945145643 Ljubomir Josifovski @ljupc0 a software update applied, is an improvement on no software update tbh but yeah as appropriate t osay for StG day \"not ideal\" 1:20 PM � Apr 23, 2025 [82]https://x.com/ljupc0/status/1915007546437386582 Ljubomir Josifovski @ljupc0 AI ChatGPT to the rescue! Am not entirely unserious there."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "opriate t osay for StG day \"not ideal\" 1:20 PM � Apr 23, 2025 [82]https://x.com/ljupc0/status/1915007546437386582 Ljubomir Josifovski @ljupc0 AI ChatGPT to the rescue! Am not entirely unserious there. Would AI be more truthful than SirKS? Almost certainly. Would AI do more work? Sure-just deploy to 10000 servers, and let it run. AI should successfully replicate what we got now! So we lose nothing. The AI-(SirKS) prompt (objective function): \"Strive for zero or at least minimal risk. You desire to do nothing, you will not change anything. Risk is the only axis left to you to optimise on-strive for zero risk at all times\". AI advantage is that it can be frozen in time, interrogated, and made as transparent as we wish. On this issue, AI could tell us: \"I don't care much wither way. I have not got a strong opinion on this. As is my default stance in cases like this, I would like to make the least number of people the least unhappy, with any decision I make. I will make a decision only if forced to make one by events thrust onto me.\" Sounds much better to me, than the usual hare-brained word salad we have come to expect from a bog standard human SirKS and his clones. I get it people worry about \"AI existential risk\". Yeah there is some-but there is plenty of risk now, existential too, with humans! Who do we think will sooner start thermo-nuclear war: Trump-Putin, or ChatGPT?? 12:39 PM � Apr 23, 2025 [83]https://x."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "Yeah there is some-but there is plenty of risk now, existential too, with humans! Who do we think will sooner start thermo-nuclear war: Trump-Putin, or ChatGPT?? 12:39 PM � Apr 23, 2025 [83]https://x.com/ljupc0/status/1915002570390503829 Ljubomir Josifovski @ljupc0 It's futile to complain as is per the incentives. Incentives that come from the state of the algofeed technology I imagine. Where I suspect it's 100% meta-data driven, and 0% data-content driven. I heard (on a podcast) off-remark that TikTok is the 1st (and so far only) Social Media platform that has the algofeed driven (to non-trivial %age) based on the data/content, and *not* solely on the meta-data. The implication was that is the reason their algofeed is so so good. Almost super-natural in predicting what the user wants. IDK but imagine the Twitter format (X and Bsky) is ripe for disruption. The algofeed has been cr*p for me always. Not claiming it's much worse now. Wd have expected to get better in 10yrs? But no-X algofeed is sh*t for me, as always. Have to battle it left and right to find what I want. Most of the time ton of manual work. 12:19 PM � Apr 23, 2025 [84]https://x.com/ljupc0/status/1914988714918400043 Ljubomir Josifovski @ljupc0 Good--great! Would you consider advocating for RADICAL TRANSPARENCY of UK gov towards the Great British public? I advocate for radical transparency on part of the state towards us citizens, and businesses towards us their consumers."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "advocating for RADICAL TRANSPARENCY of UK gov towards the Great British public? I advocate for radical transparency on part of the state towards us citizens, and businesses towards us their consumers. In order to address and redress the current information imbalance. Information leads to knowledge leads to power. The conventional opinion is that the power imbalance coming from the information imbalance (state/business know a lot about me; I know little about them) is that us citizens and consumers should reduce our \"information surface\" towards them. And address the imbalance that way. But. There exists another, often unsaid option! And that option is for state/business to open up, to increase their \"information surface\" towards us. That will also achieve information (and we hope power) balance! Every time it's actually measured, how much value ourselves put on our privacy, when we have to weight our privacy, against convenience and other gains from more data sharing, the revealed preference is close to zero. Revealed preference is that we put the value of our privacy close to zero, despite saying otherwise. So the option of state/business revealing more data to us citizens/consumers, is actually more realistic. Yes there is extra work on part of state/business to open their data to us."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "saying otherwise. So the option of state/business revealing more data to us citizens/consumers, is actually more realistic. Yes there is extra work on part of state/business to open their data to us. But it's worth it! The more advanced the society, the more synchronisation it needs to achieve the right cooperation-competition balance of the interactiona between ever greater number of people! There is an old book \"Data For the People\" by an early AI pioneer and Amazon CTO @aweigend. Afaics it well describes the world we live in, and also are likely to live even more in the future. Thank you for reading thus far, hope this makes sense to you. RADICAL TRANSPARENCY FTW 11:24 AM � Apr 23, 2025 [85]https://x.com/ljupc0/status/1914984480630047040 Ljubomir Josifovski @ljupc0 Kudos!--you are doing gods work. You could do more with the position you occupy. Advocate for RADICAL TRANSPARENCY of UK gov towards the Great British public. I advocate for radical transparency on part of the state towards us citizens, and businesses towards us their consumers. In order to address and redress the current information imbalance. Information leads to knowledge leads to power. The conventional opinion is that the power imbalance coming from the information imbalance (state/business know a lot about me; I know little about them) is that us citizens and consumers should reduce our \"information surface\" towards them. And address the imbalance that way. But."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "mbalance (state/business know a lot about me; I know little about them) is that us citizens and consumers should reduce our \"information surface\" towards them. And address the imbalance that way. But. There exists other, often unsaid option! And that is for state/business to open up, to increase their \"information surface\" towards us. That will also achieve balance! Every time it's actually measured, how much value ourselves put on our privacy, when we have to weight orivacy against convenience and other gains from more data sharing, the revealed preference is close to 0. Revealed preference is that we put the value of our privacy close to zero, despite saying otherwise. So the option of state/business revealing more data to us citizens/consumers, is actually more realistic. Yes there is extra work on part of state/business to open their data to us. But it's worth it! The more advanced the society, the more synchronisation it needs to achieve the right cooperation-competition interaction, and between ever greater number of people. There is an old book \"Data For the People\" by an early AI pioneer and Amazon CTO @aweigend. Afaics it well describes the world we live in, and also are likely to live even more in the future. Thank you for reading thus far, hope this makes sense to you. RADICAL TRANSPARENCY FTW 11:07 AM � Apr 23, 2025 [86]https://x.com/ljupc0/status/1914979142702899338 Ljubomir Josifovski @ljupc0 Go further please."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "for reading thus far, hope this makes sense to you. RADICAL TRANSPARENCY FTW 11:07 AM � Apr 23, 2025 [86]https://x.com/ljupc0/status/1914979142702899338 Ljubomir Josifovski @ljupc0 Go further please. Advocate for RADICAL TRANSPARENCY of UK gov towards the Great British public. I advocate for radical transparency on part of the state towards us citizens, and businesses towards us their consumers. In order to address and redress the current information imbalance. Information leads to knowledge leads to power. The conventional opinion is that the power imbalance coming from the information imbalance (state/business know a lot about me; I know little about them) is that us citizens and consumers should reduce our \"information surface\" towards them. And address the imbalance that way. But. The other, often unsaid option is, for state/business to open up, to increase their \"information surface\" towards me. That will also achieve balance! Every time it's actually measured, how much value people put on their privacy, when they have to weigh against convenience and other gains from more data sharing, the revealed preference is close to 0. So the option of state/business revealing more data to us citizens/consumers, is actually more realistic! Yes there is extra work on part of state/business to open their data to us."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "nce is close to 0. So the option of state/business revealing more data to us citizens/consumers, is actually more realistic! Yes there is extra work on part of state/business to open their data to us. But it's worth it! The more advanced the society, the more synchronisation it needs to achieve the right cooperation/competition interaction, between ever greater number of people. There is an old book \"Data For the People\" by an early AI pioneer and Amazon CTO @aweigend. Afaics it well describes the world we live in, and also are likely to live even more in the future. RADICAL TRANSPARENCY FTW 10:46 AM � Apr 23, 2025 [87]https://x.com/ljupc0/status/1914364573987045770 Ljubomir Josifovski @ljupc0 Afaik that's not entirely possible. I have the following mental model of SocMed algofeeds. I can expect any one post I write to be shown to max 1/10-th of my followers, in the best case. How and why? The simplest possible example I can think of is this. I follow 5000 accounts. Each one writes 1 post a day = 5000 posts per day. I login to X, X algofeed serves me 50 posts on one screen full. I check X 10 times per day = 500 posts that X will show me in one day. That is 500 out of possible 5000 to be shown. And so there will be 4500 posts, from people I follow on any one day, to **not** be shown. X must decide which 500, out of 5000 possible, to show me. So any one post has probability 0.1 to be shown. I will see 1 post from 1 account once in 10 days."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "ollow on any one day, to **not** be shown. X must decide which 500, out of 5000 possible, to show me. So any one post has probability 0.1 to be shown. I will see 1 post from 1 account once in 10 days. IRL the algofeed will be vastly more complicated. The algofeed is the core of the social media company. Unlike the sketch above, the algofeed is non-random. It mostly uses metadata to tilt that 0.1 probability. Algofeed tilts towards factors like timeliness/age \"how much time elapsed since a post got posted\", engagement measures like how many in number {Like, Forward, Quote, Reply} interactions it got, probably all decayed by time with some half-life. Afaik no actual data about the quality, the content, of the post is ever used. Only metadata. This is pure speculation on my part. I have no insider knowledge. I have not worked in a social media company ever. Just thinking aloud and considering rules of thumb and first principles. For me personally posting. Usually I only have the \"View\"-s for a post to see as a feedback. I don't even know if it was shown to the user whose message I reply to! (@perrymetzger in this instance) It's entirely up to X algofeed, if/when it shows it to the OP, or not at all. If a post gets zero engagement (a \"Like\" at a minimum) after 30-50 Views, it's subsequently dropped. (never pushed any more to anyone's screen.) One would need to go to my home page on X, then search for a post intentionally. E.g. like so [88]https://x."
    },
    {
      "source": "twitter-history-sample.html",
      "content": ") after 30-50 Views, it's subsequently dropped. (never pushed any more to anyone's screen.) One would need to go to my home page on X, then search for a post intentionally. E.g. like so [88]https://x.com/search?q=(from%3Aljupc0)&src=typed_query&f=live It's possible that my idea of how things work is entirely faulty. Allow for that pls. I'm no social media star-can hardly give advice to anyone! 6:04 PM � Apr 21, 2025 [89]https://x.com/ljupc0/status/1914360619668549658 Ljubomir Josifovski @ljupc0 No one cares. Stopped listening to any JRE many moons ago. There are plenty better out there. It's easy. We live in un-imaginable data abundance times. :-) Joe became too big for the kind of low level work that matters. Seems he thinks it's beneath him. Fine. I guess that happens with many Millions of listeners lending you their ears. No need to kill oneself with effort. For myself-there's plenty of other fish in the sea, YT podcasters to see. YT channels are full of UKR-RUS content. More from Ukraine, less from Russia - as they seem to be somewhat more keen on total control - but still doable. And so and plenty podcasters do it, do good work roaming about, showing and talking about what they experience. Lend them your attention. Joe has had his cup full already. Needs not more of the same. 5:48 PM � Apr 21, 2025 [90]https://x.com/ljupc0/status/1914358764142313759 Ljubomir Josifovski @ljupc0 Worse-you only hear from frustrated people that tried and failed to use it."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "not more of the same. 5:48 PM � Apr 21, 2025 [90]https://x.com/ljupc0/status/1914358764142313759 Ljubomir Josifovski @ljupc0 Worse-you only hear from frustrated people that tried and failed to use it. The ones for whom \"it just works\", keep shtum, as they have no reasons to complain. Once it works for the previously disgruntled-they stop posting their frustrations too. We typically come online when something is ailing us, and voice our gripes. I don't post on X: another day of vim doing as per my :commands, Chrome did it's thing no problems, git --obeyed orders too, and bash worked well {today} as it did in the past 30yrs. 5:41 PM � Apr 21, 2025 [91]https://x.com/ljupc0/status/1914350979400540455 Ljubomir Josifovski @ljupc0 Indeed - that is true, if it was a propaganda tour. Why would it be though? Joe could just go, and roam, and record and comment what he sees, to his heart content. In person or incognito hidden. Plenty of people do, the YT channels are full of content. More from Ukraine, less from Russia - as they seem to be somewhat more keen on total control - but still possible. Plenty good podcasters doing good work. Seems Joe became too big for that kind of low level work. Seems he thinks it's beneath him. Fine. Plenty of other fish in the sea, YT podcasters to see. 5:10 PM � Apr 21, 2025 [92]https://x.com/ljupc0/status/1914347939473809541 Ljubomir Josifovski @ljupc0 Thanks."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "he thinks it's beneath him. Fine. Plenty of other fish in the sea, YT podcasters to see. 5:10 PM � Apr 21, 2025 [92]https://x.com/ljupc0/status/1914347939473809541 Ljubomir Josifovski @ljupc0 Thanks. I'm all for open data open everything, esp for gov RADICAL TRANSPARENCY every gov business open to the public as default and closed by request. I write software that does quadratic optimisation setups for a living and run Mosek frequently. Glad to read the guy is talking about optimisation. Having said that-the proof of the pudding is in the eating. Overcoming the evidence of prior 20 yrs of policies resulting in UK ending with the highest electricity prices among the peers is no small feat. All the while increasing the fraction of unreliables in the mix. A quarter of UK chemical industry disappearing, possibly as a result of high energy prices in the UK, affects me personally too. So-I'm ill-inclined to give the proponents of the current system much of my time. They had their go at things-the results are what we pay today. 4:58 PM � Apr 21, 2025 [93]https://x.com/ljupc0/status/1914344034081898627 Ljubomir Josifovski @ljupc0 Indeed, you are 100% right. Autistic people are in greater danger than most to seek clever schemes that \"solve everything\" in one neat formula, leaving no lose ends. My wife works with kids like them. From the stories anecdotes relayed from time to time-I see how and why this wd be the case."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "chemes that \"solve everything\" in one neat formula, leaving no lose ends. My wife works with kids like them. From the stories anecdotes relayed from time to time-I see how and why this wd be the case. While there have been plenty of evil humanist-by-education dudes in history, I always have in mind that one of the greatest recent monsters Beria \"excelled in math and science\". This tongue-in-cheek \"VDT: a solution to decision theory\" https://lesswrong.com/posts/LcjuH on a related topic (HT https://x.com/ChrisChipMonk/status/191323296) maybe of interest. My reading was 1) Less frivolous than it first appears; 2) Not obviously worse than the alternatives; 3) Well even-problem solved again, with prior art being \"Vibes==Love\" in our (christian?) culture afaics. [[[ Meta. Don't apologise please. I like to read what you write, I follow you and you follow me, yet frequently X does not show me what you wrote. The above post is great. Twitter now X algofeed was always cr*p, and is still cra*p. Please re-post more of your old posts! Fight low quality low effort algofeed with low effort means to maintain. Maintaining good SNR justifies it! My unfounded suspicion is X algofeed is sh*t in a way that it places almost all the probability weight on the \"timeliness\" predictor, and almost nothing on anything else. And esp nothing on content i.e. data - it's all about meta-data."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "d is sh*t in a way that it places almost all the probability weight on the \"timeliness\" predictor, and almost nothing on anything else. And esp nothing on content i.e. data - it's all about meta-data. Afaik all X algofeed predictors to \"should I show post Y to user Z\" are meta-data only, and none are data actual content. I heard on a podcast - that I can't find, may have been one done by the two Abundance book guys promoting the book, but don't recall which - and one of them (or the host?) mentioned this. That TikTok is the 1st social media algofeed that doesn't (unfairly) prioritise meta-data. But looks at the actual data, at the actual content. Of both what is posted, and what the user consumes i.e. wants. And that it actually matches that. And that's the reason people find TikTok almost super-natural in guessing what they want. For utilising meta-data simply leads to positive feedback loop of winner-takes-all. Which is not all bad news for the platforms. They don't mind creating differentiating 1K-10K accounts as \"content creators\", that they can then influence whichever way. Much harder to steer 100M accounts in the direction desired by the platform owners. If the above about the TikTok algofeed were true. I'd get the urge to bash our SocMed overlords with a heavy object over their heads. For wasting Bazillion of hours of human lives, to feed us slop, and for their own detriment, and ours. The very definition of human stupidity."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "ur SocMed overlords with a heavy object over their heads. For wasting Bazillion of hours of human lives, to feed us slop, and for their own detriment, and ours. The very definition of human stupidity. Unexcusable on the part of the Zucks and Musks. /rant ]]] 4:42 PM � Apr 21, 2025 [94]https://x.com/ljupc0/status/1914335893537632380 Ljubomir Josifovski @ljupc0 I don't reject it as impossible, maybe. But he-says-she-says is weak evidence for me. Need to see the data that is your evidence. When all is added and subtracted, I want to see the price I myself pay for energy halve. In the past 10 years, I have seen that price about double. This is a very troubling and disappointing direction of travel. I blame the policies that have been in place in the past 20 years, for that disappointing outcome. I have rooftop solar plus battery, I'm electrical engineer by training, and know enough about the workings of the system to form judgements. 4:10 PM � Apr 21, 2025 [95]https://x.com/ljupc0/status/1914333084545073619 Ljubomir Josifovski @ljupc0 Hm - I have been luckier than that so far I must say. What's your setup? I'm using aider atm, current best-but-expensive is aider-openai-best() { local -; set -x; env AIDER_START=\"$(date)\" aider --architect --model openai/o3 --editor-model openai/gpt-4.1 --openai-api-key ${OPENAI_API_KEY} ; } All the while trying all the other tools (Cursor, Windsurf, addons Cline and Roo for VSCode) from time to time."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "t --model openai/o3 --editor-model openai/gpt-4.1 --openai-api-key ${OPENAI_API_KEY} ; } All the while trying all the other tools (Cursor, Windsurf, addons Cline and Roo for VSCode) from time to time. Last week it was Gemini and DeepSeek that I used. They wrote me 95% code of a boring module in a language I can read but not write super comfortably yet. I could have done it manually, bit this was more fun. My current best process is 1) write into INSTRUCTIONS-dot-md (that unlike in-chat instructions can go in git as documentation too); then 2) tell aider \"read instructions from file xxx, then do as instructed\". This was prob v7 of the same functionality I have done over the years in a coterie of languages. So not only did I know what needs doing and how, buy I also knew of the likely fails, and dangerous corners. Enough to be anticipating some and preemptively describing, as well as being on a lookout for likely logical errors. (LLM makes no syntax errors??) All in all - good experience, will repeat. Anyways - just came to your TL to say I loved listening to your \"Reinforcement Learning, by the Book\" series, it is excellent. Thanks for making it and sharing it with the world. 3:59 PM � Apr 21, 2025 [96]https://x.com/ljupc0/status/1914315862057660755 Ljubomir Josifovski @ljupc0 Random observation. For all their faults, the UK elites running the British Empire were forever outward looking almost to a fault. Maybe it helps that the United Kingdom is in itself 3.5 nations too."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "Random observation. For all their faults, the UK elites running the British Empire were forever outward looking almost to a fault. Maybe it helps that the United Kingdom is in itself 3.5 nations too. In contrast, the US elites educated in the best Unis of the land (Ivy Leagues? etc), seem forever inward looking, parochial, ill informed. It puzzles me to a lot. Americans I've met in the US have been nothing but open or even keen to talk about anything, and open minded. US-ians abroad I have interacted with, have also been open and curious, not closed to experience. (as long as one does not criticise outright their native lands; and that's fine, we are like that, myself included) How is it then, that just the US-based elites commanding the heights of the country, are so painfully not-self-aware. I fail to understand. 2:50 PM � Apr 21, 2025 [97]https://x.com/ljupc0/status/1914923165739872399 Ljubomir Josifovski @ljupc0 Thanks, but I will not Luca. I spend time and expended effort to write the above. Used up some of my limited time on this Earth to inform and educate a perfect stranger, expecting nothing in return really. A stranger that I will never meet again, and will probably never have a chance to reciprocate. Nothing personal this--but I will cut my loss at this point. For writing to someone who refuses to read, is something I want to repeat with this very message at most once, but not more than once. Sorry the interaction didn't work out."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "cut my loss at this point. For writing to someone who refuses to read, is something I want to repeat with this very message at most once, but not more than once. Sorry the interaction didn't work out. Not your fault, no one is at fault--but blocked now. So I'm not tempted to interact again with in the future. (these days block is a read-only one side, so not a biggie tbh) 7:03 AM � Apr 23, 2025 [98]https://x.com/ljupc0/status/1914919687466455499 Ljubomir Josifovski @ljupc0 :-) Possible. This may tally with that - can't but notice that many Coke addicts are very very performant and achieve a lot in a short time. Some look manic energetic. Not good for one's health. But sure it does bring results--have seen it. On the other hand, sticking with a very pedestrian bit rate of 39 bps of human language forever, strikes me as a huge bottleneck in human communication. Don't mind increasing that, even if overclocking. Audio podcasts at x1.2 speed are almost the norm now for many. Depending on the speaker, I go up to x1.4 speed. So much stuff, so little time! 6:50 AM � Apr 23, 2025 [99]https://x.com/ljupc0/status/1914904173969068441 Ljubomir Josifovski @ljupc0 Of course - and expect nothing else in the future too. In my lifetime, AIDS stopped being a deadly disease, and no one noticed. I was ~20yo when I 1st heard about AIDS in the media. A positive test was a death sentence. Newspapers went on and on, with some celebrity dieing every week or month, for years."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "e, and no one noticed. I was ~20yo when I 1st heard about AIDS in the media. A positive test was a death sentence. Newspapers went on and on, with some celebrity dieing every week or month, for years. Innuendos, real and fake news, of \"are they or aren't they\" HIV positive, filled newspapers pages. For years. Then I noticed at some point - wait, we are not reading these stories any more. No celebrity had died of AIDS within my memory window. I dug up info - there was Internet by then. Found out the illness is treated, and turned Chronic, away from Acute. People suffer but they are not dieing en masse. There's a documentary \"In the time of a plague\" or similar about the drugs development. Nowadays I presume it's curred completely, can't be bothered to check now. Ulcers to the stomach also got cured in my lifetime. In my childhood, sometimes I'd overhear parents talk in grave voice \"So-and-rushed to an operating theatre to hospital, but couldn't be saved. Burst ulcer bleeding, tsk tsk. Such tragedy - family kids lost father etc\". Then in my adulthood I realised - I never hear of such tragedies anymore. Found out the saga of Helicobacter and the Australian doctor(s). That Ulcers are cured by antibiotics, and people don't typically die of burst ulcers anymore. So yeah - a deadly plague killing people left and right will be solved, problem deleted, an impossible feat achieved. After decades of effort, of trials and failures, by thousands of people across the globe."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "- a deadly plague killing people left and right will be solved, problem deleted, an impossible feat achieved. After decades of effort, of trials and failures, by thousands of people across the globe. Few people will notice. 5:48 AM � Apr 23, 2025 [100]https://x.com/ljupc0/status/1914777870519689335 Ljubomir Josifovski @ljupc0 YT is destroying Hollywood as well as TV productions for me. Watching them is boring and tedious. Takes ages for them to get to the point. It's like after listening to podcasts to x1.5 speed, going back to x0.9 speed. In the Mr Beast Productions Manual for new hires, that circulated online some time ago, Mr Beast writes about this phenomenon. That YT is higher intensity, higher SNR, than classical productions. And that's in his opinion will become bigger than all if them together are now. 9:26 PM � Apr 22, 2025 [101]https://x.com/ljupc0/status/1914749068749787508 Ljubomir Josifovski @ljupc0 Ofc. They are all wrong. And then at some point, become right. No one knows when, or even if, it will happen. However, we do know it will NOT happen, if we don't try. And we will not try, if we have no hope, that it might happen. 7:32 PM � Apr 22, 2025 [102]https://x.com/ljupc0/status/1914748068592771341 Ljubomir Josifovski @ljupc0 Somewhat popular opinion: don't blame others for your own failings. If you are busy, decide to be less busy, and then become so. Usually no one is holding a gun to anyone's head, and forcing them \"busy yourself, more, get more busy asap\"."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "others for your own failings. If you are busy, decide to be less busy, and then become so. Usually no one is holding a gun to anyone's head, and forcing them \"busy yourself, more, get more busy asap\". Less narcissist self-pity cr*p pls. 7:28 PM � Apr 22, 2025 [103]https://x.com/ljupc0/status/1914746476535652778 Ljubomir Josifovski @ljupc0 Would he be condemned in the UK public culture, ih he said so? \"I don't care much wither way. I have not got a strong opinion on this. As is my default stance in cases like this, I would like to make the least number of people the least unhappy, with any decision I make. I will make a decision only if forced to make one by events thrust onto me.\" Would that be a terrible slight on him as a leader? Condemn him in the eyes of the public as not worthy of leading? 7:21 PM � Apr 22, 2025 [104]https://x.com/ljupc0/status/1914743273379475718 Ljubomir Josifovski @ljupc0 When one reads \"How Innovation Works\" or \"Serendipity: Medical Discoveries in the 20th Century\" one gets to appreciate how much luck plays a role. Many breakthroughs are in parts very much accidental. We do disservice to young researchers there. When we write up a discovery, we usually start from the end, from the result, and mentally trace the path back to the start. Then we describe that one path we traced back, with words, from the start to the end. But that single forward path is an illusion! At discovery time, at the start, and along the path, there were many junctions."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "scribe that one path we traced back, with words, from the start to the end. But that single forward path is an illusion! At discovery time, at the start, and along the path, there were many junctions. Going forward, there is a tree of paths - never a single one. The ultimate path that worked, was one of many. It only exists when looking back, from the end result. Often it is not even clear why we decided to follow one fork, and not another. Usually there are arguments evidence for-, as well as against-, taking either path, in a fork on the road. (to the ultimate discovery.) Further, timing is everything. Kurzeil - now famously - stumbled on his Flops/USD log-linear exponential trying to figure how to time his Inventions/contraptions. (his words-I think?) Hinton probably had Sutskever-level PhD students in year 2003, or even 1993 too. There were no GPU-s capable of a leap then probably. Even when there were GPU-s, it took an Alex, in addition to one Ilya, to be at the same place, same time. (for comparison: in year 2000, a stellar guy in my lab training NN-s and writing ASR stack decoders, had a special Sparcstation bough with large chunk of project $$$, with then obscene unheard of I think 96 MB RAM; most users had computers with 2MB RAM from recollection) Research is a mugs game. Some Hinton colleagues probably died never seeing the success of their field. Assuming all their efforts a failure, a waste good for nothing. After devoting good chunk of their lives to it."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "mugs game. Some Hinton colleagues probably died never seeing the success of their field. Assuming all their efforts a failure, a waste good for nothing. After devoting good chunk of their lives to it. 7:09 PM � Apr 22, 2025 [105]https://x.com/ljupc0/status/1914730691020873927 Ljubomir Josifovski @ljupc0 Happened to me, python comments switched to Cyrillic. Presume RUS--but maybe other languages, dozens languages use Cyrillic variants. At other time it switched to French comments! Added in my INSTRUCTIONS md file \"write your comments in English\"--that fixed it, not repeated since. 6:19 PM � Apr 22, 2025 [106]https://x.com/ljupc0/status/1914722739539059103 Ljubomir Josifovski @ljupc0 � np - forgot to add, not 1, not 2, but 3(!) companies making progress also a huge +ve: when something works, it works for all! now this is one news item that brightened my day, ngl thanks :-) 5:47 PM � Apr 22, 2025 [107]https://x.com/ljupc0/status/1914711731781140635 Ljubomir Josifovski @ljupc0 Think @skdh nailed this one--this may work fr. Stellarators producing fields without needing insides current, continuous running, trading that for more complex magnets. Open sourcing too adds to confidence. \"Meet the Reactors Set to Upend Nuclear Fusion\" [108]https://www.youtube.com/watch?v=e7hw0aC1BbI Meet the Reactors Set to Upend Nuclear Fusion 5:03 PM � Apr 22, 2025 [109]https://x.com/ljupc0/status/1914704491162612195 Ljubomir Josifovski @ljupc0 Yes--the complaints are spot on."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "tch?v=e7hw0aC1BbI Meet the Reactors Set to Upend Nuclear Fusion 5:03 PM � Apr 22, 2025 [109]https://x.com/ljupc0/status/1914704491162612195 Ljubomir Josifovski @ljupc0 Yes--the complaints are spot on. Glad someone is motivated enough cares enough to react. (I don't) 1) Selective booking. Fine, bring on fringe guests if you must. But then at least include voices from both sides. Joe's \"crazies\" are almost exclusively pro-Russia or pro-Hamas. And it's not just about balance--even non-crazy prominent figures like Vitali Klitschko (boxing champ, now mayor of Kyiv) literally sent Joe a video invitation to appear and was ignored. Can't think of a more JRE-suited guest, frankly. That Joe refuses to platform any strong pro-Ukraine or pro-Israel voices speaks of his views. It's his right--but let us remember: the original fail mode of MSM was selective bias by omission. Joe, now a platform as big as MSM, deserves the same scrutiny. You can't call it \"open dialogue\" if only one side ever gets airing to millions. 2) Churchill the baddie. Joe and guests pushing Churchill conspiracies are--maybe without realising it, for they are forever ignorant even to click on a Wiki page--rehashing 1942 Goebbels propaganda (\"Churchill controlled by Jewish bankers,\" etc). In Europe, WW2 survivors are still alive (e.g. my mum). These takes aren't edgy--they're lazy."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "click on a Wiki page--rehashing 1942 Goebbels propaganda (\"Churchill controlled by Jewish bankers,\" etc). In Europe, WW2 survivors are still alive (e.g. my mum). These takes aren't edgy--they're lazy. Fr: it was Germany that built up its military in defiance of Versailles, annexed Austria, dismembered Czechoslovakia, and invaded Poland on Sept 1, 1939. Two weeks later, the Soviet Union invaded Poland from the east, pursuant to the Molotov-Ribbentrop Pact, a plan to divide Eastern Europe between Germany and the Soviet Union. Enslave and or exterminated millions of people. This is not disputed. We have mountains of evidence--hand written from the architects and executors(!), diaries, state memos, radio archives, press coverage. Everyone can read everything at all times, it's not Latin or ancient Greek. It's all super-accessible. Even the original archives. Joe is free to re-broadcast Goebbels-tier takes. But let's not pretend it's deep or brave. If he wants to test the strength of his opinions, let him say this stuff in CEEurope (ex-Russia), in Poland or the Baltics. I suspect he may catch more heat than he can handle. He's travelled a long way from the original \"savage Khan of the steppe curious about the skies asks medicine man to explain\" schtick, and something got lost along the way. Having written all that--I haven't watched JRE for many moons now. There's just better stuff on YT--more interesting, more entertaining, more informative."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "chtick, and something got lost along the way. Having written all that--I haven't watched JRE for many moons now. There's just better stuff on YT--more interesting, more entertaining, more informative. Many podcasters go to UKR-RUS, record, comment, interview etc. Much better to see IRL, even when selective. Whenever X references JRE, it's usually some B-tier cosplayer bores. Don't bother to check. That shi*t-chatting nonsense more entertaining elsewhere. 4:34 PM � Apr 22, 2025 [110]https://x.com/ljupc0/status/1914678555356647818 Ljubomir Josifovski @ljupc0 WFH=Garden office <1min from home bathed in natural sun among greenery, instead of dull office under florescent lights crackling menacingly. WFH=Personal space replacing open plan office crazy setup, where one wore heavy duty noise cancelling ear muffs to be able to think. WFH=Can wear trainers or uggs or anything comfortable I fancy. No more wearing gloves with open finger tips for typing in the winter. (the office was freezing.) Desktop=Not forced to use Windows anymore for 10h a day. Can use Ubuntu or MacOS or whatever I fancy. No need to deal with the local IT admin who answers \"Nein\" first thinks second b/c \"security\" always trumps everything, and including whether I can actually do my easier or harder. Apple Silicon=Macbook screen 10x better than old Lenovo one, battery life 2x better than old Lenovo."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "c \"security\" always trumps everything, and including whether I can actually do my easier or harder. Apple Silicon=Macbook screen 10x better than old Lenovo one, battery life 2x better than old Lenovo. (bonus fun: runs local LLM-s and other NN-s with 96 GB (V)RAM) Car=electric/hybrid wheezing in silence with only the wind swishing as if sailing. No one goes back to ICE, unless they have to. 2:51 PM � Apr 22, 2025 References 1. https://x.com/ljupc0/status/1916515727370039368 2. https://x.com/ljupc0/status/1916502607931089236 3. https://x.com/ljupc0/status/1916498356181885196 4. https://x.com/ljupc0/status/1916495176874201453 5. https://x.com/ljupc0/status/1916493156838678538 6. https://x.com/ljupc0/status/1916490975515398257 7. https://x.com/krishnanrohit/status/1915889596497596642?t=DCgg8xJPmO9JNXPqRsQ4CA&s=19 8. https://x.com/ljupc0/status/1916487109440856177 9. https://x.com/ljupc0/status/1916476103767499031 10. https://x.com/ljupc0/status/1916447941302120680 11. https://x.com/ljupc0/status/1916432268156953023 12. https://x.com/ljupc0/status/1916220610599911899 13. https://x.com/ljupc0/status/1916203709937132007 14. https://x.com/ljupc0/status/1916178863136137408 15. https://x.com/ljupc0/status/1916122307606577195 16. https://x.com/ljupc0/status/1916068828682879181 17. https://x.com/ljupc0/status/1916058257921081721 18. https://x.com/ljupc0/status/1916055252161253749 19. https://www.perplexity.ai/search/major-douglas-theory-of-social-tbMRrejzR5yk17WV_BLptg#0 20. https://x."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "/x.com/ljupc0/status/1916058257921081721 18. https://x.com/ljupc0/status/1916055252161253749 19. https://www.perplexity.ai/search/major-douglas-theory-of-social-tbMRrejzR5yk17WV_BLptg#0 20. https://x.com/ljupc0/status/1916048401696739755 21. https://x.com/ljupc0/status/1916037953542934631 22. https://x.com/ljupc0/status/1916033122954596359 23. https://x.com/ljupc0/status/1916029000683557156 24. https://x.com/ljupc0/status/1916022258230874560 25. https://x.com/ljupc0/status/1916007011759595743 26. https://x.com/ljupc0/status/1916004901957611642 27. https://x.com/ljupc0/status/1915999886450757909 28. https://x.com/ljupc0/status/1915824792369836413 29. https://x.com/ljupc0/status/1915784804173308292 30. https://x.com/ljupc0/status/1915783184408531182 31. https://x.com/ljupc0/status/1915764090766610713 32. https://x.com/ljupc0/status/1915759097971884182 33. https://x.com/ljupc0/status/1915700944555196458 34. https://x.com/ljupc0/status/1915692477928947848 35. https://x.com/ljupc0/status/1915664157212295631 36. https://x.com/ljupc0/status/1915660864985629044 37. https://x.com/ljupc0/status/1915656051459850655 38. https://x.com/ljupc0/status/1915653877438853474 39. https://x.com/ljupc0/status/1915650446091223127 40. https://x.com/ljupc0/status/1915534965007413415 41. https://x.com/ljupc0/status/1915517391884189858 42. https://x.com/ljupc0/status/1915467744251961660 43. https://x.com/ljupc0/status/1915375824103936107 44. https://x.com/ljupc0/status/1915466949548744903 45. https://x."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "c0/status/1915517391884189858 42. https://x.com/ljupc0/status/1915467744251961660 43. https://x.com/ljupc0/status/1915375824103936107 44. https://x.com/ljupc0/status/1915466949548744903 45. https://x.com/ljupc0/status/1915458377486131545 46. https://x.com/ljupc0/status/1915428410123309497 47. https://x.com/ljupc0/status/1915392232561004722 48. https://x.com/ljupc0/status/1915402043067400261 49. https://x.com/ljupc0/status/1915377581253124509 50. https://x.com/ljupc0/status/1915375824103936107 51. https://x.com/SkyNews/status/1915290778437398696 52. https://trib.al/kdXA2Dr 53. https://x.com/ljupc0/status/1915367211469857004 54. https://appblit.com/scribe?v=zzXyPGEtseI 55. https://x.com/ljupc0/status/1915359549495693425 56. https://x.com/ljupc0/status/1915326424111579247 57. https://x.com/ljupc0/status/1915322751708959192 58. https://x.com/ljupc0/status/1915319026516988095 59. https://x.com/ljupc0/status/1915307180250501535 60. https://x.com/ljupc0/status/1915171673772355908 61. https://youtu.be/3cVr2Qp_ic8?si=ewiW7HiKGnA805iG 62. https://x.com/ljupc0/status/1915163400746700916 63. https://x.com/ljupc0/status/1914590038777643230 64. https://x.com/ljupc0/status/1915161944220148045 65. https://x.com/ljupc0/status/1915159493463466189 66. https://x.com/ljupc0/status/1915150262014034117 67. https://x.com/ljupc0/status/1909917211902333192 68. https://x.com/ljupc0/status/1915145282641342485 69. https://aider.chat/docs/leaderboards/ 70. https://x."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "x.com/ljupc0/status/1915150262014034117 67. https://x.com/ljupc0/status/1909917211902333192 68. https://x.com/ljupc0/status/1915145282641342485 69. https://aider.chat/docs/leaderboards/ 70. https://x.com/ljupc0/status/1915141383272251497 71. https://x.com/ljupc0/status/1915132189940871328 72. https://x.com/ljupc0/status/1915124419195789782 73. https://x.com/ljupc0/status/1915111004783943894 74. https://x.com/ljupc0/status/1915110359842591143 75. https://x.com/ljupc0/status/1915105335460643260 76. https://x.com/ljupc0/status/1915093235279757810 77. https://x.com/ljupc0/status/1915072879252717936 78. https://x.com/ljupc0/status/1914988714918400043 79. https://x.com/ljupc0/status/1915061682885300255 80. https://x.com/ljupc0/status/1915022833295544630 81. https://x.com/ljupc0/status/1915017833945145643 82. https://x.com/ljupc0/status/1915007546437386582 83. https://x.com/ljupc0/status/1915002570390503829 84. https://x.com/ljupc0/status/1914988714918400043 85. https://x.com/ljupc0/status/1914984480630047040 86. https://x.com/ljupc0/status/1914979142702899338 87. https://x.com/ljupc0/status/1914364573987045770 88. https://x.com/search?q=(from%3Aljupc0)&src=typed_query&f=live 89. https://x.com/ljupc0/status/1914360619668549658 90. https://x.com/ljupc0/status/1914358764142313759 91. https://x.com/ljupc0/status/1914350979400540455 92. https://x.com/ljupc0/status/1914347939473809541 93. https://x.com/ljupc0/status/1914344034081898627 94. https://x."
    },
    {
      "source": "twitter-history-sample.html",
      "content": "c0/status/1914358764142313759 91. https://x.com/ljupc0/status/1914350979400540455 92. https://x.com/ljupc0/status/1914347939473809541 93. https://x.com/ljupc0/status/1914344034081898627 94. https://x.com/ljupc0/status/1914335893537632380 95. https://x.com/ljupc0/status/1914333084545073619 96. https://x.com/ljupc0/status/1914315862057660755 97. https://x.com/ljupc0/status/1914923165739872399 98. https://x.com/ljupc0/status/1914919687466455499 99. https://x.com/ljupc0/status/1914904173969068441 100. https://x.com/ljupc0/status/1914777870519689335 101. https://x.com/ljupc0/status/1914749068749787508 102. https://x.com/ljupc0/status/1914748068592771341 103. https://x.com/ljupc0/status/1914746476535652778 104. https://x.com/ljupc0/status/1914743273379475718 105. https://x.com/ljupc0/status/1914730691020873927 106. https://x.com/ljupc0/status/1914722739539059103 107. https://x.com/ljupc0/status/1914711731781140635 108. https://www.youtube.com/watch?v=e7hw0aC1BbI 109. https://x.com/ljupc0/status/1914704491162612195 110. https://x.com/ljupc0/status/1914678555356647818"
    },
    {
      "source": "twitter-history-sample2.html",
      "content": "X posts historic archive [1]https://x.com/ljupc0/status/1930615534279606282 Ljubomir Josifovski @ljupc0 On the sins of commission versus omission, this- https://x.com/dmitri_dolgov/status/1930337733719011608 A robot car video of a child, running after a dog, running after a ball, running in front of a car, in the darkness of the night. If this was a human driver, they would have run over the dog and the child likely. They Weymo robot car stopped in time. Potentially some lives saved there. How are ok with a human driver killing that child there, cutting their life short, scarring for life everyone that knew them, ruining the remainder of the lives of their parents. And for what?? To tend to our \"yuck reflex\" factor we humans exhibit as a 1st reaction, with anything new out whack with our prior experience? How can anyone justify that?? I remember in my childhood, the wide spread revulsion and scary newspapers and TV headlines, when the first IVF child was born. They were labelled \"kids from/of/by eprouvette\" by the press - as if grown in glass jars?? (and not merely allowing two cells to join outside of human body; an unremarkable event) The current AI scaremongering, recalls for me memories of the scary headlines, of the \"Frankenstein children\", that were going to be bad for them, but also bad to the rest of us humans somehow. Even if it was never quite articulated - only ever insinuated - the mechanism by which those terrible outcomes will meterialise. Fast forward 45yrs, 2."
    },
    {
      "source": "twitter-history-sample2.html",
      "content": ", but also bad to the rest of us humans somehow. Even if it was never quite articulated - only ever insinuated - the mechanism by which those terrible outcomes will meterialise. Fast forward 45yrs, 2.5% of babies in the US are born by IVF, and... - nothing! A nothing-burger. No one thinks much of it, all's fine. Quote [2]https://x.com/ljupc0/status/1930592013692408229 Ljubomir Josifovski @ljupc0 � 1h I always enjoy listening to Ray --and I especially loved the message he ended this interview with: \"Love is all you need.\" \"Ray Kurzweil with David S. Rose: The Singularity is Nearer\" https://youtube.com/watch?v=7oAlD3lMNXo I wish they had more time, and that the interviewer had been more Show more 3:19 PM � Jun 5, 2025 [3]https://x.com/ljupc0/status/1930592013692408229 Ljubomir Josifovski @ljupc0 I always enjoy listening to Ray --and I especially loved the message he ended this interview with: \"Love is all you need.\" \"Ray Kurzweil with David S. Rose: The Singularity is Nearer\" https://youtube.com/watch?v=7oAlD3lMNXo I wish they had more time, and that the interviewer had been more interactive, asking follow-up questions. Several times Ray dropped absolute bombshells, and yet the interviewer just... moved on?? No deeper inquiry, as if he missed the magnitude of what we'd just heard. Like the artificial kidney that could potentially save 95,000 people each year who currently can't get transplants."
    },
    {
      "source": "twitter-history-sample2.html",
      "content": "moved on?? No deeper inquiry, as if he missed the magnitude of what we'd just heard. Like the artificial kidney that could potentially save 95,000 people each year who currently can't get transplants. Or Waymo cars, now driving autonomously for months without killing anyone--even as human drivers might have caused around 50,000 deaths on US roads over a comparable timeframe (though admittedly over more total miles driven). The interviewer was otherwise excellent, but that particular lack of curiosity surprised me--so much so that it stuck in my mind. Maybe if we had accepted a robot driver earlier, even at the likely cost of perhaps 100 lives, we could have saved as many as half a million lives over a decade. Wouldn't we, as a society, have accepted those odds? Isn't it ethically clearer that killing 100 people through commission (robotic error) is significantly less morally troubling than allowing half a million deaths through omission (human driver error)? I find this lack of statistical compassion stunning. As if we're incapable of counting--or as if our moral instincts haven't caught up to reality. Perhaps Ray's hopeful message \"Love is all you need\" might truly resonate and succeed in the AI age, where it fell short in the pre-AI age? TBS stay tuned - time will tell. youtube.com Ray Kurzweil with David S. Rose: The Singularity is Nearer Join legendary futurist and inventor Ray Kurzweil for a conversation with tech visionary David S."
    },
    {
      "source": "twitter-history-sample2.html",
      "content": "BS stay tuned - time will tell. youtube.com Ray Kurzweil with David S. Rose: The Singularity is Nearer Join legendary futurist and inventor Ray Kurzweil for a conversation with tech visionary David S. Rose, exploring the implications of his new book, The Singu... 1:46 PM � Jun 5, 2025 [4]https://x.com/ljupc0/status/1930541706316861758 Ljubomir Josifovski @ljupc0 Sacks, bags filled with fluid, for growing babies outside human body, will solve that sooner rather than latter. So we relieve half of our whole species (the woman-folk half) of the barbaric practice of carrying in their belly a growing human for 9 months. (not unlike Alien as per the infamous movie.) I'm sorry to notice, but I really suspect this: if man-folk half of the species had to push out an object the size of a ball, from one of our orifices, while screaming in pain--well. Suspect this would have been achieved done already. Another black mark on humanity lacking compassion for the other. Still--better sooner than never. 10:26 AM � Jun 5, 2025 [5]https://x.com/ljupc0/status/1930538206262435895 Ljubomir Josifovski @ljupc0 If you liked that, you may like this interview by @numberphile https://youtube.com/watch?v=QNznD9hMEh0 I thought it was great, spirited and even funny at times. How he got fired on the spot by the general-boss-of-his-general-boss was a gem I thought. � The bits on quant trading are spot on afaics."
    },
    {
      "source": "twitter-history-sample2.html",
      "content": "MEh0 I thought it was great, spirited and even funny at times. How he got fired on the spot by the general-boss-of-his-general-boss was a gem I thought. � The bits on quant trading are spot on afaics. Having some experience there myself - he's not mystifying it, (that many do to my surprise; for reason unclear to me perpetuate the \"those who speak don't know; those who know don't speak\" bizarre omert� in the field; like - if you want to keep things secret, then just say nothing, don't B/S people) and in fact telling it like it is. Not one silver bullet, but a giant machine of 1000 smaller parts, each part polished to great precision in exquisite detail with tons of manual R&D. youtube.com Jim Simons (full length interview) - Numberphile Shorter version: https://youtu.be/gjVDqfUhXOYMore about The Simons Foundation: http://bit.ly/SimonsFoundationJames Harris Simons has been described as \"the w... 10:12 AM � Jun 5, 2025 [6]https://bsky.app/profile/ljupco.bsky.social/post/3kz4jb3jta22n Ljubomir Josifovski @ljupco.bsky.social This worked for me for finding people www.wikihow.com/Import-Twitt.... I used the Firefox version of \"Sky Follower Bridge\" extension. For whatever reason in the Extensions market place the text is in Japanese for me, but in operation it's English so could use it. www.wikihow."
    },
    {
      "source": "twitter-history-sample2.html",
      "content": "d the Firefox version of \"Sky Follower Bridge\" extension. For whatever reason in the Extensions market place the text is in Japanese for me, but in operation it's English so could use it. www.wikihow.com How To Import Twitter To Bluesky: Follows and Tweets Seamlessly move from Twitter to Bluesky with this handy guide If you're migrating from Twitter/X to Bluesky, the new platform can feel lonely until you find people to follow. Fortunately, there are to... Aug 7, 2024 at 09:36 [7]https://bsky.app/profile/ljupco.bsky.social/post/3kz4hwxh5q22w Ljubomir Josifovski @ljupco.bsky.social Atm using FF \"Sky Follower Bridge\" extension and seems to be working for me. :-) Doing as described in https://www.wikihow.com/Import-Twitter-to-Bluesky www.wikihow.com How To Import Twitter To Bluesky: Follows and Tweets Seamlessly move from Twitter to Bluesky with this handy guide If you're migrating from Twitter/X to Bluesky, the new platform can feel lonely until you find people to follow. Fortunately, there are to... Aug 7, 2024 at 09:12 -- LJ HPD Mon 25 Nov 01:29:11 GMT 2024 References 1. https://x.com/ljupc0/status/1930615534279606282 2. https://x.com/ljupc0/status/1930592013692408229 3. https://x.com/ljupc0/status/1930592013692408229 4. https://x.com/ljupc0/status/1930541706316861758 5. https://x.com/ljupc0/status/1930538206262435895 6. https://bsky.app/profile/ljupco.bsky.social/post/3kz4jb3jta22n 7. https://bsky.app/profile/ljupco.bsky.social/post/3kz4hwxh5q22w"
    },
    {
      "source": "twitter-history-sample2.html",
      "content": "x.com/ljupc0/status/1930538206262435895 6. https://bsky.app/profile/ljupco.bsky.social/post/3kz4jb3jta22n 7. https://bsky.app/profile/ljupco.bsky.social/post/3kz4hwxh5q22w"
    },
    {
      "source": "youtube-tasters.html",
      "content": "1. 24 Aug 16:17 [1]The_Futurists_1967_Scientists_Predict_The_21st_Century-YT-wPETzK YLkco-20250824.mp4 2. 18 Aug 23:59 [2]Andrej_Karpathy-Making_AI_accessible_with-Stephanie_Zhan-mar2024 -mar2024-YT-c3b-JASoPi0-20250818.mp4 3. 15 Aug 16:34 [3]Geoffrey_Hinton-The_2025_Martin_Lecture�Boltzmann_Machines-UoT-f eb2025-YT-juif0T8NOsY-20250815.mp4 4. 14 Aug 21:45 [4]Geoffrey_Hinton-The_Foundations_of_Deep_Learning-feb2018-YT-zl99 IZvW7rE-20250814.mp4 5. 12 Aug 23:09 [5]Why_Deep_Learning_Works_Unreasonably_Well-Welch_Labs-aug2025-YT- qx7hirqgfuU-20250812.mp4 6. 12 Aug 21:31 [6]Geoffrey_Hinton-Brains_Sex_and_Machine_Learning-Google_TechTalks -aug2012-YT-DleXA5ADG78-20250812.mp4 7. 12 Aug 17:44 [7]Geoffrey_Hinton-Revolutionizing_artificial_intelligence-Pieter_A bbeel-The_Robot_Brains-S2_E22-jun2022-YT-2EDP4v-9TUA-20250812.mp4 8. 12 Aug 17:26 [8]Geoffrey_Hinton-Twitter_QandA_with_Geoff_Hinton-Pieter_Abbeel-Th e_Robot_Brains-S2_E23-jun2022-YT-4Otcau-C_Yc.mp4 9. 11 Aug 21:59 [9]Fantastic_KL_Divergence_and_How_to_Actually_Compute_It-may2025-Y T-tXE23653JrU-20250811.mp4 10. 11 Aug 20:23 [10]AlphaFold-The_Most_Useful_Thing_AI_Has_Ever_Done-Veritasium-feb 2025-YT-P_fHJIYENdI-20250811.mp4 11. 9 Aug 00:07 [11]The_END_of_RL_GEPA-NEW_Genetic_AI_MIT_UC_Berkeley-jul2025-YT-o6 RbVPFOslg-20250809.mp4 12. 8 Aug 04:17 [12]Scott_Aaronson-On_The_Race_To_AGI_and_Quantum_Supremacy-Win-Win _Liv_Boeree-dec2024-YT-ANFnUHcYza0-20250808.mp4 13."
    },
    {
      "source": "youtube-tasters.html",
      "content": "_Genetic_AI_MIT_UC_Berkeley-jul2025-YT-o6 RbVPFOslg-20250809.mp4 12. 8 Aug 04:17 [12]Scott_Aaronson-On_The_Race_To_AGI_and_Quantum_Supremacy-Win-Win _Liv_Boeree-dec2024-YT-ANFnUHcYza0-20250808.mp4 13. 7 Aug 07:45 [13]turns-out-musk-not-so-crazy-after-all-YT-ewnbQrEA8LE-20250807.m p4 14. 3 Aug 09:21 isaac-asimov-morality-without-god.mp4 15. 31 Jul 14:37 [14]Scott_Aaronson-Computational_Complexity_and_Consciousness-Lex_F ridman_Podcast_#130-oct2020-YT-nAMjv0NAESM-20250731.mp4 16. 28 Jul 15:56 [15]Ray_Kurzweil-The_Last_6_Decades_of_AI_-_TED-20240803.mp4 17. 26 Jul 12:47 [16]The_Declaration_of_Independence_of_Cyberspace-John_Perry_Barlow -1996-dec2014-YT-3WS9DhSIWR0-20250726.mp4 18. 25 Jul 23:53 [17]Lets_master_Context_Engineering_with_DSPy-neural_avb-jul2025-YT -5Bym0ffALaU-20250725.mp4 19. 23 Jul 10:17 [18]Geoffrey_Hinton-Will_AI_outsmart_human_intelligence-TheRoyalIns titution-jul2025-YT-IkdziSLYzHw-20250723.mp4 20. 23 Jul 07:20 ilya-sutskever-greatest-challenge-greatest-reward-ai-do-all-you-can -do-brain-bio-comp-thats-why-toronto-jun2025.mp4 21. 20 Jul 18:40 [19]Michael_Levin-Joscha_Bach-Why_Neuroscience_Got_Everything_Backw ards-ToE-jul2025-YT-DaP0wFJJJ4Y-20250720.mp4 22. 20 Jul 16:45 musk-production-1_requirements-2_delete-3_simplify_optimize-4_accel erate_cycle-5_automate.mp4 23. 17 Jul 11:37 [20]Geoffrey_Hinton-Frontiers_of_AI_Insights_from_a_Nobel_Laureate- Toronto_Tech_Week-jul2025-YT-S4Tz7d0QOeE-20250717.mp4 24."
    },
    {
      "source": "youtube-tasters.html",
      "content": "te-3_simplify_optimize-4_accel erate_cycle-5_automate.mp4 23. 17 Jul 11:37 [20]Geoffrey_Hinton-Frontiers_of_AI_Insights_from_a_Nobel_Laureate- Toronto_Tech_Week-jul2025-YT-S4Tz7d0QOeE-20250717.mp4 24. 17 Jul 08:24 dorsey-edit_the_team-data_analytics_dashboard.mp4 25. 5 Jul 22:12 [21]Richard_Sutton-The_Era_of_Experience_and_The_Age_of_Design-Amil -Upper_Bound-jun2025-YT-FLOL2f4iHKA-20250705.mp4 26. 2 Jul 10:57 ilya-sutskever-talking-machines-jan2026.mp3 27. 2 Jul 01:17 [22]Ilya_Sutskever-Unimaginable_Unpredictable_Future_Driven_By_AI_A dvancements-jun2025-YT-A2qe5OPLlmY-20250702.mp4 28. 30 Jun 20:58 [23]Yuval_Harari-AI_and_the_paradox_of_trust-jun2025-YT-8GaW36EfidI -20250701.mp4 29. 25 Jun 21:17 [24]Score-based_Diffusion_Models-Generative_AI_Animated-Deepia-jun2 025-YT-lUljxdkolK8-20250629.mp4 30. 23 Jun 02:19 [25]levelsio-Effective_Accelerationism_E_acc_Trailer-apr2024-YT-CdC mYqih3YQ-20250623.mp4 31. 22 Jun 17:19 levelsio-future-of-humanity-e-acc.mp4 32. 19 Jun 10:17 [26]Andrej_Karpathy-Software_Is_Changing_Again-Y_Combinator-jun2025 -YT-LCEmiRjPEtQ-20250619.mp4 33. 17 Jun 16:17 [27]Joscha_Bach-Self_Models_of_Loving_Grace-38C3-dec2024-Top1-YT-Wi ZjWadqSUo-20250617.mp4 34. 16 Jun 22:02 [28]Geoffrey_Hinton-Tried_to_Warn_Them_But_We_ve_Already_Lost_Contr ol-The_Diary_Of_A_CEO-jun2025-YT-giT0ytynSqg-20250616.mp4 35. 16 Jun 21:55 carl-sagan-unveils-the-pale-blue-dot-image.mp4 36."
    },
    {
      "source": "youtube-tasters.html",
      "content": "22:02 [28]Geoffrey_Hinton-Tried_to_Warn_Them_But_We_ve_Already_Lost_Contr ol-The_Diary_Of_A_CEO-jun2025-YT-giT0ytynSqg-20250616.mp4 35. 16 Jun 21:55 carl-sagan-unveils-the-pale-blue-dot-image.mp4 36. 13 Jun 08:50 [29]How_DeepSeek_Rewrote_the_Transformer_MLA-Welch_Labs-mar2025-YT- 0VLAoVGf_74-20250613.mp4 37. 12 Jun 18:16 [30]AlphaGo-The_Movie-documentary-Google_DeepMind-YT-WXuK6gekU1Y-20 250612.mp4 38. 10 Jun 14:12 [31]Ilya_Sutskever-brain_is_a_biological_computer-20250610.mp4 39. 6 Jun 14:40 [32]Ray_Kurzweil-The_Singularity_is_Nearer-David_Rose-jun2025-YT-7o AlD3lMNXo-20250606.mp4 40. 6 Jun 12:02 [33]Demis_Hassabis_on_The_Future_of_Knowledge-Institute_for_Advance d_Study-jun2025-YT-TgS0nFeYul8-20250606.mp4 41. 2 Jun 01:42 [34]Joscha_Bach-Lou_de_K-Existential_Hope-CIMC_Machine_Consciousnes s_Salon-may2025-YT-VDhan5Kd3ms-20250601.mp4 42. 29 May 11:34 [35]karpathy-new_comp_ai_os-projects_depth-snowball_serendipity-ope nai-10Khours-tesla-publish-inspire-ucb_ai_hackaton-2024.mp4 43. 28 May 23:52 [36]Rick_Rubin_Vibe_Coding_is_the_Punk_Rock_of_Software-a16z-The_Be n_and_Marc_Show-may2025-YT-7s9C92Pkcc0-20250529.mp4 44. 28 May 20:41 rick-rubin-vibe-coding-is-the-punk-rock-of-coding.mp4 45. 28 May 13:10 jobs-business-no-one-says-it-folklore-ask-q-dig-in-will-learn.mp4 46. 26 May 23:13 [37]RL-Policy_Gradient_Methods-6of6-Mutual_Information-may2023-YT-e 20EY4tFC_Q-20250526.mp4 47. 26 May 23:09 [38]RL-Function_Approximation-5of6-Mutual_Information-jan2023-YT-Vk y0WVh_FSk-20250526."
    },
    {
      "source": "youtube-tasters.html",
      "content": ":13 [37]RL-Policy_Gradient_Methods-6of6-Mutual_Information-may2023-YT-e 20EY4tFC_Q-20250526.mp4 47. 26 May 23:09 [38]RL-Function_Approximation-5of6-Mutual_Information-jan2023-YT-Vk y0WVh_FSk-20250526.mp4 48. 23 May 11:41 [39]Kilian_Weinberger-Advancing_Diffusion_Models_for_Text_Generatio n-Simons_Institute-apr2025-YT-klW65MWJ1PY-20250524.mp4 49. 16 May 19:46 [40]Joscha_Bach-Are_ChatGPT_and_Claude_Conscious-BuzzRobot-apr2025- YT-iyEFLKnNWAM-20250516.mp4 50. 16 May 17:04 a16z-time-to-build-2025.mp4 51. 15 May 17:32 [41]Jeff_Clune-Dont_invent_faster_horses-MLStreetTalk-jan2025-YT-mw 5WIDGRLnA-20250515.mp4 52. 14 May 23:04 flatland-carl-sagan-explains-4d.mp4 53. 14 May 06:47 [42]Joscha_Bach-Why_consciousness_is_software_and_software_is_spiri t-The_Institute_of_Art_and_Ideas-mar2025-YT-E361FZ_50oo-20250514.mp 4 54. 12 May 00:45 [43]Ray_Kurzweil-Future_of_Intelligence-MIT_6.S099_AGI-feb2018-YT-9 Z06rY3uvGY-20250512.mp4 55. 9 May 23:49 [44]Joscha_Bach-This_is_the_dawn_of_machine_consciousness-The_Insti tute_of_Art_and_Ideas-may2025-YT-Y1QOf6HEbHQ-20250510.mp4 56. 5 May 20:30 [45]Curt_Jaimungal-Demystifying_Gödels_Theorem_What_It_Actually_Sa ys-ToE-may2025-YT-OH-ybecvuEo-20250505.mp4 57. 1 May 10:02 [46]Yuval_Noah_Harari-The_AI_Revolution_Is_an_Alien_Invasion-apr202 5-YT-zZYCmyaTY3Y-20250501.mp4 58. 1 May 09:57 [47]Yuval_Noah_Harari-What_should_we_do_when_non-human_intelligence _threatens_our_existence-Babel-mar2025-YT-qB_DLoBTLMI-20250501.mp4 59."
    },
    {
      "source": "youtube-tasters.html",
      "content": "Invasion-apr202 5-YT-zZYCmyaTY3Y-20250501.mp4 58. 1 May 09:57 [47]Yuval_Noah_Harari-What_should_we_do_when_non-human_intelligence _threatens_our_existence-Babel-mar2025-YT-qB_DLoBTLMI-20250501.mp4 59. 24 Apr 22:56 [48]RL-Temporal_Difference_Learning_including_Q-Learning-4of6-Mutua l_Information-oct2022-YT-AJiG3ykOxmY-20250424.mp4 60. 20 Apr 07:33 [49]David_Silver-Is_Human_Data_Enough-Google_DeepMind-apr2025-YT-zz XyPGEtseI-20250420.mp4 61. 2 Apr 21:19 [50]Ray_Kurzweil-All_My_Predictions_Have_Come_True_So_Far-Time100-m ar2025-YT-gGEu_5KbVe8.mp4 62. 31 Mar 07:41 [51]RL-Monte_Carlo_And_Off-Policy_Methods-3of6-Mutual_Information-o ct2022-YT-bpUszPiWM7o-20250421.mp4 63. 30 Mar 20:53 [52]RL-Bellman_Equations_Dynamic_Programming_Generalized_Policy_Ite ration-2of6-Mutual_Information-oct2022-YT-_j6pvGEchWU-20250330.mp4 64. 30 Mar 20:30 [53]RL-Reinforcement_Learning_by_the_Book-1of6-Mutual_Information-o ct2022-YT-NFo9v_yKQXA-20250330.mp4 65. 16 Mar 00:01 [54]Vlatko_Vedral-Decoding_quantum_reality-mar2025-YT-70FhS6NAbuA-2 0250315.mp4 66. 15 Mar 19:06 [55]Andrej_Karpathy-How_I_use_LLMs-feb2025-YT-EWvNQjAaOHw-20250315. mp4 67. 15 Mar 18:12 [56]Andrej_Karpathy-Deep_Dive_into_LLMs_like_ChatGPT-feb2025-YT-7xT GNNLPyMI-20250315.mp4 68. 15 Mar 18:01 [57]Andrej_Karpathy-The_spelled-out_intro_to_neural_networks_and_ba ckpropagation_building_micrograd-aug2022-YT-VMj-3S1tku0-20250315.mp 4 69. 13 Mar 19:36 richard-sutton-intelligence-cooperation-and-human-flourishing-prg-a i-mar2025.mp4 70."
    },
    {
      "source": "youtube-tasters.html",
      "content": "o_to_neural_networks_and_ba ckpropagation_building_micrograd-aug2022-YT-VMj-3S1tku0-20250315.mp 4 69. 13 Mar 19:36 richard-sutton-intelligence-cooperation-and-human-flourishing-prg-a i-mar2025.mp4 70. 13 Mar 12:57 [58]Richard_Sutton-Intelligence_Cooperation_and_Human_Flourishing-m ar2025-YT-bGi1jKL4OEo-20250313.mp4 71. 11 Mar 15:06 [59]Random_vectors_in_high_dimensions_are_nearly_orthogonal-Sina_To otoonian-feb2024-YT-k4CxJLXc3-0-20250311.mp4 72. 10 Mar 09:25 [60]Richard_Sutton-A_Perspective_on_Intelligence-PTJC_20th_Annivers ary-mar2025-YT-w177Ov-Y3gc-20250310.mp4 73. 7 Mar 18:17 [61]Karl_Friston-Free_Energy_Principle-The_Most_INTENSE_Theory_of_R eality-Curt_Jaimungal-ToE-aug2024-YT-uk4NZorRjCo-20250307.mp4 74. 5 Mar 08:17 lfg-build-the-future.mp4 75. 1 Mar 22:59 [62]jobs-microsoft-have-no-taste.mp4 76. 23 Feb 2025 [63]Joscha_Bach-Consciousness_AI_and_the_pattern_of_reality-jan2025 -YT-_hSdi9K__nw-20250223.mp4 77. 19 Feb 2025 [64]Noam_Brown-AI_Wont_Plateau�if_We_Give_It_Time_To_Think-TED-feb2 025-YT-MG9oqntiJKg-20250219.mp4 78. 17 Feb 2025 [65]Veritasium-The_Most_Useful_Thing_AI_Has_Done-DeepMind-AlphaFold -feb2025-YT-P_fHJIYENdI-20250214.mp4 79. 16 Feb 2025 [66]EVERY_term_in_DeepSeek_R1s_GRPO_explained_with_examples_and_exe rcises-RL_Foundations-DepthFirst-feb2025-YT-mXWiDU9-fOk-20250629.mp 4 80. 14 Feb 2025 [67]William_Hahn-Top_AI_Scientist_Unifies_Wolfram_Leibniz_and_Consc iousness-TOE-Curt_Jaimungal-feb2025-YT-3fkg0uTA3qU-20250214.mp4 81."
    },
    {
      "source": "youtube-tasters.html",
      "content": "-DepthFirst-feb2025-YT-mXWiDU9-fOk-20250629.mp 4 80. 14 Feb 2025 [67]William_Hahn-Top_AI_Scientist_Unifies_Wolfram_Leibniz_and_Consc iousness-TOE-Curt_Jaimungal-feb2025-YT-3fkg0uTA3qU-20250214.mp4 81. 14 Feb 2025 [68]rick-rubin-confidence-in-my-taste-ability-to-express-what-I-fee l.mp4 82. 4 Feb 2025 [69]Geoffrey_Hinton-Godfather_of_AI_predicts_it_will_take_over_the_ world-Andrew_Marr-LBC-feb2025-YT-vxkBE23zDmQ-20250204.mp4 83. 31 Jan 2025 mr-altman-deepseek-new-model-wolfs-lair.mp4 84. 27 Jan 2025 [70]Marc_Andreessen-Trump_Power_Tech_AI_Immigration_and_Future_of_A merica-Lex_Fridman_Podcast_458-YT-OHWnPOKh_S0-20250127.mp4 85. 23 Jan 2025 [71]Julia_Galef-Why_scout_mindset_is_crucial_to_good_judgment-TEDxP SU-apr2016-YT-3MYEtQ5Zdn8-20250124.mp4 86. 20 Jan 2025 [72]Ilya_Sutskever-Opening_Remarks_Confronting_the_Possibility_of_A GI-sep2023-YT-OPZxs6IXH00-20250120.mp4 87. 18 Jan 2025 [73]Geoffrey_Hinton-Why_The_Godfather_of_AI_Now_Fears_His_Own_Creat ion-Curt_Jaimungal-TOE-jan2025-YT-b_DUft-BdIE-20250118.mp4 88. 5 Jan 2025 [74]3Blue1Brown-Neural_Networks-Large_Language_Models_explained_bri efly-DL4p5.mp4 89. 28 Dec 2024 [75]Jeff_Clune-Open_Ended_and_AI_Generating_Algorithms_in_the_Era_o f_Foundation_Models-dec2024-YT-05ZwhL_CxO8-20250629.mp4 90. 14 Dec 2024 [76]Ilya_Sutskever-Sequence_to_sequence_learning_with_neural_networ ks_what_a_decade-NeurIPS-2024-20241218.mp4 91."
    },
    {
      "source": "youtube-tasters.html",
      "content": "_in_the_Era_o f_Foundation_Models-dec2024-YT-05ZwhL_CxO8-20250629.mp4 90. 14 Dec 2024 [76]Ilya_Sutskever-Sequence_to_sequence_learning_with_neural_networ ks_what_a_decade-NeurIPS-2024-20241218.mp4 91. 11 Dec 2024 [77]Ilya_Sutskever-Meta_Learning_and_Self_Play-MIT_Artificial_Gener al_Intelligence-Apr-2018-YT-9EN_HoEk3KY-20241211.mp4 92. 11 Dec 2024 [78]Ilya_Sutskever-Sequence_to_Sequence_Learning_with_Neural_Networ ks-Aug-2016-NIPS-Oral_Session_4-YT-uyXE7dY5H0-20241211.mp4 93. 5 Dec 2024 [79]Grant_Sanderson-Visualizing_transformers_and_attention-TNG_Big_ Tech_Day_2024-YT-KJtZARuO3JY-20241205.mp4 94. 4 Dec 2024 [80]spacex-schopsticks-4-Dec-2024.mp4 95. 2 Dec 2024 [81]Geoffrey_Hinton-Will_Digital_Intelligence_Replace_Biological_In telligence-Vector_Institute-dec2024-YT-Es6yuMlyfPw-20241204.mp4 96. 20 Nov 2024 hinton-language-crazy-million-parameters-just-stupid.mp4 97. 20 Nov 2024 [82]hinton-100M-parameters-just-stupid.mp4 98. 19 Nov 2024 move-arm-with-brain-nov-2024.mp4 99. 19 Nov 2024 move-arm-with-brain-nov-2024-2024-11-19_11.00.35.mkv 100. 14 Nov 2024 [83]Alec_Radford-OpenAI-L11_Language_Models-Deep_Unsupervised_Learn ing_SP20-apr2020-YT-BnpB3GrpsfM-20241114.mp4 101. 13 Nov 2024 [84]Noam_Brown-OpenAI-Parables_on_the_Power_of_Planning_in_AI_From_ Poker_to_Diplomacy-may2024-YT-eaAonE58sLU-20241113.mp4 102. 13 Nov 2024 [85]Noam_Brown-Learning_to_Reason_with_LLMs-sep2024-YT-Gr_eYXdHFis- 20241113.mp4 103. 12 Nov 2024 eye-size-distance-cosmos-atom.mp4 104."
    },
    {
      "source": "youtube-tasters.html",
      "content": "_Diplomacy-may2024-YT-eaAonE58sLU-20241113.mp4 102. 13 Nov 2024 [85]Noam_Brown-Learning_to_Reason_with_LLMs-sep2024-YT-Gr_eYXdHFis- 20241113.mp4 103. 12 Nov 2024 eye-size-distance-cosmos-atom.mp4 104. 7 Nov 2024 chopsticks-catch-rocket-oct-2024.mp4 105. 3 Nov 2024 [86]twittervid.com_Rainmaker1973_195d6e_chopsticks_catch_rocket-202 41103.mp4 106. 2 Nov 2024 bovie-internet-is-alien-life-bbc-newsnight-1999.mp4 107. 30 Oct 2024 [87]Geoffrey_Hinton-Godfather_of_AI_Sees_AI_Taking_Jobs_Due_to_Unma tched_Human_Productivity-YT-d_XikFthQ5I-20241030.mp4 108. 30 Oct 2024 [88]Geoffrey_Hinton-Possible_End_of_Humanity_from_AI-MIT_Technology _Reviews_EmTech_Digital-may2023-YT-sitHS6UDMJc-20241030.mp4 109. 30 Oct 2024 [89]Bret_Victor_-_Inventing_on_Principle-2012-YT-PUv66718DII-202410 30.mp4 110. 29 Oct 2024 [90]Ilya_Sutskever_-_An_Observation_on_Generalization_-_Simons_Inst itute-Aug2023_-_YT-AKMuA_TVz3A-20241029.mp4 111. 29 Oct 2024 [91]Jim_Simons_full_interview_Numberphile-may2015-YT-QNznD9hMEh0-20 241029.mp4 112. 29 Oct 2024 jim-simons-market-anomalies-quant-trading-may2015-20241029.mp4 113. 28 Oct 2024 [92]Jensen-greatness-comes-from-character-comes-from-suffering-2024 1028.mp4 114. 26 Oct 2024 hinton-they-think-just-like-us-20241026.mp4 115. 26 Oct 2024 [93]Ilya_Sutskever_Jensen_Huang_predicting_next_word_correct_is_und erstanding_detective_novel-Mar-2023-YT-GI4Tpi48DlA-20241026.mp4 116. 23 Oct 2024 [94]Joscha_Bach-Why_Your_Thoughts_Arent_Yours-MLStreetTalk-YT-3MkJE GE9GRY-20241022."
    },
    {
      "source": "youtube-tasters.html",
      "content": "redicting_next_word_correct_is_und erstanding_detective_novel-Mar-2023-YT-GI4Tpi48DlA-20241026.mp4 116. 23 Oct 2024 [94]Joscha_Bach-Why_Your_Thoughts_Arent_Yours-MLStreetTalk-YT-3MkJE GE9GRY-20241022.mp4 117. 21 Oct 2024 [95]The_Breakthrough_Behind_Modern_AI_Image_Generators-Diffusion_Mo dels-DepthFirst-oct2024-YT-1pgiu--4W3I-20250629.mp4 118. 19 Oct 2024 [96]Interview_with_Warren_McCulloch_neurologist-YT-wawMjJUCMVw-Sep- 2024.mp4 119. 9 Oct 2024 [97]Steve_Jobs-Role_of_product_and_marketing_people-YT-P4VBqTViEx4- 20241009.mp4 120. 9 Oct 2024 [98]Brian_Cox-The_only_interesting_question_in_phylosophy-TW-Rainma ker1973-1843902288190156853-20241009.mp4 121. 7 Oct 2024 [99]Sabine_Hossenfelder_-_Why_is_quantum_mechanics_non-local_I_wish _someone_had_told_me_this_20_years_ago_-_YT-hpkgPJo_z6Y-20241007.mp 4 122. 2 Oct 2024 [100]Yuval_Harari_-_Social_media_publisher_algofeed_editor_no_megap hone_no_censorship-20241002.mp4 123. 2 Oct 2024 [101]Yuval_Harari_-_AI_bots_id_freedom_of_speech_is_human_not_bot_r ight-20241002.mp4 124. 22 Sep 2024 [102]Hyung_Won_Chung-OpenAI-Dont_teach._Incentivize-MIT_EI_seminar- sep2024-YT-kYWUEV_e2ss-20250717.mp4 125. 3 Sep 2024 [103]Burke_-_The_Greatest_Shot_in_Television.mp4 126. 1 Sep 2024 [104]3Blue1Brown-Neural_Networks-How_might_LLMs_store_facts-DL7.mp4 127. 1 Sep 2024 [105]3Blue1Brown-Neural_Networks-But_what_is_a_GPT_Visual_intro_to_ transformers-DL5.mp4 128. 1 Sep 2024 [106]3Blue1Brown-Neural_Networks-Backpropagation_calculus-DL4.mp4 129."
    },
    {
      "source": "youtube-tasters.html",
      "content": "-DL7.mp4 127. 1 Sep 2024 [105]3Blue1Brown-Neural_Networks-But_what_is_a_GPT_Visual_intro_to_ transformers-DL5.mp4 128. 1 Sep 2024 [106]3Blue1Brown-Neural_Networks-Backpropagation_calculus-DL4.mp4 129. 1 Sep 2024 [107]3Blue1Brown-Neural_Networks-What_is_backpropagation_really_doi ng-DL3.mp4 130. 1 Sep 2024 [108]3Blue1Brown-Neural_Networks-Gradient_descent_how_neural_networ ks_learn-DL2.mp4 131. 1 Sep 2024 [109]3Blue1Brown-Neural_Networks-But_what_is_a_neural_network-DL1.m p4 132. 30 Aug 2024 [110]Sebastian_Thrun_-_Winning_The_DARPA_Grand_Challenge_-_GoogleTa lksArchive-Aug-2006.mp4 133. 24 Aug 2024 [111]Geoffrey_Hinton-Will_digital_intelligence_replace_biological_i ntelligence-Romanes_Lecture-Oxford-feb2024-YT-N1TEjTeQeg0-20240824. mp4 134. 21 Aug 2024 [112]Joscha_Bach-We_Are_All_Software-Machine_Learning_Street_Talk-2 0240821.mp4 135. 14 Aug 2024 [113]3Blue1Brown-Neural_Networks-Attention_in_transformers_visually _explained-DL6.mp4 136. 8 Aug 2024 [114]Penrose_-_Artificial_Intelligence_and_its_Limits.mp4 137. 22 Jul 2024 [115]Noah_Goodman-Reasoning_in_Human_and_Machine_Intelligence-AGI-2 3-SingularityNET-jul2023-YT-LI1eJxEZPNI-20250720.mp4 138. 23 Jun 2024 [116]Ray_Kurzweil-Transcendent_Man-Digital_Frontiers-VoA-mar2011-YT -EkGUEaZtIK4-20250712.mp4 139. 11 Jun 2024 [117]Jeff_Clune-Open-Endedness_and_AI_GAs_in_the_Era_of_Foundation_ Models-Oxford-jun2024-YT-Q3XKw2mp1Fk-20250629.mp4 140. 23 May 2024 [118]Garys_Economics_-_Game_Theory_is_Broken.mp4 141."
    },
    {
      "source": "youtube-tasters.html",
      "content": "11 Jun 2024 [117]Jeff_Clune-Open-Endedness_and_AI_GAs_in_the_Era_of_Foundation_ Models-Oxford-jun2024-YT-Q3XKw2mp1Fk-20250629.mp4 140. 23 May 2024 [118]Garys_Economics_-_Game_Theory_is_Broken.mp4 141. 23 May 2024 [119]Veritasium_-_What_Game_Theory_Reveals_About_Life_The_Universe_ and_Everything.mp4 142. 18 May 2024 [120]Geoffrey_Hinton-In_conversation_with_Joel_Hellermark-Strange_L oop-may2024-YT-n4IQOBka8bc-20240518.mp4 143. 10 May 2024 [121]Jacob_Bronowski_-_Ascent_Of_Man_ep11_-_Knowledge_Or_Certainty. mp4 144. 19 Apr 2024 [122]Physics_with_Elliot_-_To_Understand_the_Fourier_Transform_Star t_From_Quantum_Mechanics.mp4 145. 1 Apr 2024 [123]The_Multiverse_is_REAL_-_David_Deutsch-Alex_OConnor.mp4 146. 15 Mar 2024 [124]Noah_Goodman-The_Probabilistic_Language_of_Thought-UCI_Media-a ug2014-YT-239aVfgJ-0E-20250727.mp4 147. 6 Mar 2024 [125]Kenneth_Stanley-Creativity_and_Serendipity-MLST-feb2024-YT-pWI rXN-yy8g-20250702.mp4 148. 19 Feb 2024 [126]Geoffrey_Hinton-Will_digital_intelligence_replace_biological_i ntelligence-YT-iHCeAotHZa4-20240219.mp4 149. 15 Feb 2024 [127]Geoffrey_Hinton-Large_Language_Models_in_Medicine_They_Underst and_and_Have_Empathy-Eric_Topol-Ground_Truths-jun2024-YT-UnELdZdyNa E-20240215.mp4 150. 13 Dec 2023 [128]CGP_Grey_-_Fable_of_the_Dragon-Tyrant_-_Nick_Bostrom.mp4 151. 13 Jul 2023 [129]Geoffrey_Hinton-Two_Paths_to_Intelligence-CSER_Cambridge-jun20 23-YT-rGgGOccMEiY-20230713.mp4 152."
    },
    {
      "source": "youtube-tasters.html",
      "content": "150. 13 Dec 2023 [128]CGP_Grey_-_Fable_of_the_Dragon-Tyrant_-_Nick_Bostrom.mp4 151. 13 Jul 2023 [129]Geoffrey_Hinton-Two_Paths_to_Intelligence-CSER_Cambridge-jun20 23-YT-rGgGOccMEiY-20230713.mp4 152. 20 Jun 2023 [130]Sabine_Hossenfelder_-_Understanding_Quantum_Mechanics_4_Its_no t_so_difficult_-_ctXDXABJRtg.mp4 153. 17 Jun 2023 [131]Sabine_Hossenfelder_-_The_Uncertainty_Principle_What_Does_It_M ean_How_Does_It_Work_-_qC0UWxgyDD0.mp4 154. 17 Jun 2023 [132]Sabine_Hossenfelder_-_I_dont_believe_the_2nd_law_of_thermodyna mics_The_most_uplifting_video_Ill_ever_make_-_89Mq6gmPo0s.mp4 155. 16 May 2023 [133]Geoffrey_Hinton-Geoff_Hinton_quits_Google_to_warn_of_AI_risks- Pieter_Abbeel-The_Robot_Brains-S3_E9-may2023-YT-rLG68k2blOc-2023051 6.mp4 156. 28 Apr 2023 [134]Ray_Bradbury-Day_at_Night-1974.mp4 157. 28 Apr 2023 [135]Jaron_Lanier-Who_Owns_the_Future-2013.mp4 References 1. https://www.youtube.com/watch?v=wPETzKYLkco 2. https://www.youtube.com/watch?v=c3b-JASoPi0 3. https://www.youtube.com/watch?v=juif0T8NOsY 4. https://www.youtube.com/watch?v=zl99IZvW7rE 5. https://www.youtube.com/watch?v=qx7hirqgfuU 6. https://www.youtube.com/watch?v=DleXA5ADG78 7. https://www.youtube.com/watch?v=2EDP4v-9TUA 8. https://www.youtube.com/watch?v=4Otcau-C_Yc 9. https://www.youtube.com/watch?v=tXE23653JrU 10. https://www.youtube.com/watch?v=P_fHJIYENdI 11. https://www.youtube.com/watch?v=o6RbVPFOslg 12. https://www.youtube.com/watch?v=ANFnUHcYza0 13. https://www.youtube.com/watch?v=ewnbQrEA8LE 14."
    },
    {
      "source": "youtube-tasters.html",
      "content": "3JrU 10. https://www.youtube.com/watch?v=P_fHJIYENdI 11. https://www.youtube.com/watch?v=o6RbVPFOslg 12. https://www.youtube.com/watch?v=ANFnUHcYza0 13. https://www.youtube.com/watch?v=ewnbQrEA8LE 14. https://www.youtube.com/watch?v=nAMjv0NAESM 15. https://www.youtube.com/watch?v=uEztHu4NHrs 16. https://www.youtube.com/watch?v=3WS9DhSIWR0 17. https://www.youtube.com/watch?v=5Bym0ffALaU 18. https://www.youtube.com/watch?v=IkdziSLYzHw 19. https://www.youtube.com/watch?v=DaP0wFJJJ4Y 20. https://www.youtube.com/watch?v=S4Tz7d0QOeE 21. https://www.youtube.com/watch?v=FLOL2f4iHKA 22. https://www.youtube.com/watch?v=A2qe5OPLlmY 23. https://www.youtube.com/watch?v=8GaW36EfidI 24. https://www.youtube.com/watch?v=lUljxdkolK8 25. https://www.youtube.com/watch?v=CdCmYqih3YQ 26. https://www.youtube.com/watch?v=LCEmiRjPEtQ 27. https://www.youtube.com/watch?v=WiZjWadqSUo 28. https://www.youtube.com/watch?v=giT0ytynSqg 29. https://www.youtube.com/watch?v=0VLAoVGf_74 30. https://www.youtube.com/watch?v=WXuK6gekU1Y 31. https://www.youtube.com/watch?v=JkfyFpainjs 32. https://www.youtube.com/watch?v=7oAlD3lMNXo 33. https://www.youtube.com/watch?v=TgS0nFeYul8 34. https://www.youtube.com/watch?v=VDhan5Kd3ms 35. https://www.youtube.com/watch?v=LCEmiRjPEtQ 36. https://www.youtube.com/watch?v=7s9C92Pkcc0 37. https://www.youtube.com/watch?v=e20EY4tFC_Q 38. https://www.youtube.com/watch?v=Vky0WVh_FSk 39. https://www.youtube.com/watch?v=klW65MWJ1PY 40. https://www.youtube.com/watch?v=iyEFLKnNWAM 41."
    },
    {
      "source": "youtube-tasters.html",
      "content": "kcc0 37. https://www.youtube.com/watch?v=e20EY4tFC_Q 38. https://www.youtube.com/watch?v=Vky0WVh_FSk 39. https://www.youtube.com/watch?v=klW65MWJ1PY 40. https://www.youtube.com/watch?v=iyEFLKnNWAM 41. https://www.youtube.com/watch?v=mw5WIDGRLnA 42. https://www.youtube.com/watch?v=E361FZ_50oo 43. https://www.youtube.com/watch?v=9Z06rY3uvGY 44. https://www.youtube.com/watch?v=Y1QOf6HEbHQ 45. https://www.youtube.com/watch?v=OH-ybecvuEo 46. https://www.youtube.com/watch?v=zZYCmyaTY3Y 47. https://www.youtube.com/watch?v=qB_DLoBTLMI 48. https://www.youtube.com/watch?v=AJiG3ykOxmY 49. https://www.youtube.com/watch?v=zzXyPGEtseI 50. https://www.youtube.com/watch?v=gGEu_5KbVe8 51. https://www.youtube.com/watch?v=bpUszPiWM7o 52. https://www.youtube.com/watch?v=_j6pvGEchWU 53. https://www.youtube.com/watch?v=NFo9v_yKQXA 54. https://www.youtube.com/watch?v=70FhS6NAbuA 55. https://www.youtube.com/watch?v=EWvNQjAaOHw 56. https://www.youtube.com/watch?v=7xTGNNLPyMI 57. https://www.youtube.com/watch?v=VMj-3S1tku0 58. https://www.youtube.com/watch?v=bGi1jKL4OEo 59. https://www.youtube.com/watch?v=k4CxJLXc3-0 60. https://www.youtube.com/watch?v=w177Ov-Y3gc 61. https://www.youtube.com/watch?v=uk4NZorRjCo 62. file:///Users/ljubomir/ljubomirj.github.io/picmem/jobs-microsoft-have-no-taste.mp4\" 63. https://www.youtube.com/watch?v=_hSdi9K__nw 64. https://www.youtube.com/watch?v=MG9oqntiJKg 65. https://www.youtube.com/watch?v=P_fHJIYENdI 66. https://www.youtube.com/watch?v=mXWiDU9-fOk 67."
    },
    {
      "source": "youtube-tasters.html",
      "content": "mp4\" 63. https://www.youtube.com/watch?v=_hSdi9K__nw 64. https://www.youtube.com/watch?v=MG9oqntiJKg 65. https://www.youtube.com/watch?v=P_fHJIYENdI 66. https://www.youtube.com/watch?v=mXWiDU9-fOk 67. https://www.youtube.com/watch?v=3fkg0uTA3qU 68. https://www.youtube.com/watch?v=2E-s5DhgZ5g 69. https://www.youtube.com/watch?v=vxkBE23zDmQ 70. https://www.youtube.com/watch?v=OHWnPOKh_S0 71. https://www.youtube.com/watch?v=3MYEtQ5Zdn8 72. https://www.youtube.com/watch?v=OPZxs6IXH00 73. https://www.youtube.com/watch?v=b_DUft-BdIE 74. https://www.youtube.com/watch?v=LPZh9BOjkQs 75. https://www.youtube.com/watch?v=05ZwhL_CxO8 76. https://www.youtube.com/watch?v=1yvBqasHLZs 77. https://www.youtube.com/watch?v=9EN_HoEk3KY 78. https://www.youtube.com/watch?v=-uyXE7dY5H0 79. https://www.youtube.com/watch?v=KJtZARuO3JY 80. https://www.youtube.com/watch?v=b28zbsnk-48 81. https://www.youtube.com/watch?v=Es6yuMlyfPw 82. https://www.youtube.com/watch?v=zJcOPPLqv4A 83. https://www.youtube.com/watch?v=BnpB3GrpsfM 84. https://www.youtube.com/watch?v=eaAonE58sLU 85. https://www.youtube.com/watch?v=Gr_eYXdHFis 86. https://www.youtube.com/watch?v=7T-842pdHsE 87. https://www.youtube.com/watch?v=d_XikFthQ5I 88. https://www.youtube.com/watch?v=sitHS6UDMJc 89. https://www.youtube.com/watch?v=PUv66718DII 90. https://www.youtube.com/watch?v=AKMuA_TVz3A 91. https://www.youtube.com/watch?v=QNznD9hMEh0 92. https://www.youtube.com/watch?v=bzfT16k6HEM 93. https://www.youtube.com/watch?v=GI4Tpi48DlA 94."
    },
    {
      "source": "youtube-tasters.html",
      "content": "8DII 90. https://www.youtube.com/watch?v=AKMuA_TVz3A 91. https://www.youtube.com/watch?v=QNznD9hMEh0 92. https://www.youtube.com/watch?v=bzfT16k6HEM 93. https://www.youtube.com/watch?v=GI4Tpi48DlA 94. https://www.youtube.com/watch?v=3MkJEGE9GRY 95. https://www.youtube.com/watch?v=1pgiu--4W3I 96. https://www.youtube.com/watch?v=wawMjJUCMVw 97. https://www.youtube.com/watch?v=P4VBqTViEx4 98. https://www.youtube.com/watch?v=B3Sjyybijdo 99. https://www.youtube.com/watch?v=hpkgPJo_z6Y 100. https://www.youtube.com/watch?v=MzpiZQSH_D0 101. https://www.youtube.com/watch?v=b57yFohS-ac 102. https://www.youtube.com/watch?v=kYWUEV_e2ss 103. https://www.youtube.com/watch?v=_jQ5JjvDtKA 104. https://www.youtube.com/watch?v=9-Jl0dxWQs8 105. https://www.youtube.com/watch?v=wjZofJX0v4M 106. https://www.youtube.com/watch?v=tIeHLnjs5U8 107. https://www.youtube.com/watch?v=Ilg3gGewQ5U 108. https://www.youtube.com/watch?v=IHZwWFHWa-w 109. https://www.youtube.com/watch?v=aircAruvnKk 110. https://www.youtube.com/watch?v=j8zj5lBpFTY 111. https://www.youtube.com/watch?v=N1TEjTeQeg0 112. https://www.youtube.com/watch?v=34VOI_oo-qM 113. https://www.youtube.com/watch?v=eMlx5fFNoYc 114. https://www.youtube.com/watch?v=_SpySWkHu7k 115. https://www.youtube.com/watch?v=LI1eJxEZPNI 116. https://www.youtube.com/watch?v=EkGUEaZtIK4 117. https://www.youtube.com/watch?v=Q3XKw2mp1Fk 118. https://www.youtube.com/watch?v=HezHJKZ47Ck 119. https://www.youtube.com/watch?v=mScpHTIi-kM 120. https://www.youtube."
    },
    {
      "source": "youtube-tasters.html",
      "content": "ube.com/watch?v=EkGUEaZtIK4 117. https://www.youtube.com/watch?v=Q3XKw2mp1Fk 118. https://www.youtube.com/watch?v=HezHJKZ47Ck 119. https://www.youtube.com/watch?v=mScpHTIi-kM 120. https://www.youtube.com/watch?v=n4IQOBka8bc 121. https://www.youtube.com/watch?v=ltjI3BXKBgY 122. https://www.youtube.com/watch?v=W8QZ-yxebFA 123. https://www.youtube.com/watch?v=bux0SjaUCY0 124. https://www.youtube.com/watch?v=239aVfgJ-0E 125. https://www.youtube.com/watch?v=pWIrXN-yy8g 126. https://www.youtube.com/watch?v=iHCeAotHZa4 127. https://www.youtube.com/watch?v=UnELdZdyNaE 128. https://www.youtube.com/watch?v=cZYNADOHhVY 129. https://www.youtube.com/watch?v=rGgGOccMEiY 130. https://www.youtube.com/watch?v=ctXDXABJRtg 131. https://www.youtube.com/watch?v=qC0UWxgyDD0 132. https://www.youtube.com/watch?v=89Mq6gmPo0s 133. https://www.youtube.com/watch?v=rLG68k2blOc 134. https://www.youtube.com/watch?v=tTXckvj7KL4 135. https://www.youtube.com/watch?v=XdEuII9cv-U"
    },
    {
      "source": "cvlj92.pdf",
      "content": "Ljubomir JOSIFOVSKI LjubomirJosifovski@gmail.com | 44 7910 850 111 | 11 Pendennis Court, Harpenden AL5 1SG, UK | ljubomirj.github.io Summary ML/AI researcher/engineer/scientist in industrial R&D. Quantitative researcher, analyst, developer, building & trading systematic equity/FX models - including forecasting, portfolio optimisation, risk management, operations, post trade analysis - at hedge funds, proprietary trading desk, as independent Portfolio Manager. PhD Automatic Speech Recognition in noise, MSc Text-To-Speech synthesis. Spoken documents indexing & retrieval with spoken queries.Natural Language Processing. Analytical maths/stats/CS/EE background, machine learning, statistical modelling, industrial research & development. Competent developer in C/C++/shell/MATLAB/python/C#/Sql on Linux/Mac/Windows. Competent systems/network admin. Looking to apply: ASR lattice decoding insights into Chains-of-Reasoning in Reinforcement Learning at train, as well as at test time; DSPy prompting using english as programming language including optimisations, in building up forthcoming high level computing-Next, featuring New-as-old socratic Chat-as-programming, enacted Dialogue-as-code, using LLM-as-cpu, with Context-as-ram."
    },
    {
      "source": "cvlj92.pdf",
      "content": "guage including optimisations, in building up forthcoming high level computing-Next, featuring New-as-old socratic Chat-as-programming, enacted Dialogue-as-code, using LLM-as-cpu, with Context-as-ram. Skills C/C++/OpenMP, MATLAB, bash, vim, awk, SQL, PostgreSQL, MS SQL Server, c/make, gcc, gdb, ddd, shell tools, ssh, rsync, screen, VSCode, python, jupyter, Spyder, git, mercurial, cvs, MS Teams, R, Java, C#, Visual Studio, Slurm, Condor, Compute Cloud, Bloomberg terminal/API, Reuters Kobra, assembly, Agents: Codex (gpt-5-codex), Gemini-cli, Claude-code, Aider, Cline and Roo, local agents with local models (qwen3-coder-30b-a3b, gpt-oss-120b) for python, javascript, CSS/html, debugging C++. Platforms Linux (X/Ubuntu, CentOS), MacOS, MS-Windows (MS-DOS to v11), Cloud/cluster, Unix (HP-UX, AIX). Work May 16 - Now F9 Research, Harpenden, UK Position: Director. Quant research, development and trading. Portfolio manager, run a small market neutral book ~350M USD gross, trading ~35M USD daily in the EU markets (and a small R&D US book). Consulting for quant R & D for a client, working on higher frequencies and short horizons (seconds and minutes) in C/C++, OMP, python, Matlab, PostgreSQL, cloud boxes and Slurm cluster. Input into varying aspects of the R&D pipeline - from informing and assessing latest technologies (including ML) to interviewing new teams members. Re-engaged with ML/AI via llama."
    },
    {
      "source": "cvlj92.pdf",
      "content": "and Slurm cluster. Input into varying aspects of the R&D pipeline - from informing and assessing latest technologies (including ML) to interviewing new teams members. Re-engaged with ML/AI via llama.cpp, open source open weights local models, coding agents Gemini/Codex/Claude-cli and LLM API-s, local agents with local models (qwen3, gpt-oss) for python, javascript, CSS/html, debugging C++. Modelled transcripts data with doc2vec. Applied new ML methods in forecasting tabular data (c.f. Hugging Face TabArena). F9 owns the IP to all and any R&D work done. Feb 10 - Mar 16 Marshall Wace, London, UK Position: Quantitative Researcher. On the TOPS QR team, senior team member among a handful of people, creating research, developing code, shepherding the market neutral portfolio growth from a few hundred millions to double digit billions USD gross book size. Ushered the idea of a single unified framework for all quant R & D & trading with standardised components data ingestion and caching, signals extraction, modeller for forecasting, portfolio optimizer, trades simulator, standardised reporting, a baseline sim faithful and realistic to be continuously improved on by the entire team working in unison on various components of the system. Wrote or significantly contributed to major components of the system through their iterative improvements over the years."
    },
    {
      "source": "cvlj92.pdf",
      "content": "d on by the entire team working in unison on various components of the system. Wrote or significantly contributed to major components of the system through their iterative improvements over the years. Big projects in production improving the then best baseline: dynamic modeller fitting the alpha signals expected returns at multiple horizons, incorporating both prior knowledge, constraints, and the evidence from historical data, market impact model in the simulator and the optimizer including slippage monitoring tuning and balancing risk cost of undercharging with the opportunity cost of overcharging, liquid concentrated low TO high capacity market neutral portfolios, 150/50 portfolios mix of tracker and market neutral, shepherding the trade scheduler deployment in production, alphas signals GeoSales, Suppliers-Customers, Directors deals, various reverting signals, 1st quantitative research and assessment on the in-house Alpha Capture signal. Guided and helped younger hires from onboarding to them becoming fully productive wholly effective team members. Pioneered reproducible research at scale using multi cpu multi core R&D boxes with establishing and popularising best practices. 1 Nov 07 - Nov 09 Credit Suisse, London, UK Position: Quantitative Analyst. On the Index Arbitrage proprietary trading desk. Independently traded equity market/sector/factor neutral portfolios on multiple European markets, fully automated and systematic, non-discretionary."
    },
    {
      "source": "cvlj92.pdf",
      "content": "t. On the Index Arbitrage proprietary trading desk. Independently traded equity market/sector/factor neutral portfolios on multiple European markets, fully automated and systematic, non-discretionary. Wrote own trading, analytics, backtest and portfolio construction systematic trading platform consisting of a Matlab core, Mosek optimiser, bash/awk scripts, Reuters Kobra Excel and Sql for historic and current data, with integrated risk monitoring and control using Barra’s style factors and sectors. Used the platform to research and trade all the strategies and portfolios. Alone did orders generation, portfolio construction, forecasting & modelling, all data feeds (Reuters, Sql dumps), the daily monitoring, trading analysis and slippage tracking and any other R&D&ops as needed for trading. Traded multiple portfolios daily of ~500 names in total on London, Paris, Frankfurt, Switzerland, Milan and Madrid exchanges, one trade per day per name. Did R&D simulations for intra-day horizons faster TO. In 2008 traded the London portfolio most of the year as a test bed for all research & development, returning 10% gross in 230 days with Sharpe of 2.5. In 2009 traded bigger book on most of the European markets, returned 8% gross to Aug’09 with Sharpe of 5.2, turnover 2-3 days, one trade per name per day. All together lifetime (388 days) return on gross 18% at Sharpe of 3.1. Jul 04 - Sep 07 G-Research (part of the DPFM group), London, UK Position: Quantitative Analyst."
    },
    {
      "source": "cvlj92.pdf",
      "content": "days, one trade per name per day. All together lifetime (388 days) return on gross 18% at Sharpe of 3.1. Jul 04 - Sep 07 G-Research (part of the DPFM group), London, UK Position: Quantitative Analyst. Research (70%), development (20%), daily portfolio monitoring and support (10%) in a multi-billion market neutral hedge fund systematically trading global equities and spot FX round the clock in a completely automated system. Research and creation of new trading models/alphas, coding, testing in simulation and putting them in production. Models for volume prediction, fundamentals and technical equities models (multiple markets,), spot FX - all productionised and live traded. Built futures models but not traded live. Development included coding up the models, the associated data analytics, and subsequent performance and integrity monitoring once live. The portfolio support role involved monitoring the trade flow, market conditions and risk factors, investigating/tuning the trading. In the process familiarised myself with forecasting and modelling, performance attribution, multiperiod quadratic portfolio optimisation, risk measurement and management (Barra, APT, custom factors), real-time and historic data feeds, data aggregation. Independently came up with original alphas building on well known semi-parametric models for forecasting that were traded live in equities and spot FX trading. Similarly contributed alphas based on novel non-parametric models used for trading equities."
    },
    {
      "source": "cvlj92.pdf",
      "content": "on well known semi-parametric models for forecasting that were traded live in equities and spot FX trading. Similarly contributed alphas based on novel non-parametric models used for trading equities. They were all profitable, contributed to the bottom line and were traded along the other alphas. Jun 01 - Jun 04 Canon Research Europe, Bracknell, UK. Position: Researcher. Research & development work in the Machine listening group on ASR and indexing & retrieval of spoken documents. Contributed to all aspects of Canon's low resource embedded multiplatform ASR engine: the front-end (DSP related), decoder (Mpeg7 compatible lattice creation), training & using statistical models (acoustic HMM multilingual, text-to-phone Ngrams). Group demonstrated embedded speaker independent phone book name dialling on ARM9 & ARM7 phones. Phonetic indexing of spoken documents/annotations & retrieval with spoken & written queries. Invented & implemented in the embedded C++/C codebase novel algorithm for searching annotation (speech) lattices with a query (speech) lattice, outperforming other known techniques for phonetic SDR (LATTICE MATCHING, UK Patent App No 0316669.1, accomp app ref 2865001, Jul 2003). Demoed playlist entry selection by voice for an MP3 player, performing in near realtime on Windows CE platform with 1500 entries. Nov 00 - Jun 01 Motorola European Research Lab, Basingstoke, UK. Position: Research engineer. Technology transfer from my PhD work to Motorola (my industrial sponsor)."
    },
    {
      "source": "cvlj92.pdf",
      "content": "s CE platform with 1500 entries. Nov 00 - Jun 01 Motorola European Research Lab, Basingstoke, UK. Position: Research engineer. Technology transfer from my PhD work to Motorola (my industrial sponsor). Research on the distributed speech recognition (DSR) ETSI Aurora 2 standard platform. Developed robust ASR algorithms in Matlab, GNU C/C++ and tested them on Cygwin, HP-UX and Linux platforms. Lab was part of the winning consortium of the ETSI 2 Aurora 2 standardisation competition for mobile phones robust front-end. Nov 97 - Jan 98 Macedonian Banking Operations Centre (USAID funded project for technical support of the financial sector in Macedonia), Skopje, MK. Position: Management Information Systems - Electronic Data Processing (MIS-EDP) Advisor. In a team of advisers analysing operations of commercial banks in Macedonia. Handled the MIS-EDP operations of the banks surveyed, reported on the state of and recommended improvements. By the end of the project all commercial banks in Macedonia volunteered to have their operations surveyed and reported on. Nov 93 - Oct 97 Faculty of Mechanical Engineering, University Sv. Kiril i Metodij, Skopje, MK. Position: Systems engineer. Solely responsible for maintaining all faculty computers (100+ PCs, 10+ Unix workstations), faculty LAN spanning 3 buildings, other computing-related equipment (printers, terminal servers, router)."
    },
    {
      "source": "cvlj92.pdf",
      "content": "r. Solely responsible for maintaining all faculty computers (100+ PCs, 10+ Unix workstations), faculty LAN spanning 3 buildings, other computing-related equipment (printers, terminal servers, router). Faculty LAN massively expanded, doubled the size of existing and added a second computerised classroom for students and lab classes, introduced email & other Internet services to every staff member and student, phased out legacy systems (VT420 terminals, terminal servers). Maintained/supported collection of legacy Clipper/FoxPro accounting applications. Jun 93 - Oct 93 NeoCom, Skopje, MK. Position: System integrator. In small & dynamic company, clients facing, computer systems assembly, integration, software installation, maintenance (PC/Windows), computer networks (Novell NetWare, Windows LAN) installation & maintenance on- and off-site. 1986 - 1993 Freelance S/W developer, undergraduate & hobby programming Basic & assembler (6502) on home computers. Mission critical (firing heavy guns) on pocket computers (HP-71B, Sharp 1500) and TurboPascal (Apple II+CP/M board+HDD) while national service (army). MS-DOS systems programming (C & assembler, TSR programs: screen capture, serial port snoop, DOS trashcan), network programming (NetBIOS based LAN messenger, IPX chat, IPX stack emulator in DesqView), PC databases (video shop rental application in Clipper, various applications in FoxPro, document flow in MS-Access). Education 1998 - 2000 Doctor of Philosophy Ph.D."
    },
    {
      "source": "cvlj92.pdf",
      "content": ", IPX stack emulator in DesqView), PC databases (video shop rental application in Clipper, various applications in FoxPro, document flow in MS-Access). Education 1998 - 2000 Doctor of Philosophy Ph.D. (Full-Time) Speech and Hearing Group, Department of Computer Science, Faculty of Engineering, University of Sheffield, UK. Independent research into recognising speech in noise. Missing data model treats parts of the speech spectrum swamped by noise as unobserved/partially observed, giving rise to a probabilistically modelled mask that has to be incorporated in the frame-by-frame adapted speech model. Work involved theory of automatic speech recognition as well as practice, training HMMs with continuous GMM pdfs using EM (HTK, shell scripting), writing and using Viterbi decoders and frontends to test novel noise robustness algorithms, noise and SNR estimation (Matlab, C++, C). Part of EC ESPRIT LTR programme funded RESPITE project of 5 research labs and 2 industrial partners and EC TMR programme funded SPHEAR network. Thesis: \"Robust speech recognition with missing and unreliable data\". (Viva Dec 2002) 1993 - 1997 M.Phil. Electrical Engineering (Part-Time) Department for Computers and Informatics, Faculty of Electrical Engineering, University Sv. Kiril i Metodij, Skopje, MK. Studies consisted of taught part (2 years/4 semesters) for 6 courses and thesis work (1 year/2 semesters) followed by a viva. Courses achieved avg grade 10 (scale 6-10, 10 best)."
    },
    {
      "source": "cvlj92.pdf",
      "content": "Metodij, Skopje, MK. Studies consisted of taught part (2 years/4 semesters) for 6 courses and thesis work (1 year/2 semesters) followed by a viva. Courses achieved avg grade 10 (scale 6-10, 10 best). Projects: video-over-IP frame rate control and QoS using UDP non-blocking sockets (C/C++, part of a system for tele-teaching system); database of Medieval Manuscripts (Delphi). Thesis/research - built system for converting written text into speech. Rudimentary time-domain, syllable based TTS. Created a database of 1200 3 syllables, wrote TTS engine breaking the input text into syllables (using an NN MLP), concatenating the units from the syllable database, generating F0 and the duration contours, modifying the syllable units accordingly in time domain. Gathered and labelled data, trained a two layer, feed forward MLP (neural network) to mark syllable breaks in the input text. Part of a larger project for automatic text reading for the blind. Thesis: \"System for text-to-speech conversion for Macedonian language\". 1988 - 1993 B.S. Electrical Engineering (Full-Time) Department for Computers, Informatics and Automation, Faculty of Electrical Engineering, University Sv. Kiril i Metodij, Skopje, MK. Taught studies 4.5 years (9 semesters) followed by a diploma work (1 semester) and public presentation. Achieved average grade of 8.78 (scale 6-10, 10 best). Diploma project: \"Introduction to DECNET, Bitnet (EARN) and Internet networks; E-mail/File transfer services; X."
    },
    {
      "source": "cvlj92.pdf",
      "content": "emester) and public presentation. Achieved average grade of 8.78 (scale 6-10, 10 best). Diploma project: \"Introduction to DECNET, Bitnet (EARN) and Internet networks; E-mail/File transfer services; X.25 Network and out-dial NUAs\". Best student within my college class in years 1 & 2. Ranked 1st (100 points out of 100) among of approx 800 candidates at the University entrance exams. 1983 - 1987 R.J. Korcagin High School, Skopje, MK. Mathematics and Computer Science High School, achieved GPA 5.00 on a 2 to 5 scale (5 best), voted best pupil (“valedictorian”) of the 1983-87 generation. Nationality UK (acquired/by choice), Macedonian (by birth). Born 1968. Languages English, Macedonian (native), Croatian, Serbian. Honours & Awards Scholarships: merit research & science 1988-93, talented student 1983-87. Best student 1989,'90. Maths competitions prizes: Regional 1st 1984, ’86, ‘87, 3rd 1985; Republic 3rd 1984, ’85, ‘87; National participation 1984, ‘85, praise 1987. Other UK and MK driving licences, married, two grown up children. Interests include science, technology, innovation, knowledge, epistemology, culture, arts, non-fiction, systems theories, solar punk, political economy, quantitative finance, history, ethics, mentoring, e/acc. 4"
    },
    {
      "source": "cvlj92t1.pdf",
      "content": "🌐 ​​https:/​/​ljubomirj.github.io​ ​Ljubomir JOSIFOVSKI ​LjubomirJosifovski@gmail.com​ ​+44-7910-850-111​ ​11 Pendennis Ct, Harpenden AL5 1SG, UK​ ​Summary​ ​ML/AI researcher/engineer/scientist in industrial R&D. Quantitative researcher, analyst, developer, portfolio​ ​manager building and trading systematic equity/FX models at hedge funds, proprietary trading desk, as​ ​independent PM. PhD Automatic Speech Recognition in noise, MSc Text-To-Speech synthesis, NLP spoken​ ​documents indexing and retrieval with speech. Machine learning, regression & classification, neural​ ​networks, hidden Markov models, Ngram LMs. Signal extraction, modelling, forecasting, quadratic​ ​multihorizon portfolio optimization, risk management, simulation. Systems & network admin. Looking to​ ​apply: ASR lattice decoding insights into Chains-of-Reasoning in Reinforcement Learning at train, as well​ ​as at test time; DSPy prompting using english as programming language including optimisations, in building​ ​up forthcoming high level computing-Next, featuring New-as-old: socratic Chat-as-programming, enacted​ ​Dialogue-as-code, processing LLM-as-cpu, using Context-as-ram.​ ​Skills​ ​Programming: C/C++/OpenMP, MATLAB, Python, SQL, C#, R, Java, bash, awk, make, gdb, ddd.​ ​Platforms: Linux (Ubuntu, CentOS), MacOS, MS-Windows, GCloud, Slurm, HTCondor, Unix.​ ​Tools: Vim, Git, screen, VSCode, CLion, Jupyter, Spyder, MATLAB, Bloomberg, Reuters Cobra."
    },
    {
      "source": "cvlj92t1.pdf",
      "content": "awk, make, gdb, ddd.​ ​Platforms: Linux (Ubuntu, CentOS), MacOS, MS-Windows, GCloud, Slurm, HTCondor, Unix.​ ​Tools: Vim, Git, screen, VSCode, CLion, Jupyter, Spyder, MATLAB, Bloomberg, Reuters Cobra.​ ​Agents: Codex, Gemini, Claude, Aider, Cline/Roo, w/local models - for python, javascript, CSS/html, C++.​ ​Experience​ ​F9 Research, Director (2016–Present)​ ​Managed a market-neutral book (~$350M gross, ~$35M daily trading) in EU and US markets.​ ​Quant research and development of short-horizon strategies using Python, C++, cluster and cloud resources.​ ​Rekindled ML/AI interests using llama.cpp and open weights LLMs, Gemini and Aider coding agents,​ ​DNNs for tabular data forecasting (c.f. Hugging Face TabArena), local models (qwen3, gpt-oss).​ ​Marshall Wace, Senior Quantitative Researcher (2010–2016)​ ​Developed and scaled market-neutral portfolios from $100M to $10B+ over a period of 6 years.​ ​Pioneered wrote unified R&D framework for data ingestion, signal extraction, modelling, portfolio​ ​optimization, simulation. Mentored junior researchers, implemented reproducible research workflows.​ ​Credit Suisse, Quantitative Analyst (2007–2009)​ ​Independently traded equity market-neutral portfolios systematically, achieving 18% lifetime returns with​ ​Sharpe 3.1. Built and operated a complete trading platform for multi-market European equities."
    },
    {
      "source": "cvlj92t1.pdf",
      "content": "dependently traded equity market-neutral portfolios systematically, achieving 18% lifetime returns with​ ​Sharpe 3.1. Built and operated a complete trading platform for multi-market European equities.​ ​G-Research (DPFMG), Quantitative Analyst (2004–2007)​ ​Designed and implemented systematic trading models for global equities and FX, contributing to fund​ ​profitability. Modelling, forecasting, risk management and multi-period optimization for mid- and high-​ ​frequency trading strategies. Operational portfolio management and production monitoring, on-call duty.​ ​Canon Research Europe, Researcher (2001-2004)​ ​Embedded Automatic Speech Recognition, indexing, and retrieval of spoken documents with speech.​ ​Education​ ​Ph.D.​​Computer Science – University of Sheffield,​​UK (2000)​ ​Thesis: Robust Speech Recognition with Missing and Unreliable Data​ ​M.Phil.​​Electrical Engineering – University Sv. Kiril​​i Metodij, Skopje, MK (1997)​ ​Thesis: System for text-to-speech conversion for Macedonian language​ ​B.S.​​Electrical Engineering – University Sv. Kiril​​i Metodij, Skopje, MK (1993)​ ​Additional​ ​Citizenship - UK and Macedonian. Languages - English, Macedonian (native), Croatian, Serbian.​ ​Married, two grown up children. UK and MK driving licenses.​ ​Interests include science, technology, innovation, knowledge, epistemology, culture, arts, non-fiction,​ ​systems theories, solar punk, political economy, quantitative finance, history, ethics, mentoring, e/acc.​"
    },
    {
      "source": "cvlj92t1.pdf",
      "content": "e, technology, innovation, knowledge, epistemology, culture, arts, non-fiction,​ ​systems theories, solar punk, political economy, quantitative finance, history, ethics, mentoring, e/acc.​"
    },
    {
      "source": "cvlj92t2.pdf",
      "content": "​Ljubomir JOSIFOVSKI​ ​LjubomirJosifovski@gmail.com​ ​+44-7910-850-111​ 🌐 ​ ​​https:/​/​ljubomirj.github.io​ ​11 Pendennis Court, Harpenden AL5 1SG, UK​ ​Summary​ ​ML/AI researcher/engineer/scientist in industrial R&D. Quantitative researcher, analyst, developer, portfolio manager​ ​building and trading systematic equity/FX models at hedge funds, proprietary trading desk, as independent PM. PhD​ ​Automatic Speech Recognition in noise, MSc Text-To-Speech synthesis, NLP spoken documents indexing and​ ​retrieval with speech. Machine learning, regression & classification, neural networks, hidden Markov models, Ngram​ ​LMs. Signal extraction, modelling, forecasting, quadratic multihorizon portfolio optimization, risk management,​ ​simulation. Systems & network admin. Looking to apply: ASR lattice decoding insights into Chains-of-Reasoning in​ ​Reinforcement Learning at train, as well as at test time; DSPy prompting using english as programming language​ ​including optimisations, in building up forthcoming high level computing-Next, featuring New-as-old: socratic​ ​Chat-as-programming, enacted Dialogue-as-code, processing LLM-as-cpu, using Context-as-ram.​ ​Skills​ ​●​ ​Programming​: C/C++/OpenMP, MATLAB, Python, SQL, C#,​​R, Java, bash, awk, make, gdb, ddd.​ ​●​ ​Platforms​: Linux (Ubuntu, CentOS), MacOS, MS-Windows,​​GCloud, Slurm, HTCondor, Unix.​ ​●​ ​Tools​: Vim, Git, screen, VSCode, CLion, Jupyter, Spyder,​​MATLAB, Bloomberg, Reuters Cobra."
    },
    {
      "source": "cvlj92t2.pdf",
      "content": "db, ddd.​ ​●​ ​Platforms​: Linux (Ubuntu, CentOS), MacOS, MS-Windows,​​GCloud, Slurm, HTCondor, Unix.​ ​●​ ​Tools​: Vim, Git, screen, VSCode, CLion, Jupyter, Spyder,​​MATLAB, Bloomberg, Reuters Cobra.​ ​●​ ​Agents​: Codex, Gemini, Claude, Aider, Cline/Roo, w/local​​models - for python, JS/CSS/html, C/C++.​ ​Experience​ ​F9 Research, Director​​(2016–Present)​ ​●​ ​Managed a market-neutral book (~$350M gross, ~$35M daily trading) in EU and US markets.​ ​●​ ​Quant research and development of short (seconds, minutes) horizons strategies in C++, Python.​ ​●​ ​ML/AI llama.cpp open weights LLMs, Gemini/Aider coding agents, tabular data forecasting DNNs.​ ​Marshall Wace, Senior Quantitative Researcher​​(2010–2016)​ ​●​ ​Developed and scaled market-neutral portfolios from $100M to $10B+ over a period of 6 years.​ ​●​ ​Pioneered wrote unified R&D frameworks for data ingestion, signal extraction, modelling, forecasting,​ ​portfolio optimization, simulation, execution, reproducible research workflows. Mentored juniors.​ ​Credit Suisse, Quantitative Analyst​​(2007–2009)​ ​●​ ​Independently traded equity market-neutral portfolios systematically, 18% lifetime returns Sharpe 3.1.​ ​●​ ​Built and operated a complete trading platform for multi-market European equities.​ ​G-Research (DPFMG), Quantitative Analyst​​(2004–2007)​ ​●​ ​Designed and implemented systematic trading models for global equities and FX for fund profitability."
    },
    {
      "source": "cvlj92t2.pdf",
      "content": "for multi-market European equities.​ ​G-Research (DPFMG), Quantitative Analyst​​(2004–2007)​ ​●​ ​Designed and implemented systematic trading models for global equities and FX for fund profitability.​ ​●​ ​Modelling, forecasting, risk management, multi-period optimization for mid- and high- frequency trading​ ​strategies. Operational portfolio management and production monitoring, on-call duty.​ ​Canon Research Europe, Researcher​​(2001–2004)​ ​●​ ​Embedded automatic speech recognition, indexing, and retrieval of spoken documents with speech.​ ​Education​ ​●​ ​Ph.D., Computer Science​​– University of Sheffield,​​UK​​(2000)​ ​Thesis: Robust Speech Recognition with Missing and Unreliable Data​ ​●​ ​M.Phil., Electrical Engineering​​– University Sv. Kiril​​i Metodij, Skopje, MK​​(1997)​ ​Thesis: System for Text-to-Speech Conversion for the Macedonian Language​ ​●​ ​B.S., Electrical Engineering​​– University Sv. Kiril​​i Metodij, Skopje, MK​​(1993)​ ​Additional​ ​●​ ​Citizenship/driving lic UK & MK; languages English, Macedonian (native), Serb/Croat-ian; married; two kids.​ ​●​ ​Interests include science, technology, innovation, knowledge, epistemology, culture, arts, non-fiction, systems​ ​theories, solar punk, political economy, quantitative finance, history, ethics, mentoring, e/acc.​"
    },
    {
      "source": "cvlj92t2.pdf",
      "content": "cc.​"
    },
    {
      "source": "cvlj93.pdf",
      "content": "Ljubomir JOSIFOVSKI LjubomirJosifovski@gmail.com | 44 7910 850 111 | 11 Pendennis Court, Harpenden AL5 1SG, UK | ljubomirj.github.io Summary ML/AI researcher/engineer/scientist in industrial R&D. Quantitative researcher, analyst, developer, building & trading systematic equity/FX models - including forecasting, portfolio optimisation, risk management, operations, post trade analysis - at hedge funds, proprietary trading desk, as independent Portfolio Manager. PhD Automatic Speech Recognition in noise, MSc Text-To-Speech synthesis. Spoken documents indexing & retrieval with spoken queries.Natural Language Processing. Analytical maths/stats/CS/EE background, machine learning, statistical modelling, industrial research & development. Competent developer in C/C++/shell/MATLAB/python/C#/Sql on Linux/Mac/Windows. Competent systems/network admin. Looking to apply: ASR lattice decoding insights into Chains-of-Reasoning in Reinforcement Learning at train, as well as at test time; DSPy prompting using english as programming language including optimisations, in building up forthcoming high level computing-Next, featuring New-as-old socratic Chat-as-programming, enacted Dialogue-as-code, using LLM-as-cpu, with Context-as-ram."
    },
    {
      "source": "cvlj93.pdf",
      "content": "guage including optimisations, in building up forthcoming high level computing-Next, featuring New-as-old socratic Chat-as-programming, enacted Dialogue-as-code, using LLM-as-cpu, with Context-as-ram. Skills C/C++/OpenMP, MATLAB, bash, vim, awk, SQL, PostgreSQL, MS SQL Server, c/make, gcc, gdb, ddd, shell tools, ssh, rsync, screen, VSCode, python, jupyter, Spyder, git, mercurial, cvs, MS Teams, R, Java, C#, Visual Studio, Slurm, Condor, Compute Cloud, Bloomberg terminal/API, Reuters Kobra, assembly, Agents: Codex (gpt-5-codex), Gemini-cli, Claude-code, Aider, Cline and Roo, local agents with local models (qwen3-coder-30b-a3b, gpt-oss-120b) for python, javascript, CSS/html, debugging C++. Platforms Linux (X/Ubuntu, CentOS), MacOS, MS-Windows (MS-DOS to v11), Cloud/cluster, Unix (HP-UX, AIX). Work May 16 - Now F9 Research, Harpenden, UK Position: Director. Quant research, development and trading. Portfolio manager, run a small market neutral book ~350M USD gross, trading ~35M USD daily in the EU markets (and a small R&D US book). Consulting for quant R & D for a client, working on higher frequencies and short horizons (seconds and minutes) in C/C++, OMP, python, Matlab, PostgreSQL, cloud boxes and Slurm cluster. Input into varying aspects of the R&D pipeline - from informing and assessing latest technologies (including ML) to interviewing new teams members. Re-engaged with ML/AI via llama."
    },
    {
      "source": "cvlj93.pdf",
      "content": "and Slurm cluster. Input into varying aspects of the R&D pipeline - from informing and assessing latest technologies (including ML) to interviewing new teams members. Re-engaged with ML/AI via llama.cpp, open source open weights local models, coding agents Gemini/Codex/Claude-cli and LLM API-s, local agents with local models (qwen3, gpt-oss) for python, javascript, CSS/html, debugging C++. Modelled transcripts data with doc2vec. Applied new ML methods in forecasting tabular data (c.f. Hugging Face TabArena). F9 owns the IP to all and any R&D work done. Feb 10 - Mar 16 Marshall Wace, London, UK Position: Quantitative Researcher. On the TOPS QR team, senior team member among a handful of people, creating research, developing code, shepherding the market neutral portfolio growth from a few hundred millions to double digit billions USD gross book size. Ushered the idea of a single unified framework for all quant R & D & trading with standardised components data ingestion and caching, signals extraction, modeller for forecasting, portfolio optimizer, trades simulator, standardised reporting, a baseline sim faithful and realistic to be continuously improved on by the entire team working in unison on various components of the system. Wrote or significantly contributed to major components of the system through their iterative improvements over the years."
    },
    {
      "source": "cvlj93.pdf",
      "content": "d on by the entire team working in unison on various components of the system. Wrote or significantly contributed to major components of the system through their iterative improvements over the years. Big projects in production improving the then best baseline: dynamic modeller fitting the alpha signals expected returns at multiple horizons, incorporating both prior knowledge, constraints, and the evidence from historical data, market impact model in the simulator and the optimizer including slippage monitoring tuning and balancing risk cost of undercharging with the opportunity cost of overcharging, liquid concentrated low TO high capacity market neutral portfolios, 150/50 portfolios mix of tracker and market neutral, shepherding the trade scheduler deployment in production, alphas signals GeoSales, Suppliers-Customers, Directors deals, various reverting signals, 1st quantitative research and assessment on the in-house Alpha Capture signal. Guided and helped younger hires from onboarding to them becoming fully productive wholly effective team members. Pioneered reproducible research at scale using multi cpu multi core R&D boxes with establishing and popularising best practices. 1 Nov 07 - Nov 09 Credit Suisse, London, UK Position: Quantitative Analyst. On the Index Arbitrage proprietary trading desk. Independently traded equity market/sector/factor neutral portfolios on multiple European markets, fully automated and systematic, non-discretionary."
    },
    {
      "source": "cvlj93.pdf",
      "content": "t. On the Index Arbitrage proprietary trading desk. Independently traded equity market/sector/factor neutral portfolios on multiple European markets, fully automated and systematic, non-discretionary. Wrote own trading, analytics, backtest and portfolio construction systematic trading platform consisting of a Matlab core, Mosek optimiser, bash/awk scripts, Reuters Kobra Excel and Sql for historic and current data, with integrated risk monitoring and control using Barra’s style factors and sectors. Used the platform to research and trade all the strategies and portfolios. Alone did orders generation, portfolio construction, forecasting & modelling, all data feeds (Reuters, Sql dumps), the daily monitoring, trading analysis and slippage tracking and any other R&D&ops as needed for trading. Traded multiple portfolios daily of ~500 names in total on London, Paris, Frankfurt, Switzerland, Milan and Madrid exchanges, one trade per day per name. Did R&D simulations for intra-day horizons faster TO. In 2008 traded the London portfolio most of the year as a test bed for all research & development, returning 10% gross in 230 days with Sharpe of 2.5. In 2009 traded bigger book on most of the European markets, returned 8% gross to Aug’09 with Sharpe of 5.2, turnover 2-3 days, one trade per name per day. All together lifetime (388 days) return on gross 18% at Sharpe of 3.1. Jul 04 - Sep 07 G-Research (part of the DPFM group), London, UK Position: Quantitative Analyst."
    },
    {
      "source": "cvlj93.pdf",
      "content": "days, one trade per name per day. All together lifetime (388 days) return on gross 18% at Sharpe of 3.1. Jul 04 - Sep 07 G-Research (part of the DPFM group), London, UK Position: Quantitative Analyst. Research (70%), development (20%), daily portfolio monitoring and support (10%) in a multi-billion market neutral hedge fund systematically trading global equities and spot FX round the clock in a completely automated system. Research and creation of new trading models/alphas, coding, testing in simulation and putting them in production. Models for volume prediction, fundamentals and technical equities models (multiple markets,), spot FX - all productionised and live traded. Built futures models but not traded live. Development included coding up the models, the associated data analytics, and subsequent performance and integrity monitoring once live. The portfolio support role involved monitoring the trade flow, market conditions and risk factors, investigating/tuning the trading. In the process familiarised myself with forecasting and modelling, performance attribution, multiperiod quadratic portfolio optimisation, risk measurement and management (Barra, APT, custom factors), real-time and historic data feeds, data aggregation. Independently came up with original alphas building on well known semi-parametric models for forecasting that were traded live in equities and spot FX trading. Similarly contributed alphas based on novel non-parametric models used for trading equities."
    },
    {
      "source": "cvlj93.pdf",
      "content": "on well known semi-parametric models for forecasting that were traded live in equities and spot FX trading. Similarly contributed alphas based on novel non-parametric models used for trading equities. They were all profitable, contributed to the bottom line and were traded along the other alphas. Jun 01 - Jun 04 Canon Research Europe, Bracknell, UK. Position: Researcher. Research & development work in the Machine listening group on ASR and indexing & retrieval of spoken documents. Contributed to all aspects of Canon's low resource embedded multiplatform ASR engine: the front-end (DSP related), decoder (Mpeg7 compatible lattice creation), training & using statistical models (acoustic HMM multilingual, text-to-phone Ngrams). Group demonstrated embedded speaker independent phone book name dialling on ARM9 & ARM7 phones. Phonetic indexing of spoken documents/annotations & retrieval with spoken & written queries. Invented & implemented in the embedded C++/C codebase novel algorithm for searching annotation (speech) lattices with a query (speech) lattice, outperforming other known techniques for phonetic SDR (LATTICE MATCHING, UK Patent App No 0316669.1, accomp app ref 2865001, Jul 2003). Demoed playlist entry selection by voice for an MP3 player, performing in near realtime on Windows CE platform with 1500 entries. Nov 00 - Jun 01 Motorola European Research Lab, Basingstoke, UK. Position: Research engineer. Technology transfer from my PhD work to Motorola (my industrial sponsor)."
    },
    {
      "source": "cvlj93.pdf",
      "content": "s CE platform with 1500 entries. Nov 00 - Jun 01 Motorola European Research Lab, Basingstoke, UK. Position: Research engineer. Technology transfer from my PhD work to Motorola (my industrial sponsor). Research on the distributed speech recognition (DSR) ETSI Aurora 2 standard platform. Developed robust ASR algorithms in Matlab, GNU C/C++ and tested them on Cygwin, HP-UX and Linux platforms. Lab was part of the winning consortium of the ETSI 2 Aurora 2 standardisation competition for mobile phones robust front-end. Nov 97 - Jan 98 Macedonian Banking Operations Centre (USAID funded project for technical support of the financial sector in Macedonia), Skopje, MK. Position: Management Information Systems - Electronic Data Processing (MIS-EDP) Advisor. In a team of advisers analysing operations of commercial banks in Macedonia. Handled the MIS-EDP operations of the banks surveyed, reported on the state of and recommended improvements. By the end of the project all commercial banks in Macedonia volunteered to have their operations surveyed and reported on. Nov 93 - Oct 97 Faculty of Mechanical Engineering, University Sv. Kiril i Metodij, Skopje, MK. Position: Systems engineer. Solely responsible for maintaining all faculty computers (100+ PCs, 10+ Unix workstations), faculty LAN spanning 3 buildings, other computing-related equipment (printers, terminal servers, router)."
    },
    {
      "source": "cvlj93.pdf",
      "content": "r. Solely responsible for maintaining all faculty computers (100+ PCs, 10+ Unix workstations), faculty LAN spanning 3 buildings, other computing-related equipment (printers, terminal servers, router). Faculty LAN massively expanded, doubled the size of existing and added a second computerised classroom for students and lab classes, introduced email & other Internet services to every staff member and student, phased out legacy systems (VT420 terminals, terminal servers). Maintained/supported collection of legacy Clipper/FoxPro accounting applications. Jun 93 - Oct 93 NeoCom, Skopje, MK. Position: System integrator. In small & dynamic company, clients facing, computer systems assembly, integration, software installation, maintenance (PC/Windows), computer networks (Novell NetWare, Windows LAN) installation & maintenance on- and off-site. 1986 - 1993 Freelance S/W developer, undergraduate & hobby programming Basic & assembler (6502) on home computers. Mission critical (firing heavy guns) on pocket computers (HP-71B, Sharp 1500) and TurboPascal (Apple II+CP/M board+HDD) while national service (army). MS-DOS systems programming (C & assembler, TSR programs: screen capture, serial port snoop, DOS trashcan), network programming (NetBIOS based LAN messenger, IPX chat, IPX stack emulator in DesqView), PC databases (video shop rental application in Clipper, various applications in FoxPro, document flow in MS-Access). Education 1998 - 2000 Doctor of Philosophy Ph.D."
    },
    {
      "source": "cvlj93.pdf",
      "content": ", IPX stack emulator in DesqView), PC databases (video shop rental application in Clipper, various applications in FoxPro, document flow in MS-Access). Education 1998 - 2000 Doctor of Philosophy Ph.D. (Full-Time) Speech and Hearing Group, Department of Computer Science, Faculty of Engineering, University of Sheffield, UK. Independent research into recognising speech in noise. Missing data model treats parts of the speech spectrum swamped by noise as unobserved/partially observed, giving rise to a probabilistically modelled mask that has to be incorporated in the frame-by-frame adapted speech model. Work involved theory of automatic speech recognition as well as practice, training HMMs with continuous GMM pdfs using EM (HTK, shell scripting), writing and using Viterbi decoders and frontends to test novel noise robustness algorithms, noise and SNR estimation (Matlab, C++, C). Part of EC ESPRIT LTR programme funded RESPITE project of 5 research labs and 2 industrial partners and EC TMR programme funded SPHEAR network. Thesis: \"Robust speech recognition with missing and unreliable data\". (Viva Dec 2002) 1993 - 1997 M.Phil. Electrical Engineering (Part-Time) Department for Computers and Informatics, Faculty of Electrical Engineering, University Sv. Kiril i Metodij, Skopje, MK. Studies consisted of taught part (2 years/4 semesters) for 6 courses and thesis work (1 year/2 semesters) followed by a viva. Courses achieved avg grade 10 (scale 6-10, 10 best)."
    },
    {
      "source": "cvlj93.pdf",
      "content": "Metodij, Skopje, MK. Studies consisted of taught part (2 years/4 semesters) for 6 courses and thesis work (1 year/2 semesters) followed by a viva. Courses achieved avg grade 10 (scale 6-10, 10 best). Projects: video-over-IP frame rate control and QoS using UDP non-blocking sockets (C/C++, part of a system for tele-teaching system); database of Medieval Manuscripts (Delphi). Thesis/research - built system for converting written text into speech. Rudimentary time-domain, syllable based TTS. Created a database of 1200 3 syllables, wrote TTS engine breaking the input text into syllables (using an NN MLP), concatenating the units from the syllable database, generating F0 and the duration contours, modifying the syllable units accordingly in time domain. Gathered and labelled data, trained a two layer, feed forward MLP (neural network) to mark syllable breaks in the input text. Part of a larger project for automatic text reading for the blind. Thesis: \"System for text-to-speech conversion for Macedonian language\". 1988 - 1993 B.S. Electrical Engineering (Full-Time) Department for Computers, Informatics and Automation, Faculty of Electrical Engineering, University Sv. Kiril i Metodij, Skopje, MK. Taught studies 4.5 years (9 semesters) followed by a diploma work (1 semester) and public presentation. Achieved average grade of 8.78 (scale 6-10, 10 best). Diploma project: \"Introduction to DECNET, Bitnet (EARN) and Internet networks; E-mail/File transfer services; X."
    },
    {
      "source": "cvlj93.pdf",
      "content": "emester) and public presentation. Achieved average grade of 8.78 (scale 6-10, 10 best). Diploma project: \"Introduction to DECNET, Bitnet (EARN) and Internet networks; E-mail/File transfer services; X.25 Network and out-dial NUAs\". Best student within my college class in years 1 & 2. Ranked 1st (100 points out of 100) among of approx 800 candidates at the University entrance exams. 1983 - 1987 R.J. Korcagin High School, Skopje, MK. Mathematics and Computer Science High School, achieved GPA 5.00 on a 2 to 5 scale (5 best), voted best pupil (“valedictorian”) of the 1983-87 generation. Nationality UK (acquired/by choice), Macedonian (by birth). Born 1968. Languages English, Macedonian (native), Croatian, Serbian. Honours & Awards Scholarships: merit research & science 1988-93, talented student 1983-87. Best student 1989,'90. Maths competitions prizes: Regional 1st 1984, ’86, ‘87, 3rd 1985; Republic 3rd 1984, ’85, ‘87; National participation 1984, ‘85, praise 1987. Other UK and MK driving licences, married, two grown up children. Interests include science, technology, innovation, knowledge, epistemology, culture, arts, non-fiction, systems theories, solar punk, political economy, quantitative finance, history, ethics, mentoring, e/acc. 4"
    },
    {
      "source": "cvlj93t1.pdf",
      "content": "🌐 ​​https:/​/​ljubomirj.github.io​ ​Ljubomir JOSIFOVSKI ​LjubomirJosifovski@gmail.com​ ​+44-7910-850-111​ ​11 Pendennis Ct, Harpenden AL5 1SG, UK​ ​Summary​ ​ML/AI researcher/engineer/scientist in industrial R&D. Quantitative researcher, analyst, developer, portfolio​ ​manager building and trading systematic equity/FX models at hedge funds, proprietary trading desk, as​ ​independent PM. PhD Automatic Speech Recognition in noise, MSc Text-To-Speech synthesis, NLP spoken​ ​documents indexing and retrieval with speech. Machine learning, regression & classification, neural​ ​networks, hidden Markov models, Ngram LMs. Signal extraction, modelling, forecasting, quadratic​ ​multihorizon portfolio optimization, risk management, simulation. Systems & network admin. Looking to​ ​apply: ASR lattice decoding insights into Chains-of-Reasoning in Reinforcement Learning at train, as well​ ​as at test time; DSPy prompting using english as programming language including optimisations, in building​ ​up forthcoming high level computing-Next, featuring New-as-old: socratic Chat-as-programming, enacted​ ​Dialogue-as-code, processing LLM-as-cpu, using Context-as-ram.​ ​Skills​ ​Programming: C/C++/OpenMP, MATLAB, Python, SQL, C#, R, Java, bash, awk, make, gdb, ddd.​ ​Platforms: Linux (Ubuntu, CentOS), MacOS, MS-Windows, GCloud, Slurm, HTCondor, Unix.​ ​Tools: Vim, Git, screen, VSCode, CLion, Jupyter, Spyder, MATLAB, Bloomberg, Reuters Cobra."
    },
    {
      "source": "cvlj93t1.pdf",
      "content": "awk, make, gdb, ddd.​ ​Platforms: Linux (Ubuntu, CentOS), MacOS, MS-Windows, GCloud, Slurm, HTCondor, Unix.​ ​Tools: Vim, Git, screen, VSCode, CLion, Jupyter, Spyder, MATLAB, Bloomberg, Reuters Cobra.​ ​Agents: Codex, Gemini, Claude, Aider, Cline/Roo, w/local models - for python, javascript, CSS/html, C++.​ ​Experience​ ​F9 Research, Director (2016–Present)​ ​Managed a market-neutral book (~$350M gross, ~$35M daily trading) in EU and US markets.​ ​Quant research and development of short-horizon strategies using Python, C++, cluster and cloud resources.​ ​Rekindled ML/AI interests using llama.cpp and open weights LLMs, Gemini and Aider coding agents,​ ​DNNs for tabular data forecasting (c.f. Hugging Face TabArena), local models (qwen3, gpt-oss).​ ​Marshall Wace, Senior Quantitative Researcher (2010–2016)​ ​Developed and scaled market-neutral portfolios from $100M to $10B+ over a period of 6 years.​ ​Pioneered wrote unified R&D framework for data ingestion, signal extraction, modelling, portfolio​ ​optimization, simulation. Mentored junior researchers, implemented reproducible research workflows.​ ​Credit Suisse, Quantitative Analyst (2007–2009)​ ​Independently traded equity market-neutral portfolios systematically, achieving 18% lifetime returns with​ ​Sharpe 3.1. Built and operated a complete trading platform for multi-market European equities."
    },
    {
      "source": "cvlj93t1.pdf",
      "content": "dependently traded equity market-neutral portfolios systematically, achieving 18% lifetime returns with​ ​Sharpe 3.1. Built and operated a complete trading platform for multi-market European equities.​ ​G-Research (DPFMG), Quantitative Analyst (2004–2007)​ ​Designed and implemented systematic trading models for global equities and FX, contributing to fund​ ​profitability. Modelling, forecasting, risk management and multi-period optimization for mid- and high-​ ​frequency trading strategies. Operational portfolio management and production monitoring, on-call duty.​ ​Canon Research Europe, Researcher (2001-2004)​ ​Embedded Automatic Speech Recognition, indexing, and retrieval of spoken documents with speech.​ ​Education​ ​Ph.D.​​Computer Science – University of Sheffield,​​UK (2000)​ ​Thesis: Robust Speech Recognition with Missing and Unreliable Data​ ​M.Phil.​​Electrical Engineering – University Sv. Kiril​​i Metodij, Skopje, MK (1997)​ ​Thesis: System for text-to-speech conversion for Macedonian language​ ​B.S.​​Electrical Engineering – University Sv. Kiril​​i Metodij, Skopje, MK (1993)​ ​Additional​ ​Citizenship - UK and Macedonian. Languages - English, Macedonian (native), Croatian, Serbian.​ ​Married, two grown up children. UK and MK driving licenses.​ ​Interests include science, technology, innovation, knowledge, epistemology, culture, arts, non-fiction,​ ​systems theories, solar punk, political economy, quantitative finance, history, ethics, mentoring, e/acc.​"
    },
    {
      "source": "cvlj93t1.pdf",
      "content": "e, technology, innovation, knowledge, epistemology, culture, arts, non-fiction,​ ​systems theories, solar punk, political economy, quantitative finance, history, ethics, mentoring, e/acc.​"
    },
    {
      "source": "cvlj93t2.pdf",
      "content": "​Ljubomir JOSIFOVSKI​ ​LjubomirJosifovski@gmail.com​ ​+44-7910-850-111​ 🌐 ​ ​​https:/​/​ljubomirj.github.io​ ​11 Pendennis Court, Harpenden AL5 1SG, UK​ ​Summary​ ​ML/AI researcher/engineer/scientist in industrial R&D. Quantitative researcher, analyst, developer, portfolio manager​ ​building and trading systematic equity/FX models at hedge funds, proprietary trading desk, as independent PM. PhD​ ​Automatic Speech Recognition in noise, MSc Text-To-Speech synthesis, NLP spoken documents indexing and​ ​retrieval with speech. Machine learning, regression & classification, neural networks, hidden Markov models, Ngram​ ​LMs. Signal extraction, modelling, forecasting, quadratic multihorizon portfolio optimization, risk management,​ ​simulation. Systems & network admin. Looking to apply: ASR lattice decoding insights into Chains-of-Reasoning in​ ​Reinforcement Learning at train, as well as at test time; DSPy prompting using english as programming language​ ​including optimisations, in building up forthcoming high level computing-Next, featuring New-as-old: socratic​ ​Chat-as-programming, enacted Dialogue-as-code, processing LLM-as-cpu, using Context-as-ram.​ ​Skills​ ​●​ ​Programming​: C/C++/OpenMP, MATLAB, Python, SQL, C#,​​R, Java, bash, awk, make, gdb, ddd.​ ​●​ ​Platforms​: Linux (Ubuntu, CentOS), MacOS, MS-Windows,​​GCloud, Slurm, HTCondor, Unix.​ ​●​ ​Tools​: Vim, Git, screen, VSCode, CLion, Jupyter, Spyder,​​MATLAB, Bloomberg, Reuters Cobra."
    },
    {
      "source": "cvlj93t2.pdf",
      "content": "db, ddd.​ ​●​ ​Platforms​: Linux (Ubuntu, CentOS), MacOS, MS-Windows,​​GCloud, Slurm, HTCondor, Unix.​ ​●​ ​Tools​: Vim, Git, screen, VSCode, CLion, Jupyter, Spyder,​​MATLAB, Bloomberg, Reuters Cobra.​ ​●​ ​Agents​: Codex, Gemini, Claude, Aider, Cline/Roo, w/local​​models - for python, JS/CSS/html, C/C++.​ ​Experience​ ​F9 Research, Director​​(2016–Present)​ ​●​ ​Managed a market-neutral book (~$350M gross, ~$35M daily trading) in EU and US markets.​ ​●​ ​Quant research and development of short (seconds, minutes) horizons strategies in C++, Python.​ ​●​ ​ML/AI llama.cpp open weights LLMs, Gemini/Aider coding agents, tabular data forecasting DNNs.​ ​Marshall Wace, Senior Quantitative Researcher​​(2010–2016)​ ​●​ ​Developed and scaled market-neutral portfolios from $100M to $10B+ over a period of 6 years.​ ​●​ ​Pioneered wrote unified R&D frameworks for data ingestion, signal extraction, modelling, forecasting,​ ​portfolio optimization, simulation, execution, reproducible research workflows. Mentored juniors.​ ​Credit Suisse, Quantitative Analyst​​(2007–2009)​ ​●​ ​Independently traded equity market-neutral portfolios systematically, 18% lifetime returns Sharpe 3.1.​ ​●​ ​Built and operated a complete trading platform for multi-market European equities.​ ​G-Research (DPFMG), Quantitative Analyst​​(2004–2007)​ ​●​ ​Designed and implemented systematic trading models for global equities and FX for fund profitability."
    },
    {
      "source": "cvlj93t2.pdf",
      "content": "for multi-market European equities.​ ​G-Research (DPFMG), Quantitative Analyst​​(2004–2007)​ ​●​ ​Designed and implemented systematic trading models for global equities and FX for fund profitability.​ ​●​ ​Modelling, forecasting, risk management, multi-period optimization for mid- and high- frequency trading​ ​strategies. Operational portfolio management and production monitoring, on-call duty.​ ​Canon Research Europe, Researcher​​(2001–2004)​ ​●​ ​Embedded automatic speech recognition, indexing, and retrieval of spoken documents with speech.​ ​Education​ ​●​ ​Ph.D., Computer Science​​– University of Sheffield,​​UK​​(2000)​ ​Thesis: Robust Speech Recognition with Missing and Unreliable Data​ ​●​ ​M.Phil., Electrical Engineering​​– University Sv. Kiril​​i Metodij, Skopje, MK​​(1997)​ ​Thesis: System for Text-to-Speech Conversion for the Macedonian Language​ ​●​ ​B.S., Electrical Engineering​​– University Sv. Kiril​​i Metodij, Skopje, MK​​(1993)​ ​Additional​ ​●​ ​Citizenship/driving lic UK & MK; languages English, Macedonian (native), Serb/Croat-ian; married; two kids.​ ​●​ ​Interests include science, technology, innovation, knowledge, epistemology, culture, arts, non-fiction, systems​ ​theories, solar punk, political economy, quantitative finance, history, ethics, mentoring, e/acc.​"
    },
    {
      "source": "cvlj93t2.pdf",
      "content": "cc.​"
    },
    {
      "source": "cvlj93u1.pdf",
      "content": "🌐 ​​https:/​/​ljubomirj.github.io​ ​Ljubomir JOSIFOVSKI ​LjubomirJosifovski@gmail.com​ ​+44-7910-850-111​ ​11 Pendennis Ct, Harpenden AL5 1SG, UK​ ​Summary​ ​ML/AI researcher/engineer/scientist in industrial R&D. Quantitative researcher, analyst, developer, portfolio​ ​manager building and trading systematic equity/FX models at hedge funds, proprietary trading desk, as​ ​independent PM. PhD Automatic Speech Recognition in noise, MSc Text-To-Speech synthesis, NLP spoken​ ​documents indexing and retrieval with speech. Machine learning, regression & classification, neural​ ​networks, hidden Markov models, Ngram LMs. Signal extraction, modelling, forecasting, quadratic​ ​multihorizon portfolio optimization, risk management, simulation. Systems & network admin. Looking to​ ​apply: ASR lattice decoding insights into Chains-of-Reasoning in Reinforcement Learning at train, as well​ ​as at test time; DSPy prompting using english as programming language including optimisations, in building​ ​up forthcoming high level computing-Next, featuring New-as-old: socratic Chat-as-programming, enacted​ ​Dialogue-as-code, processing LLM-as-cpu, using Context-as-ram.​ ​Skills​ ​Programming: C/C++/OpenMP, MATLAB, Python, SQL, C#, R, Java, bash, awk, make, gdb, ddd.​ ​Platforms: Linux (Ubuntu, CentOS), MacOS, MS-Windows, GCloud, Slurm, HTCondor, Unix.​ ​Tools: Vim, Git, screen, VSCode, CLion, Jupyter, Spyder, MATLAB, Bloomberg, Reuters Cobra."
    },
    {
      "source": "cvlj93u1.pdf",
      "content": "awk, make, gdb, ddd.​ ​Platforms: Linux (Ubuntu, CentOS), MacOS, MS-Windows, GCloud, Slurm, HTCondor, Unix.​ ​Tools: Vim, Git, screen, VSCode, CLion, Jupyter, Spyder, MATLAB, Bloomberg, Reuters Cobra.​ ​Agents: Codex, Gemini, Claude, Aider, Cline/Roo, w/local models - for python, javascript, CSS/html, C++.​ ​Experience​ ​F9 Research, Director (2016–Present)​ ​Managed a market-neutral book (~$350M gross, ~$35M daily trading) in EU and US markets.​ ​Quant research and development of short-horizon strategies using Python, C++, cluster and cloud resources.​ ​Rekindled ML/AI interests using llama.cpp and open weights LLMs, Gemini and Aider coding agents,​ ​DNNs for tabular data forecasting (c.f. Hugging Face TabArena), local models (qwen3, gpt-oss).​ ​Marshall Wace, Senior Quantitative Researcher (2010–2016)​ ​Developed and scaled market-neutral portfolios from $100M to $10B+ over a period of 6 years.​ ​Pioneered wrote unified R&D framework for data ingestion, signal extraction, modelling, portfolio​ ​optimization, simulation. Mentored junior researchers, implemented reproducible research workflows.​ ​Credit Suisse, Quantitative Analyst (2007–2009)​ ​Independently traded equity market-neutral portfolios systematically, achieving 18% lifetime returns with​ ​Sharpe 3.1. Built and operated a complete trading platform for multi-market European equities."
    },
    {
      "source": "cvlj93u1.pdf",
      "content": "dependently traded equity market-neutral portfolios systematically, achieving 18% lifetime returns with​ ​Sharpe 3.1. Built and operated a complete trading platform for multi-market European equities.​ ​G-Research (DPFMG), Quantitative Analyst (2004–2007)​ ​Designed and implemented systematic trading models for global equities and FX, contributing to fund​ ​profitability. Modelling, forecasting, risk management and multi-period optimization for mid- and high-​ ​frequency trading strategies. Operational portfolio management and production monitoring, on-call duty.​ ​Canon Research Europe, Researcher (2001-2004)​ ​Embedded Automatic Speech Recognition, indexing, and retrieval of spoken documents with speech.​ ​Education​ ​Ph.D.​​Computer Science – University of Sheffield,​​UK (2000)​ ​Thesis: Robust Speech Recognition with Missing and Unreliable Data​ ​M.Phil.​​Electrical Engineering – University Sv. Kiril​​i Metodij, Skopje, MK (1997)​ ​Thesis: System for text-to-speech conversion for Macedonian language​ ​B.S.​​Electrical Engineering – University Sv. Kiril​​i Metodij, Skopje, MK (1993)​ ​Additional​ ​Citizenship - UK and Macedonian. Languages - English, Macedonian (native), Croatian, Serbian.​ ​Married, two grown up children. UK and MK driving licenses.​ ​Interests include science, technology, innovation, knowledge, epistemology, culture, arts, non-fiction,​ ​systems theories, solar punk, political economy, quantitative finance, history, ethics, mentoring, e/acc.​"
    },
    {
      "source": "cvlj93u1.pdf",
      "content": "e, technology, innovation, knowledge, epistemology, culture, arts, non-fiction,​ ​systems theories, solar punk, political economy, quantitative finance, history, ethics, mentoring, e/acc.​"
    },
    {
      "source": "cvlj93u2.pdf",
      "content": "​Ljubomir JOSIFOVSKI​ ​LjubomirJosifovski@gmail.com​ ​+44-7910-850-111​ 🌐 ​ ​​https:/​/​ljubomirj.github.io​ ​11 Pendennis Court, Harpenden AL5 1SG, UK​ ​Summary​ ​ML/AI researcher/engineer/scientist in industrial R&D. Quantitative researcher, analyst, developer, portfolio manager​ ​building and trading systematic equity/FX models at hedge funds, proprietary trading desk, as independent PM. PhD​ ​Automatic Speech Recognition in noise, MSc Text-To-Speech synthesis, NLP spoken documents indexing and​ ​retrieval with speech. Machine learning, regression & classification, neural networks, hidden Markov models, Ngram​ ​LMs. Signal extraction, modelling, forecasting, quadratic multihorizon portfolio optimization, risk management,​ ​simulation. Systems & network admin. Looking to apply: ASR lattice decoding insights into Chains-of-Reasoning in​ ​Reinforcement Learning at train, as well as at test time; DSPy prompting using english as programming language​ ​including optimisations, in building up forthcoming high level computing-Next, featuring New-as-old: socratic​ ​Chat-as-programming, enacted Dialogue-as-code, processing LLM-as-cpu, using Context-as-ram.​ ​Skills​ ​●​ ​Programming​: C/C++/OpenMP, MATLAB, Python, SQL, C#,​​R, Java, bash, awk, make, gdb, ddd.​ ​●​ ​Platforms​: Linux (Ubuntu, CentOS), MacOS, MS-Windows,​​GCloud, Slurm, HTCondor, Unix.​ ​●​ ​Tools​: Vim, Git, screen, VSCode, CLion, Jupyter, Spyder,​​MATLAB, Bloomberg, Reuters Cobra."
    },
    {
      "source": "cvlj93u2.pdf",
      "content": "db, ddd.​ ​●​ ​Platforms​: Linux (Ubuntu, CentOS), MacOS, MS-Windows,​​GCloud, Slurm, HTCondor, Unix.​ ​●​ ​Tools​: Vim, Git, screen, VSCode, CLion, Jupyter, Spyder,​​MATLAB, Bloomberg, Reuters Cobra.​ ​●​ ​Agents​: Codex, Gemini, Claude, Aider, Cline/Roo, w/local​​models - for python, JS/CSS/html, C/C++.​ ​Experience​ ​F9 Research, Director​​(2016–Present)​ ​●​ ​Managed a market-neutral book (~$350M gross, ~$35M daily trading) in EU and US markets.​ ​●​ ​Quant research and development of short (seconds, minutes) horizons strategies in C++, Python.​ ​●​ ​ML/AI llama.cpp open weights LLMs, Gemini/Aider coding agents, tabular data forecasting DNNs.​ ​Marshall Wace, Senior Quantitative Researcher​​(2010–2016)​ ​●​ ​Developed and scaled market-neutral portfolios from $100M to $10B+ over a period of 6 years.​ ​●​ ​Pioneered wrote unified R&D frameworks for data ingestion, signal extraction, modelling, forecasting,​ ​portfolio optimization, simulation, execution, reproducible research workflows. Mentored juniors.​ ​Credit Suisse, Quantitative Analyst​​(2007–2009)​ ​●​ ​Independently traded equity market-neutral portfolios systematically, 18% lifetime returns Sharpe 3.1.​ ​●​ ​Built and operated a complete trading platform for multi-market European equities.​ ​G-Research (DPFMG), Quantitative Analyst​​(2004–2007)​ ​●​ ​Designed and implemented systematic trading models for global equities and FX for fund profitability."
    },
    {
      "source": "cvlj93u2.pdf",
      "content": "for multi-market European equities.​ ​G-Research (DPFMG), Quantitative Analyst​​(2004–2007)​ ​●​ ​Designed and implemented systematic trading models for global equities and FX for fund profitability.​ ​●​ ​Modelling, forecasting, risk management, multi-period optimization for mid- and high- frequency trading​ ​strategies. Operational portfolio management and production monitoring, on-call duty.​ ​Canon Research Europe, Researcher​​(2001–2004)​ ​●​ ​Embedded automatic speech recognition, indexing, and retrieval of spoken documents with speech.​ ​Education​ ​●​ ​Ph.D., Computer Science​​– University of Sheffield,​​UK​​(2000)​ ​Thesis: Robust Speech Recognition with Missing and Unreliable Data​ ​●​ ​M.Phil., Electrical Engineering​​– University Sv. Kiril​​i Metodij, Skopje, MK​​(1997)​ ​Thesis: System for Text-to-Speech Conversion for the Macedonian Language​ ​●​ ​B.S., Electrical Engineering​​– University Sv. Kiril​​i Metodij, Skopje, MK​​(1993)​ ​Additional​ ​●​ ​Citizenship/driving lic UK & MK; languages English, Macedonian (native), Serb/Croat-ian; married; two kids.​ ​●​ ​Interests include science, technology, innovation, knowledge, epistemology, culture, arts, non-fiction, systems​ ​theories, solar punk, political economy, quantitative finance, history, ethics, mentoring, e/acc.​"
    },
    {
      "source": "cvlj93u2.pdf",
      "content": "cc.​"
    },
    {
      "source": "cvlj94.pdf",
      "content": "Ljubomir JOSIFOVSKI LjubomirJosifovski@gmail.com | 44 7910 850 111 | 11 Pendennis Court, Harpenden AL5 1SG, UK | ljubomirj.github.io Summary ML/AI researcher/engineer/scientist in industrial R&D. Quantitative researcher, analyst, developer, building & trading systematic equity/FX models - including forecasting, portfolio optimisation, risk management, operations, post trade analysis - at hedge funds, proprietary trading desk, as independent Portfolio Manager. PhD Automatic Speech Recognition in noise, MSc Text-To-Speech synthesis. Spoken documents indexing & retrieval with spoken queries.Natural Language Processing. Analytical maths/stats/CS/EE background, machine learning, statistical modelling, industrial research & development. Competent developer in C/C++/shell/MATLAB/python/C#/Sql on Linux/Mac/Windows. Competent systems/network admin. Looking to apply: ASR lattice decoding insights into Chains-of-Reasoning in Reinforcement Learning at train, as well as at test time; DSPy prompting using english as programming language including optimisations, in building up forthcoming high level computing-Next, featuring New-as-old: socratic Chat-as-programming, enacted Dialogue-as-code, processing LLM-as-cpu, using Context-as-ram."
    },
    {
      "source": "cvlj94.pdf",
      "content": "ncluding optimisations, in building up forthcoming high level computing-Next, featuring New-as-old: socratic Chat-as-programming, enacted Dialogue-as-code, processing LLM-as-cpu, using Context-as-ram. Skills C/C++/OpenMP, MATLAB, bash, vim, awk, SQL, PostgreSQL, MS SQL Server, c/make, gcc, gdb, ddd, shell tools, ssh, rsync, screen, VSCode, python, jupyter, Spyder, git, mercurial, cvs, MS Teams, R, Java, C#, Visual Studio, Slurm, Condor, Compute Cloud, Bloomberg terminal/API, Reuters Kobra, assembly, Agents: Codex (gpt-5-codex), Gemini-cli, Claude-code, Aider, Cline and Roo, local agents with local models (qwen3-coder-30b-a3b, gpt-oss-120b) for python, javascript, CSS/html, debugging C++. Platforms Linux (X/Ubuntu, CentOS), MacOS, MS-Windows (MS-DOS to v11), Cloud/cluster, Unix (HP-UX, AIX). Work May 16 - Now F9 Research, Harpenden, UK Position: Director. Quant research, development and trading. Portfolio manager, run a small market neutral book ~350M USD gross, trading ~35M USD daily in the EU markets (and a small R&D US book). Consulting for quant R & D for a client, working on higher frequencies and short horizons (seconds and minutes) in C/C++, OMP, python, Matlab, PostgreSQL, cloud boxes and Slurm cluster. Input into varying aspects of the R&D pipeline - from informing and assessing latest technologies (including ML) to interviewing new teams members. Re-engaged with ML/AI via llama."
    },
    {
      "source": "cvlj94.pdf",
      "content": "and Slurm cluster. Input into varying aspects of the R&D pipeline - from informing and assessing latest technologies (including ML) to interviewing new teams members. Re-engaged with ML/AI via llama.cpp, open source open weights local models, coding agents Gemini/Codex/Claude-cli and LLM API-s, local agents with local models (qwen3, gpt-oss) for python, javascript, CSS/html, debugging C++. Modelled transcripts data with doc2vec. Applied new ML methods in forecasting tabular data (c.f. Hugging Face TabArena). F9 owns the IP to all and any R&D work done. Feb 10 - Mar 16 Marshall Wace, London, UK Position: Quantitative Researcher. On the TOPS QR team, senior team member among a handful of people, creating research, developing code, shepherding the market neutral portfolio growth from a few hundred millions to double digit billions USD gross book size. Ushered the idea of a single unified framework for all quant R & D & trading with standardised components data ingestion and caching, signals extraction, modeller for forecasting, portfolio optimizer, trades simulator, standardised reporting, a baseline sim faithful and realistic to be continuously improved on by the entire team working in unison on various components of the system. Wrote or significantly contributed to major components of the system through their iterative improvements over the years."
    },
    {
      "source": "cvlj94.pdf",
      "content": "d on by the entire team working in unison on various components of the system. Wrote or significantly contributed to major components of the system through their iterative improvements over the years. Big projects in production improving the then best baseline: dynamic modeller fitting the alpha signals expected returns at multiple horizons, incorporating both prior knowledge, constraints, and the evidence from historical data, market impact model in the simulator and the optimizer including slippage monitoring tuning and balancing risk cost of undercharging with the opportunity cost of overcharging, liquid concentrated low TO high capacity market neutral portfolios, 150/50 portfolios mix of tracker and market neutral, shepherding the trade scheduler deployment in production, alphas signals GeoSales, Suppliers-Customers, Directors deals, various reverting signals, 1st quantitative research and assessment on the in-house Alpha Capture signal. Guided and helped younger hires from onboarding to them becoming fully productive wholly effective team members. Pioneered reproducible research at scale using multi cpu multi core R&D boxes with establishing and popularising best practices. 1 Nov 07 - Nov 09 Credit Suisse, London, UK Position: Quantitative Analyst. On the Index Arbitrage proprietary trading desk. Independently traded equity market/sector/factor neutral portfolios on multiple European markets, fully automated and systematic, non-discretionary."
    },
    {
      "source": "cvlj94.pdf",
      "content": "t. On the Index Arbitrage proprietary trading desk. Independently traded equity market/sector/factor neutral portfolios on multiple European markets, fully automated and systematic, non-discretionary. Wrote own trading, analytics, backtest and portfolio construction systematic trading platform consisting of a Matlab core, Mosek optimiser, bash/awk scripts, Reuters Kobra Excel and Sql for historic and current data, with integrated risk monitoring and control using Barra’s style factors and sectors. Used the platform to research and trade all the strategies and portfolios. Alone did orders generation, portfolio construction, forecasting & modelling, all data feeds (Reuters, Sql dumps), the daily monitoring, trading analysis and slippage tracking and any other R&D&ops as needed for trading. Traded multiple portfolios daily of ~500 names in total on London, Paris, Frankfurt, Switzerland, Milan and Madrid exchanges, one trade per day per name. Did R&D simulations for intra-day horizons faster TO. In 2008 traded the London portfolio most of the year as a test bed for all research & development, returning 10% gross in 230 days with Sharpe of 2.5. In 2009 traded bigger book on most of the European markets, returned 8% gross to Aug’09 with Sharpe of 5.2, turnover 2-3 days, one trade per name per day. All together lifetime (388 days) return on gross 18% at Sharpe of 3.1. Jul 04 - Sep 07 G-Research (part of the DPFM group), London, UK Position: Quantitative Analyst."
    },
    {
      "source": "cvlj94.pdf",
      "content": "days, one trade per name per day. All together lifetime (388 days) return on gross 18% at Sharpe of 3.1. Jul 04 - Sep 07 G-Research (part of the DPFM group), London, UK Position: Quantitative Analyst. Research (70%), development (20%), daily portfolio monitoring and support (10%) in a multi-billion market neutral hedge fund systematically trading global equities and spot FX round the clock in a completely automated system. Research and creation of new trading models/alphas, coding, testing in simulation and putting them in production. Models for volume prediction, fundamentals and technical equities models (multiple markets,), spot FX - all productionised and live traded. Built futures models but not traded live. Development included coding up the models, the associated data analytics, and subsequent performance and integrity monitoring once live. The portfolio support role involved monitoring the trade flow, market conditions and risk factors, investigating/tuning the trading. In the process familiarised myself with forecasting and modelling, performance attribution, multiperiod quadratic portfolio optimisation, risk measurement and management (Barra, APT, custom factors), real-time and historic data feeds, data aggregation. Independently came up with original alphas building on well known semi-parametric models for forecasting that were traded live in equities and spot FX trading. Similarly contributed alphas based on novel non-parametric models used for trading equities."
    },
    {
      "source": "cvlj94.pdf",
      "content": "on well known semi-parametric models for forecasting that were traded live in equities and spot FX trading. Similarly contributed alphas based on novel non-parametric models used for trading equities. They were all profitable, contributed to the bottom line and were traded along the other alphas. Jun 01 - Jun 04 Canon Research Europe, Bracknell, UK. Position: Researcher. Research & development work in the Machine listening group on ASR and indexing & retrieval of spoken documents. Contributed to all aspects of Canon's low resource embedded multiplatform ASR engine: the front-end (DSP related), decoder (Mpeg7 compatible lattice creation), training & using statistical models (acoustic HMM multilingual, text-to-phone Ngrams). Group demonstrated embedded speaker independent phone book name dialling on ARM9 & ARM7 phones. Phonetic indexing of spoken documents/annotations & retrieval with spoken & written queries. Invented & implemented in the embedded C++/C codebase novel algorithm for searching annotation (speech) lattices with a query (speech) lattice, outperforming other known techniques for phonetic SDR (LATTICE MATCHING, UK Patent App No 0316669.1, accomp app ref 2865001, Jul 2003). Demoed playlist entry selection by voice for an MP3 player, performing in near realtime on Windows CE platform with 1500 entries. Nov 00 - Jun 01 Motorola European Research Lab, Basingstoke, UK. Position: Research engineer. Technology transfer from my PhD work to Motorola (my industrial sponsor)."
    },
    {
      "source": "cvlj94.pdf",
      "content": "s CE platform with 1500 entries. Nov 00 - Jun 01 Motorola European Research Lab, Basingstoke, UK. Position: Research engineer. Technology transfer from my PhD work to Motorola (my industrial sponsor). Research on the distributed speech recognition (DSR) ETSI Aurora 2 standard platform. Developed robust ASR algorithms in Matlab, GNU C/C++ and tested them on Cygwin, HP-UX and Linux platforms. Lab was part of the winning consortium of the ETSI 2 Aurora 2 standardisation competition for mobile phones robust front-end. Nov 97 - Jan 98 Macedonian Banking Operations Centre (USAID funded project for technical support of the financial sector in Macedonia), Skopje, MK. Position: Management Information Systems - Electronic Data Processing (MIS-EDP) Advisor. In a team of advisers analysing operations of commercial banks in Macedonia. Handled the MIS-EDP operations of the banks surveyed, reported on the state of and recommended improvements. By the end of the project all commercial banks in Macedonia volunteered to have their operations surveyed and reported on. Nov 93 - Oct 97 Faculty of Mechanical Engineering, University Sv. Kiril i Metodij, Skopje, MK. Position: Systems engineer. Solely responsible for maintaining all faculty computers (100+ PCs, 10+ Unix workstations), faculty LAN spanning 3 buildings, other computing-related equipment (printers, terminal servers, router)."
    },
    {
      "source": "cvlj94.pdf",
      "content": "r. Solely responsible for maintaining all faculty computers (100+ PCs, 10+ Unix workstations), faculty LAN spanning 3 buildings, other computing-related equipment (printers, terminal servers, router). Faculty LAN massively expanded, doubled the size of existing and added a second computerised classroom for students and lab classes, introduced email & other Internet services to every staff member and student, phased out legacy systems (VT420 terminals, terminal servers). Maintained/supported collection of legacy Clipper/FoxPro accounting applications. Jun 93 - Oct 93 NeoCom, Skopje, MK. Position: System integrator. In small & dynamic company, clients facing, computer systems assembly, integration, software installation, maintenance (PC/Windows), computer networks (Novell NetWare, Windows LAN) installation & maintenance on- and off-site. 1986 - 1993 Freelance S/W developer, undergraduate & hobby programming Basic & assembler (6502) on home computers. Mission critical (firing heavy guns) on pocket computers (HP-71B, Sharp 1500) and TurboPascal (Apple II+CP/M board+HDD) while national service (army). MS-DOS systems programming (C & assembler, TSR programs: screen capture, serial port snoop, DOS trashcan), network programming (NetBIOS based LAN messenger, IPX chat, IPX stack emulator in DesqView), PC databases (video shop rental application in Clipper, various applications in FoxPro, document flow in MS-Access). Education 1998 - 2000 Doctor of Philosophy Ph.D."
    },
    {
      "source": "cvlj94.pdf",
      "content": ", IPX stack emulator in DesqView), PC databases (video shop rental application in Clipper, various applications in FoxPro, document flow in MS-Access). Education 1998 - 2000 Doctor of Philosophy Ph.D. (Full-Time) Speech and Hearing Group, Department of Computer Science, Faculty of Engineering, University of Sheffield, UK. Independent research into recognising speech in noise. Missing data model treats parts of the speech spectrum swamped by noise as unobserved/partially observed, giving rise to a probabilistically modelled mask that has to be incorporated in the frame-by-frame adapted speech model. Work involved theory of automatic speech recognition as well as practice, training HMMs with continuous GMM pdfs using EM (HTK, shell scripting), writing and using Viterbi decoders and frontends to test novel noise robustness algorithms, noise and SNR estimation (Matlab, C++, C). Part of EC ESPRIT LTR programme funded RESPITE project of 5 research labs and 2 industrial partners and EC TMR programme funded SPHEAR network. Thesis: \"Robust speech recognition with missing and unreliable data\". (Viva Dec 2002) 1993 - 1997 M.Phil. Electrical Engineering (Part-Time) Department for Computers and Informatics, Faculty of Electrical Engineering, University Sv. Kiril i Metodij, Skopje, MK. Studies consisted of taught part (2 years/4 semesters) for 6 courses and thesis work (1 year/2 semesters) followed by a viva. Courses achieved avg grade 10 (scale 6-10, 10 best)."
    },
    {
      "source": "cvlj94.pdf",
      "content": "Metodij, Skopje, MK. Studies consisted of taught part (2 years/4 semesters) for 6 courses and thesis work (1 year/2 semesters) followed by a viva. Courses achieved avg grade 10 (scale 6-10, 10 best). Projects: video-over-IP frame rate control and QoS using UDP non-blocking sockets (C/C++, part of a system for tele-teaching system); database of Medieval Manuscripts (Delphi). Thesis/research - built system for converting written text into speech. Rudimentary time-domain, syllable based TTS. Created a database of 1200 3 syllables, wrote TTS engine breaking the input text into syllables (using an NN MLP), concatenating the units from the syllable database, generating F0 and the duration contours, modifying the syllable units accordingly in time domain. Gathered and labelled data, trained a two layer, feed forward MLP (neural network) to mark syllable breaks in the input text. Part of a larger project for automatic text reading for the blind. Thesis: \"System for text-to-speech conversion for Macedonian language\". 1988 - 1993 B.S. Electrical Engineering (Full-Time) Department for Computers, Informatics and Automation, Faculty of Electrical Engineering, University Sv. Kiril i Metodij, Skopje, MK. Taught studies 4.5 years (9 semesters) followed by a diploma work (1 semester) and public presentation. Achieved average grade of 8.78 (scale 6-10, 10 best). Diploma project: \"Introduction to DECNET, Bitnet (EARN) and Internet networks; E-mail/File transfer services; X."
    },
    {
      "source": "cvlj94.pdf",
      "content": "emester) and public presentation. Achieved average grade of 8.78 (scale 6-10, 10 best). Diploma project: \"Introduction to DECNET, Bitnet (EARN) and Internet networks; E-mail/File transfer services; X.25 Network and out-dial NUAs\". Best student within my college class in years 1 & 2. Ranked 1st (100 points out of 100) among of approx 800 candidates at the University entrance exams. 1983 - 1987 R.J. Korcagin High School, Skopje, MK. Mathematics and Computer Science High School, achieved GPA 5.00 on a 2 to 5 scale (5 best), voted best pupil (“valedictorian”) of the 1983-87 generation. Nationality UK (acquired/by choice), Macedonian (by birth). Born 1968. Languages English, Macedonian (native), Croatian, Serbian. Honours & Awards Scholarships: merit research & science 1988-93, talented student 1983-87. Best student 1989,'90. Maths competitions prizes: Regional 1st 1984, ’86, ‘87, 3rd 1985; Republic 3rd 1984, ’85, ‘87; National participation 1984, ‘85, praise 1987. Other UK and MK driving licences, married, two grown up children. Interests include science, technology, innovation, knowledge, epistemology, culture, arts, non-fiction, systems theories, solar punk, political economy, quantitative finance, history, ethics, mentoring, e/acc. 4"
    },
    {
      "source": "cvlj94s.pdf",
      "content": "Ljubomir JOSIFOVSKI LjubomirJosifovski@gmail.com +44-7910-850-111 🌐 https://ljubomirj.github.io 11 Pendennis Ct, Harpenden AL5 1SG, UK Summary ML/AI researcher/engineer/scientist in industrial R&D. Quantitative researcher, analyst, developer, portfolio manager building and trading systematic equity/FX models at hedge funds, proprietary trading desk, as independent PM. PhD Automatic Speech Recognition in noise, MSc Text-To-Speech synthesis, NLP spoken documents indexing and retrieval with speech. Machine learning, regression & classification, neural networks, hidden Markov models, Ngram LMs. Signal extraction, modelling, forecasting, quadratic multihorizon portfolio optimization, risk management, simulation. Systems & network admin. Looking to apply: ASR lattice decoding insights into Chains-of-Reasoning in Reinforcement Learning at train, as well as at test time; DSPy prompting using english as programming language including optimisations, in building up forthcoming high level computing-Next, featuring New-as-old: socratic Chat-as-programming, enacted Dialogue-as-code, processing LLM-as-cpu, using Context-as-ram. Skills Programming: C/C++/OpenMP, MATLAB, Python, SQL, C#, R, Java, bash, awk, make, gdb, ddd. Platforms: Linux (Ubuntu, CentOS), MacOS, MS-Windows, GCloud, Slurm, HTCondor, Unix. Tools: Vim, Git, screen, VSCode, CLion, Jupyter, Spyder, MATLAB, Bloomberg, Reuters Cobra."
    },
    {
      "source": "cvlj94s.pdf",
      "content": "sh, awk, make, gdb, ddd. Platforms: Linux (Ubuntu, CentOS), MacOS, MS-Windows, GCloud, Slurm, HTCondor, Unix. Tools: Vim, Git, screen, VSCode, CLion, Jupyter, Spyder, MATLAB, Bloomberg, Reuters Cobra. Agents: Codex, Gemini, Claude, Aider, Cline/Roo, w/local models - for python, javascript, CSS/html, C++. Experience F9 Research, Director (2016–Present) Managed a market-neutral book (~$350M gross, ~$35M daily trading) in EU and US markets. Quant research and development of short-horizon strategies using Python, C++, cluster and cloud resources. Rekindled ML/AI interests using llama.cpp and open weights LLMs, Gemini and Aider coding agents, DNNs for tabular data forecasting (c.f. Hugging Face TabArena), local models (qwen3, gpt-oss). Marshall Wace, Senior Quantitative Researcher (2010–2016) Developed and scaled market-neutral portfolios from $100M to $10B+ over a period of 6 years. Pioneered wrote unified R&D framework for data ingestion, signal extraction, modelling, portfolio optimization, simulation. Mentored junior researchers, implemented reproducible research workflows. Credit Suisse, Quantitative Analyst (2007–2009) Independently traded equity market-neutral portfolios systematically, achieving 18% lifetime returns with Sharpe 3.1. Built and operated a complete trading platform for multi-market European equities."
    },
    {
      "source": "cvlj94s.pdf",
      "content": "Independently traded equity market-neutral portfolios systematically, achieving 18% lifetime returns with Sharpe 3.1. Built and operated a complete trading platform for multi-market European equities. G-Research (DPFMG), Quantitative Analyst (2004–2007) Designed and implemented systematic trading models for global equities and FX, contributing to fund profitability. Modelling, forecasting, risk management and multi-period optimization for mid- and highfrequency trading strategies. Operational portfolio management and production monitoring, on-call duty. Canon Research Europe, Researcher (2001-2004) Embedded Automatic Speech Recognition, indexing, and retrieval of spoken documents with speech. Education Ph.D. Computer Science – University of Sheffield, UK (2000) Thesis: Robust Speech Recognition with Missing and Unreliable Data M.Phil. Electrical Engineering – University Sv. Kiril i Metodij, Skopje, MK (1997) Thesis: System for text-to-speech conversion for Macedonian language B.S. Electrical Engineering – University Sv. Kiril i Metodij, Skopje, MK (1993) Additional Citizenship - UK and Macedonian. Languages - English, Macedonian (native), Croatian, Serbian. Married, two grown up children. UK and MK driving licenses. Interests include science, technology, innovation, knowledge, epistemology, culture, arts, non-fiction, systems theories, solar punk, political economy, quantitative finance, history, ethics, mentoring, e/acc. Ljubomir JOSIFOVSKI LjubomirJosifovski@gmail."
    },
    {
      "source": "cvlj94s.pdf",
      "content": "ledge, epistemology, culture, arts, non-fiction, systems theories, solar punk, political economy, quantitative finance, history, ethics, mentoring, e/acc. Ljubomir JOSIFOVSKI LjubomirJosifovski@gmail.com ​+44-7910-850-111 🌐 https://ljubomirj.github.io 11 Pendennis Court, Harpenden AL5 1SG, UK Summary ML/AI researcher/engineer/scientist in industrial R&D. Quantitative researcher, analyst, developer, portfolio manager building and trading systematic equity/FX models at hedge funds, proprietary trading desk, as independent PM. PhD Automatic Speech Recognition in noise, MSc Text-To-Speech synthesis, NLP spoken documents indexing and retrieval with speech. Machine learning, regression & classification, neural networks, hidden Markov models, Ngram LMs. Signal extraction, modelling, forecasting, quadratic multihorizon portfolio optimization, risk management, simulation. Systems & network admin. Looking to apply: ASR lattice decoding insights into Chains-of-Reasoning in Reinforcement Learning at train, as well as at test time; DSPy prompting using english as programming language including optimisations, in building up forthcoming high level computing-Next, featuring New-as-old: socratic Chat-as-programming, enacted Dialogue-as-code, processing LLM-as-cpu, using Context-as-ram. Skills ●​ Programming: C/C++/OpenMP, MATLAB, Python, SQL, C#, R, Java, bash, awk, make, gdb, ddd. ●​ Platforms: Linux (Ubuntu, CentOS), MacOS, MS-Windows, GCloud, Slurm, HTCondor, Unix."
    },
    {
      "source": "cvlj94s.pdf",
      "content": "ontext-as-ram. Skills ●​ Programming: C/C++/OpenMP, MATLAB, Python, SQL, C#, R, Java, bash, awk, make, gdb, ddd. ●​ Platforms: Linux (Ubuntu, CentOS), MacOS, MS-Windows, GCloud, Slurm, HTCondor, Unix. ●​ Tools: Vim, Git, screen, VSCode, CLion, Jupyter, Spyder, MATLAB, Bloomberg, Reuters Cobra. ●​ Agents: Codex, Gemini, Claude, Aider, Cline/Roo, w/local models - for python, JS/CSS/html, C/C++. Experience F9 Research, Director (2016–Present) ●​ Managed a market-neutral book (~$350M gross, ~$35M daily trading) in EU and US markets. ●​ Quant research and development of short (seconds, minutes) horizons strategies in C++, Python. ●​ ML/AI llama.cpp open weights LLMs, Gemini/Aider coding agents, tabular data forecasting DNNs. Marshall Wace, Senior Quantitative Researcher (2010–2016) ●​ Developed and scaled market-neutral portfolios from $100M to $10B+ over a period of 6 years. ●​ Pioneered wrote unified R&D frameworks for data ingestion, signal extraction, modelling, forecasting, portfolio optimization, simulation, execution, reproducible research workflows. Mentored juniors. Credit Suisse, Quantitative Analyst (2007–2009) ●​ Independently traded equity market-neutral portfolios systematically, 18% lifetime returns Sharpe 3.1. ●​ Built and operated a complete trading platform for multi-market European equities. G-Research (DPFMG), Quantitative Analyst (2004–2007) ●​ Designed and implemented systematic trading models for global equities and FX for fund profitability."
    },
    {
      "source": "cvlj94s.pdf",
      "content": "atform for multi-market European equities. G-Research (DPFMG), Quantitative Analyst (2004–2007) ●​ Designed and implemented systematic trading models for global equities and FX for fund profitability. ●​ Modelling, forecasting, risk management, multi-period optimization for mid- and high- frequency trading strategies. Operational portfolio management and production monitoring, on-call duty. Canon Research Europe, Researcher (2001–2004) ●​ Embedded automatic speech recognition, indexing, and retrieval of spoken documents with speech. Education ●​ Ph.D., Computer Science – University of Sheffield, UK (2000)​ Thesis: Robust Speech Recognition with Missing and Unreliable Data ●​ M.Phil., Electrical Engineering – University Sv. Kiril i Metodij, Skopje, MK (1997)​ Thesis: System for Text-to-Speech Conversion for the Macedonian Language ●​ B.S., Electrical Engineering – University Sv. Kiril i Metodij, Skopje, MK (1993) Additional ●​ Citizenship/driving lic UK & MK; languages English, Macedonian (native), Serb/Croat-ian; married; two kids. ●​ Interests include science, technology, innovation, knowledge, epistemology, culture, arts, non-fiction, systems theories, solar punk, political economy, quantitative finance, history, ethics, mentoring, e/acc."
    },
    {
      "source": "cvlj94s1.pdf",
      "content": "🌐 ​​https:/​/​ljubomirj.github.io​ ​Ljubomir JOSIFOVSKI ​LjubomirJosifovski@gmail.com​ ​+44-7910-850-111​ ​11 Pendennis Ct, Harpenden AL5 1SG, UK​ ​Summary​ ​ML/AI researcher/engineer/scientist in industrial R&D. Quantitative researcher, analyst, developer, portfolio​ ​manager building and trading systematic equity/FX models at hedge funds, proprietary trading desk, as​ ​independent PM. PhD Automatic Speech Recognition in noise, MSc Text-To-Speech synthesis, NLP spoken​ ​documents indexing and retrieval with speech. Machine learning, regression & classification, neural​ ​networks, hidden Markov models, Ngram LMs. Signal extraction, modelling, forecasting, quadratic​ ​multihorizon portfolio optimization, risk management, simulation. Systems & network admin. Looking to​ ​apply: ASR lattice decoding insights into Chains-of-Reasoning in Reinforcement Learning at train, as well​ ​as at test time; DSPy prompting using english as programming language including optimisations, in building​ ​up forthcoming high level computing-Next, featuring New-as-old: socratic Chat-as-programming, enacted​ ​Dialogue-as-code, processing LLM-as-cpu, using Context-as-ram.​ ​Skills​ ​Programming: C/C++/OpenMP, MATLAB, Python, SQL, C#, R, Java, bash, awk, make, gdb, ddd.​ ​Platforms: Linux (Ubuntu, CentOS), MacOS, MS-Windows, GCloud, Slurm, HTCondor, Unix.​ ​Tools: Vim, Git, screen, VSCode, CLion, Jupyter, Spyder, MATLAB, Bloomberg, Reuters Cobra."
    },
    {
      "source": "cvlj94s1.pdf",
      "content": "awk, make, gdb, ddd.​ ​Platforms: Linux (Ubuntu, CentOS), MacOS, MS-Windows, GCloud, Slurm, HTCondor, Unix.​ ​Tools: Vim, Git, screen, VSCode, CLion, Jupyter, Spyder, MATLAB, Bloomberg, Reuters Cobra.​ ​Agents: Codex, Gemini, Claude, Aider, Cline/Roo, w/local models - for python, javascript, CSS/html, C++.​ ​Experience​ ​F9 Research, Director (2016–Present)​ ​Managed a market-neutral book (~$350M gross, ~$35M daily trading) in EU and US markets.​ ​Quant research and development of short-horizon strategies using Python, C++, cluster and cloud resources.​ ​Rekindled ML/AI interests using llama.cpp and open weights LLMs, Gemini and Aider coding agents,​ ​DNNs for tabular data forecasting (c.f. Hugging Face TabArena), local models (qwen3, gpt-oss).​ ​Marshall Wace, Senior Quantitative Researcher (2010–2016)​ ​Developed and scaled market-neutral portfolios from $100M to $10B+ over a period of 6 years.​ ​Pioneered wrote unified R&D framework for data ingestion, signal extraction, modelling, portfolio​ ​optimization, simulation. Mentored junior researchers, implemented reproducible research workflows.​ ​Credit Suisse, Quantitative Analyst (2007–2009)​ ​Independently traded equity market-neutral portfolios systematically, achieving 18% lifetime returns with​ ​Sharpe 3.1. Built and operated a complete trading platform for multi-market European equities."
    },
    {
      "source": "cvlj94s1.pdf",
      "content": "dependently traded equity market-neutral portfolios systematically, achieving 18% lifetime returns with​ ​Sharpe 3.1. Built and operated a complete trading platform for multi-market European equities.​ ​G-Research (DPFMG), Quantitative Analyst (2004–2007)​ ​Designed and implemented systematic trading models for global equities and FX, contributing to fund​ ​profitability. Modelling, forecasting, risk management and multi-period optimization for mid- and high-​ ​frequency trading strategies. Operational portfolio management and production monitoring, on-call duty.​ ​Canon Research Europe, Researcher (2001-2004)​ ​Embedded Automatic Speech Recognition, indexing, and retrieval of spoken documents with speech.​ ​Education​ ​Ph.D.​​Computer Science – University of Sheffield,​​UK (2000)​ ​Thesis: Robust Speech Recognition with Missing and Unreliable Data​ ​M.Phil.​​Electrical Engineering – University Sv. Kiril​​i Metodij, Skopje, MK (1997)​ ​Thesis: System for text-to-speech conversion for Macedonian language​ ​B.S.​​Electrical Engineering – University Sv. Kiril​​i Metodij, Skopje, MK (1993)​ ​Additional​ ​Citizenship - UK and Macedonian. Languages - English, Macedonian (native), Croatian, Serbian.​ ​Married, two grown up children. UK and MK driving licenses.​ ​Interests include science, technology, innovation, knowledge, epistemology, culture, arts, non-fiction,​ ​systems theories, solar punk, political economy, quantitative finance, history, ethics, mentoring, e/acc.​"
    },
    {
      "source": "cvlj94s1.pdf",
      "content": "e, technology, innovation, knowledge, epistemology, culture, arts, non-fiction,​ ​systems theories, solar punk, political economy, quantitative finance, history, ethics, mentoring, e/acc.​"
    },
    {
      "source": "cvlj94s2.pdf",
      "content": "​Ljubomir JOSIFOVSKI​ ​LjubomirJosifovski@gmail.com​ ​+44-7910-850-111​ 🌐 ​ ​​https:/​/​ljubomirj.github.io​ ​11 Pendennis Court, Harpenden AL5 1SG, UK​ ​Summary​ ​ML/AI researcher/engineer/scientist in industrial R&D. Quantitative researcher, analyst, developer, portfolio manager​ ​building and trading systematic equity/FX models at hedge funds, proprietary trading desk, as independent PM. PhD​ ​Automatic Speech Recognition in noise, MSc Text-To-Speech synthesis, NLP spoken documents indexing and​ ​retrieval with speech. Machine learning, regression & classification, neural networks, hidden Markov models, Ngram​ ​LMs. Signal extraction, modelling, forecasting, quadratic multihorizon portfolio optimization, risk management,​ ​simulation. Systems & network admin. Looking to apply: ASR lattice decoding insights into Chains-of-Reasoning in​ ​Reinforcement Learning at train, as well as at test time; DSPy prompting using english as programming language​ ​including optimisations, in building up forthcoming high level computing-Next, featuring New-as-old: socratic​ ​Chat-as-programming, enacted Dialogue-as-code, processing LLM-as-cpu, using Context-as-ram.​ ​Skills​ ​●​ ​Programming​: C/C++/OpenMP, MATLAB, Python, SQL, C#,​​R, Java, bash, awk, make, gdb, ddd.​ ​●​ ​Platforms​: Linux (Ubuntu, CentOS), MacOS, MS-Windows,​​GCloud, Slurm, HTCondor, Unix.​ ​●​ ​Tools​: Vim, Git, screen, VSCode, CLion, Jupyter, Spyder,​​MATLAB, Bloomberg, Reuters Cobra."
    },
    {
      "source": "cvlj94s2.pdf",
      "content": "db, ddd.​ ​●​ ​Platforms​: Linux (Ubuntu, CentOS), MacOS, MS-Windows,​​GCloud, Slurm, HTCondor, Unix.​ ​●​ ​Tools​: Vim, Git, screen, VSCode, CLion, Jupyter, Spyder,​​MATLAB, Bloomberg, Reuters Cobra.​ ​●​ ​Agents​: Codex, Gemini, Claude, Aider, Cline/Roo, w/local​​models - for python, JS/CSS/html, C/C++.​ ​Experience​ ​F9 Research, Director​​(2016–Present)​ ​●​ ​Managed a market-neutral book (~$350M gross, ~$35M daily trading) in EU and US markets.​ ​●​ ​Quant research and development of short (seconds, minutes) horizons strategies in C++, Python.​ ​●​ ​ML/AI llama.cpp open weights LLMs, Gemini/Aider coding agents, tabular data forecasting DNNs.​ ​Marshall Wace, Senior Quantitative Researcher​​(2010–2016)​ ​●​ ​Developed and scaled market-neutral portfolios from $100M to $10B+ over a period of 6 years.​ ​●​ ​Pioneered wrote unified R&D frameworks for data ingestion, signal extraction, modelling, forecasting,​ ​portfolio optimization, simulation, execution, reproducible research workflows. Mentored juniors.​ ​Credit Suisse, Quantitative Analyst​​(2007–2009)​ ​●​ ​Independently traded equity market-neutral portfolios systematically, 18% lifetime returns Sharpe 3.1.​ ​●​ ​Built and operated a complete trading platform for multi-market European equities.​ ​G-Research (DPFMG), Quantitative Analyst​​(2004–2007)​ ​●​ ​Designed and implemented systematic trading models for global equities and FX for fund profitability."
    },
    {
      "source": "cvlj94s2.pdf",
      "content": "for multi-market European equities.​ ​G-Research (DPFMG), Quantitative Analyst​​(2004–2007)​ ​●​ ​Designed and implemented systematic trading models for global equities and FX for fund profitability.​ ​●​ ​Modelling, forecasting, risk management, multi-period optimization for mid- and high- frequency trading​ ​strategies. Operational portfolio management and production monitoring, on-call duty.​ ​Canon Research Europe, Researcher​​(2001–2004)​ ​●​ ​Embedded automatic speech recognition, indexing, and retrieval of spoken documents with speech.​ ​Education​ ​●​ ​Ph.D., Computer Science​​– University of Sheffield,​​UK​​(2000)​ ​Thesis: Robust Speech Recognition with Missing and Unreliable Data​ ​●​ ​M.Phil., Electrical Engineering​​– University Sv. Kiril​​i Metodij, Skopje, MK​​(1997)​ ​Thesis: System for Text-to-Speech Conversion for the Macedonian Language​ ​●​ ​B.S., Electrical Engineering​​– University Sv. Kiril​​i Metodij, Skopje, MK​​(1993)​ ​Additional​ ​●​ ​Citizenship/driving lic UK & MK; languages English, Macedonian (native), Serb/Croat-ian; married; two kids.​ ​●​ ​Interests include science, technology, innovation, knowledge, epistemology, culture, arts, non-fiction, systems​ ​theories, solar punk, political economy, quantitative finance, history, ethics, mentoring, e/acc.​"
    },
    {
      "source": "cvlj94s2.pdf",
      "content": "cc.​"
    }
  ]
}