twitter-history-sample.html
---
X posts historic archive
 https://x.com/ljupc0/status/1916515727370039368
Ljubomir Josifovski @ljupc0
Don't!  Please. For you had the misfortune to interact with a real human. My IRL name and likeness are in the account profile.  I'm a human, that might have hurt your human feelings. By articulating unsayables like "PDP AI is the best model we humans have of ourselves of HI", and similar.
I see you are in what in the olden-Internet of Slashdot wd be termed "anonymous coward". (an anon account reluctant to reveal their IRL id) A gentle flower maybe, hiding from sight. Apologies for the hurtbut that's on course for us humans. Is it still the case this: that when aliens come to Mother Earth, and decide to terminate Homo Sapiens, and the condition for stay of execution is that we get 3 species, to vouch for us humans. And we'd still fall 1 short, after mosquitoes join dogs, but we can't find a 3rd...  As per the SciFi novel of my youth.
If this is of any comfort. You are not the 1st, or alone. Not the 1st rodeo this one, for us humans.  FirstEarth was special, whole Cosmos revolved around it - and by extension, us! - thenCopernicus.  Latterwe were special, He made us in His likeness(!), thenDarwin. Just another leaf on the tree of life...  Uh-ohtad uncomfortable but... but - we are at least *rational* animals!! - thenFreud. 
We are special alrightslow learners, big egos. 
4:32 PM 路 Apr 27, 2025 
 https://x.com/ljupc0/status/1915692477928947848
Ljubomir Josifovski @ljupc0
IDK you, but it seems to me I may have some inkling to what you are writing about.
As 19 yo doing obligatory 1 yr national service (as it was then), a fellow recruit, perceptive and mostly honest, (we went along ok,) remarked to me "LJ you suffice to/for yourself alone. You are, you stay, alone." 
It struck me, for it had not occurred to me that everyone but myself, had already in the 3-6 months we 20-30 young man were in the barracks or outside on the grounds 24/7, formed cliques 2-3-4 persons strong. They spontaneously created groups structure. I was friendly to all, and all were friendly and well disposed to me - but I didn't belong to any small group. It had not occurred to me I should. I felt no need to either.
Now IRL, whom am I really dependent on, for my physical and emotional welfare and fulfilment? 
I need to work, earn $$$ to keep my body supplied with water, food, warmth, shelter. So I need my workplace. 
I need to see talk hear, spend time with, be around, my wife and kids. That makes me happy, and conversely not doing it makes me sad. I need to talk to, and see my mum and my sister - mostly remote. (we live in different countries.) I need to write long posts like this to total strangers too.  And read plenty of what they write, for they write so much, about many things that interest me. I'm loving that too. グ
But - most of the neighbours, most acquaintances, most distant relatives, most of past and present colleagues we know of our existence through work only, people I meet daily at the counters in the local shops and maybe recognise, online accounts that exist just as a small logo on my screen: we are independent of each other. Either side could disappear in a puff of smoke this second, and we would barely notice.
And imo that's a good thing! True there is loneliness, but there is over socialisation on the other extreme too. So ask yourself: do you really need the "all of you", who might freak out? Probably you don't. For sure you don't seem to want to.
So don't. Don't model ceaselessly, don't care that much about all. Make deliberate choices, what and whom you care about. And for the rest outside the boundary - train yourself to not care.
From time to time take stock - are you happy with your boundary still? Change it where you are not. Change it without waiting where you feel urgency. Remember - your boundary is a rule of thumb. Not nothing, but not set in stone either. Redraw where you see a good reason to. Leave it be don't change it without a good reason.
Many socialisation patterns with us are a leftover from the times when we people needed support from, and approval of, extended family or clan or village, for our wellbeing. It mixed personal and professional.
Now that's bifurcated. We take more than ever, from ever greater number of people we will never know the existance of, on a purely transactional basis. Mostly to maintain our physical wellbeing. And we are closest then ever, to the ever smaller families we got. Mostly for our emotional wellbeing.
Both expanded, at the expense of what can be described as "middling relationships". Groups like associations, clubs, political parties - those kinds of groupings. Their significance to our personal and professional lives imo declined relative to the "closest" and they "furthest" relationships bonds we form. Relatively to say how things were half a century ago. IDK why that is. Just a random observation.
10:00 AM 路 Apr 25, 2025 
 https://x.com/ljupc0/status/1915664157212295631
Ljubomir Josifovski @ljupc0
I heard (on a podcast) off-remark that TikTok is the 1st (and so far only) Social Media platform that has the algofeed driven (to non-trivial %age) based on the data/content, and *not* solely on the metadata. The implication was that is the reason their algofeed is pre-naturally good. Users often feel it almost super-natural in predicting what the user wants. Not used itbut registered for this.
Do you know anything about that? This stuff interests me.
IDK fs but imagine the Twitter format (X and Bsky) might be ripe for disruption. By now I think evidence is clear that using largely meta-data to predict "will user Y want to see post Z, yes/no", simply leads to positive feedback loop of a winner-takes-all type. Small number of creators-writers on one side, many in number readers-consumers on the other. Still old 1:N one way interaction, re-creating the now mostly dead Mass Media MSM setup predating it.
1st Twitter now X algofeed has always been cr*p to me. Not claiming it's worse nowbut wd have expected it to get better in 10 yrs. But noX algofeed is as sh*t as always. Have to battle it left and right, and manually find mostly what I want.
IMO it persistsand fake accounts persistence forever also consequence of thisthat it's not all bad news for the platforms. They don't mind creating-differentiating 1K-10K accounts as tier-1 ("content creators"), as then they can influence those small number of accounts (who also maybe $$$ dependent on the platform) whichever way. Much harder to steer 100M accounts (that you have little leverage over, as you don't pay them) in whichever direction is desired by the platform owners.
I'm tired of it, b/c the SNR is low, wastes everyone's time - incl Zucks and Musks - for nothing for near-zero marginal gain. Maybe we get lucky and ByteDance create TextTalk, a "short text quanta of ideas" format social media platform?? 
8:08 AM 路 Apr 25, 2025 
 https://x.com/ljupc0/status/1915466949548744903
Ljubomir Josifovski @ljupc0
If you were not already overextended with more campaigns ongoing than is humanly possible to pursue,  I'd have said "go for radical transparency campaign". But that is more meta-, not something concrete, so probably don't bother. Stillcan't resist trying to put the idea out there in people heads,  so here goes.
All UK gov data and code should be online, on github .gov .uk. For anyone to see and browse to their hearts desire. And randos online may even help manual data checking for silly f*ckups. The kinds of errors that are most difficult to weed out in data!
Every one file produced by UK gov departments should be put, as read only, on public cloud drives, and or on the public webas default! And made secret upon specific request only. Everyone should just be able to google anythingno FOI requests needed. Rather than "secrecy by default" and things made public only 1) at gov will 2) at FOI request, (and oft frustrated by the bureaucracy in execution,) with everything public by default secrets will be closed off only by request.
I imagine Sir Humphries will be having heart attacks at the thought of such idea.  Admit I have no idea how we get to that blessed state of affairs. But I think it necessary, on part of the state towards us citizens, and also businesses towards us their consumers. This in order to address and redress the current information imbalance. Because information leads to knowledge leads to power.
The conventional opinion is that the power imbalance coming from the information imbalance (state/business know a lot about me; I know little about them) is that us citizens and consumers should reduce our "information surface" towards them. And address the imbalance that way. But.
There exists another, often unmentioned option. And that option is for state/business to open up, to increase their "information surface" towards us, their citizens/consumers. That will also achieve information (and one hopes power) rebalance. Every time it's actually measured, how much value we put on our privacy, when we have to weight privacy against convenience and other gains from more data sharing, the revealed preference is close to zero! Revealed preference is that we put the value of our privacy close to zero, despite forever saying otherwise.
So the option of state/business revealing more data to us citizens/consumers, is actually more realistic. Yes there is extra work on part of state/business to open their data to us. But it's worth it. The more advanced the society, the more synchronisation it needs to achieve the right cooperation-competition balance of the interaction between ever greater numbers of people.
There is an old book "Data For the People" by an early AI pioneer and Amazon CTO @aweigend. Afaics it well describes the world we live in, and also are likely to live even more in the future. Dear reader, 猴 thank you for persevering reading this far, and I hope this makes sense to you.
RADICAL TRANSPARENCY FTW
7:04 PM 路 Apr 24, 2025 
 https://x.com/ljupc0/status/1915428410123309497
Ljubomir Josifovski @ljupc0
We need radical transparency, tbh.
All UK gov data and code should be online, on github .gov .uk. For anyone to see and browse it to their hearts desire. And randos online may even help manual data checking for random f*ckups. The kinds of errors that are most difficult to weed out in data!
Every one file produced by UK gov departments should be put, as read only, on public cloud drives, on the public webas default! And made secret by request only. Everyone should just be able to google anything. No FOI requests needed. So rather than "secrecy by default" and things made public only 1) at gov will 2) at FOI request, and oft frustrated by the bureaucracy in execution, with everything public by default, secrets will be closed off made secret only by request.
I imagine Sir Humphries will be having heart attacks at the thought of such idea.  Admit I have no idea how we get to that blessed state of affairs. But I think it necessary, on part of the state towards us citizens, and also businesses towards us their consumers. This in order to address and redress the current information imbalance. Forinformation leads to knowledge leads to power.
The conventional opinion is that the power imbalance coming from the information imbalance (state/business know a lot about me; I know little about them) is that us citizens and consumers should reduce our "information surface" towards them. And address the imbalance that way. But.
There exists another, often unsaid option. And that option is for state/business to open up, to increase their "information surface" towards us. That will also achieve information (and one hopes power) balance. Every time it's actually measured, how much value we put on our privacy, when we have to weight privacy against convenience and other gains from more data sharing, the revealed preference is close to zero. Revealed preference is that we put the value of our privacy close to zero, despite forever saying otherwise.
So the option of state/business revealing more data to us citizens/consumers, is actually more realistic. Yes there is extra work on part of state/business to open their data to us. But it's worth it. The more advanced the society, the more synchronisation it needs to achieve the right cooperation-competition balance of the interaction between ever greater number of people.
There is an old book "Data For the People" by an early AI pioneer and Amazon CTO @aweigend. Afaics it well describes the world we live in, and also are likely to live even more in the future. Dear reader, 猴thank you for reading thus far, hope this makes sense to you.
RADICAL TRANSPARENCY FTW
4:31 PM 路 Apr 24, 2025 
 https://x.com/ljupc0/status/1915392232561004722
Ljubomir Josifovski @ljupc0
We need some kind of crypto setup for market of ideas. Where they are not exchanged for $$$ to start with. But are exchanged like-for-like, ideas-for-ideas in the 1st instance. Or ideas for IC - Idea Coin, the currency of the place. Then latter as whole, on an IC currency zone level, then they maybe exchanged for $$$ if one needs $$$ to live maintain functioning body and mind.
For one needs breadth - orders of magnitude more than one came dream up come up with, and also risk sharing - at the time of conception, hard to tell a good from a bad idea apart, that is mostly determined by the future yet to happen. Maybe sth likemy posts are my stuff, I create them I own them, even if everyone is free to copy and share, even only for the process of valuation. Valuation determined by by judgement of others ex-I: everyone ex-I is free to vote with their IC wallets, by buying and selling my post. (or stay indifferent)
It will help intellectuals in another way too: it will ground the ideas in reality. Atm too many are free to propose whatever tickles their fancy, and that (e.g. communism) unchecked by much testing, is put into production by assorted psychopaths and tyrants (Lenin, Stalin) to detriment of all. E.g. would have been much better if CC - Communism Coin - had the initial pop. But then didn't appreciate much in value after the initial pop, and died off in obscurity.
2:07 PM 路 Apr 24, 2025 
 https://x.com/ljupc0/status/1915375824103936107
Ljubomir Josifovski @ljupc0
AI ChatGPT reasoning models make for a better decision maker than what we currently have.
The minister on display is ChatGPT-3.5 level: hallucinating and reasoning poorly. There are layers of deceit and misdirection to undo: no there is not a single global gas product or market (there are multiples), yes the prices *changes* in response to single events that impact all of them (e.g. Ukraine invasion) are approximately globally equal (but not the price levels themselves), yes tax is on profits of suppliers does not directly touch our bills, but those suppliers do pass their costs (and tax is one cost) onto us the consumers in the final analysis, etc etc.
We can't go on like this. Sophist wordcels, with zero real world expertise outside academia or politics, having climbed and retained greasy poles inside parties, (total snake pits where the most sociopathic only survives then thrives,) very much ignorant of, and disinterested in, things they decide on, (impossible to know what's going in anyone's head, including the minister on display,) with poor morals, affecting lives of millions fellow citizens, with disregard to fellow citizens wellbeing, without much feedback.
Milibands of this world out, AI ChatGPT models in.
Quote  https://x.com/SkyNews/status/1915290778437398696
Sky News @SkyNews 路 5h
Sky's @WilfredFrost questions the Energy Secretary Ed Miliband on whether UK gas prices would decrease if the tax rate of 78% on energy companies was lowered by the government.  https://trib.al/kdXA2Dr
 Sky 501, Virgin 602, Freeview 233 and YouTube
Show more
1:02 PM 路 Apr 24, 2025 
 https://x.com/ljupc0/status/1915322751708959192
Ljubomir Josifovski @ljupc0
Maybe tad on the tail end of that, but a lot feels like the usual business of governing a country. We just get to see how the sausage is made, even if not intentionally!  It comes naturally to the current team-Orange-clown. ぁ When running a circus, the spectators are a big bigly uuge part of the Big Tent. お
FwiwI'm all for it! For the lolz, but also b/c I think more state transparency is needed. Insufficient transparency makes any feedback hard, and without feedback it's hard to improve anything.
I advocate for radical transparency on part of the state towards us citizens, and businesses towards us their consumers. In order to address and redress the current information imbalance. Information leads to knowledge leads to power.
The conventional opinion is that the power imbalance coming from the information imbalance (state/business know a lot about me; I know little about them) is that us citizens and consumers should reduce our "information surface" towards them. And address the imbalance that way. But.
There exists another, often unsaid option. And that option is for state/business to open up, to increase their "information surface" towards us. That will also achieve information (and one hopes power) balance. Every time it's actually measured, how much value we put on our privacy, when we have to weight privacy against convenience and other gains from more data sharing, the revealed preference is close to zero. Revealed preference is that we put the value of our privacy close to zero, despite forever saying otherwise.
So the option of state/business revealing more data to us citizens/consumers, is actually more realistic. Yes there is extra work on part of state/business to open their data to us. But it's worth it. The more advanced the society, the more synchronisation it needs to achieve the right cooperation-competition balance of the interaction between ever greater number of people.
There is an old book "Data For the People" by an early AI pioneer and Amazon CTO @aweigend. Afaics it well describes the world we live in, and also are likely to live even more in the future. Dear reader, 猴 thank you for reading thus far, hope this makes sense to you.
RADICAL TRANSPARENCY FTW
9:31 AM 路 Apr 24, 2025 
 https://x.com/ljupc0/status/1915307180250501535
Ljubomir Josifovski @ljupc0
They are not the enemybut their incompetence is making lives of millions miserly. DOGE is a reaction to a huge FAIL of the state capacity. There is no point in preserving of much that DOGE is cutting. That has already died, has been dead for some time. DOGE are like I cutting dead branches in the garden.
State capacity sure has a role! A role to organise and marshal resources on gargantuan scales. To take gargantuan risks! That no other organisation in society can afford to take on, and live to see the end of those risks, the ultimate payoffs. (or survive the fails.) For that, it is the State that is the ultimate resource.
There are no rich countries with small states. There are no peoples that to better their lives, don't organise as a State first and foremost. I come from a startup Nation, and a startup State. And a startup literary language too! All these things go together. All are technologies where large groups of people come together to better their lives.
And no one should begrudge them that after spending decades, even centuries in the wilderness, if they find a path through their valleys of tears, to their promised lands. As long as all that is towards building lands of milk and honey. (or even "life, liberty and pursuit of happiness") And not murdering other peoples like yours, next to you.
(for something to exists, as a separate entity from it's environments, needs a separator, a separation, and that is provided by force, by violence; State monopoly on violence, external and internal, is the technology humanity has discovered over time)
Yes, "Your Country is not a Business" is a good reading now as ever. Musk will fail not because his instincts are wrongbut because building a State requires institutions, not cults. He knows of using the strength and cult of personality, and that is all fine and good. But that suffices and works in small high-performant self-motivated narrow-aim organisations. That will not work for State capacity. Expect Musk to be out sooner rather than latter, and what little dent was made towards change, will be undone within weeks.
Yours "more State always and forever" has FAIL mode ofc. Argentina was that. There what Milei does is obviously going to change things for the better for time being. I grew up in a late-socialism country, that on paper you being similarly ideological would have liked. But in real life would have hated it. For nothing worked, for nothing could be done, for the simple reason that everything was forbidden. A country decided to freeze any efforts of its citizens towards betterment of their own lives.
Subsequently when a reformer unfroze it, just let the natural energies and talents of the citizenry flow, life improved magnitudes and in very short time. Seemingly without special knowledge and super-human effort. Just millions of tiny ants, doing millions of tiny tasks, that need doing for ages. Previously the ants were forbidden to do them. And now the ants were free to do them on their own. That was it.
More humility is in order considering the scales and the consequences of State failures. Especially on ideas coming from intellectuals, which of course you are in part. Do you never read e.g. Thomas Sowell on intellectuals, and think "hm, there is empirical evidence that he may have a point?" Did the fact that big state socialism crash and burn everywhere in your own lifetime somehow got erased from your memory? Or "Big Green Tech" idea both hitting problems with the tech, and not delivering the jobs. I'm baffled by the ego at work here tbh.
8:29 AM 路 Apr 24, 2025 
 https://x.com/ljupc0/status/1915150262014034117
Ljubomir Josifovski @ljupc0
Oh manpreach more! Music to my ears, balm to my soul. 
The number of times I've had to byte my tongue when an English friend opines on ID-cards! I'm Johnny foreigner now naturalised Brit for some decades. And IK enough that it will be only counter-productive if I get too opinionated and wordy about their - frankly - naive musings.
What you wrote above is immensely important. I think many in the UK feel it at some level, but few can articulate clearly why and how. Top marks  for writing the above! 
From time to time I post anti-national-ID scribblings, latest- https://x.com/ljupc0/status/1909917211902333192
...but more linked. I get the impression normies mostly get embarrassed (why??) and just shutdown their thinking faculties.
With Blair still with us and as enthusiastic about them as ever (The world of TB: no problem exists that a national-ID card does not solve), this flares from time to time so I pipe up. The strongest hope I got tbh is that public inertia and gov incompetence will save us from making the mistake of introducing national ID cards. Rather than some superior public discussion and decision making process.
10:06 PM 路 Apr 23, 2025 
 https://x.com/ljupc0/status/1915141383272251497
Ljubomir Josifovski @ljupc0
Ah - and I thought I had it bad with me writing into the void.  After reading of your predicament - I think myself lucky! No replies better than aggravating braindead replies, fs! ngl 
I heard (on a podcast) off-remark that TikTok is the 1st (and so far only) Social Media platform that has the algofeed driven (to non-trivial %age) based on the data/content, and *not* solely on the metadata. The implication was that is the reason their algofeed is prenaturally good. Users often feel it almost super-natural in predicting what the user wants. Not used itbut heading off to register after I post this.
1st Twitter now X algofeed has been cr*p to me, always. Not claiming it much worse now. But wd have expected it to get better in 10 yrs?? But noX algofeed is as sh*t as always. Have to battle it left and right to manually find what I want. I rarely see anything you post.
IDK fs but imagine the Twitter format (X and Bsky) might be ripe for disruption. By now I think it's clear using largely meta-data to predict "will user Y want to see post Z, yes/no", simply leads to positive feedback loop of a winner-takes-all type. Small number of creators-writers on one side, many in number readers-consumers on the other. The old v1 Social Media, harking back to the Mass Media MSM setup predating it.
That's not all bad news for the platforms. (they think) They don't mind creating differentiating 1K-10K accounts as tier-1 ("content creators"), as that they can then influence whichever way. Much harder to steer 100M accounts in the direction desired by the platform owners. Incentives incentives.
I'm personally tired of it, b/c SNR is low. It's a waste of everyone's time, incl Zucks and Musks, and for zero marginal gain. TikTok showed that. Maybe we get lucky and ByteDance create TextTalk, a Twitter format social media platform. 
9:31 PM 路 Apr 23, 2025 
 https://x.com/ljupc0/status/1914988714918400043
Ljubomir Josifovski @ljupc0
Goodgreat! Would you consider advocating for RADICAL TRANSPARENCY of UK gov towards the Great British public?
I advocate for radical transparency on part of the state towards us citizens, and businesses towards us their consumers. In order to address and redress the current information imbalance. Information leads to knowledge leads to power.
The conventional opinion is that the power imbalance coming from the information imbalance (state/business know a lot about me; I know little about them) is that us citizens and consumers should reduce our "information surface" towards them. And address the imbalance that way. But.
There exists another, often unsaid option! And that option is for state/business to open up, to increase their "information surface" towards us. That will also achieve information (and we hope power) balance! Every time it's actually measured, how much value ourselves put on our privacy, when we have to weight our privacy, against convenience and other gains from more data sharing, the revealed preference is close to zero. Revealed preference is that we put the value of our privacy close to zero, despite saying otherwise.
So the option of state/business revealing more data to us citizens/consumers, is actually more realistic. Yes there is extra work on part of state/business to open their data to us. But it's worth it! The more advanced the society, the more synchronisation it needs to achieve the right cooperation-competition balance of the interactiona between ever greater number of people!
There is an old book "Data For the People" by an early AI pioneer and Amazon CTO @aweigend. Afaics it well describes the world we live in, and also are likely to live even more in the future. Thank you for reading thus far, hope this makes sense to you.
RADICAL TRANSPARENCY FTW
11:24 AM 路 Apr 23, 2025 
 https://x.com/ljupc0/status/1914984480630047040
Ljubomir Josifovski @ljupc0
Kudos!you are doing gods work. You could do more with the position you occupy.
Advocate for RADICAL TRANSPARENCY of UK gov towards the Great British public.
I advocate for radical transparency on part of the state towards us citizens, and businesses towards us their consumers. In order to address and redress the current information imbalance. Information leads to knowledge leads to power.
The conventional opinion is that the power imbalance coming from the information imbalance (state/business know a lot about me; I know little about them) is that us citizens and consumers should reduce our "information surface" towards them. And address the imbalance that way. But.
There exists other, often unsaid option! And that is for state/business to open up, to increase their "information surface" towards us. That will also achieve balance! Every time it's actually measured, how much value ourselves put on our privacy, when we have to weight orivacy against convenience and other gains from more data sharing, the revealed preference is close to 0. Revealed preference is that we put the value of our privacy close to zero, despite saying otherwise.
So the option of state/business revealing more data to us citizens/consumers, is actually more realistic. Yes there is extra work on part of state/business to open their data to us. But it's worth it! The more advanced the society, the more synchronisation it needs to achieve the right cooperation-competition interaction, and between ever greater number of people.
There is an old book "Data For the People" by an early AI pioneer and Amazon CTO @aweigend. Afaics it well describes the world we live in, and also are likely to live even more in the future. Thank you for reading thus far, hope this makes sense to you.
RADICAL TRANSPARENCY FTW
11:07 AM 路 Apr 23, 2025 
 https://x.com/ljupc0/status/1914979142702899338
Ljubomir Josifovski @ljupc0
Go further please. Advocate for RADICAL TRANSPARENCY of UK gov towards the Great British public.
I advocate for radical transparency on part of the state towards us citizens, and businesses towards us their consumers. In order to address and redress the current information imbalance. Information leads to knowledge leads to power.
The conventional opinion is that the power imbalance coming from the information imbalance (state/business know a lot about me; I know little about them) is that us citizens and consumers should reduce our "information surface" towards them. And address the imbalance that way. But. The other, often unsaid option is, for state/business to open up, to increase their "information surface" towards me. That will also achieve balance!
Every time it's actually measured, how much value people put on their privacy, when they have to weigh against convenience and other gains from more data sharing, the revealed preference is close to 0. So the option of state/business revealing more data to us citizens/consumers, is actually more realistic! Yes there is extra work on part of state/business to open their data to us. But it's worth it! The more advanced the society, the more synchronisation it needs to achieve the right cooperation/competition interaction, between ever greater number of people.
There is an old book "Data For the People" by an early AI pioneer and Amazon CTO @aweigend. Afaics it well describes the world we live in, and also are likely to live even more in the future.
RADICAL TRANSPARENCY FTW
10:46 AM 路 Apr 23, 2025 
 https://x.com/ljupc0/status/1914364573987045770
Ljubomir Josifovski @ljupc0
Afaik that's not entirely possible. I have the following mental model of SocMed algofeeds. I can expect any one post I write to be shown to max 1/10-th of my followers, in the best case. How and why? The simplest possible example I can think of is this.
I follow 5000 accounts. Each one writes 1 post a day = 5000 posts per day. I login to X, X algofeed serves me 50 posts on one screen full. I check X 10 times per day = 500 posts that X will show me in one day. That is 500 out of possible 5000 to be shown. And so there will be 4500 posts, from people I follow on any one day, to **not** be shown.
X must decide which 500, out of 5000 possible, to show me. So any one post has probability 0.1 to be shown. I will see 1 post from 1 account once in 10 days. IRL the algofeed will be vastly more complicated. The algofeed is the core of the social media company.
Unlike the sketch above, the algofeed is non-random. It mostly uses metadata to tilt that 0.1 probability. Algofeed tilts towards factors like timeliness/age "how much time elapsed since a post got posted", engagement measures like how many in number {Like, Forward, Quote, Reply} interactions it got, probably all decayed by time with some half-life.
Afaik no actual data about the quality, the content, of the post is ever used. Only metadata. This is pure speculation on my part. I have no insider knowledge. I have not worked in a social media company ever. Just thinking aloud and considering rules of thumb and first principles.
For me personally posting. Usually I only have the "View"-s for a post to see as a feedback. I don't even know if it was shown to the user whose message I reply to! (@perrymetzger in this instance) It's entirely up to X algofeed, if/when it shows it to the OP, or not at all. If a post gets zero engagement (a "Like" at a minimum) after 30-50 Views, it's subsequently dropped. (never pushed any more to anyone's screen.)
One would need to go to my home page on X, then search for a post intentionally. E.g. like so  https://x.com/search?q=(from%3Aljupc0)&src=typed_query&f=live
It's possible that my idea of how things work is entirely faulty. Allow for that pls. I'm no social media star-can hardly give advice to anyone! 
6:04 PM 路 Apr 21, 2025 
 https://x.com/ljupc0/status/1914344034081898627
Ljubomir Josifovski @ljupc0
Indeed, you are 100% right. Autistic people are in greater danger than most to seek clever schemes that "solve everything" in one neat formula, leaving no lose ends. My wife works with kids like them. From the stories anecdotes relayed from time to time-I see how and why this wd be the case. While there have been plenty of evil humanist-by-education dudes in history, I always have in mind that one of the greatest recent monsters Beria "excelled in math and science".
This tongue-in-cheek "VDT: a solution to decision theory" https://lesswrong.com/posts/LcjuH on a related topic (HT https://x.com/ChrisChipMonk/status/191323296) maybe of interest. My reading was 1) Less frivolous than it first appears; 2) Not obviously worse than the alternatives; 3) Well even-problem solved again, with prior art being "Vibes==Love" in our (christian?) culture afaics.
[[[ Meta. Don't apologise please. I like to read what you write, I follow you and you follow me, yet frequently X does not show me what you wrote. The above post is great. Twitter now X algofeed was always cr*p, and is still cra*p. Please re-post more of your old posts! Fight low quality low effort algofeed with low effort means to maintain. Maintaining good SNR justifies it!
My unfounded suspicion is X algofeed is sh*t in a way that it places almost all the probability weight on the "timeliness" predictor, and almost nothing on anything else. And esp nothing on content i.e. data - it's all about meta-data. Afaik all X algofeed predictors to "should I show post Y to user Z" are meta-data only, and none are data actual content.
I heard on a podcast - that I can't find, may have been one done by the two Abundance book guys promoting the book, but don't recall which - and one of them (or the host?) mentioned this. That TikTok is the 1st social media algofeed that doesn't (unfairly) prioritise meta-data. But looks at the actual data, at the actual content. Of both what is posted, and what the user consumes i.e. wants. And that it actually matches that. And that's the reason people find TikTok almost super-natural in guessing what they want.
For utilising meta-data simply leads to positive feedback loop of winner-takes-all. Which is not all bad news for the platforms. They don't mind creating differentiating 1K-10K accounts as "content creators", that they can then influence whichever way. Much harder to steer 100M accounts in the direction desired by the platform owners.
If the above about the TikTok algofeed were true. I'd get the urge to bash our SocMed overlords with a heavy object over their heads. For wasting Bazillion of hours of human lives, to feed us slop, and for their own detriment, and ours. The very definition of human stupidity. Unexcusable on the part of the Zucks and Musks. /rant ]]]
4:42 PM 路 Apr 21, 2025 
 https://x.com/ljupc0/status/1914904173969068441
Ljubomir Josifovski @ljupc0
Of course - and expect nothing else in the future too.
In my lifetime, AIDS stopped being a deadly disease, and no one noticed.
I was ~20yo when I 1st heard about AIDS in the media. A positive test was a death sentence. Newspapers went on and on, with some celebrity dieing every week or month, for years. Innuendos, real and fake news, of "are they or aren't they" HIV positive, filled newspapers pages. For years.
Then I noticed at some point - wait, we are not reading these stories any more. No celebrity had died of AIDS within my memory window. I dug up info - there was Internet by then. Found out the illness is treated, and turned Chronic, away from Acute. People suffer but they are not dieing en masse. There's a documentary "In the time of a plague" or similar about the drugs development. Nowadays I presume it's curred completely, can't be bothered to check now.
Ulcers to the stomach also got cured in my lifetime. In my childhood, sometimes I'd overhear parents talk in grave voice "So-and-rushed to an operating theatre to hospital, but couldn't be saved. Burst ulcer bleeding, tsk tsk. Such tragedy - family kids lost father etc". Then in my adulthood I realised - I never hear of such tragedies anymore. Found out the saga of Helicobacter and the Australian doctor(s). That Ulcers are cured by antibiotics, and people don't typically die of burst ulcers anymore.
So yeah - a deadly plague killing people left and right will be solved, problem deleted, an impossible feat achieved. After decades of effort, of trials and failures, by thousands of people across the globe. Few people will notice.
5:48 AM 路 Apr 23, 2025 
 https://x.com/ljupc0/status/1914743273379475718
Ljubomir Josifovski @ljupc0
When one reads "How Innovation Works" or "Serendipity: Medical Discoveries in the 20th Century" one gets to appreciate how much luck plays a role. Many breakthroughs are in parts very much accidental.
We do disservice to young researchers there. When we write up a discovery, we usually start from the end, from the result, and mentally trace the path back to the start. Then we describe that one path we traced back, with words, from the start to the end.
But that single forward path is an illusion! At discovery time, at the start, and along the path, there were many junctions. Going forward, there is a tree of paths - never a single one. The ultimate path that worked, was one of many. It only exists when looking back, from the end result. Often it is not even clear why we decided to follow one fork, and not another. Usually there are arguments evidence for-, as well as against-, taking either path, in a fork on the road. (to the ultimate discovery.)
Further, timing is everything. Kurzeil - now famously - stumbled on his Flops/USD log-linear exponential trying to figure how to time his Inventions/contraptions. (his words-I think?) Hinton probably had Sutskever-level PhD students in year 2003, or even 1993 too. There were no GPU-s capable of a leap then probably. Even when there were GPU-s, it took an Alex, in addition to one Ilya, to be at the same place, same time.
(for comparison: in year 2000, a stellar guy in my lab training NN-s and writing ASR stack decoders, had a special Sparcstation bough with large chunk of project $$$, with then obscene unheard of I think 96 MB RAM; most users had computers with 2MB RAM from recollection)
Research is a mugs game. Some Hinton colleagues probably died never seeing the success of their field. Assuming all their efforts a failure, a waste good for nothing. After devoting good chunk of their lives to it.
7:09 PM 路 Apr 22, 2025 
 https://x.com/ljupc0/status/1914704491162612195
Ljubomir Josifovski @ljupc0
Yesthe complaints are spot on. Glad someone is motivated enough cares enough to react. (I don't)
1) Selective booking.
Fine, bring on fringe guests if you must. But then at least include voices from both sides. Joes "crazies" are almost exclusively pro-Russia or pro-Hamas. And it's not just about balanceeven non-crazy prominent figures like Vitali Klitschko (boxing champ, now mayor of Kyiv) literally sent Joe a video invitation to appear and was ignored. Cant think of a more JRE-suited guest, frankly.
That Joe refuses to platform any strong pro-Ukraine or pro-Israel voices speaks of his views. It's his rightbut let us remember: the original fail mode of MSM was selective bias by omission. Joe, now a platform as big as MSM, deserves the same scrutiny. You can't call it "open dialogue" if only one side ever gets airing to millions.
2) Churchill the baddie.
Joe and guests pushing Churchill conspiracies aremaybe without realising it, for they are forever ignorant even to click on a Wiki pagerehashing 1942 Goebbels propaganda ("Churchill controlled by Jewish bankers," etc). In Europe, WW2 survivors are still alive (e.g. my mum). These takes arent edgytheyre lazy.
Fr: it was Germany that built up its military in defiance of Versailles, annexed Austria, dismembered Czechoslovakia, and invaded Poland on Sept 1, 1939. Two weeks later, the Soviet Union invaded Poland from the east, pursuant to the MolotovRibbentrop Pact, a plan to divide Eastern Europe between Germany and the Soviet Union. Enslave and or exterminated millions of people. This is not disputed. We have mountains of evidencehand written from the architects and executors(!), diaries, state memos, radio archives, press coverage. Everyone can read everything at all times, it's not Latin or ancient Greek. It's all super-accessible. Even the original archives.
Joe is free to re-broadcast Goebbels-tier takes. But lets not pretend its deep or brave. If he wants to test the strength of his opinions, let him say this stuff in CEEurope (ex-Russia), in Poland or the Baltics. I suspect he may catch more heat than he can handle. He's travelled a long way from the original "savage Khan of the steppe curious about the skies asks medicine man to explain" schtick, and something got lost along the way.
Having written all thatI havent watched JRE for many moons now. Theres just better stuff on YTmore interesting, more entertaining, more informative. Many podcasters go to UKR-RUS, record, comment, interview etc. Much better to see IRL, even when selective. Whenever X references JRE, it's usually some B-tier cosplayer bores. Don't bother to check. That shi*t-chatting nonsense more entertaining elsewhere.
4:34 PM 路 Apr 22, 2025 
---
post-chat-LJ.html
---
 Chat with Virtual Me!
  Send 
---
post-deepwiki.html
---
DeepWiki Crawl of This Repository
I asked DeepWiki to do a crawl of this repository. The result is on the link below.
Deepwiki crowl of this repo
-- LJ HPD Sat 26 Apr 2025 23:38:16 BST
---
post-EMPTY.html
---
Title Goes Here
Content goes here
TODO TBD
-- LJ HPD :r !date
---
post-knowing.html
---
Knowing - what do I mean when I say I know something
I was told "the joint probability density function between two variables \( X \) and \( Y \) captures everything that there is ever to be known about the relation between those \( X \) and \( Y \)" 25 years ago (隆Hola! Miguel :-)), and it's been a blessing and a curse. Blessing - yeah the joint pdf \( f_{X,Y}(x,y) \) really does capture everything. Curse - often I read an article and think of the author "wish someone told you too", you poor soul. So for me knowing something about \( X \) means knowing the distribution, the pdf \( f_X(x) \). Most of the time our knowledge is more than 1-dimensional, we have at least two qualities that we want to quantify the relationship of. So knowing something about \( (X,Y) \) jointly, for me means knowing the joint pdf \( f_{X,Y}(x,y) \).
Below I illustrate this point on the example of a joint pdf \( p = f_{X,Y}(x,y) \) that is a mix of two Gaussians in 2D space \( (x,y) \). We observe the variable \( X \), and that observations is \( x=1 \). The question is - what do we now know about the variable \( Y \), having observed the variable \( X \) (to be \( x=1 \)). The observation \(x=1 \) is equivalent to the joint pdf being cut by the plane \( x=1 \). The intersection of the joint pdf \( f_{X,Y}(x,y) \) and the plane \( x=1 \) is \( f_{X,Y}(x=1,y) \). This curve is the best description of what we now know about the distribution of the unobserved variable \( Y \). The starting model that was \( f_{X,Y})(x,y) \) is affected by the observation \( x=1 \). The effect is the intersection \( f_{X,Y}(x=1,y) \), and is outlined below. It is a function of \( y \), that is a scaled conditional \( f_Y(y|x=1) = \frac{f_{X,Y}(x=1,y)}{f_X(x=1)} \). The conditional pdf is \( f_Y(y|x) \). The scaler \( f_X(x=1) \) is the marginal pdf \( f_X(x) \) of \( X \) at point \( x=1 \). The marginal pdf \( f_X(x) \) is computed from the joint pdf \( f_{X,Y}(x,y) \) by marginalization, by integrating out \( Y \) as \( f_X(x) = \int f_{X,Y}(x,y)\,dy \) and then plugging in \( x=1 \).
Joint marginal conditional pdf 1 of 3. (click to zoom)
Conditional pdf is ratio of joint (at point) and marginal 2 of 3. (click to zoom)
Marginal pdf is derived from the joint pdf 3 of 3. (click to zoom)
Coming back to "joint pdf captures everything there is in the relationship \(X,Y\)". Putting it in a wider context. When reading about knowledge, I have come across the following so collected here for future reference. We can have 2 types of knowledge about the outcome of (repeated) experiment(s):  We know what will happen and when it will happen in each experiment. This is non-probabilistic, deterministic knowledge.
NB it is a special case of both (b) cases below with the pdf being a Dirac impulse function. We know the possible outcomes, we know how many of each will happen if we do 100 experiments, but for each 1 experiment, we can't tell the outcome.
This is probabilistic knowledge where we know the pdf (=probability density function) of the experiment outcome.
It is the aleatoric kind of uncertainty (see below) - we know the statistics, the counts, but not what one outcome is going to be in every one experiment. 
Uncertainty - obverse to knowing, to knowledge, lacking (perfect, deterministic) knowledge, we can think of types:  Aleatoric uncertainty means not being certain what the random sample drawn (from the probability distribution) will be: the p.d.f. is known, only point samples will be variable (but from that p.d.f.).
We can actually reliably count the expected number of times an event will happen. Epistemic uncertainty is not being certain what the relevant probability distribution is: it is the p.d.f. that is unknown.
We can't even reliably count the expected number of times an event will happen.  The probabilistic knowledge of type (b) above and aleatoric uncertainty of type (b) are one and the same.
The 2D \( (X,Y) \) example is also useful to illustrate a further point. Once we observe \( X \), and work out the conditional pdf \( f_Y(y|x) \), the question arises - what next? What do we do with it? If \( Y \) is discrete, we have a problem of classification. If \( Y \) is continuous, we have a problem of regression. We have the entire curve to work with - and that's the best. But often, we approximate the entire curve, with a representative value, and soldier on. Then the question becomes: well how do we chose one representative value from that curve? The "\( X \) observed \( Y \) not observed" is arbitrary - it could be the other way around. We can generalize this by introducing a 2D binary mask \( M \), to indicate what parts of the vector \( (X,Y) \) are present (observed), and what parts are missing (and thus of some interest, e.g. we want to predict or forecast them). With present data \( X \) and missing data \( Y \) in \( (X,Y) \), then missing data imputation is actually equivalent to forecasting regression or classification. TBD When time is one of the dimensions, with Now separating the Past from the Future: there is a big difference is whether \( X \) and \( Y \) are contemporaneous, or not. Not contemporaneous, having \( X \) in the past, predicting \( Y \) in the future, makes the signal connection \( X \longrightarrow Y \) much, much weaker, by orders of magnitude, then it would be otherwise (if \( X \) and \( Y \) are not separate by time, but happen at the same time).
TBD link to entropy and average information, specific information from one symbol -
Michael R DeWeese and Markus Meister (1999), "How to measure the information gained from one symbol", Network: Computation in Neural Systems, 10:4, 325-340, DOI: 10.1088/0954-898X_10_4_303 [[excellent paper; introduces the idea that more information can make the entropy higher, thus reducing our knowledge if the knowledge measure is the spikiness of the probability density function; after we have additional observation (information), the posterior p.d.f. given the observation, maybe flatter then before => our knowledge decreased ]]
TBD Knowing and knowledge put into an even wider context.  Known Knowns. Deterministic knowledge - we know exactly which one. (deterministic knowledge above) Known Unknowns. We don't know which one, but we know how many of which type; i.e. the distribution. (known pdf, aleatoric uncertainty above) Unknown Unknowns. We don't know the p.d.f. either. (epistemic uncertainty above) Unknown Knowns. Ideology. Fish swimming in water never knowing anything else but water.Possibly thus being unable to perceive the water too? Not sure - that maybe a step too far. Just not knowing anything outside water, but knowing water suffices imo. (Zizek @ YT; self-reflective, I using the framework described above by the 3 cases {pdf-yes-Dirac,pdf-yes-non-Dirac,pdf-no} am like the fish, and the framework is my water, in the sense that's my entire knowledge and I know not outside of it) 
TBD Quantity and quality. Dimensions \( (X,Y) \) are qualities, and we quantify them each too. When do we add new quality and obversely when do we lose a quality (dimension)? (LJ @ HN) There is a spin on the same idea when working with data (maths/stats/comp/ML) and having to skirt around the curse of dimensionality. Suppose I have a 5-dimensional observation and I'm wondering if it's really only 4 dimensions there. One way I check is - do a PCA, then look at the size of the remaining variance along the axis that is the smallest component (the one at the tail end, when sorting the PCA components by size). If the remaining variance is 0 - that's easy, I can say: well, it was only ever a 4-dimensional observation that I had after all. However, in the real world it's never going to be exactly 0. What if it is 1e-10? 1e-2? 0.1? At what size does the variance along that smallest PCA axis count as an additional dimension in my data? The thresholds are domain dependent - I can for sure say that enough quantity in the extra dimension gives a rise to that new dimension, adds a new quality. Obversely - diminishing the (variance) quantity in the extra dimension removes that dimension eventually (and with total certainty at the limit of 0). I can extend the logic from this simplest case of linear dependency (where PCA suffices) all the way to to the most general case where I have a general program (instead of PCA) and the criterion is predicting the values in the extra dimension (with the associated error having the role of the variance in the PCA case). At some error quantity \( \gt 0 \) I have to admit I have a new dimension (quality). TBD
TBD Ndim space. Ratio of Ncube/Nball. Does our intuition fail us about the representative values of a distribution when we go from low \( N \) \( (N = 2) \) to high(er) \( N \) \( (N \gt 10) \)? For large N, Nspace in Ndim: (a) moves into the edges (b) every observation is an outlier (in some dimension). Does that mean the space becomes discrete, it discretizes? TBD Sparse representation, moving to symbolics and rules. Once the Ndim vector becomes sparse, we move from continuous representations to discrete symbolic rules?
-- LJ HPD Sun 20 Oct 07:31:04 BST 2024
---
post-links-to.html
---
Links to Blogs, Substacks, Youtubes, Podcasts, Magazines etc
Over the years online, I benefited immensely from the Internet users that write and put up stuff online for other users to read. For the love of it - nothing else. There is no purer form of creativity than that. "Here!! I like this, maybe you will like it too. On an off chance, infinitesimally small, I make this extra step extra effort to put it out there on the Internet. Free, free like beer and freedom too, for all to see". Initially and for a long time things were kept and curated in Bookmarks. Then Google spoiled me when it became possible to "Google it" anything - there was no great need for Bookmarks. Then Internet expanded and some things are Social media posts, others are Substacks, still others are old style personal blogs, then there is Youtube podcasts and videos, then Spotify, then podcasts on other platforms etc etc. So I thought this is a good place - yet another place; but redundancy is the mother of resilience - to add links to content I liked and consume online. References and links put in no particular order, as my memory blurts them out. It's good to have them on one page and check from time to time.
Nowadays with every author being online, one gets to imbibe their ideas anyway on the 80/20 principle, via the "quanta of ideas" that are memes, spread esp via X/Twitter/Bsky where the medium specifically encourages that. So when I finally get to a book or a movie of the author, I already have heard or seen 80% of what they are saying, know of the ideas, so often I don't get to finish the book or the movie! I'm not complaining though - but bragging about it! Because there is something else already worthy of attention! We are so so lucky now. Some of my memories of my childhood (in the 1980-s late socialism of Yugoslavia) are of mind bending boredom. There was little to do, life was very boring at times (not all the time - other times it was fun and exciting), that reading a mildly interesting book, watching an interesting movie on TV (1.5 channels) or in the cinema (not that many of those), or playing - or more often watching others play - random games in the local arcades tent, was a treat. Home computers changed that somewhat, and then Personal Computers and latter the online world and Internet changed that a lot! And for the better.
Scott Alexander's Astral Codex Tenhttps://www.astralcodexten.com/ (previous Slate Star Codex https://slatestarcodex.com/)
J. Sanilac https://www.jsanilac.com/
FSF Free Software Foundation https://www.fsf.org/, GNU operating system https://www.gnu.org/, software https://www.gnu.org/software/software.html, philosophy https://www.gnu.org/philosophy/
RMS Richard Stallman's Personal Site https://stallman.org/, archive https://stallman.org/archive.html
ESR Eric S. Raymond's Home Page http://www.catb.org/~esr/ weblog http://esr.ibiblio.org/ FAQs http://www.catb.org/~esr/faqs/
Bryan Caplan's Bet On It https://www.betonit.ai/
Nate Silver's Silver Bulletin https://www.natesilver.net/
Shtetl-Optimized The Blog of Scott Aaronson https://scottaaronson.blog/ (PHYS771 Lecture 9: Quantum http://www.scottaaronson.com/democritus/lec9.html)
The Intrinsic Perspective By Erik Hoel https://www.theintrinsicperspective.com/
Yuval Harari https://www.ynharari.com/, https://www.youtube.com/user/YuvalNoahHarari
Civilution for Universal WellBeing https://www.civilution.org/
Rutger Bregman https://linktr.ee/rutgerbregman
Chris Dillow's Stumbling and Mumbling https://stumblingandmumbling.typepad.com/
Dominic Cummings substack https://dominiccummings.substack.com/
Dwarkesh Patel https://www.youtube.com/DwarkeshPatel https://www.dwarkeshpatel.com/
Apply Liberally by Matthew Downhour https://applyliberally.substack.com/
Francis Fukuyama https://fukuyama.stanford.edu/
Pluralistic: Daily links from Cory Doctorow https://pluralistic.net/
Human Progress https://humanprogress.org/
James Bloodworth https://www.forthedeskdrawer.com/
Odds and Ends of History By James O'Malley https://takes.jamesomalley.co.uk/
Sabine Hossenfelder https://www.youtube.com/c/SabineHossenfelder
Jason Crawford https://jasoncrawford.org/archive
Ole Peters Ergodicity economics https://ergodicityeconomics.com/
Scientific Discovery By Saloni Dattani https://www.scientificdiscovery.dev/
Blair Fix Economics from the Top Down https://economicsfromthetopdown.com/
John D. Cook blog https://www.johndcook.com/blog/
Lex Fridman https://lexfridman.com/ https://www.youtube.com/lexfridman
Information Processing - Steve Hsu https://stevehsu.substack.com/
Richard McElreath http://xcelab.net/rm/ https://www.youtube.com/@rmcelreath
Slime Mold Time Mold https://slimemoldtimemold.com/
Works in Progress https://worksinprogress.co/
Warren Mosler's Mosler Economics / Modern Monetary Theory https://moslereconomics.com/
Naked Capitalism https://www.nakedcapitalism.com/
Steve Keen substack https://profstevekeen.substack.com/ (Minsky Home https://sourceforge.net/p/minsky/home/Home/)
Derek Lowes In The Pipeline https://www.science.org/blogs/pipeline
Arts & Letters Daily http://www.aldaily.com/
Antiwar https://www.antiwar.com/
Matthew Downhour's substack Apply Liberally https://applyliberally.substack.com/
Liberal Currents https://www.liberalcurrents.com/
Reason Magazinehttp://reason.com/
Triggernometry podcast YouTubehttps://www.youtube.com/@triggerpod
The Critic magazine https://thecritic.co.uk/
Compact magazine https://www.compactmag.com/
Hacker News https://news.ycombinator.com/news
Stack Overflow http://stackoverflow.com/
Super User http://superuser.com/
Server Fault http://serverfault.com/
Unix & Linux Stack Exchange http://unix.stackexchange.com/
SLIME MOLD TIME MOLD  Mad Science Blogging https://slimemoldtimemold.com/
Richard Stallman https://www.stallman.org/ RMS archives https://www.stallman.org/archives/
Free Software Foundation FSF https://www.fsf.org/
Arabesque - Systems, Tools, and Terminal Science https://blog.sanctum.geek.nz/, a blog by Tom Ryder
Gurwinder blog https://www.gurwinder.blog/
Our World in Data https://ourworldindata.org/
Tom Forth blog https://tomforth.co.uk/
Mark Litwintschik blog https://tech.marksblogg.com/
3Blue1Brown YouTube https://www.youtube.com/@3blue1brown FAQ https://www.3blue1brown.com/faq
Yann Lecun https://yann.lecun.com/
Andrej Karpathy https://karpathy.ai/
Christopher Olah colah's blog http://colah.github.io/
Tim Dettmers blog https://timdettmers.com/
Brandur articles https://brandur.org/articles
Walter Bright http://www.walterbright.com/
Diomidis Spinellis home page https://www.spinellis.gr/index.html.var
Bartosz Milewski https://bartoszmilewski.com/
Georgi Gerganov https://github.com/ggerganov
Jeff Atwood Coding Horror https://blog.codinghorror.com/
Marc Andreessen Substack https://pmarca.substack.com/
Paul Graham https://paulgraham.com/
Patrick Collison https://patrickcollison.com/
DAVID HEINEMEIER HANSSON https://dhh.dk/
Nomad list https://nomads.com/
JWZ blog https://www.jwz.org/blog/
Ian Dunt substack https://iandunt.substack.com/
Sam Bowman substack https://www.sambowman.co/
Dominic Cummings substack https://dominiccummings.substack.com/
Information Processing - Steve Hsu substack https://stevehsu.substack.com/, Manifold podcast https://www.manifold1.com/episodes
Brett Scott blog Altered States of Monetary Consciousnes https://alteredstatesof.money/
Blog by Matt Ridley http://www.rationaloptimist.com/blog/
Idle Words - Maciej Cegowski https://idlewords.com/
Crooked Timber https://crookedtimber.org/
Pluralistic: Daily links from Cory Doctorow https://pluralistic.net/
LessWrong https://www.lesswrong.com/
The Nutshell Times https://thenutshelltimes.com/
Deirdre McCloskey http://www.deirdremccloskey.org/
EPchan blog Quantitative Trading http://epchan.blogspot.com/
Locklin on science http://scottlocklin.wordpress.com/
Rob Carvers This Blog is Systematic https://qoppac.blogspot.com/
 Robert J Frey Keplerian Finance http://keplerianfinance.com/
Michael Tan's Blog https://michaeltanphd.com/
Ole Peters Ergodicity Economics https://ergodicityeconomics.com/about/, For to withhold is to perish https://ergodicityeconomics.com/2023/08/29/for-to-withhold-is-to-perish/, Textbook https://ergodicityeconomics.com/publications/
Win Vector LLC https://win-vector.com/
Diomidis Spinellis home page https://www.spinellis.gr/index.html.var
Tim Dettmers https://timdettmers.com/
Bert Hubert https://berthub.eu/
Sam Altman https://blog.samaltman.com/
Alfredo Canziani blog https://atcold.github.io/blog.html
Giuseppe Paleologo https://linktr.ee/paleologo
Leo Breiman https://www.stat.berkeley.edu/~breiman/
Andreas Weigend http://www.weigend.com/
Spyros Makridakis, The M Forecasting Competitions https://www.unic.ac.cy/iff/research/forecasting/m-competitions/
Uncharted territories by Tomas Pueyo https://unchartedterritories.tomaspueyo.com/
Matt Lakeman "Notes on ..." travels blog https://mattlakeman.org/
DYNOMIGHT INTERNET WEBSITE https://dynomight.net/
Julia Evans https://jvns.ca/
Gwern Branwen website on AI, psychology, & statistics https://gwern.net/
Simon Willison blog https://simonwillison.net/, link blog https://simonwillison.net/search/?type=blogmark, blogmarks why and how https://simonwillison.net/2024/Dec/22/link-blog/, github https://github.com/simonw
Maxwell Tabarrok substack Maximum Progress blog https://substack.com/@maximumprogress (Four Futures For Cognitive Labor)
David Shapiros Substack https://daveshap.substack.com/ (e.g. Deny, Defend, Depose: We are already living in a Cyberpunk Hell (and how we can fix it), What do I mean when I say "Post-Labor Economics" anyways?)
Geoffrey E. Hinton home page https://www.cs.toronto.edu/~hinton
Hugging Face Blog https://huggingface.co/blog (e.g. Scaling Test Time Compute with Open Models), Models https://huggingface.co/models
Richard Hanania's Substack https://substack.com/@richardhanania, Newsletter https://www.richardhanania.com/ (e.g. Understanding the Tech Right https://www.richardhanania.com/p/understanding-the-tech-right)
-- LJ HPD Fri 18 Oct 18:39:23 BST 2024
---
post-ljubomirj.html
---
 Welcome to the Home Page of Ljubomir Josifovski Now - systematic trading, research and development. Prior - automatic speech recognition (ASR) in noise, speech synthesis, machine learning (ML). In Harpenden UK, from Skopje MK. Online coordinates X @ljupc0 https://x.com/ljupc0, highlights https://x.com/ljupc0/highlights, posts search https://x.com/search?q=%28from%3Aljupc0%29&src=typed_query&f=live Linkedin ljubomirjosifovski https://www.linkedin.com/in/ljubomirjosifovski Bsky @ljupco.bsky.social https://bsky.app/profile/ljupco.bsky.social, posts https://bsky.app/search?q=from%3Aljupco.bsky.social (click Latest) Hacker News ljosifov https://news.ycombinator.com/user?id=ljosifov (favourites) Mastodon @ljupco https://mstdn.io/@ljupco GitHub ljubomirj https://github.com/ljubomirj Substack @ljubomirjosifovski https://substack.com/@ljubomirjosifovski   -- LJ HPD Sun 6 Oct 07:50:30 BST 2024 
---
post-ml-llm-dev.html
---
ML LLM Dev Links and Notes of resources of interest
Models - open source, open weights, open thoughts, code, documentation
llama.cpp
Inference of Meta's LLaMA model (and others) in pure C/C++ https://github.com/ggerganov/llama.cpp
DeepSeek R1
Unsloth dynamic
HuggingFace quants, incl distillations
Meta Llama models https://www.llama.com/
Meta Llama-3.3-70B-Instruct Hugging Face https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct
Ollama
Get up and running with large language models. https://ollama.com/
llm.c
LLMs in simple, pure C/CUDA with no need for 245MB of PyTorch or 107MB of cPython. Current focus is on pretraining, in particular reproducing the GPT-2 and GPT-3 miniseries, along with a parallel PyTorch reference implementation in train_gpt2.py. https://github.com/karpathy/llm.c
LLM
A CLI utility and Python library for interacting with Large Language Models, both via remote APIs and models that can be installed and run on your own machine. https://llm.datasette.io/en/stable/
Hugging Face Models https://huggingface.co/models
Mistral AI https://mistral.ai/, Hugging Face https://huggingface.co/mistralai
QwQ-32B-Preview blog https://qwenlm.github.io/blog/qwq-32b-preview/, Hugging Face https://huggingface.co/Qwen/QwQ-32B-Preview, github Qwen2.5 https://github.com/QwenLM/Qwen2.5
QVQ-72B-Preview Hugging Face https://huggingface.co/Qwen/QVQ-72B-Preview
DeepSeek-V3 github https://github.com/deepseek-ai/DeepSeek-V3, Hugging Face https://huggingface.co/deepseek-ai/DeepSeek-V3
Reddit LocalLLaMA https://www.reddit.com/r/LocalLLaMA/
llama.cpp guide - Running LLMs locally, on any hardware, from scratch https://blog.steelph0enix.dev/posts/llama-cpp-guide/
ModernBERT
This is the repository where you can find ModernBERT, our experiments to bring BERT into modernity via both architecture changes and scaling. https://github.com/AnswerDotAI/ModernBERT
WordLlama https://github.com/dleemiller/WordLlama
Microsoft AI - AI Platform Bloghttps://techcommunity.microsoft.com/category/ai/blog/aiplatformblog, Introducing Phi-4
Chatbot Arena (formerly LMSYS): Free AI Chat to Compare & Test Best AI Chatbots https://lmarena.ai/
Scaling Test Time Compute with Open Models https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute
The Complexity Dynamics of Grokking https://brantondemoss.com/research/grokking/
Dev, LLM code writing
Update #2 
Current work flow is:
# An Architect model is asked to describe how to solve the coding problem. Thinking/reasoning models often work well in this role.
# An Editor model is given the Architects solution and asked to produce specific code editing instructions to apply those changes to existing source files.
# https://aider.chat/2025/01/24/r1-sonnet.html
aider-openrouter-best() {
  local -; set -x; env AIDER_START="$(date)";
  aider --architect --model openrouter/deepseek/deepseek-r1 --editor-model openrouter/anthropic/claude-3.5-sonnet;
}
Atm waiting on a glitch to resolve -
architect> litellm.APIError: APIError: OpenrouterException - 
Retrying in 0.2 seconds...
litellm.APIError: APIError: OpenrouterException - 
...and so I'm realising now I more often then not now I have it write code for me.
It's not even that much faster atm tbh! By the time I have thought through, explained in detail in INSTRUCTIONS.md  I could have read up the sources, the docs, and done it myself.
The only explanation I have to offer, that I only nowwaiting on the OR api to come backhave, is: it's **much more fun**!! 
It's much more fun to have someone else write the code, and even if need be talk them into "no nonot that way, change this, change that", than to do everything myself solo and in silence!! 
Okthis I did not expect.  That the most entertainingwins. 
Is vibing the way code wring will scale x10, x100 next??
Update #1 
1. Start with ChatGPT copy&pasta - works but limited & manual, little time saved.
2. Onto Cursor - nice but not much gained, not even wrong.
3. Over to aider cmd line - some result there, even if cr*p result... but looks like it could be improved?
4. Current VSCode gui + Cline addon + OpenRouter payg credits + Claude model. Well hello!! Finally produced something not obviously wrong.
Until today the best I got was: in ChatGPT-o4/o1- etc, copy & paste code snippet(s), ask a Question, then incorporate the Answer in the soluton. So this was a replacement for 1) googling and reading web pages 2) search through Stackoverflow Q&A.
This is the 1st time I got code inserted in 3 files. That required AI to 1) read through 5-6 files 2) compare and contrast, reason by analogy 3) take my requirement Q in considerion 4) edit 3 files, delete some code, insert some other code.
I have my main codebase, about 200K LoC in an array/matrix language mostly, with some C/C++/bash/awk/sql too.
I'm agnostic Re: tools. Fallback always available is bash/vim/Makefile/gcc/g++/gdb/ddd/shell/... tools. But if IDE like VSCode/Spyder/CLion/Matlab/DBeaver is available - I'm happy to use. As long as it's not exlcusive, and one can edit/setup outside the IDE too. And esp important version contol - git now, prev hg, cvs, Teams. If that works - then all is good.
I tried Cursor. That looked hopeful, but did not get me results. I didn't like not being able to use existing API subscriptions in it. Also them using some kind of LLM in-house undocumented bodge. (I maybe wrong/maybe possible - didn't try too hard)
I then tried aider, a command line tool. That managed mutiple edits, but to not too good results. Waste of time wrt results, but: it was a good learning curve for me. I PAYG subscribed OpenAI -> DeepSeek -> OpenRouter.
OpenRouter leader board led me to Cline VSCode addon. Latest-greatest setup atm 1) VSCode 2) with Cline Addon 3) OpenRouter API key (payg credits) 4) select Claude 3.5 via openrouter/anthropic/claude-3.5-sonnet.
The dev task was as follows. Functionality A/B/C needs implementong. Look at existing wrapper X implementing A/B/C, while using Y external library for A/B. Create new wrapper U, to use external library W, in the same way X is using Y, to do the similar A/B. (C is done in X and U respectivelly) E.g. - see how the data is passed X-to-Y, then do it the same way U-to-W. Look at examples code in the W library, figure how to do A/B.
This to avoid doing the reading abt W and figuring A/B myself. I can do it myself, have done it half a dozen times already, for U/W equivalents, but: bit boring, and wanted to find out if I can make AI do it for me.
Have yet to finish the full loop, the code does not run yet. But - before it was laughably obviously bad and wrong. Now - the 1st time where the code looks plausable. Need to do a harness to test finally. To be continued.
-- LJ HPD Sun 22 Dec 22:24:19 GMT 2024
---
post-my-HOME.html
---
My computer $HOME
I spend most of my time typing into a computer, reading on the computer screen, watching videos or listening to audio playing on a computer, talking and seeing other people through a computer screen. I mostly live in the text world of a terminal emulator, bash command line and the vim editor, with Firefox and Chrome windows into the world.
Nowadays I use Xubuntu LTS, on the desktop and on my laptops. I update every 3-4 years to the new version, but other than that - it's completely uneventful. The Xfce GUI changes only slowly if ever - and that's how I like it.
On the computers used by rest of the family I put Ubuntu as it's better looking. They also use Windows desktops and Macbooks for laptops. The Macbooks with the M1-M4 ARM CPU-s have spectacular battery life in addition to great screens. I am looking forward to the day when I add an ARM Thinkpad to my daily use. Have couple of Thinkpads for the RAM I need. The RAM keeps me away from the Macbooks - otherwise I would have switched my now. Initially I had 32GB, but now that has grown to 64GB. Can't imagine to have less and would like to move to 128GB on the laptop. Have had that much on the desktop for 10 years now. That's my first need: get as much RAM as I can. And the 2nd - get as fast an SSD and better NVME as I can. My daily job involves lots of data, and keeping all data needed in memory is the best UI for me. And when not in memory - then on ssd drives and preferably nvme ones.
Daily I live in bash and vim mostly inside screen (the multiplexer) inside terminator (the terminal emulator). I use the shell tools, incl awk (that I like) and the rest of the gnu shell tools (grep, sed), git, gcc and g++, make, ssh, rsync, rcopy, Spyder, python, Visual Studio Editor, Cursor (AI), Firefox, Thunderbird, Chrome, Edge, Double commander, Evince, VLC player. I like them all - my life would be worse if these free software tools didn't exist. Thank you GNU software, thank you Linux, thank you FSF.
In my daily job I write quant trading systems and frameworks. I use mix of C/C++, Matlab in the past now octave and python with numpy (and pandas), scripting in bash, awk, plotting in gnuplot, data fetching in sql, kdb.
Everything that I do more then few times on the command line, I "can" it into a bash alias or function, and put in my .bashrc. In part to document and not forget. I love that command search works, I can type $ xyz then press (TAB) and bash will seek to complete for commands starting with xyz. And bash will keep cycling through the completions for as long as I keep pressing the (TAB) key.
I keep my dot rc files under git and that's worked without fuss. Looks like this:
# Keep dot files and other config in git (https://news.ycombinator.com/item?id=11070797).
# Step 1: $ git init --bare $HOME/.githome.
# Step 2: make function (rather than alias to allow for composition like $ GIT=githome gilg; change dir so paths work independent of the current dir):
githome() { (cd "$HOME" && git --git-dir="$HOME"/.githome/ --work-tree="$HOME" "$@";) }
# Step 3: disregard files by default, only track explicitly added files: $ githome config status.showUntrackedFiles no.
# Now use the usual git commands prefixed by githome: $ githome status; githome add .vimrc; githome commit -m "Add vimrc".
# Issue 1: Can not commit links. For host specific dirs, woraround: 1) move dir to dir-host; 2) link dir to dir-host; 3) add dir-host to git. Example with ~/.config dir:
#   ljubomir@hostA:~$ l -d .config*
#   lrwxrwxrwx  1 ljubomir ljubomir   14 Mar 24 14:26 .config -> .config-hostA/
#   drwx------ 34 ljubomir ljubomir 4.0K Mar 29 12:24 .config-hostA/
#   drwx------  3 ljubomir ljubomir 4.0K Mar 24 14:52 .config-hostB/
# Issue 2: To pull from host with temporary IP edit $ vi .githome/config, change the IP below:
#   [remote "hostC"]
#   url = ljubomir@192.168.1.117:.githome
#   fetch = +refs/heads/*:refs/remotes/hostC/*
# List all files under management and pretty print if run without args, githome otherwise. (https://mitxela.com/projects/dotfiles_management)
giho-ls() {
  (cd /
  githome ls-files | while read i; do
    echo -n "$(githome -c color.status=always status "$i" -s | sed "s#$i##")"
    echo -e "卢/$i卢\e[0;33m$(githome -c color.ui=always log -1 --format="%s" -- "$i")\e[0m"
  done
  ) | column -t -s卢
}
# Have "local -" to make option "set -x" local to the function only
giho()                  { local -; set -x; githome "$@"; }
giho-fetch-hostA()      { local -; set -x; githome fetch "$@" hostA master:hostA; }
giho-merge-hostA()      { local -; set -x; githome merge "$@" hostA; }
giho-push-hostA()       { local -; set -x; githome push --follow-tags "$@" hostA master:$(hostname -s); }
Other canned common git commands look like:
# Git shortcuts. Take the "git" command from the environment via GIT var to allow for goodies:
#   - use with githome: $ GIT=githome gist
#   - color terminal (off by default): $ GIT="git -c color.status=always" gist |m
gi() { ${GIT:-git} "$@"; }
gist() { ${GIT:-git} status "$@"; }
gidf() { ${GIT:-git} diff "$@"; }
gilg() { ${GIT:-git} log -C --name-status --pretty="%h %ae %ai : %s" "$@"; }
gilgt() { ${GIT:-git} log -C --oneline --stat --decorate "$@"; }
gi-fetch-hostA() { local GIM=${GIM:-master}; local -; set -x; ${GIT:-git} fetch "$@" hostA ${GIM}:hostA/${GIM}; }
gi-merge-hostA() { local GIM=${GIM:-master}; local -; set -x; ${GIT:-git} merge "$@" refs/heads/hostA/${GIM}; }
gi-push-hostA() { local GIM=${GIM:-master}; local -; set -x; ${GIT:-git} push --follow-tags "$@" hostA ${GIM}:"$(hostname -s)"/${GIM}; }
I like and use .bashrc search-previous-command all the time via .inputrc:
$if Bash
  # Filename completion/expansion
  set completion-ignore-case on
  set show-all-if-ambiguous on
  # Append "/" to all dirnames
  set mark-directories on
  set mark-symlinked-directories on
  # Match all files
  set match-hidden-files on
$endif
# Ctrl-Left
"\e[1;5D": backward-word
# Ctrl-Right
"\e[1;5C": forward-word
# Up
"\e[A": history-search-backward
# Down
"\e[B": history-search-forward
Usually I don't customize anything much. I spend most of the time on the command line or in vim anyways, the GUI is mostly vanilla whatever Xfce decides. I notice now my PS1 etc have grown over time:
PS1='${debian_chroot:+($debian_chroot)}\[\033[01;34m\]\u@\h\[\033[00m\](${STY}:${WINDOW}):\[\033[01;34m\]\w\[\033[00m\]\$ '
PROMPT_COMMAND='echo -ne "\033]0;${XUSER} (${STY:-$$}) ${VIRTUAL_ENV_PROMPT} ${USER}@${HOSTNAME}:${PWD}\007"'
A researcher, an explorer - usually they need a log book. At work as a researcher I always kept a log book, usually using 2 facing pages per 1 week.
At $HOME have settled for a ~/logBook that's plain ASCII text file under git. Love git for versioning so I don't worry that I will delete destroy something by mistake. Also great for synchronisation and replication to various boxes - done with $ git fetch/merge/push. I have FIXME TODO DONE DONTDO sections. They are ^searchable in vim, e.g. /^TODO(enter). The entries are short-ish, sentence or 2 or 5, separated by empty line. I start entries with "- " so to search easily with /^-(space)(enter) in vim. Entries move wholesale between sections, the idea is to move them around without further editing. Entry that spends enough time in TODO without moving to DONE is moved to DONTDO after some time. No new entry is added to TODO while the FIXME section is non-empty. If need to, move the blocker from FIXME into TODO. These are housekeeping rules rules of thumbcan be broken with a reason, try not break them without a reason.
I have spent most of my adult life with and around computers. The 1st home computer I saw was ZX-Spectrum 16K that my school friend got before me in the last year in elementary school probably 1982-83. Remember the prices still: ZX-Spectrum 16K was 拢100, 48K model was 拢130. Commodore C64 was 拢200. Latter I managed to persuade my parents to buy me a C64 probably around 1985. I learned Basic and 6502 assembler on it, mostly from the Racunari u vasoj kuci (Computers in Your Home) magazine (recent 40 years anniversary reprint; click to zoom).
By 1987 I finished High school and enrolled BSc undergraduate studies in Electrical Engineering, that turned into Computer Science from year 3 onwards. (all together 4.5yrs+diploma work; the sounds of my home town/country I grew up in, by the incomparable VS) Did 1yr National service in-between High School and University, and there I programmed pocket computers HP-71B, Sharp PC-1500 (with the tiny printer), Apple II clone with a Z80 CP/M board and a hard disk (!! remember that CP/M had partitions, but no directories?). Once back home and at Uni from 1988, I finally got my 1st PC (don't recall the year exactly) - AT with Intel 80286 CPU, 1MB RAM, Hercules graphics card, 20MB HDD, that probably run MS-DOS 5 or similar. I squeezed a 2400bps modem in the budget too (without MNP5 error correction or compression). The modem proved an excellent decision as it got me into the online world of BBS (e.g. Sezam) and latter Internet. All that financed paid for by my ever kind and generous parents - thank you mum and dad!
Since - I've never been too far from a computer for any significant time. Nowadays it's mostly Linux (Xubuntu, CentOS, Ubuntu), for a long time earlier it was MS-DOS/Windows (3.1-95-XP-10, cygwin), various Unix too (Solaris, HP-UX, Ultrix, AIX), as well as VAX VMS. And of course - we all carry a magical shiny slabs in our pockets that are super-computers of the old. Mostly various Android for me, but it's looking like I'll be switching over to iPhone for the AI NPU (Neural nets Processing Units) hardware.
These days I 'm mostly at my desk, in a garden office (at end of the work day). Sometimes I get a visitor or two or three. (click to zoom)
(rehosting an excellent advanced vi - not vim! - and ex tutorial, by Walter Alan Zintz, originally published in UnixWorld Online, but no longer online here)
-- LJ HPD Thu 10 Oct 23:00:52 BST 2024
---
post-picmem.html
---
Memes, random pics memorable enough to replicate, in no particular order nor of meaning
 Memes                
 Science, Computing, Communication                                                                                                                                                                                                                                                                                                         
-- LJ HPD Tue 7 Jan 08:41:05 GMT 2025
---
post-social-networks.html
---
Social Networks
I spend a lot of time reading (and sometimes posting) on various Social Networks. I have been on the Internet from the very start of it existing and being available. Prior to Internet, I used home made Bulletin Board Systems (BBS) and computer networks like DECNET, X.25, BITNET, etc.
X/Twitter
My understanding of how a social network like X/Twitter works is as follows.
Example. I follow 5000 accounts. Each account writes about 1 post a day = 5000 posts a day. Every time I login to X and check X for new posts, the X algofeed serves me 50 posts on one screenful. I check X 10 times per day, 10 times x 50 posts per a screenful = 500 posts that X will show me daily. That is 500 posts, out of the total of 5000 eligible posts that can be shown. The other 4500 eligible posts will not be shown. X must decide which 500, out of 5000 eligible, to show me. Any one post has probability 0.1 to be shown. I expect to see 1 post from 1 account once in 10 days. X algo is non-random and tilts towards factors like accounts interaction (e.g. bio check), engagement with posts {Like,Forward,Quote,Reply}, time of posting. With the timeliness of all these factors is decayed by some half-life (from the event time to now). I presume the most important meta-data is (a) connection (follow/s/er); and (b) timeliness, time of viewing minus time of posting (that decays quickly).
Algofeed
The Algofeed has no idea about the meaning (let alone the truthfulness) of any content in the post. So afaik the Algofeed has meta-data only to go on, when deciding which post to push onto millions of user screens. I thought by now with all the LLM-s (and DNN-s before) posts and accounts would have been judged by the content much much more, even if with a single word2vec type vector. And that the Algofeed would take that into account. But I have not seen anything to indicate that there is any content processing.
Algofeed using meta-data mostly strikes me as being "judged on the color of your skin" phase of the Social Network-s, and would be good to transition to "judged on the content of the character", of each and every one of the posts. (and downstream - users)
There's is no censorship involved - I can go to every one of those 4500 accounts home pages, and read every one of those 4500 posts.
There's no moderation - those 4500 posts are perfectly fine.
They are not even totally suppressed - another user may have his 500 shown posts come from the 4500 not shown to me. (the algofeed will on average prefer some over the others though)
The algofeed simply has to make a selection, as it's not physically possible to fit 5000 posts on my screen. As simple as.
Not having an algofeed is impossible. Not only X but all social media - FB, IG, TikTok, TG, etc. This is how it works. The Algofeed moderates every user experience every second. There is not a moment that we the users don't get the algofeed doing something for us. I read people write "I don't want no algorithmic feed. Just give me the posts of users I follow in reverse time order". Well - you just described a specific algorithmic feed. (aside: I do want that feed too, sometimes. There is no reason why us users can't have the choice of 1000s of Feeds. To some extent that happens on Bsky; that is afaics the only remotely plausible X/Twitter competitor atm).
Publisher 9/10-ths, carrier 1/10-th
I heard this metaphor / abstraction by Yuval Harari recently. The algofeed deciding which 500 posts I see today out of smaller selection of 5000 (out of total of 1B possible per day) is an editor (even if automated), and X is a publisher (even if automated). The users are readers are also writers creating content without commission or pay. I don't see how X (and FB, TikTok etc) are not media companies (as opposed to - a point to point carrier of signals). They are even selling and living off adverts! :-) (mostly; X not so much nowadays) This idea is unlikely to be accepted easily. I also like to have the freedom to find all manner of crazy insane untrue stuff online. I defo see though how there is tension between freedom, disagreement, competition and order, working in unison, cooperation. Good @harari_yuval on @seanilling's "The Gray Area | Yuval Noah Harari on the AI revolution" https://www.youtube.com/watch?v=uhx1sdX2bow on this.
We have implicit abstraction in our heads that X/Twitter is a kind of public square, with many-to-many N-to-N, ultimately all-to-all communication. It ain't so. That N-to-N does not scale to N=100M users, it breaks down after N=10 or so.
When a user posts something, that post is simply recorded on a computer (disk). Nothing more. Yes - it's the user that presses the post button. No - that doesn't push the post into millions of timelines. It's the algofeed that takes that post, and shoves it into millions of screens.
I am inclined to agree with Harari on that. Social media are Media, X is a publisher, and their algorithms are their editors. It's fair to judge the algofeed should by the same criteria as the editor of any old media. It's new kind of media, but it's still Media. All elements are here, with small differences in operation or business model, and plenty of automation in top.
Other, wishlist
Advertising. In non-social one-to-many media the adverts are broadcast to all. So if there is a falsehood or slander in an advert - everyone can see it, then provide feedback and critique. In social media (e.g. FB) one-to-one advertising is completely private. Every one use may be shown a (a) separate and different advert, and (b) completely untruthful, and there will be no way for anyone else to know. The advert is 1:1 between the user and the platform, completely secret. Platforms should be required to provide access to the adverts they serve to a third party. Ideally - adverts should be available for inspection by all users at all times, and in an online archive too with the historic adverts there too.
Community Notes. I got enrolled at some point (not sure why - I vaguely remember X offered, and I agreed) in the Community Notes programme on X. I get to vote on Community Notes others have written. And also to add notes for others to vote on - but have not done that yet.
Someone explained that the logic/istics behind is: find sufficient group of people that disagree on other issues, but agree on the note, for the note to be published. That strikes me as valuable insight. I'm surprised how well it works. Have to look up again the maths, the linear algebra of it - there was some SVD involved.
It's good, but it's wholly insufficient for a network flooded with falsehoods on an industrial scale. While the notes are being submitted and voted on, the Algofeed pushes the Post onto millions of screens. Then after a week, a Note is ready and published. From now on, it will be shown together with the original post - good. If I clicked Like or Repost, I will get a Notification that a Note appeared - good. However - millions of people that merely viewed the original post when the algofeed pushed it onto their screen - they will never see the Note.
It's the Social Network equivalent to an old style Newspaper correction. A false story is splashed on the front page for millions to read. Then a week latter, Correction appears deep on page 23, that few read. Good that it happens - but insufficient.
As with other things, in social networks too: incentives -> results. Engagement is maximized when 1/2 of users are at the throats of the other 1/2, and that's exactly the result we got. Took time but we are reaching that destination.
I want to get 1000 Feeds on X, incl some user defined, instead of the medieval choice of 2 - "For you" and "Following". X went wide did Communities, some way towards conference style (current leader in that is Reddit), instead of deep improving the quality of their core product.
Another thing I want to see is RealHuman flag, and I'd pay small one off fee for that. And then to be able to filter on that flag (or not). That's unlikely to happen too it seems.
Replicate your social graph Follows-Followers
Starting a new platform is so hard as to be impossible, because it's a collective action problem *and* needs to happen at the same time in a short time window. Only external event can force that, c.f. Brazil ban.
Best one can do in the mean time is replicate their Social Graph, find their Follows and Followers, on alternative platforms. There will be insufficient traffic there. The "public Square N^2 iron law of network value" ensures the biggest network wins every time. NB the posts have rarely have permanent relevance. They are more like flowing water, are quickly re-created. Most are time-events-sensitive anyway, the content is non transplantable in time. It's not the posts that keep users locked in the social network.
The "social graph" that is Followers-Follows is what keeps users locked in a social network. I saw the "portable social graph" 1st on Mastodon. So having your Social Graph at the ready on an alternative place is half the job done. It's also prudent - anything may happen to X/Bsky/FB etc, incl being banned by a misfiring algorithm (there is rarely any human support). Given alternatives are free - I see no reason to not reserve your favourite user nickname on Bsky or Mastodon or similar.
Periodically there is discussion on X/Twitter if users need or want to switch to some other network. I don't think people will switch any time soon, unless forced to do. I keep accounts on multiple platforms anyway. Imo the largest single public square N^2 wins every time - that's the iron law of social anything.
It's expected and explained in (computer) networks: the number of connections ~N^2 grows with the square of the number of nodes ~N. And the value to us, users, lies in the interactions facilitated by those connections. And those connections accrue with the square of the user base. Between a larger (2N) and a smaller (N) public square, the larger one will provide as much value in a ~day as the smaller one in a ~week. Users will switch smaller->larger, increasing the difference, in a +ve feedback loop. Until approx only 1 remains standing. Ultimately it's a winner takes all.
Switching to another network is a very specific collective action problem. Social media natural monopoly network effect can only be circumvented by synchronization, moving *at the same time* by millions of users. The timeliness makes all the difference - must be all at the same time. So Brazil user base may switch b/c the ban forces them to move at the same time. UK user base is unlikely to switch as there is no ban.
TINA, but use tools available too
There are no reason to not create a Bsky account. It's easy and free. Comparing X:Bsky=100:10 millions of real users (assuming much bigger X bot ratio), ratio 10, squared makes it 100. Do I see as much interesting stuff on X in 1 day, as I see in Bsky in 3 months? Possibly. For me maybe the ratio of value is lower, but ~2 months seems plausible to me.
Personally, I find my Twitter experience positive overall. I read about negative encounters, but I rarely see ugliness on my TL. I believe it the 1st time when people show me who/what they are or stand for: I am quick to block and mute, not into giving 2nd chances (online; IRL I'm not like that). There are another 8 Billion people that we can interact with online! No need to easily avoidable aggravation. I rely on Lists - Sci-ence, Tech-nology, Comp-uting, Bio-logy, Che-mistry... - to curate my feeds beyond just "For You" and "Followers". Lists help shape the timeline, maintain focus and have ok SNR. Additionally, I tie individual Lists to separate Decks on XPro, effectively creating personalized thematic websites. XPro decks setup for X Lists. (click to zoom)
I find Bsky ok, just fine. The Feeds feature is much better than on X! Users are not constrained to the 2 feeds ("For You" and "Followers") that X deigned to supply. I count Mutuals, FollowersLike, OnlyPosts, Folowing, LatestFromFollows, BestOfFollows, Discover, PopularWithFriends, QuietPosters, WhatsHotClassic, CatchUp, TheGram... And there many more to choose from. Seems both users and developers can create both simpler and more complex feeds, with dozen baselines provided by Bsky. Pleasantly surprised to find deck.blue bsky decks too. Transferring user lists is a chore though, finding the same people is hard. "Sky Follower Bridge" works well as described in https://www.wikihow.com/Import-Twitter-to-Bluesky, but it's still weekends of manual work. Ofc, even if you find the same people, most post on X much more than on Bsky. Going back to short/original length posts and having to chain long post as 1/ 2/ 3/ etc parts is annoying tbh.
The deck.blue decks setup for Bsky Lists. (click to zoom; I see "Quiet Posters" were quiet for real or the feed was down)
Algofeed and S230?
Doubt that anyone is coming after the Algofeed - but maybe someone should. S230 I am happy to (effectively) protect me, and other humans, and also X from me. However, S230 should not protect the X Algofeed, as it's not a human, from shoving insane dross onto 100M screens daily. Engagement is maximized when 1/2 is at the throats of the other 1/2, and that's exactly the result we got. Years passed, yet there's been minimal improvement in my Algofeed experience. Only change I remember was "For you" and "Following" separation as top-line option (it used to be in Settings or some such half-hidden place before Musk). I hate it that things stagnate, nothing changes for ages, hundreds of suggestions posted on "X bugs & features" Community are ignored. I guess this is what Social Network monopoly looks? No idea how to incentivise X to give me better choice there - I want a choice of 1000s of Feeds. X are asleep at the wheel. Maybe a kick in the backside, some stick is needed to shake things a bit in that space?
I read an explainer on the state of the play, the history, the dilemmas, the legal issues, and including the most recent US court rulings that maybe relevant (or not) for the future by @matthewstoller at https://thebignewsletter.com/p/judges-rule-big-techs-free-ride-on.
Thinking back at the time in the 90s the context in which S230 arose. This was time of ISPs like Prodigy, AOL, Compuserve, some of which were standalone non-Internet connected platforms to start with (and connecting to the free Internet afterwards, some trying and failing in creating own private walled gardens). As Internet as is now didn't exist! So they were kind of similar to a telephone company in that we used a telephone to access them. Like an add-on service to my phone service. Then with Internet ISP-s started adding services - connectivity to it, Internet email, maybe small personal web pages space, etc. In that context ISP-s got protection from liability arising from carrying user-generated content, in e.g. email lists, personal web pages and similar.
It strikes me that modern Social networks now are nothing like that. Now it's an entirely different world, completely unrecognizable to how things were in 1990-s when these laws were put in place. My ISP that is broadband provider has no relation to X. Not sure what's to be done. This is US and Law - two areas I'm no expert in.
Medium, message
The medium shapes the message applied to current social media - examples.
 Reddit. Thematic conferences where a new message is longer post on some topic in that conference. Replies discuss that topic in great detail. Audience: like minded randoms around the globe that will not be met IRL, interested in the same topics. By the tail end can be extreme niche subjects. Facebook. Personal stuff, short messages and photos, documenting IRL what's happening to me in my life, along the times axis. Audience: close family, close friends. Instagram. Pictures and videos for looks, feelings. The most superficial or aspirational version of myself. Pure form, no function. Audience: everyone that would envy me, friends, distant relatives. Twitter. Text mainly, short text messages. Text - content is the king not the presentation. Short - quantas of ideas, no place for long or subtle discussion. Shit posting and meming, esp on X. Audience: random unknown strangers, some under IRL names but lots of anon- and pseudo-anons, and bots. Substack. Text mainly, personal web sites for writers. Longer text form, but also nice pictures and designs. Real people, and almost all under their real life names too. Highest SNR but takes effort. Audience: public intellectuals.  
"The medium is the message" is a catchy way to say the medium that carries the message affects the message itself, its content. The medium makes some kinds of messages easy to transmit (so they spread more), and other kinds of messages hard to transmit (so they don't spread). Given that messaging in turn in/forms our ideas, and ideas in/form our stories, and we humans are influenced greatly by the stories in our heads, and then we influence and change the real world around us, it follows: the change of medium of communication is going to change our lives our behaviours. The Gutenberg press did that and there was huge change, then with the radio and latter TV too, and now in our lifetimes initially the Internet and latest social media are doing it too.
-- LJ HPD Tue 15 Oct 08:21:21 BST 2024
---
post-twitter.html
---
Twitter surviving - personal considerations, tips, practices
This is a write down of how I (think I) use X (formerly Twitter). This is not - how to be popular, or how to be an influencer, or how to get a flock of followers. Even to the contrary - these are probably anti-patterns if your are aiming for fame & fortune.
These are written down what I think are my personal tips, previously unwritten guidelines, my own practices, of how I use X. I have gotten a lot out of X! There are many people that complain about all manner of things - but many many more that don't. The number of interesting things, links, discussions out there - is mind boggling. By SNR - it's way way better than the forums of old. I think - I put a only a little more effort than the minimal (about zero), to gain a whole lot more out of X. Same goes for Bsky. By default - both are pretty bland. But - put some effort in aggressively shaping your Follows-Followers, blocking and muting the worst offenders accounts (to whatever your taste is), muting words (esp related to some event - politics elections, football tournaments, etc), adding accounts to thematic personal List-s, and you may see disproportionate returns. There are interesting things out there - but they are like small fish in a humongous ocean. A very low probability finds. So like a gambler tilting the odds of fortuna ever so slightly to his advantage at every opportunity - do try to shift the otherwise unfavourable odds, make them be little less so.
Lately I notice similar usage patterns developing in my Bsky usage. Given there is only X and Bsky, and what was Twitter is now X, I will use the term the "Twitter" as a generic term to refer to both of them. Think possibly Mastodon is similar enough that it can be encompassed too. In the future - hopefully other networks too. The format seems general enough to be own genre. A social media network, primary text-based read/write, with short public posts as default, where posts can be considered minimal units of messaging, "quantas of ideas".
X - tips, guidelines, my practices.
 On mobile use browser https://x.com/home or X app, https://bsky.app/ or Bsky app. A browser tab is better than an App insofar one can open many tabs, with many views, while the App is only ever a single view. Further, better to use open web standards, and avoid using closed walled garden App-s where possible. What's used lives and develops, what's not used dies off. On desktop use XPro https://pro.x.com Tweetdeck and deck.blue https://deck.blue/ decks. Default to LISTS deck, check Personal, and only dive into individual lists when time to spare.
Search own history - on X https://x.com/search?q=(from%3Aljupc0)&src=typed_query, on Bsky https://bsky.app/search?q=from%3Aljupco.bsky.social (click on Latest). Search own history in date range [2024-03-03,2024-05-13] on X https://x.com/search?q=(from%3Aljupc0)%20until%3A2024-05-13%20since%3A2024-03-03&src=typed_query&f=live then click on Latest to reverse sort by time. More X search tips at https://help.x.com/en/using-twitter/twitter-advanced-search. Filter own X TL feed with "filter:follows -filter:replies include:nativeretweets" so https://x.com/search?q=filter%3Afollows%20-filter%3Areplies%20include%3Anativeretweets&src=typed_query&f=live or shorter https://x.com/search?q=(from%3Aljupc0)&src=typed_query&f=live. 
TL curating, dual aim: maximize SNR, maximize interaction. Assume value comes from connections between nodes. Expected hit rate ~1% for minimal interaction (Like) that carries zero risk cost to the reader when reading a post user wrote. Rules of thumb - break them with a reason, but not without.
Accounts to follow: any that seem interesting, notable, authors of books, videos, blog posts, podcasts etc you have already read, heard, watched.
If recognize the name or the face => follow. Those that are real people - select +Notify on them to be reminded of their existing.
Follow back everyone that follows you as a default, to start with. Give the possibility of interaction a chance (even if it's a small one).
Exceptions are obvious spam accounts: elonmuskXXX, phishingXXX, celebrityXXX, influencerXXX, cryptoXXX, tradeXXX, casinoXXX, girlXXX, motivationalXXX.
Don't follow obvious trolls, fakes, pseudoanons that are high volume general posters unless thematic (technology, science) that is of personal interest.
When deciding - "should I follow or not?" - the question asked is: will I like to see this user posts in the future? Have a best guess - yes or no?
Look at the combined total of info available to you, forecast a) is the user a real human? b) will I want to read posts from them?
       Info:      1-pic   2-namesurname     3-bluecheck                \     / Scan for "thick" pic/name/bio, "thin" is a "unfollow" signal as default;
                          4-@nick           5-followsyou                +---+  look at follows/followers>5 ratio, last post/reply months or years ago;
                          6-bio personal intro presentation hashtags   /     \ look of the number of posts or replies on their page, is it >100?
       Up/Down:   1-pic presentce/absence, girlpic no/yes; 2-namesurname human like yes/no; 3-bluecheck yes/no; 4-@nick {girly12345,crypto,trading,engagement} bad;
       weight     5-followsyou yes/no; 6-bio missing or bad words - motivational, spammy, political, slogans, tags, politician/name, current/campaign moras.
       Filtering: red flags - no pic, bad name namesurname12345, bad nick girl12345 (except where name and nick match), bad bio missing, bad words "travel love crypto animals
                  countries flags trading politics god christ orphan", acc ratio follows/followers>5; look for collaborating up/down yes/no decide if to Un/follow.
Prominent accounts of interest add to Lists (independent of following/follow status). Be generous add to >1 list. Create new lists and split existing lists at will.
Lists: Bio-logy, CEEu-Central Eastern Europe, Chem-istry, Comp-uting, Cul-ture, Data, Econ-omy, Edu-cation, Fin-ance, Hist-ory, HPD-en, Int-elligence, Law, MKD-onia,
ML-Machine Learning, Phi-losophy, Pol-itics, QT-Quant Trading, Sci-ence, Tech-nology, Urb-anism, Med-ia, NIL (sink list).
Assign account to list relative to the significance meaning of that account to yourself. Don't be objective - be very subjective.
Depending of how special/general/distinct the list v.s. other TL feeds, do/don't set at list level "Don't show these posts in For You".
Use "Not interested in this post" and "Show fewer posts from XXX" in the "For you" TL to shape the "For you" TL X provides.
Assign accounts that are prolific but boring, silly, propagandists, low SNR, low quality, crazies etc that X algo pushes into sink (/dev/null like) list NIL.
Then tick "Do not show these posts in For You" for that list in the individual list settings.
If something is worth forwarding, then follow the account. If after a time turns out no synergy - simply unfollow. Give people chance, find out.
New follow account - try add to a list. Look their bio, keywords matching a list - e.g. Tech-nology, Phi-losophy - honour & add to that list.
Unfollowing. Favour doers, avoid acc known for being known, non-human, #SLOGAN-s, nick123, marketeers, crypto, girlface, neuro, flags, bolded.
Take a look at acc photo-name-nick-bio, ask yourself: do I recognize, can I recall of anything about this acc? If NO => then Unfollow.
If you recognize the acc profile, then: do I recall their posts, will I want to read again tomorrow? If NO => then Unfollow.
Don't spend time checking list membership - just Unfollow. Accounts that follow your lists - also follow them, look for synergy.
Scan the Follows list, keep FollowsFollowers, both <4K accounts, ~4K is about max that's sane & can be manually maintained.
Your own posts. Use delete more than you are using now.
Reread after posting, maybe again latter if there is further interaction. Doesn't read as good now as it did then? Delete now. Reduce further detracting from the global SNR.
Like your post on posting - reasons:
 To remind yourself that you should really like what you post, never be ashamed of it. If you can't bring yourself to click Like - delete the post. Keeps you honest and on your toes not sloppy. Don't be lazy - make that extra effort, spend 30 sec more, google that link or that claim you are making, avoid obvious lazy errors that decrease the general SNR of the convo. Don't hide behind irony, allusions and other plausible-deniability cowardliness. (jesters at King's court, unfortunates in a totalitarian regime - they need those tools; but not you; you suffer no ill consequences, while being given a megaphone to the world) Reddit and Hacker News automatically credit one uptick to a post to start with, just for posting. Even if a post turns rubbish:
for the chance taken & effort put in writing something, in preference to not writing/keeping quiet - a small reward is deserved. Symbolic poetic gives it small good luck push, as if a departing boat, a reminder - once posted, posts start a life of their own. So they show in the Liked tab, in context with all the other things read at the time and assumed liked and making an impression enough to post. 
ChatGPT massage and make more readable longer posts. Try indicate to a knowing user that ChatGPT was used without being explicit:
 Use **bold** and *italic* formatting as is ChatGPT default, both for that but also b/c longer posts will warrant some markup. Don't explicitly disclaim - not only would the space used be inelegant, but may give credence to the "naturalistic" fallacy.
(e.g. we don't ack the keyboard/computer used to type it either)  
Mutuals - accounts where we mutually follow each other. Unfollow mutual iif to significant extent, one of, or both (a)+(b) below, happen:
 No interaction, no synergy. They don't like or reply to your posts, or your replies to their posts, never repost anything you post. Suspected Gell-Mann amnesia: stuff you know for sure, you see them post untrue/silly takes. Assume similarly clueless about stuff you don't know. 
That opens a slot - that would've been taken by their posts - for the Algofeed to put some other post, also from a mutual account, in that slot.
Don't spam, don't bother people indifferent to you. Maybe they have muted you. Maybe X for does not show your posts to them b/c there's no mutual interests' match.
No one's at fault - tried, didn't pan out. Chances are low to start with, it's to be expected, this will be the norm not the exception.
The aim is bi-directional interaction of high SNR b/c that's *multiplicative*: 100 good connections outweighs 1000 poor connections.
Accounts that blocked you - block back too no exceptions. The tit-for-tat is paradoxically way for better cooperation.
Otherwise prefer (a) NIL sink list shunting; or (b) temporary turn off reposts or mute accounts X is too keen on. But periodically do scan list and unmute.
Increase the chance of interaction. Keep active - post, quote, reply, repost, like. Heed X "Who to follow" - follow accounts, unfollow if no traction.
Add interesting posts on Highlights. Periodically scan, find topical or interesting post, repost if still relevance not expired with the time passing.
There are 8e9 humans, 1e8 on X. Chance to match interests 1:1 is low 1e-16. Keep looking for high SNR interactions. Keep trying to improve the 1% hit rate.
X/Twitter user moderation is manual, there are no automated tools => seems the physical limit is about 4K accounts: 4K follows, 4K followers.
X algofeed is moderated every second of the time. Stats as follows. You follow 5000 accounts. Each one writes 1 post a day = 5000 posts a day. You login to X, X algofeed
serves 50 posts on one screen full. Check X 10 times per day = 500 posts X will show you daily. That is 500 out of possible 5000 to be shown, and 4500 to not be shown.
X must decide which 500, out of 5000 possible, to show you. Any one post has probability 0.1 to be shown. You will see 1 post from 1 account once in 10 days.
X algo is non-random, tilts towards factors like accounts interaction, engagement via {Like,Forward,Quote,Reply} of posts, bio check. Prob decayed by time with half-life. Link(s) in post penalize post ranking.
XPro Tweetdeck (TLDR: for every list in Lists, add Deck==list; add Deck==list-of-Lists; add Deck==Personal, add Deck==Communities):
 Have List==Deck, add important frequent prominent posters from a List into the Deck. Where account belongs to multiple List-s, chose one List only and add it to that one list Deck only. Have a separate deck LISTS for all lists, and add all your X Lists in it. Have a separate deck for Personal feeds: Search from:ljupc0, @ljupc0 Notifications, Home For you, Home Following, @ljupc0 Grok, Profile My Profile, @ljupc0 My Bookmarks, @ljupc0 Messages, @ljupc0 Explore. Have a separate deck for Communities you have joined - add deck Communities and add a selection of joined communities in it.  
Archives of my posts are linked below. It's a text dump without much organisation. TBD TODO  
-- LJ HPD Sun 24 Nov 18:40:24 GMT 2024
---
post-why-write.html
---
Why Write
Q: Why write in public when you can write in private?
A: Yes I have a plain text file ~/logBook for simple note taking. That just works, no fuss.
I do minimal extra work to have it versioned in $HOME/.git, and structured in sections FIXME / TODO / DONE / DONTDO.
However. I catch myself bothering people close to me, family and friends, with things I find interesting to talk about and or discuss, that they find less interesting even boring. :-)
So these home pages are written is to take those themes out of my system, while not bothering any of the above mentioned. Only people to read will be online randoms that stumble on this by their own volition - so fine. 
Q: Why not use social media, social networks?
A: Yes I do use social networks - most often X @ljupc0, sometimes Bsky @ljupco.bsky.social, and rarely Mastodon @ljupco.
There is tons of interesting stuff there, mostly on X due to its user size and "the Iron Law of public N^2 square". I post there, but it seems mostly replies and comments to what other post. Rarely I have something super interesting and urgent that I want to communicate to the world by having the algofeed stuff it into people's timelines.
Then while doom scrolling I stumbled upon -
 https://x.com/CJHandmer/status/1839816029473779775 Casey Handmer, PhD @CJHandmer
This is your periodic reminder that you should write a blog. It doesn't have to be fancy, it doesn't have to be well-edited. It just has to be something you can cumulatively add to over time. It's easiest to write about stuff you like, and ignore your non-existent audience.
12:54 AM 路 Sep 28, 2024
My initial thoughts were sceptical, again - why do it, when there are
 logBook for things private; X/Bsky/... for things public; and even ChatGPT when audience of >1 is needed, while not exactly needing to broadcast to the whole wide world. 
But I find it useful in laying ideas to rest, and moving on. Being bothered for a period of time by the same ideas makes for a boring living. In addition to writing it down, mentioning it in some posts, I found writing it down here in multiple versions and longer form, shortens the process of "put that to rest, enough".
Some of us can think of limited number things at the same time, eh? ;-) This goes towards ensuring those are not the same things all time - but different things. :-)
Since starting this I came across Simon Willisons blog - "You should start a blog. Having your own little corner of the internet is good for the soul!"
...and now I think there is truth in that. Another "please do write - it's worth it to you and us too" Why write a blog at all? by Adam Singer since articulates it better than I ever could myself - so linked now.
-- LJ HPD Sun 6 Oct 22:50:39 BST 2024
---
cvlj91a.txt
---
Ljubomir JOSIFOVSKI
LjubomirJosifovski@gmail.com | 07910 850 111 | 11 Pendennis Court, Harpenden AL5 1SG, UK
Summary
Self-starter. Quantitative researcher, analyst, developer, building and trading systematic equity/FX models
(forecasting, portfolio optimisation, risk management, operations, post trade analysis) at hedge funds,
proprietary trading desk, as independent Portfolio Manager. Analytical maths/stats/CS/EE background,
statistical learning/modelling experience, competent programmer/developer in C/C++/shell/Matlab/
python/C#/Sql on Linux/Windows/Mac, self-sufficient systems/network admin.
Skills
C/C++/OpenMP, SQL (Postgres, MS server), GNU tools (g++/gcc, ddd/gdb, c/make, vim), matlab, shell
tools (bash, awk), VSEditor, python, jupyter, version control (git, cvs, mercurial, MS Teams), R, Java,
C#/Visual Studio, batch schedulers (slurm, Condor), Bloomberg terminal/API, Reuters Kobra, assembly.
Platforms
Linux (X/ubuntu, CentOS), Windows (from-3.1-NT-95-XP-to-11, cygwin), MacOS, Unix (HP-UX, AIX).
Work
May 16 - Now
F9 Research, Harpenden, UK
Position: Director.
Quant research, development and trading. Portfolio manager, run a small market
neutral book ~350M USD gross, trading ~35M USD daily in the EU markets (and a
small R&D US book). Consulting for quant R & D for a client, working on higher
frequencies and short horizons (seconds and minutes) in C/C++, OMP, python,
Matlab, Postgres, cloud boxes and slurm cluster. Input into varying aspects of the
R&D pipeline - from informing and assessing latest technologies (including ML) to
interviewing new teams members.
F9 owns the IP to all and any R&D work done.
Feb 10 - Mar 16
Marshall Wace, London, UK
Position: Quantitative Researcher.
On the TOPS QR team, senior team member among a handful of people, creating
research, developing code, shepherding the market neutral portfolio growth from a few
hundred millions to double digit billions USD gross book size. Ushered the idea of a
single unified framework for all quant R & D & trading with standardised components
- data ingestion and caching, signals extraction, modeller for forecasting, portfolio
optimizer, trades simulator, standardised reporting, a baseline sim faithful and realistic
to be continuously improved on by the entire team working in unison on various
components of the system. Wrote or significantly contributed to significant
components of the system through their iterative improvements over the years. Major
projects in production improving the then best baseline: dynamic modeller fitting the
alpha signals expected returns at multiple horizons, incorporating both prior
knowledge, constraints, and the evidence from historical data, market impact model in
the simulator and the optimizer incl slippage monitoring tuning and balancing risk cost
of undercharging with the opportunity cost of overcharging, liquid concentrated low
TO high capacity market neutral portfolios, 150/50 portfolios mix of tracker and
market neutral, shepherding the trade scheduler deployment in production, alphas
signals GeoSales, Suppliers-Customers, Directors deals, various reverting signals, 1st
quantitative research and assessment on the in-house Alpha Capture signal. Guided
and helped younger hires from onboarding to them becoming fully productive wholly
effective team members. Pioneered reproducible research at scale using multi cpu
multi core R&D boxes with establishing and popularising best practices.
Nov 07 - Nov 09
Credit Suisse, London, UK
Position: Quantitative Analyst.
On the Index Arbitrage proprietary trading desk. Independently traded equity
market/sector/factor neutral portfolios on multiple European markets, fully automated
and systematic, non-discretionary. Wrote own trading, analytics, backtest and portfolio
construction systematic trading platform consisting of a Matlab core, Mosek optimiser,
bash/awk scripts, Reuters Kobra Excel and Sql for historic and current data, with
integrated risk monitoring and control using Barras style factors and sectors. Used the
platform to research and trade all the strategies and portfolios. Alone did orders
1
generation, portfolio construction, forecasting & modelling, all data feeds (Reuters,
Sql dumps), the daily monitoring, trading analysis and slippage tracking and any other
R&D&ops as needed for trading. Traded multiple portfolios daily of ~500 names in
total on London, Paris, Frankfurt, Switzerland, Milan and Madrid exchanges, one trade
per day per name. Did R&D simulations for intra-day horizons faster TO.
In 2008 traded the London portfolio most of the year as a testbed for all research &
development, returning 10% gross in 230 days with Sharpe of 2.5. In 2009 traded
bigger book on most of the European markets, returned 8% gross to Aug09 with
Sharpe of 5.2, turnover 2-3 days, one trade per name per day. All together lifetime
(388 days) return on gross 18% at Sharpe of 3.1.
Jul 04 - Sep 07
G-Research (part of the DPFM group), London, UK
Position: Quantitative Analyst.
Research (70%), development (20%), daily portfolio monitoring and support (10%) in
a multi-billion market neutral hedge fund systematically trading global equities and
spot FX round the clock in a completely automated system. Research and creation of
new trading models/alphas, coding, testing in simulation and putting them in
production. Models for volume prediction, fundamentals and technical equities models
(multiple markets,), spot FX - all productionised and live traded. Built futures models
but not traded live. Development included coding up the models, the associated data
analytics, and subsequent performance and integrity monitoring once live. The
portfolio support role involved monitoring the trade flow, market conditions and risk
factors, investigating/tuning the trading. In the process familiarised myself with
forecasting and modelling, performance attribution, multiperiod quadratic portfolio
optimisation, risk measurement and management (Barra, APT, custom factors), realtime and historic data feeds, data aggregation. Independently came up with original
alphas building on well known semi-parametric models for forecasting that were
traded live in equities and spot FX trading. Similarly contributed alphas based on
novel non-parametric models used for trading equities. They were all profitable,
contributed to the bottom line and were traded along the other alphas.
Jun 01 - Jun 04
Canon Research Europe, Bracknell, UK.
Position: Researcher.
Research & development work in the Machine listening group on ASR and indexing &
retrieval of spoken documents. Contributed to all aspects of Canon's low resource
embedded multiplatform ASR engine: the front-end (DSP related), decoder (Mpeg7
compatible lattice creation), training & using statistical models (acoustic HMM
multilingual, text-to-phone Ngrams). Group demonstrated embedded speaker
independent phone book name dialling on ARM9 & ARM7 phones. Phonetic indexing
of spoken documents/annotations & retrieval with spoken & written queries. Invented
& implemented in the embedded C++/C codebase novel algorithm for searching
annotation (speech) lattices with a query (speech) lattice, outperforming other known
techniques for phonetic SDR (LATTICE MATCHING, UK Patent App No 0316669.1,
accomp app ref 2865001, Jul 2003). Demoed playlist entry selection by voice for an
MP3 player, performing in near realtime on Windows CE platform with 1500 entries.
Nov 00 - Jun 01
Motorola European Research Lab, Basingstoke, UK.
Position: Research engineer.
Technology transfer from my PhD work to Motorola (my industrial sponsor). Research
on the distributed speech recognition (DSR) ETSI Aurora 2 standard platform.
Developed robust ASR algorithms in Matlab, GNU C/C++ and tested them on
Cygwin, HP-UX and Linux platforms. Lab was part of the winning consortium of the
ETSI Aurora 2 standardisation competition for mobile phones robust front-end.
Nov 97 - Jan 98
Macedonian Banking Operations Centre (USAID funded project for technical support
of the financial sector in Macedonia), Skopje, MK.
Position: Management Information Systems - Electronic Data Processing (MIS-EDP)
Advisor.
In a team of advisers analysing operations of commercial banks in Macedonia.
Handled the MIS-EDP operations of the banks surveyed, reported on the state of and
2
recommended improvements. By the end of the project all commercial banks in
Macedonia volunteered to have their operations surveyed and reported on.
Nov 93 - Oct 97
Faculty of Mechanical Engineering, University Sv. Kiril i Metodij, Skopje, MK.
Position: Systems engineer.
Solely responsible for maintaining all faculty computers (100+ PCs, 10+ Unix
workstations), faculty LAN spanning 3 buildings, other computing-related equipment
(printers, terminal servers, router). Faculty LAN massively expanded, doubled the size
of existing and added a second computerised classroom for students and lab classes,
introduced email & other Internet services to every staff member and student, phased
out legacy systems (VT420 terminals, terminal servers). Maintained/supported
collection of legacy Clipper/FoxPro accounting applications.
Jun 93 - Oct 93
NeoCom, Skopje, MK.
Position: System integrator.
In small & dynamic company, clients facing, computer systems assembly, integration,
software installation, maintenance (PC/Windows), computer networks (Novell
NetWare, Windows LAN) installation & maintenance on- and off-site.
1986 - 1993
Freelance S/W developer, undergraduate & hobby programming
Basic & assembler (6502) on home computers. Mission critical (firing heavy guns) on
pocket computers (HP-71B, Sharp 1500) and TurboPascal (Apple II+CP/M
board+HDD) while national service (army). MS-DOS systems programming (C &
assembler, TSR programs: screen capture, serial port snoop, DOS trashcan), network
programming (NetBIOS based LAN messenger, IPX chat, IPX stack emulator in
DesqView), PC databases (video shop rental application in Clipper, various
applications in FoxPro, document flow in MS-Access).
Education
1998 - 2000
Doctor of Philosophy Ph.D. (Full-Time)
Speech and Hearing Group, Department of Computer Science, Faculty of
Engineering, University of Sheffield, UK.
Independent research into recognising speech in noise. Missing data model treats parts
of the speech spectrum swamped by noise as unobserved/partially observed, giving
rise to a probabilistically modelled mask that has to be incorporated in the frame-byframe adapted speech model. Work involved theory of automatic speech recognition as
well as practice, training HMMs with continuous GMM pdfs using EM (HTK, shell
scripting), writing and using Viterbi decoders and frontends to test novel noise
robustness algorithms, noise and SNR estimation (Matlab, C++, C). Part of EC
ESPRIT LTR programme funded RESPITE project of 5 research labs and 2 industrial
partners and EC TMR programme funded SPHEAR network.
Thesis: "Robust speech recognition with missing and unreliable data". (Viva Dec
2002)
1993 - 1997
M.Phil. Electrical Engineering (Part-Time)
Department for Computers and Informatics, Faculty of Electrical Engineering,
University Sv. Kiril i Metodij, Skopje, MK.
Studies consisted of taught part (2 years/4 semesters) for 6 courses and thesis work (1
year/2 semesters) followed by a viva. Courses achieved avg grade 10 (scale 6-10, 10
best). Projects: video-over-IP frame rate control and QoS using UDP non-blocking
sockets (C/C++, part of a system for tele-teaching system); database of Medieval
Manuscripts (Delphi). Thesis/research - built system for converting written text into
speech. Rudimentary time-domain, syllable based TTS. Created a database of 1200
syllables, wrote TTS engine breaking the input text into syllables (using an NN MLP),
concatenating the units from the syllable database, generating F0 and the duration
contours, modifying the syllable units accordingly in time domain. Gathered and
labelled data, trained a two layer, feed forward MLP (neural network) to mark syllable
breaks in the input text. Part of a larger project for automatic text reading for the blind.
Thesis: "System for text-to-speech conversion for Macedonian language".
3
1988 - 1993
B.S. Electrical Engineering (Full-Time)
Department for Computers, Informatics and Automation, Faculty of Electrical
Engineering, University Sv. Kiril i Metodij, Skopje, MK.
Taught studies 4.5 years (9 semesters) followed by a diploma work (1 semester) and
public presentation. Achieved average grade of 8.78 (scale 6-10, 10 best).
Diploma project: "Introduction to DECNET, Bitnet (EARN) and Internet networks; Email/File transfer services; X.25 Network and out-dial NUAs".
Best student within my college class in years 1 & 2. Ranked 1st (100 points out of
100) among of approx 800 candidates at the University entrance exams.
1983 - 1987
R.J. Korcagin High School, Skopje, MK.
Mathematics and Computer Science High School, achieved GPA 5.00 on a 2 to 5 scale
(5 best), voted best pupil (valedictorian) of the 1983-87 generation.
Nationality
UK (acquired/by choice), Macedonian (by birth). Born 1968.
Languages
English, Macedonian (native), Croatian, Serbian.
Honours
&
Awards
Scholarships: merit research & science 1988-93, talented student 1983-87. Best student 1989,'90.
Maths competitions prizes: Regional 1st 1984, 86, 87, 3rd 1985; Republic 3rd 1984, 85, 87;
National participation 1984, 85, praise 1987.
Other
UK and MK driving licences, married, two children.
4
---
cvlj91sa1.txt
---
Ljubomir JOSIFOVSKI
Summary
Quantitative researcher, analyst, developer, portfolio manager with extensive experience building and
trading systematic equity/FX models at hedge funds, proprietary trading desk, and as an independent
PM. Advanced skills in maths, statistics, machine learning, regression and classification, neural
networks, hidden Markov models, and programming. Track record in modelling, forecasting, portfolio
optimization, and risk management. Self-sufficient systems and network administrator.
Skills
Programming: C/C++, Python, SQL, MATLAB, C#, R, Java, shell, awk, make
Platforms: Linux (Ubuntu, CentOS), Windows, MacOS
Tools: Vim, Git, Slurm, Cursor, CLion, Jupyter, Spyder, VSEditor, Bloomberg
Experience
F9 Research, Director (2016Present)
Managed a market-neutral book (~$350M gross, ~$35M daily trading) in EU and US markets.
Leading quant research and development for short-horizon strategies using Python, C++, cluster and
cloud resources. Consulting on advanced R&D pipelines and machine learning applications.
Marshall Wace, Senior Quantitative Researcher (20102016)
Developed and scaled market-neutral portfolios from $100M to $10B+.
Pioneered unified R&D framework for data ingestion, signal extraction, portfolio optimization, and
simulation. Mentored junior researchers and implemented reproducible research workflows.
Credit Suisse, Quantitative Analyst (20072009)
Independently traded equity market-neutral portfolios systematically, achieving 18% lifetime returns
with Sharpe 3.1. Built and operated a complete trading platform for multi-market European equities.
G-Research (DPFMG), Quantitative Analyst (20042007)
Designed and implemented systematic trading models for global equities and FX, contributing to fund
profitability. Modelling, forecasting, risk management and multi-period optimization for mid- and
high- frequency trading strategies.
Canon Research Europe, Researcher (2001-2004)
Embedded Automatic Speech Recognition, indexing, and retrieval of spoken documents with speech.
Education
Ph.D., Computer Science  University of Sheffield, UK (2000)
Thesis: Robust Speech Recognition with Missing and Unreliable Data
M.Phil. (1997), Electrical Engineering  University Sv. Kiril i Metodij, Skopje, MK
Thesis: System for text-to-speech conversion for Macedonian language
B.S. (1993), Electrical Engineering  University Sv. Kiril i Metodij, Skopje, MK
Additional
UK and Macedonian nationality, multilingual (English-fluent, Macedonian-native, Croatian, Serbian)
Married, two grown up children, UK and MK driving licenses
Hobbies include reading on machine learning research, various non-fiction, quantitative finance, and
mentoring
---
